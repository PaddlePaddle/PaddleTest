case:
  linux_convergence:
    train:
      -
        name: prepare_datasets_seqlen128
        path: model_zoo/bert/
        cmd: wget https://bert-data.bj.bcebos.com/benchmark_sample%2Fhdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5.tar.gz && tar -xzvf benchmark_sample%2Fhdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5.tar.gz
      -
        name: dy2st_prim_cinn
        path: model_zoo/bert
        cmd: python run_pretrain.py
        params:
          - --max_predictions_per_seq 20
          - --learning_rate 1e-4
          - --weight_decay 1e-2
          - --adam_epsilon 1e-6
          - --warmup_steps 10000
          - --output_dir ./primcinn/
          - --logging_steps 1
          - --save_steps 100000
          - --model_type bert
          - --fuse_transformer false
          - --input_dir ./hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/training/
          - --model_name_or_path bert-base-uncased
          - --max_step 500000
          - --batch_size 128
          - --seed 42
          - --to_static True
          - --use_amp True
          - --amp_level O2
      -
        name: dy2st_baseline
        path: model_zoo/bert
        cmd: python run_pretrain.py
        params:
          - --max_predictions_per_seq 20
          - --learning_rate 1e-4
          - --weight_decay 1e-2
          - --adam_epsilon 1e-6
          - --warmup_steps 10000
          - --output_dir ./dy2st/
          - --logging_steps 1
          - --save_steps 100000
          - --model_type bert
          - --fuse_transformer false
          - --input_dir ./hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/training/
          - --model_name_or_path bert-base-uncased
          - --max_step 500000
          - --batch_size 128
          - --seed 42
          - --to_static True
          - --use_amp True
          - --amp_level O2
