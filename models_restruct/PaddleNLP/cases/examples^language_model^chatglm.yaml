case:
  linux:
    train:
      -
        name: multi
        path: examples/language_model/chatglm
        cmd: python -m paddle.distributed.launch --gpus "0,1" finetune_generation.py
        params:
          - --model_name_or_path THUDM/chatglm-6b
          - --task_name_or_path './AdvertiseGen/'
          - --learning_rate 3e-5
          - --warmup_steps 20
          - --max_steps 2
          - --save_steps 2
          - --eval_steps 2
          - --logging_steps 1
          - --save_steps 1000
          - --save_total_limit 1
          - --output_dir ./checkpoints/chatglm-6b
          - --src_length 64
          - --tgt_length 64
          - --per_device_eval_batch_size 2
          - --per_device_train_batch_size 2
          - --gradient_accumulation_steps 16
          - --fp16
          - --fp16_opt_level O2
          - --recompute True
          - --do_train
          - --do_eval
          - --tensor_parallel_degree 2
          - --lora True
    eval:
      -
        name: eval
        path: examples/language_model/chatglm
        cmd: python predict_generation.py
        params:
          - --model_name_or_path  ./checkpoints/chatglm-6b
    infer: skipped
    export:
      -
        name: export
        path: examples/language_model/chatglm
        cmd: export_generation_model.py
        params:
          - --model_name_or_path ./checkpoints/chatglm-6b
          - --output_path ./checkpoints/infer/chatglm
          - --dtype "float32"
    predict: skipped
  windows:
    train: skipped
    eval: skipped
    infer: skipped
    export: skipped
    predict: skipped

  windows_cpu:
    train: skipped
    eval: skipped
    infer: skipped
    export: skipped
    predict: skipped

  mac:
    train: skipped
    eval: skipped
    infer: skipped
    export: skipped
    predict: skipped
