case:
  linux_convergence:
    train:
      -
        name: datasets
        path: legacy/model_zoo/gpt-3/tasks/gpt
        cmd: mkdir data && cd data && wget https://bj.bcebos.com/paddlenlp/models/transformers/llama/data/llama_openwebtext_100k_ids.npy && wget https://bj.bcebos.com/paddlenlp/models/transformers/llama/data/llama_openwebtext_100k_idx.npz

      -
        name: dy2st_baseline
        path: legacy/model_zoo/gpt-3/tasks/gpt
        cmd: GLOG_vmodule=generated_vjp=4 python train_pir.py
        params:
          - -c ../../ppfleetx/configs/nlp/gpt/pretrain_gpt_345M_single_card.yaml
          - -o Global.micro_batch_size=2
          - -o Global.local_batch_size=2
          - -o Global.to_static=False
          - -o Engine.max_steps=5000

      -
        name: dy2st_pir
        path: legacy/model_zoo/gpt-3/tasks/gpt
        cmd: GLOG_vmodule=generated_vjp=4 python train_pir.py
        params:
          - -c ../../ppfleetx/configs/nlp/gpt/pretrain_gpt_345M_single_card.yaml
          - -o Global.micro_batch_size=2
          - -o Global.local_batch_size=2
          - -o Global.to_static=True
          - -o Engine.max_steps=5000
      -
        name: dy2st_pir_prim
        path: legacy/model_zoo/gpt-3/tasks/gpt
        cmd: GLOG_vmodule=generated_vjp=4 python train_pir.py
        params:
          - -c ../../ppfleetx/configs/nlp/gpt/pretrain_gpt_345M_single_card.yaml
          - -o Global.micro_batch_size=2
          - -o Global.local_batch_size=2
          - -o Global.to_static=True
          - -o Engine.max_steps=5000
