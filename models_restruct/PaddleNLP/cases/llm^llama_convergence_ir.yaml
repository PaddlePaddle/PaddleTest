case:
  linux_convergence:
    train:
      -
        name: prepare_datasets_seqlen128
        path: llm/llama
        cmd: mkdir data && cd data && wget https://bj.bcebos.com/paddlenlp/models/transformers/llama/data/llama_openwebtext_100k_ids.npy && wget https://bj.bcebos.com/paddlenlp/models/transformers/llama/data/llama_openwebtext_100k_idx.npz
      -
        name: baseline
        path: llm/llama
        cmd:  python run_pretrain.py
        params:
          - --model_type "llama"
          - --model_name_or_path "__internal_testing__/tiny-random-llama"
          - --tokenizer_name_or_path "__internal_testing__/tiny-random-llama"
          - --input_dir "./data"
          - --output_dir "./output/llama_output_baseline"
          - --split 949,50,1
          - --max_seq_length 2048
          - --per_device_train_batch_size 4
          - --per_device_eval_batch_size 1
          - --use_flash_attention 0
          - --use_fused_rms_norm 0
          - --scale_loss 1024
          - --learning_rate 0.00001
          - --min_learning_rate 0.000005
          - --lr_scheduler_type "cosine"
          - --max_steps 3000
          - --save_steps 3000
          - --weight_decay 0.01
          - --warmup_ratio 0.01
          - --max_grad_norm 1.0
          - --logging_steps 1
          - --dataloader_num_workers 1
          - --eval_steps 3000
          - --report_to "visualdl"
          - --disable_tqdm true
          - --continue_training 0
          - --recompute 0
          - --do_train
          - --do_eval
          - --device "gpu"
          - --seed 2023
          - --use_fused_rms_norm False
      -
        name: ir
        path: llm/llama
        cmd: python run_pretrain.py
        params:
          - --model_type "llama"
          - --model_name_or_path "__internal_testing__/tiny-random-llama"
          - --tokenizer_name_or_path "__internal_testing__/tiny-random-llama"
          - --input_dir "./data"
          - --output_dir "./llama_output_pir_prim"
          - --split 949,50,1
          - --max_seq_length 2048
          - --per_device_train_batch_size 4
          - --per_device_eval_batch_size 1
          - --use_flash_attention 0
          - --use_fused_rms_norm 0
          - --scale_loss 1024
          - --learning_rate 0.00001
          - --min_learning_rate 0.000005
          - --lr_scheduler_type "cosine"
          - --max_steps 3000
          - --save_steps 3000
          - --weight_decay 0.01
          - --warmup_ratio 0.01
          - --max_grad_norm 1.0
          - --logging_steps 1
          - --dataloader_num_workers 1
          - --eval_steps 3000
          - --report_to "visualdl"
          - --disable_tqdm true
          - --continue_training 0
          - --recompute 0
          - --do_train
          - --do_eval
          - --device "gpu"
          - --seed 2023
          - --use_fused_rms_norm False
