case:
  linux:
    train:
      -
        name: data_tools
        path: model_zoo/ernie-1.0/data_tools
        cmd: sed -i "s/python3/python/g" Makefile && sed -i "s/python-config/python3.7m-config/g" Makefile
      -
        name: prepare_data
        path: model_zoo/gpt
        cmd: mkdir pre_data && cd pre_data && wget -q https://bj.bcebos.com/paddlenlp/models/transformers/gpt/data/gpt_en_dataset_300m_ids.npy && wget -q https://bj.bcebos.com/paddlenlp/models/transformers/gpt/data/gpt_en_dataset_300m_idx.npz
      -
        name: pretrain
        path: model_zoo/gpt
        cmd: python -m paddle.distributed.launch run_pretrain.py
        params:
          - --model_name_or_path "__internal_testing__/gpt"
          - --model_type gpt
          - --input_dir "./pre_data"
          - --output_dir "output"
          - --weight_decay 0.01
          - --grad_clip 1.0
          - --max_steps 2
          - --save_steps 2
          - --decay_steps 320000
          - --warmup_rate 0.01
          - --micro_batch_size 2
          - --device gpu
        result:
          loss:
            base: 1.456469
            threshold: 0.01
            evaluation: "="
      - 
        name: single_dy2st
        path: model_zoo/gpt
        cmd: mkdir data && cd data && wget https://paddlenlp.bj.bcebos.com/models/transformers/gpt/data/gpt_en_dataset_300m_idx.npz && wget https://paddlenlp.bj.bcebos.com/models/transformers/gpt/data/gpt_en_dataset_300m_ids.npy && cd ..; python run_pretrain.py
        params:
          - --model_type gpt
          - --model_name_or_path gpt2-en
          - --input_dir "./data"
          - --output_dir "output_base"
          - --weight_decay 0.01
          - --grad_clip 1.0
          - --max_steps 100
          - --save_steps 100
          - --decay_steps 320000
          - --warmup_rate 0.01
          - --micro_batch_size 2
          - --device gpu
          - --to_static
          - --seed 100
      - 
        name: multi_dy2st
        path: model_zoo/gpt
        cmd: mkdir data && cd data && wget https://paddlenlp.bj.bcebos.com/models/transformers/gpt/data/gpt_en_dataset_300m_idx.npz && wget https://paddlenlp.bj.bcebos.com/models/transformers/gpt/data/gpt_en_dataset_300m_ids.npy && cd ..; python -m paddle.distributed.launch run_pretrain.py
        params:
          - --model_type gpt
          - --model_name_or_path gpt2-en
          - --input_dir "./data"
          - --output_dir "output_base"
          - --weight_decay 0.01
          - --grad_clip 1.0
          - --max_steps 100
          - --save_steps 100
          - --decay_steps 320000
          - --warmup_rate 0.01
          - --micro_batch_size 2
          - --device gpu
          - --to_static
          - --seed 100
      
    eval: skipped
    infer: skipped
    export:
      -
        name: export
        path: model_zoo/gpt
        cmd:  python export_model.py
        params:
          - --model_type=gpt
          - --model_path=gpt2-medium-en
          - --output_path=./infer_model/model
    predict:
      -
        name: predict
        path: model_zoo/gpt/deploy/python/
        cmd: python inference.py
        params:
          - --model_type gpt
          - --model_path ../../infer_model/model

  windows:
    train: skipped
    eval: skipped
    infer: skipped
    export: skipped
    predict: skipped

  windows_cpu:
    train: skipped
    eval: skipped
    infer: skipped
    export: skipped
    predict: skipped

  mac:
    train: skipped
    eval: skipped
    infer: skipped
    export: skipped
    predict: skipped
