case:
  linux:
    train:
      -
        name: prepare
        path: legacy/model_zoo/bert
        cmd: wget -q https://paddle-qa.bj.bcebos.com/paddlenlp/bert.tar.gz && tar -xzvf bert.tar.gz
      -
        name: pretrain
        path: legacy/model_zoo/bert
        cmd: python -m paddle.distributed.launch  run_pretrain.py
        params:
          - --model_type bert
          - --model_name_or_path bert-base-uncased
          - --batch_size 4
          - --input_dir bert/
          - --output_dir pretrained_models/
          - --logging_steps 1
          - --save_steps 2
          - --max_steps 2
          - --device gpu
          - --use_amp True
          - --to_static True
        result:
          loss:
            base: 6.28685
            threshold: 0.01
            evaluation: "="
      -
        name: fintune_glue
        path: legacy/model_zoo/bert
        cmd: python -m paddle.distributed.launch run_glue_trainer.py
        params:
          - --model_name_or_path bert-base-uncased
          - --task_name SST2
          - --max_seq_length 128
          - --per_device_train_batch_size 32
          - --per_device_eval_batch_size 32
          - --learning_rate 2e-5
          - --num_train_epochs 3
          - --logging_steps 1
          - --save_steps 1
          - --max_steps 1
          - --output_dir ./tmp/
          - --device gpu
          - --fp16 False
          - --do_train
          - --do_eval
        result:
          loss:
            base: 1.456469
            threshold: 0.01
            evaluation: "="
      -
        name: single_dy2st
        path: legacy/model_zoo/bert
        cmd: cd data && wget -q https://bj.bcebos.com/paddlenlp/datasets/benchmark_wikicorpus_en_seqlen128.tar && tar -xf benchmark_wikicorpus_en_seqlen128.tar && cd ..; python run_pretrain.py
        params:
          - --max_predictions_per_seq 20
          - --learning_rate 1e-4
          - --weight_decay 1e-2
          - --adam_epsilon 1e-6
          - --warmup_steps 10000
          - --output_dir ./dy2st/
          - --logging_steps 1
          - --save_steps 100
          - --model_type bert
          - --fuse_transformer false
          - --input_dir ./data/wikicorpus_en_seqlen128/
          - --model_name_or_path bert-base-uncased
          - --max_step 100
          - --batch_size 128
          - --seed 42
          - --to_static True
          - --use_amp True
          - --amp_level O2
      -
        name: multi_dy2st
        path: legacy/model_zoo/bert
        cmd: cd data && wget -q https://bj.bcebos.com/paddlenlp/datasets/benchmark_wikicorpus_en_seqlen128.tar && tar -xf benchmark_wikicorpus_en_seqlen128.tar && cd ..; python -m paddle.distributed.launch run_pretrain.py
        params:
          - --max_predictions_per_seq 20
          - --learning_rate 1e-4
          - --weight_decay 1e-2
          - --adam_epsilon 1e-6
          - --warmup_steps 10000
          - --output_dir ./dy2st/
          - --logging_steps 1
          - --save_steps 100
          - --model_type bert
          - --fuse_transformer false
          - --input_dir ./data/wikicorpus_en_seqlen128/
          - --model_name_or_path bert-base-uncased
          - --max_step 100
          - --batch_size 128
          - --seed 42
          - --to_static True
          - --use_amp True
          - --amp_level O2
    eval: skipped
    infer: skipped
    export:
      -
        name: export
        path: legacy/model_zoo/bert
        cmd:  python -u ./export_model.py
        params:
          - --model_type bert
          - --model_path bert-base-uncased
          - --output_path ./infer_model/model
    predict: skipped

  windows:
    train:
      -
        name: prepare
        path: legacy/model_zoo/bert
        cmd: wget -q https://paddle-qa.bj.bcebos.com/paddlenlp/bert.tar.gz && tar -xzvf bert.tar.gz
      -
        name: pretrain
        path: legacy/model_zoo/bert
        cmd: python -m paddle.distributed.launch  run_pretrain.py
        params:
          - --model_type bert
          - --model_name_or_path bert-base-uncased
          - --batch_size 16
          - --input_dir bert/
          - --output_dir pretrained_models/
          - --logging_steps 1
          - --save_steps 2
          - --max_steps 2
          - --device gpu
          - --use_amp True
        result:
          loss:
            base: 6.28685
            threshold: 0.01
            evaluation: "="
      -
        name: fintune_glue
        path: legacy/model_zoo/bert
        cmd: python -m paddle.distributed.launch run_glue.py
        params:
          - --model_type bert
          - --model_name_or_path bert-base-uncased
          - --task_name SST2
          - --logging_steps 1
          - --save_steps 2
          - --max_steps 2
          - --output_dir ./tmp/
          - --device gpu
          - --use_amp True
        result:
          loss:
            base: 1.456469
            threshold: 0.01
            evaluation: "="
    eval: skipped
    infer: skipped
    export:
      -
        name: export
        path: legacy/model_zoo/bert
        cmd:  python -u ./export_model.py
        params:
          - --model_type bert
          - --model_path bert-base-uncased
          - --output_path ./infer_model/model
    predict:
      -
        name: predict
        path: legacy/model_zoo/bert
        cmd: python -u ./predict_glue.py
        params:
          - --task_name SST2
          - --model_type bert
          - --model_path ./infer_model/model
          - --batch_size 32
          - --max_seq_length 128
      -
        name: predict_sample_data_SST2
        path: legacy/model_zoo/bert
        cmd: python -u ./predict_glue.py
        params:
          - --task_name SST2
          - --model_type bert
          - --model_path ./infer_model/model
          - --device gpu
          - --max_seq_length 128

  windows_cpu:
    train: skipped
    eval: skipped
    infer: skipped
    export: skipped
    predict: skipped

  mac:
    train: skipped
    eval: skipped
    infer: skipped
    export: skipped
    predict: skipped
