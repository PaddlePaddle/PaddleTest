import os
os.environ['FLAGS_cinn_new_group_scheduler'] = '1'
os.environ['FLAGS_group_schedule_tiling_first'] = '1'
os.environ['FLAGS_enable_pir_api'] = '1'
os.environ['FLAGS_cinn_bucket_compile'] = '1'
import sys
import unittest
import numpy as np
from dataclasses import dataclass
import typing as t
import itertools

@dataclass
class Stage:
    name: str
    env_vars: t.Dict[str, str]

cinn_stages = [
    Stage(
        name="dynamic_to_static",
        env_vars=dict(
            PADDLE_DEBUG_ENABLE_CINN=False,
            FLAGS_prim_all=False,
            FLAGS_prim_enable_dynamic=False,
        ),
    ),
    Stage(
        name="prim",
        env_vars=dict(
            PADDLE_DEBUG_ENABLE_CINN=False,
            FLAGS_prim_all=True,
            FLAGS_prim_enable_dynamic=True,
        ),
    ),
    Stage(
        name="infer_symbolic",
        env_vars=dict(
            PADDLE_DEBUG_ENABLE_CINN=False,
            FLAGS_prim_all=True,
            FLAGS_prim_enable_dynamic=True,
            FLAGS_use_cinn=False,
            FLAGS_check_infer_symbolic=True,
        ),
    ),
	Stage(
        name="frontend",
        env_vars=dict(
            PADDLE_DEBUG_ENABLE_CINN=True,
            FLAGS_prim_all=True,
            FLAGS_prim_enable_dynamic=True,
            FLAGS_use_cinn=True,
            FLAGS_check_infer_symbolic=False,
            FLAGS_enable_fusion_fallback=True,
        ), 
    ),
    Stage(
        name="backend",
        env_vars=dict(
            PADDLE_DEBUG_ENABLE_CINN=True,
            FLAGS_prim_all=True,
            FLAGS_prim_enable_dynamic=True,
            FLAGS_use_cinn=True,
            FLAGS_check_infer_symbolic=False,
            FLAGS_enable_fusion_fallback=False,
        ), 
    ),
]

def GetCinnStageByName(name):
    for stage in cinn_stages:
        if stage.name == name:
            return stage
    return None

def GetCurrentCinnStage():
    name = os.getenv('PADDLE_DEBUG_CINN_STAGE_NAME')
    if name is None:
        return None
    stage_names = [stage.name for stage in cinn_stages]
    assert name in stage_names, (
        f"PADDLE_DEBUG_CINN_STAGE_NAME should be in {stage_names}"
    )
    return GetCinnStageByName(name)

def GetPrevCinnStage(stage):
    for i in range(1, len(cinn_stages)):
        if stage is cinn_stages[i]:
            return cinn_stages[i - 1]
    return None

def IsCinnStageEnableDiff():
    value = os.getenv('PADDLE_DEBUG_CINN_STAGE_ENABLE_DIFF')
    enabled = value in {
        '1',
        'true',
        'True',
    }
    if enabled:
        assert GetCurrentCinnStage() is not None
    return enabled

def GetExitCodeAndStdErr(cmd, env):
    env = {
        k:v
        for k, v in env.items()
        if v is not None
    }
    import subprocess
    result = subprocess.run(
        cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
        env=env,
    )
    return result.returncode, result.stderr

def GetStageExitCodeAndStdErr(stage):
    return GetExitCodeAndStdErr(
        [sys.executable, __file__],
        env=dict(
            PADDLE_DEBUG_CINN_STAGE_NAME=stage.name,
            PADDLE_DEBUG_CINN_STAGE_ENABLE_DIFF='0',
            PYTHONPATH=os.getenv('PYTHONPATH'),
            ATHENA_ENABLE_TRY_RUN="False",
        ),
    )

def AthenaTryRunEnabled():
    return os.getenv('ATHENA_ENABLE_TRY_RUN') not in {
        "0",
        "False",
        "false",
        "OFF"
    }

def GetNeedSkipAndSkipMessage():
    current_stage = GetCurrentCinnStage()
    assert current_stage is not None
    if not IsCinnStageEnableDiff():
        return False, ""
    last_stage = GetPrevCinnStage(current_stage)
    if last_stage is None:
        return False, ""
    exitcode, stderr = GetStageExitCodeAndStdErr(last_stage)
    if exitcode != 0:
        return True, "last stage failed."
    return False, ""

def GetCurrentStageTryRunExitCodeAndStdErr():
    if not AthenaTryRunEnabled():
        return False, ""
    current_stage = GetCurrentCinnStage()
    assert current_stage is not None
    return GetStageExitCodeAndStdErr(current_stage)

def SetDefaultEnv(**env_var2value):
    for env_var, value in env_var2value.items():
        if os.getenv(env_var) is None:
            os.environ[env_var] = str(value)

SetDefaultEnv(
    ATHENA_ENABLE_TRY_RUN=False,
    PADDLE_DEBUG_CINN_STAGE_NAME="backend",
    PADDLE_DEBUG_CINN_STAGE_ENABLE_DIFF=False,
    PADDLE_DEBUG_ENABLE_CINN=True,
    FLAGS_enable_pir_api=True,
    FLAGS_prim_all=True,
    FLAGS_prim_enable_dynamic=True,
    FLAGS_use_cinn=False,
    FLAGS_check_infer_symbolic=False,
    FLAGS_enable_fusion_fallback=False,
)

import paddle

def SetEnvVar(env_var2value):
    for env_var, value in env_var2value.items():
        os.environ[env_var] = str(value)
    paddle.set_flags({
        env_var:value
        for env_var, value in env_var2value.items()
        if env_var.startswith('FLAGS_')
    })

if GetCurrentCinnStage() is not None:
    SetEnvVar(GetCurrentCinnStage().env_vars)

def GetEnvVarEnableJit():
    enable_jit = os.getenv('PADDLE_DEBUG_ENABLE_JIT')
    return enable_jit not in {
        "0",
        "False",
        "false",
        "OFF",
    }

def GetEnvVarEnableCinn():
    enable_cinn = os.getenv('PADDLE_DEBUG_ENABLE_CINN')
    if enable_cinn is None:
        return True
    return enable_cinn not in {
        "0",
        "False",
        "false",
        "OFF",
    }


def GetTolerance(dtype):
    if dtype == np.float16:
        return GetFloat16Tolerance()
    if dtype == np.float32:
        return GetFloat32Tolerance()
    return 1e-6

def GetFloat16Tolerance():
    try:
        return float(os.getenv('PADDLE_DEBUG_FLOAT16_TOL'))
    except:
        return 1e-3

def GetFloat32Tolerance():
    try:
        return float(os.getenv('PADDLE_DEBUG_FLOAT32_TOL'))
    except:
        return 1e-6

def IsInteger(dtype):
    return np.dtype(dtype).char in np.typecodes['AllInteger']

def ApplyToStatic(net, use_cinn):
    build_strategy = paddle.static.BuildStrategy()
    build_strategy.build_cinn_pass = use_cinn
    return paddle.jit.to_static(
        net,
        input_spec=net.get_input_spec(),
        build_strategy=build_strategy,
        full_graph=True,
    )

class InstanceTrait:

    @classmethod
    def instance(cls):
        if cls.instance_ is None:
            cls.instance_ = cls()
        return cls.instance_

    @classmethod
    def static_instance_with_cinn(cls):
        if cls.static_instance_with_cinn_ is None:
            cls.static_instance_with_cinn_ = ApplyToStatic(
                cls.instance(),
                use_cinn=True
            )
        return cls.static_instance_with_cinn_

    @classmethod
    def static_instance_without_cinn(cls):
        if cls.static_instance_without_cinn_ is None:
            cls.static_instance_without_cinn_ = ApplyToStatic(
                cls.instance(),
                use_cinn=False
            )
        return cls.static_instance_without_cinn_


class CinnTestBase:

    def setUp(self):
        paddle.seed(2024)
        self.prepare_data()

    def _test_entry(self):
        dy_outs = self.train(use_cinn=False)
        cinn_outs = self.train(use_cinn=GetEnvVarEnableCinn())

        for cinn_out, dy_out in zip(cinn_outs, dy_outs):
          if type(cinn_out) is list and type(dy_out) is list:
            for x, y in zip(cinn_out, dy_out):
              self.assert_all_close(x, y)
          else:
            self.assert_all_close(cinn_out, dy_out)

    def train(self, use_cinn):
        if GetEnvVarEnableJit():
            net = self.prepare_static_net(use_cinn)
        else:
            net = self.prepare_net()
        paddle.seed(2024)
        out = net(*self.inputs)
        return out
    
    def prepare_data(self):
        self.inputs = self.get_inputs()
        for input in self.inputs:
            input.stop_gradient = True

    def prepare_net(self):
        return self.get_test_class().instance()

    def prepare_static_net(self, use_cinn):
        if use_cinn:
            return self.get_test_class().static_instance_with_cinn()
        else:
            return self.get_test_class().static_instance_without_cinn()

    def assert_all_close(self, x, y):
        if (hasattr(x, "numpy") and hasattr(y, "numpy")):
            x_numpy = x.numpy()
            y_numpy = y.numpy()
            assert x_numpy.dtype == y_numpy.dtype
            if IsInteger(x_numpy.dtype):
                np.testing.assert_equal(x_numpy, y_numpy)
            else:
                tol = GetTolerance(x_numpy.dtype)
                np.testing.assert_allclose(x_numpy, y_numpy, atol=tol, rtol=tol)
        else:
            assert x == y





need_skip, skip_message = GetNeedSkipAndSkipMessage()
try_run_exit_code, try_run_stderr = GetCurrentStageTryRunExitCodeAndStdErr()
class TestTryRun(unittest.TestCase):
    def test_panic(self):
        if not AthenaTryRunEnabled():
            return
        if try_run_exit_code == 0:
            # All unittest cases passed.
            return
        if try_run_exit_code > 0:
            # program failed but not panic.
            return
        # program panicked.
        kOutputLimit = 65536
        message = try_run_stderr[-kOutputLimit:]
        raise RuntimeError(f"panicked. last {kOutputLimit} characters of stderr: \n{message}")
class PrimitiveOp_ffe6db27f65675d4da5719adfa845451(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a0d38740a2d3f8c885e570c02f81c159(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.00963907316327095, 0.4952716529369354, 0.4298664927482605, 0.1898406445980072, 0.09366508573293686, 0.22005803883075714, 0.09544357657432556, 0.4099632799625397, 0.10906834155321121, 0.3631872236728668, 0.10855133831501007, 0.07764644175767899, 0.383882075548172, 0.4136222004890442, 0.16006900370121002, 0.45305028557777405, 5.282094934955239e-05, 0.25554874539375305, 0.3602435886859894, 0.3348648250102997, 0.2983684837818146, 0.30115029215812683, 0.1923469603061676, 0.31077080965042114], dtype='float32').reshape([24]),
            paddle.to_tensor([0.19193211197853088, 0.20530351996421814, 0.29504451155662537, 0.0791202113032341, 0.3969559073448181, 0.30510783195495605, 0.47212767601013184, 0.2411944717168808, 0.4894741177558899, 0.2651132345199585, 0.3872438371181488, 0.07113046199083328, 0.4509156346321106, 0.13936014473438263, 0.2208334058523178, 0.055066633969545364, 0.4304090738296509, 0.13155527412891388, 0.16349200904369354, 0.39094066619873047, 0.04404998943209648, 0.46518296003341675, 0.006557495333254337, 0.10919535160064697], dtype='float32').reshape([24]),
            paddle.to_tensor([0.05450006201863289, 0.3982734978199005, 0.05617506429553032, 0.11740963160991669, 0.1392359882593155, 0.16406205296516418, 0.2876022756099701, 0.2510375380516052, 0.03513907268643379, 0.12879565358161926, 0.007613968104124069, 0.2476000338792801, 0.10909519344568253, 0.48613035678863525, 0.17029590904712677, 0.22623467445373535, 0.2966732084751129, 0.4956979751586914, 0.25022125244140625, 0.3559741675853729, 0.370802640914917, 0.05038881674408913, 0.22164352238178253, 0.07103568315505981], dtype='float32').reshape([24]),
            paddle.to_tensor([0.08098900318145752, 0.3215682804584503, 0.12781865894794464, 0.4035007655620575, 0.441372811794281, 0.4919942617416382, 0.1776186227798462, 0.26314300298690796, 0.4025031328201294, 0.3414134979248047, 0.05998397246003151, 0.2167266458272934, 0.429374098777771, 0.1401931345462799, 0.3931504189968109, 0.36554649472236633, 0.21730168163776398, 0.2958480417728424, 0.008897439576685429, 0.23111370205879211, 0.0361039862036705, 0.004783615469932556, 0.4361465275287628, 0.06803616136312485], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6d1651bc954ee08a9954a4407da6536d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 35, 35], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_30c38f8dc41268019ecbe5f53d72dfec(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 10, 10], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e04456d72d844f99eca273034cf94962(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f87f70ff66ab19612182b5ca6d312ccb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_26b78b45c5ecb0689810cbf1999646ba(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 32, 64], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.28612735867500305, 0.3815174102783203, 0.04523709788918495, 0.41154447197914124, 0.012097364291548729, 0.24104639887809753, 0.1860925704240799, 0.14015264809131622, 0.12076213210821152, 0.0953383520245552, 0.023564934730529785, 0.33590543270111084, 0.11393000185489655, 0.02695397101342678, 0.15377986431121826, 0.46985894441604614, 0.4773605763912201, 0.24655763804912567], dtype='float32').reshape([18]),
            paddle.to_tensor([0.400625616312027, 0.04865507036447525, 0.17108222842216492, 0.09716106951236725, 0.42031919956207275, 0.4964427351951599, 0.16315966844558716, 0.24377118051052094, 0.49886709451675415, 0.4831445515155792, 0.13408535718917847, 0.39345604181289673, 0.13180424273014069, 0.29617658257484436, 0.11809489130973816, 0.26376238465309143, 0.11091621965169907, 0.4428212642669678], dtype='float32').reshape([18]),
            paddle.to_tensor([0.17512081563472748, 0.4246291518211365, 0.2884640097618103, 0.3089365065097809, 0.17548227310180664, 0.10033627599477768, 0.4758952856063843, 0.021007629111409187, 0.4140338599681854, 0.21276377141475677, 0.46250930428504944, 0.37780097126960754, 0.3952149450778961, 0.48805150389671326, 0.08734383434057236, 0.14759793877601624, 0.4277113974094391, 0.03290049731731415], dtype='float32').reshape([18]),
            paddle.to_tensor([0.15028055012226105, 0.36400270462036133, 0.2230176478624344, 0.1947799175977707, 0.2613155245780945, 0.08722823858261108, 0.08220459520816803, 0.41392484307289124, 0.27216437458992004, 0.1290919929742813, 0.1331608146429062, 0.3051297068595886, 0.4251135289669037, 0.1342073529958725, 0.2245788872241974, 0.4651903510093689, 0.03736390918493271, 0.16861912608146667], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2bb16f3ff48c9151c04390b857d10394(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ea5ea90d18a13c9aa536ece771219a45(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_402d6809ddeea494c760b086db9e1992(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_21ee9057091a4e3158d8abba1bd59407(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7c8599970484a1a4a9353937b3009a2d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4e1f773ab7a48527986a1f0a5ef95399(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 60, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_254f2485822edb85fb71e7fe415e3b62(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6df6442cdee458b7279e4e30db7bb7fc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 16, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c948fddb089f4720cf7851472391c496(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 60, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0da7a17ccc2d701c83061be77b8d94ec(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 8, 6], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bc367bf32b0d133d1369c450a910d77f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 16, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f1995b567ca84fb86ff9727d7aa7f227(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.022794805467128754, 0.31372082233428955, 0.4853174090385437, 0.1695024073123932, 0.2330080270767212, 0.2648053467273712, 0.2983473241329193, 0.06046704947948456, 0.16892948746681213, 0.2577167749404907, 0.2643447518348694, 0.16891705989837646, 0.1741299033164978, 0.16887323558330536, 0.28091898560523987, 0.23739829659461975, 0.1603555679321289, 0.37162959575653076, 0.06476794928312302, 0.03082238882780075, 0.1153540313243866, 0.38787463307380676, 0.23233449459075928, 0.08267935365438461, 0.12616658210754395, 0.13427697122097015, 0.33569467067718506, 0.2966068387031555, 0.3127885162830353, 0.15964461863040924], dtype='float32').reshape([30]),
            paddle.to_tensor([0.40069785714149475, 0.13796988129615784, 0.4456109404563904, 0.016152583062648773, 0.3678443133831024, 0.1604910045862198, 0.23435468971729279, 0.13910433650016785, 0.41660821437835693, 0.4662500321865082, 0.48799046874046326, 0.286388635635376, 0.2817871570587158, 0.3192797601222992, 0.21530190110206604, 0.43153852224349976, 0.1644577980041504, 0.34754589200019836, 0.45063912868499756, 0.009481239132583141, 0.14555799961090088, 0.21220165491104126, 0.1634119302034378, 0.36466994881629944, 0.1303023248910904, 0.020108666270971298, 0.48589083552360535, 0.09501297026872635, 0.33239802718162537, 0.2811816334724426], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2842411994934082, 0.2329847365617752, 0.23689687252044678, 0.22371718287467957, 0.4492878317832947, 0.38369694352149963, 0.3086232542991638, 0.09423711150884628, 0.41254526376724243, 0.19071811437606812, 0.3097197711467743, 0.23951365053653717, 0.4058573544025421, 0.11154100298881531, 0.2737465500831604, 0.1008879542350769, 0.42196226119995117, 0.1086006835103035, 0.1393393576145172, 0.46816983819007874, 0.05262444540858269, 0.011845803819596767, 0.28781113028526306, 0.09682745486497879, 0.2939113676548004, 0.08875320106744766, 0.02658291906118393, 0.15330862998962402, 0.02569638378918171, 0.033666323870420456], dtype='float32').reshape([30]),
            paddle.to_tensor([0.22048716247081757, 0.22762472927570343, 0.05654966086149216, 0.35305142402648926, 0.3811964690685272, 0.10765687376260757, 0.36284229159355164, 0.03450128808617592, 0.09915314614772797, 0.36430788040161133, 0.2787925601005554, 0.21989503502845764, 0.44945788383483887, 0.2634105980396271, 0.03233135864138603, 0.09975109249353409, 0.1650729477405548, 0.32790255546569824, 0.1413305699825287, 0.45455536246299744, 0.440006822347641, 0.270102858543396, 0.3092101812362671, 0.3716546893119812, 0.4853034019470215, 0.09566760808229446, 0.03550741821527481, 0.2356245219707489, 0.17129240930080414, 0.4407794773578644], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6e5f135ada486e2e5ef5850981cce2de(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 32, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_79ba5207c0893535c3bd029585d91ca0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4518556296825409, 0.3015404939651489, 0.00042317277984693646, 0.17076319456100464, 0.017208101227879524, 0.19962890446186066, 0.4460568130016327, 0.3372807204723358, 0.46451109647750854, 0.47289419174194336, 0.10209739953279495, 0.49384117126464844, 0.2568758428096771, 0.32096347212791443, 0.33385971188545227, 0.30675172805786133, 0.18098057806491852, 0.14428937435150146, 0.14999428391456604, 0.3270135223865509, 0.06696426123380661, 0.02895934320986271, 0.495164692401886, 0.23217540979385376, 0.40101176500320435, 0.4100981652736664, 0.3360123932361603, 0.3784080147743225], dtype='float32').reshape([28]),
            paddle.to_tensor([0.2590256631374359, 0.12345674633979797, 0.28409069776535034, 0.34905433654785156, 0.06485819071531296, 0.06963902711868286, 0.20946335792541504, 0.33083394169807434, 0.29485970735549927, 0.3119255602359772, 0.4018772542476654, 0.24854794144630432, 0.3356415629386902, 0.3526861071586609, 0.4308042526245117, 0.2971215844154358, 0.34784960746765137, 0.35327911376953125, 0.33516377210617065, 0.005817042663693428, 0.2770463824272156, 0.39075735211372375, 0.4887378215789795, 0.10310503095388412, 0.18217502534389496, 0.3259231150150299, 0.25215619802474976, 0.3063662350177765], dtype='float32').reshape([28]),
            paddle.to_tensor([0.30844199657440186, 0.44639238715171814, 0.31129157543182373, 0.06555965542793274, 0.4807398319244385, 0.3386821150779724, 0.008549605496227741, 0.144614115357399, 0.36142051219940186, 0.333913654088974, 0.26547837257385254, 0.05581993609666824, 0.010070874355733395, 0.10335276275873184, 0.07331177592277527, 0.25474074482917786, 0.2445862591266632, 0.19972378015518188, 0.29133743047714233, 0.15360146760940552, 0.03183317929506302, 0.023570183664560318, 0.312635213136673, 0.007366131991147995, 0.38922056555747986, 0.2224033623933792, 0.44970571994781494, 0.15509119629859924], dtype='float32').reshape([28]),
            paddle.to_tensor([0.317325234413147, 0.43955516815185547, 0.40611958503723145, 0.06829517334699631, 0.2645970284938812, 0.39450252056121826, 0.2714134454727173, 0.4543478190898895, 0.36471027135849, 0.47323331236839294, 0.300085186958313, 0.39533382654190063, 0.4684881865978241, 0.3097500801086426, 0.173407644033432, 0.3755795955657959, 0.4111587703227997, 0.4112352132797241, 0.42969441413879395, 0.4460598826408386, 0.4981117844581604, 0.33102428913116455, 0.487263947725296, 0.004734368063509464, 0.13374727964401245, 0.2765214741230011, 0.037784285843372345, 0.269283264875412], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_884e90d17543233ff5da98db14e81c68(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6991acb578d63d7a96a98974729f033e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a52cf2f4474f86d3fd2f00977ac48cc4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aeaccb6aaec9a8deb6cd9f9770c45bff(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 16, 12], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_77bb8a4408a2318bd84ca4b7e86d7f21(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ebd98e67874c4016388cc10f700c7254(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 8, 6], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f98f754c929d0f939a0cb2bcc292b30c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b75b756f6021ff9b7bb10517473b8b36(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 16, 12], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e24a1d70c071470dc40929c8d6a20f04(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b864045efed9812c8f1d9d9149d7f729(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.26459500193595886, 0.07534153759479523, 0.4638489782810211, 0.21093754470348358, 0.11446517705917358, 0.3769133985042572, 0.41198280453681946, 0.04102326184511185, 0.16313236951828003, 0.03940339386463165, 0.33997318148612976, 0.14279715716838837, 0.4233648180961609, 0.4152408242225647, 0.3595559895038605, 0.020471982657909393, 0.49651047587394714, 0.19884635508060455, 0.06053716316819191, 0.4502752423286438, 0.10839392244815826, 0.0554499514400959, 0.22332200407981873, 0.42712995409965515], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4124518632888794, 0.44532594084739685, 0.28687724471092224, 0.045382022857666016, 0.3577042818069458, 0.3701724410057068, 0.3456260859966278, 0.34853291511535645, 0.045268841087818146, 0.3423847556114197, 0.46353280544281006, 0.38374248147010803, 0.23556530475616455, 0.02615380473434925, 0.3224770426750183, 0.12375756353139877, 0.2228168398141861, 0.16692836582660675, 0.0700727179646492, 0.09646471589803696, 0.11484690755605698, 0.06984750181436539, 0.02825174294412136, 0.16739654541015625], dtype='float32').reshape([24]),
            paddle.to_tensor([0.38191258907318115, 0.20422759652137756, 0.32892248034477234, 0.12404190003871918, 0.37711301445961, 0.2934609353542328, 0.4588153064250946, 0.47037473320961, 0.3321378231048584, 0.20379091799259186, 0.06838517636060715, 0.14986547827720642, 0.10052016377449036, 0.20952649414539337, 0.03327200561761856, 0.35358211398124695, 0.45332619547843933, 0.3841250538825989, 0.4972397983074188, 0.22873906791210175, 0.4640447497367859, 0.19136948883533478, 0.4656026065349579, 0.21561992168426514], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3224215805530548, 0.4978771507740021, 0.41778597235679626, 0.024584485217928886, 0.3967316448688507, 0.03814435377717018, 0.40905097126960754, 0.4871983230113983, 0.46186718344688416, 0.027126314118504524, 0.2504512369632721, 0.2626974582672119, 0.09779027849435806, 0.2882637679576874, 0.05307028815150261, 0.2795204222202301, 0.17914515733718872, 0.2098335474729538, 0.09189873188734055, 0.19220656156539917, 0.030031070113182068, 0.2343224585056305, 0.042443789541721344, 0.09574221819639206], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_19bbf54b3b172f82632cf2aa2930ea60(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6b42aa8964a0794f53bbbcf03b94ff6a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 576, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9e9e21ba6e3b42db722178216a22107b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b16eea4d8ba079e86ccdeb097b2181be(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 720, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0d5999babc5d0a2ae1f86584f3c9f84f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_63b81c7484a01b682abfc18debc685ea(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 4, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4608170986175537, 0.08453350514173508, 0.009229354560375214, 0.4211324155330658], dtype='float32').reshape([4]),
            paddle.to_tensor([0.43909233808517456, 0.39608219265937805, 0.2670497000217438, 0.1142483800649643], dtype='float32').reshape([4]),
            paddle.to_tensor([0.03336121514439583, 0.41264215111732483, 0.43556395173072815, 0.02160768397152424], dtype='float32').reshape([4]),
            paddle.to_tensor([0.4882653057575226, 0.006902588531374931, 0.2844823896884918, 0.46747028827667236], dtype='float32').reshape([4]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f55aa30235ec209cc51128d51f604693(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 26, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.09095475822687149, 0.38007164001464844, 0.17437444627285004, 0.2829025685787201, 0.3437459468841553, 0.46307703852653503, 0.4096253216266632, 0.12780900299549103, 0.47862616181373596, 0.47635623812675476, 0.13245441019535065, 0.16400401294231415, 0.16082626581192017, 0.44399330019950867, 0.3122132420539856, 0.10963353514671326, 0.212215855717659, 0.07488247752189636, 0.4961725175380707, 0.22658590972423553, 0.275383323431015, 0.0042361547239124775, 0.0032966367434710264, 0.3946513235569, 0.290060818195343, 0.10937797278165817], dtype='float32').reshape([26]),
            paddle.to_tensor([0.07204672694206238, 0.2674996256828308, 0.1830175668001175, 0.08042365312576294, 0.1348547786474228, 0.42361563444137573, 0.2072220891714096, 0.4208945035934448, 0.4629594385623932, 0.38014885783195496, 0.4236551821231842, 0.41128939390182495, 0.022223670035600662, 0.40853965282440186, 0.26927655935287476, 0.49387234449386597, 0.2672613263130188, 0.13087381422519684, 0.1241375058889389, 0.1877552568912506, 0.11155883222818375, 0.3645593822002411, 0.038118381053209305, 0.30731409788131714, 0.010550808161497116, 0.44019100069999695], dtype='float32').reshape([26]),
            paddle.to_tensor([0.22312627732753754, 0.13395154476165771, 0.13632085919380188, 0.30193403363227844, 0.17851176857948303, 0.42859768867492676, 0.1639188975095749, 0.44539201259613037, 0.07805680483579636, 0.39747875928878784, 0.34603849053382874, 0.05371436849236488, 0.21124957501888275, 0.23474366962909698, 0.4976405203342438, 0.38237664103507996, 0.28307387232780457, 0.05642851069569588, 0.29883915185928345, 0.041815631091594696, 0.013774212449789047, 0.321554034948349, 0.12959672510623932, 0.4350857138633728, 0.32611116766929626, 0.08882298320531845], dtype='float32').reshape([26]),
            paddle.to_tensor([0.4457911252975464, 0.22861367464065552, 0.11801556497812271, 0.2950004041194916, 0.2504446804523468, 0.3717225193977356, 0.4970388412475586, 0.017750022932887077, 0.49538150429725647, 0.08136588335037231, 0.45661571621894836, 0.40685927867889404, 0.2872142493724823, 0.19246797263622284, 0.4127097725868225, 0.13498389720916748, 0.42113935947418213, 0.3851394057273865, 0.14311076700687408, 0.2135351300239563, 0.2021312713623047, 0.4888601303100586, 0.1056235283613205, 0.06441653519868851, 0.3747296929359436, 0.12548036873340607], dtype='float32').reshape([26]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_0336412036bb12c89542873a9d0cc312(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_836d093e333a5d848ba3704d86eb8b8b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_0336412036bb12c89542873a9d0cc312
    def get_inputs(self):
        return [
            paddle.uniform([1, 1152, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f6b8b08952d3f8537713400028d92dbd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.10350922495126724, 0.4375012218952179, 0.2317131906747818, 0.4073106050491333, 0.024302221834659576, 0.43194156885147095, 0.3992253243923187, 0.020460333675146103, 0.2981383204460144, 0.25185778737068176, 0.4245513379573822, 0.04054906964302063, 0.20325149595737457, 0.17903673648834229, 0.3270125687122345, 0.32908397912979126, 0.20056310296058655, 0.1776854693889618, 0.2167927324771881, 0.21677501499652863], dtype='float32').reshape([20]),
            paddle.to_tensor([0.20197880268096924, 0.00047169224126264453, 0.046344976872205734, 0.1687551736831665, 0.20955732464790344, 0.41359272599220276, 0.036743007600307465, 0.16558796167373657, 0.09242777526378632, 0.07162678986787796, 0.06663908809423447, 0.326638787984848, 0.32372573018074036, 0.44536298513412476, 0.06051410362124443, 0.0113832363858819, 0.25743454694747925, 0.3680356740951538, 0.34743383526802063, 0.27529922127723694], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3366315960884094, 0.13995860517024994, 0.3034348785877228, 0.4457821547985077, 0.46063536405563354, 0.22464852035045624, 0.1386556327342987, 0.10301446914672852, 0.04470366612076759, 0.46645915508270264, 0.14206674695014954, 0.3531925678253174, 0.3319450616836548, 0.14181019365787506, 0.4917222857475281, 0.28947171568870544, 0.19300995767116547, 0.16588731110095978, 0.19773276150226593, 0.22329959273338318], dtype='float32').reshape([20]),
            paddle.to_tensor([0.22209033370018005, 0.34280523657798767, 0.3234633505344391, 0.06122342497110367, 0.14576996862888336, 0.45813900232315063, 0.20779132843017578, 0.23755484819412231, 0.4638170301914215, 0.12312138825654984, 0.03407592698931694, 0.26310691237449646, 0.0449223667383194, 0.4420398473739624, 0.045374564826488495, 0.18904539942741394, 0.3945362865924835, 0.390705943107605, 0.37790989875793457, 0.014491356909275055], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_55605fe11e4c824c1bbb3913d64484f6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ee268bc7c8cadd48923afd08a84f76b8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_63b09fc6a2eeb614063ba79c6939e1d9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 256, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bdd52776afc3c30960c4121da655ac22(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.25638866424560547, 0.1658860296010971, 0.4017789959907532, 0.18613633513450623, 0.07462523132562637, 0.20435473322868347, 0.08785209059715271, 0.007821167819201946, 0.11530051380395889, 0.4854789972305298, 0.21540464460849762, 0.21928054094314575, 0.23381663858890533, 0.14796355366706848, 0.4220860004425049, 0.08378773927688599, 0.09810946881771088, 0.0036581438034772873, 0.4094467759132385, 0.12658795714378357, 0.1154491975903511, 0.17335599660873413, 0.11468929052352905, 0.2787286937236786, 0.24123011529445648, 0.26493212580680847, 0.30613958835601807, 0.12157651036977768], dtype='float32').reshape([28]),
            paddle.to_tensor([0.18297292292118073, 0.15671595931053162, 0.14094899594783783, 0.29996368288993835, 0.2881063222885132, 0.047189462929964066, 0.08272796869277954, 0.011134210042655468, 0.16441483795642853, 0.08841980248689651, 0.08070426434278488, 0.3011088967323303, 0.40583351254463196, 0.07208459824323654, 0.33378124237060547, 0.3613925576210022, 0.3948356807231903, 0.3527202606201172, 0.3923737704753876, 0.12996424734592438, 0.3186510503292084, 0.36164116859436035, 0.42627274990081787, 0.14000922441482544, 0.31304460763931274, 0.4135626256465912, 0.0767584890127182, 0.08648249506950378], dtype='float32').reshape([28]),
            paddle.to_tensor([0.21413879096508026, 0.43453747034072876, 0.16907839477062225, 0.35189947485923767, 0.16713206470012665, 0.18372176587581635, 0.03048045188188553, 0.07347283512353897, 0.16470025479793549, 0.32231438159942627, 0.44371625781059265, 0.20763936638832092, 0.4193250238895416, 0.11860676109790802, 0.2739194929599762, 0.07911001145839691, 0.007721604313701391, 0.08248189836740494, 0.283875435590744, 0.3119998574256897, 0.18235327303409576, 0.002009949879720807, 0.0330706350505352, 0.40481746196746826, 0.1495068371295929, 0.45345431566238403, 0.3980529308319092, 0.2357337921857834], dtype='float32').reshape([28]),
            paddle.to_tensor([0.12405792623758316, 0.1950102150440216, 0.10731049627065659, 0.1822555661201477, 0.2419614940881729, 0.0897345021367073, 0.10237854719161987, 0.355510413646698, 0.3588543236255646, 0.10265734791755676, 0.29615411162376404, 0.1263541728258133, 0.15481220185756683, 0.10742159932851791, 0.0030773943290114403, 0.2744813561439514, 0.009586289525032043, 0.10744132101535797, 0.13037560880184174, 0.2534988224506378, 0.031471312046051025, 0.27978020906448364, 0.4210316240787506, 0.3673045337200165, 0.4288535714149475, 0.11892824620008469, 0.35432127118110657, 0.0037087774835526943], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9a73330ad1aa60619f27a0c03ea3642f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_190f5489288e3db92fdca6b9490f8160(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fd39978cb9862e76f6760bc916c2b47f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_190f5489288e3db92fdca6b9490f8160
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_47f9cc6282f97bf05aec68eb849e584d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4a55fce74f8756843ff46eb13745fba0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_569138bef40dec381c33ec25b9378aed(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_06959d6335eb2c83af1fc28c55b8ff77(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dc16e425cb1302829419e3cac36f94e9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_24290bbf9921cfad13520de2c0a50b34(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_0336412036bb12c89542873a9d0cc312
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 75, 75], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0e98183d1f2233c73cda7c194af1f939(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 4, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8196c634e5a63dea9fcd031aaff7f5af(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_de04791b3e07d23cf112406f644e8539(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 1280, 1, 1], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fd78f08d0cdff66b5b01fc60ffc7a614(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1df16875c00b1b82d1453c26672da833(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 640, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_82f11c5489a2e63c9d6da8b63418b3d4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_190f5489288e3db92fdca6b9490f8160
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 32, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.15557165443897247, 0.45402589440345764, 0.24867750704288483, 0.37298932671546936, 0.20754678547382355, 0.3396739065647125, 0.043512821197509766, 0.10532744228839874], dtype='float32').reshape([8]),
            paddle.to_tensor([0.3736773133277893, 0.006872091442346573, 0.4296315312385559, 0.17855378985404968, 0.09778033941984177, 0.4649389088153839, 0.14668124914169312, 0.23476345837116241], dtype='float32').reshape([8]),
            paddle.to_tensor([0.35530173778533936, 0.18556751310825348, 0.23393917083740234, 0.27278822660446167, 0.4277087450027466, 0.14110548794269562, 0.10962697863578796, 0.4010295867919922], dtype='float32').reshape([8]),
            paddle.to_tensor([0.29915106296539307, 0.07538880407810211, 0.16827981173992157, 0.11380844563245773, 0.305363267660141, 0.055884312838315964, 0.43805959820747375, 0.42258158326148987], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5b53506667997f7be2b96988fb857bfb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4354703426361084, 0.0913039818406105, 0.03414691612124443, 0.47517523169517517, 0.3937543034553528, 0.19127026200294495, 0.17634911835193634, 0.35009539127349854, 0.36319559812545776, 0.4713064730167389, 0.1912776082754135, 0.1052272617816925, 0.14264144003391266, 0.31010159850120544, 0.3480187654495239, 0.08571989089250565, 0.23950991034507751, 0.13960015773773193, 0.4080495834350586, 0.10894452035427094, 0.356588751077652, 0.17127592861652374, 0.02636166289448738, 0.3850691020488739], dtype='float32').reshape([24]),
            paddle.to_tensor([0.05964948982000351, 0.3529506027698517, 0.03488554060459137, 0.08849645406007767, 0.2290201485157013, 0.13260018825531006, 0.04100894555449486, 0.16218777000904083, 0.1687854379415512, 0.28978702425956726, 0.40597063302993774, 0.1241808608174324, 0.331339955329895, 0.48486441373825073, 0.046973153948783875, 0.49442997574806213, 0.04627183824777603, 0.27135154604911804, 0.2259267419576645, 0.18281744420528412, 0.2489662617444992, 0.25985637307167053, 0.35887303948402405, 0.29466673731803894], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2166750282049179, 0.2250286340713501, 0.3904089629650116, 0.16794411838054657, 0.18886351585388184, 0.0496429018676281, 0.1478791981935501, 0.08193104714155197, 0.13257847726345062, 0.096767857670784, 0.42802944779396057, 0.29138585925102234, 0.043617356568574905, 0.008948288857936859, 0.16100431978702545, 0.03687656670808792, 0.15753836929798126, 0.4028261601924896, 0.3840809166431427, 0.2035684585571289, 0.0510169193148613, 0.01766827516257763, 0.16017235815525055, 0.14192987978458405], dtype='float32').reshape([24]),
            paddle.to_tensor([0.33076637983322144, 0.20713216066360474, 0.3452901542186737, 0.2721608579158783, 0.041447047144174576, 0.04309919849038124, 0.2211088389158249, 0.17925074696540833, 0.49293506145477295, 0.033950041979551315, 0.34544849395751953, 0.06489952653646469, 0.30190256237983704, 0.0010093178134411573, 0.12882088124752045, 0.25918206572532654, 0.09212527424097061, 0.027688562870025635, 0.01649770699441433, 0.17391395568847656, 0.1994611769914627, 0.422043114900589, 0.37462934851646423, 0.3467988967895508], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_551b7b803f5be7f435279fa2aa263ba7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_045492a46ac6c5191b46438c2f21e392(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_551b7b803f5be7f435279fa2aa263ba7
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a3152497b30c9e24a4c82d19ac9cca3c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5badcc214d24d00513f079fd6f9228a9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_799ebcf9dc2553fd65b1dd0a1983c3b6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 60, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cbacb53cb571b58fcf6a12f943aba1c2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 4, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_018e51ea0f426873386f4527f44dd9d5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5f3fd775ba993baa1b2ce64734e2dabb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 1, 1], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fe2d76c778d1194a773503fd62591848(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 240, 240], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.05424648895859718, 0.03144332021474838, 0.3521142899990082, 0.16078568994998932, 0.08524231612682343, 0.4486231505870819, 0.023560287430882454, 0.2812679409980774, 0.1626359224319458, 0.36857667565345764, 0.3340449929237366, 0.07377607375383377, 0.23304052650928497, 0.3860880136489868, 0.1010371744632721, 0.29857680201530457], dtype='float32').reshape([16]),
            paddle.to_tensor([0.04396132752299309, 0.18105930089950562, 0.057482559233903885, 0.10331139713525772, 0.06479281187057495, 0.498113214969635, 0.3471298813819885, 0.4496530294418335, 0.43459486961364746, 0.0338829942047596, 0.11197560280561447, 0.4616166651248932, 0.19938018918037415, 0.27698004245758057, 0.01627938449382782, 0.3418782651424408], dtype='float32').reshape([16]),
            paddle.to_tensor([0.25290897488594055, 0.2570442259311676, 0.2179965376853943, 0.3324406147003174, 0.4164692461490631, 0.388457715511322, 0.1329885870218277, 0.31467682123184204, 0.019957929849624634, 0.3805794417858124, 0.09739530086517334, 0.03486305847764015, 0.09000634402036667, 0.009917384013533592, 0.13000591099262238, 0.09457934647798538], dtype='float32').reshape([16]),
            paddle.to_tensor([0.137641042470932, 0.48628395795822144, 0.4675997793674469, 0.22732888162136078, 0.3666112720966339, 0.40219855308532715, 0.40486979484558105, 0.04661799594759941, 0.4245246648788452, 0.06695600599050522, 0.07074204832315445, 0.3865964114665985, 0.047126226127147675, 0.08429522812366486, 0.4785028398036957, 0.47035688161849976], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_41b514341d31766d3bc3d2efa068c3e8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 1584, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1584], dtype='float32', min=0, max=0.5),
            paddle.uniform([1584], dtype='float32', min=0, max=0.5),
            paddle.uniform([1584], dtype='float32', min=0, max=0.5),
            paddle.uniform([1584], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e5f851a453b1991713e7aeb873a0a6c7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4302850663661957, 0.13489335775375366, 0.3634539544582367, 0.1471184641122818, 0.18079186975955963, 0.1170794814825058, 0.16709338128566742, 0.49413514137268066, 0.340761661529541, 0.34969255328178406, 0.10427352041006088, 0.15821218490600586, 0.36374521255493164, 0.15457534790039062, 0.1473153978586197, 0.3766234517097473, 0.08194763958454132, 0.3208276033401489, 0.16475889086723328, 0.2741300165653229, 0.18259766697883606, 0.3801257312297821, 0.07409727573394775, 0.09679041057825089], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3244454860687256, 0.38225287199020386, 0.2013530284166336, 0.3622274398803711, 0.4700028896331787, 0.07543723285198212, 0.09469465166330338, 0.005603897385299206, 0.4840991497039795, 0.16329601407051086, 0.3507671654224396, 0.03105047531425953, 0.0759570375084877, 0.1806868314743042, 0.32444503903388977, 0.3023732602596283, 0.305645227432251, 0.2881982922554016, 0.34267914295196533, 0.4099988639354706, 0.2841653525829315, 0.4697672724723816, 0.37029892206192017, 0.4662044644355774], dtype='float32').reshape([24]),
            paddle.to_tensor([0.10478995740413666, 0.40864935517311096, 0.016138838604092598, 0.429513156414032, 0.48441365361213684, 0.2091531604528427, 0.4995878338813782, 0.006512475199997425, 0.08939997106790543, 0.06489542871713638, 0.4323040843009949, 0.23402763903141022, 0.37983712553977966, 0.01937948912382126, 0.23846247792243958, 0.49028971791267395, 0.2013600766658783, 0.42286187410354614, 0.18096910417079926, 0.02194424346089363, 0.13227114081382751, 0.4203052222728729, 0.19079042971134186, 0.13192398846149445], dtype='float32').reshape([24]),
            paddle.to_tensor([0.11823331564664841, 0.31966981291770935, 0.3226107954978943, 0.26514971256256104, 0.18346218764781952, 0.05976405739784241, 0.3334977626800537, 0.2694248557090759, 0.24176280200481415, 0.07807822525501251, 0.46272528171539307, 0.377981573343277, 0.05895180627703667, 0.3030843138694763, 0.43495193123817444, 0.33230870962142944, 0.1382901519536972, 0.024191904813051224, 0.038734350353479385, 0.47426486015319824, 0.06631603091955185, 0.360516756772995, 0.2106352150440216, 0.2674599289894104], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e1e34d45d69f39ee10f288765a4d30c8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1bcc673261dc3e35beeac7e216098c58(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3802843e483a45394c120998fe468a01(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_31325c76c0401d3683b3bdddc7193ed2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 32, 100], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fd80c4d337a527bee889e3b8b93e4fe1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_566a55236470b7635e875f253ff157c7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 30, 30], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_233dec0c039bb81170a9b1f27b586c25(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 81, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([81], dtype='float32', min=0, max=0.5),
            paddle.uniform([81], dtype='float32', min=0, max=0.5),
            paddle.uniform([81], dtype='float32', min=0, max=0.5),
            paddle.uniform([81], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2e0e3514b3ec5a21530545450c0d055a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2506409db234899426c64d9d345a8a37(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_657b2f129c3fd1ea6d9fdd6cd061965a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1b0957892017d79f2a2d5b5d805cfd14(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_22b77575a5bc0bc434de27b1d873db4a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0230d2904a343362d25bb197cd7a3cdf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_51342b998dd055c1e393f7e6a4621bcb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c7b1730eddaacb8ace34814620ae9926(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 26], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cecf965576de71520c43c3f132d7a9df(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_26db71f4758c174ed6889ab920e82e7d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6e11e4b7cce119417281263bfebda796(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b15dbec5f36221bbbaf6ba9f5383af6a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b4386fa827e91ccedcfeedfc49a1266f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0907aab0cb52f8fb5a5962ba05bcfbdf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_0336412036bb12c89542873a9d0cc312
    def get_inputs(self):
        return [
            paddle.uniform([1, 112, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ce5c5fa73bfe552d0949034c12613ba8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5ba7a92fbaa8fe6c7f911f8a5a2fcd26(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.04860871285200119, 0.47327637672424316, 0.27734804153442383, 0.4145191013813019, 0.3995008170604706, 0.3452295660972595, 0.37910446524620056, 0.3556990325450897, 0.45805883407592773, 0.4356819689273834, 0.49224621057510376, 0.15916313230991364, 0.09771028161048889, 0.38037121295928955, 0.3398989140987396, 0.2680998146533966, 0.3742370903491974, 0.43741104006767273, 0.19776952266693115, 0.3201477825641632], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4261849820613861, 0.007117441855370998, 0.3069949448108673, 0.29014042019844055, 0.09921535104513168, 0.020874476060271263, 0.4967038631439209, 0.2666604816913605, 0.39914655685424805, 0.18968959152698517, 0.3797762393951416, 0.3199397623538971, 0.4684675633907318, 0.21983595192432404, 0.05949723720550537, 0.45794183015823364, 0.33804482221603394, 0.4177098572254181, 0.33076873421669006, 0.27582618594169617], dtype='float32').reshape([20]),
            paddle.to_tensor([0.1535036414861679, 0.3644997477531433, 0.18700292706489563, 0.38374167680740356, 0.2598721981048584, 0.3758178651332855, 0.23676888644695282, 0.43062344193458557, 0.21174168586730957, 0.3838135004043579, 0.0958828330039978, 0.18389523029327393, 0.2118738293647766, 0.3629392683506012, 0.11933835595846176, 0.03983624652028084, 0.1148252859711647, 0.3050476610660553, 0.27625322341918945, 0.2981572151184082], dtype='float32').reshape([20]),
            paddle.to_tensor([0.12661230564117432, 0.39888808131217957, 0.3876330256462097, 0.42630666494369507, 0.1311868578195572, 0.17859593033790588, 0.08144524693489075, 0.20896196365356445, 0.3478817939758301, 0.15033015608787537, 0.4731173515319824, 0.011769495904445648, 0.3339359760284424, 0.2035197764635086, 0.026680193841457367, 0.3214057981967926, 0.10093709826469421, 0.09969199448823929, 0.17811647057533264, 0.05728210508823395], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_db3174add07d1e54e9eb6c708b115f6d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0d36e4eeadea9f1d26732f1bcd63eb48(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 35, 35], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_704951b2499fdcf02742326b8371051a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 10, 10], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ceb431deda754baf4259fd92dd70d22f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_75f8bbc26007da5fdc0ccaa08d0ad0e5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4504424035549164, 0.0025267633609473705, 0.199647918343544, 0.465385764837265, 0.38986724615097046, 0.31649282574653625, 0.4935428202152252, 0.3687841594219208, 0.490682989358902, 0.3397189974784851, 0.46063968539237976, 0.49662208557128906, 0.14232096076011658, 0.41474923491477966, 0.0797487273812294, 0.40684759616851807, 0.4959411919116974, 0.2856130301952362, 0.3166672885417938, 0.28726816177368164, 0.3573475778102875, 0.18999841809272766, 0.33660396933555603, 0.08986067026853561, 0.11212407797574997, 0.19124208390712738, 0.2597047984600067, 0.2972245514392853, 0.12087026238441467, 0.07593927532434464], dtype='float32').reshape([30]),
            paddle.to_tensor([0.27010592818260193, 0.24901451170444489, 0.37958917021751404, 0.11173272132873535, 0.07677853852510452, 0.225805401802063, 0.1702677607536316, 0.4766160547733307, 0.4186742603778839, 0.1794816553592682, 0.11099779605865479, 0.09807228296995163, 0.3834799528121948, 0.01773998513817787, 0.16076652705669403, 0.08383207768201828, 0.4413616955280304, 0.2923206090927124, 0.14007507264614105, 0.06265349686145782, 0.0032330933026969433, 0.15622958540916443, 0.28954577445983887, 0.26743292808532715, 0.21545332670211792, 0.12447720766067505, 0.3978480398654938, 0.4117598533630371, 0.09532480686903, 0.20953010022640228], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4372706711292267, 0.0827009379863739, 0.09488098323345184, 0.042476776987314224, 0.20819582045078278, 0.04238879680633545, 0.19166487455368042, 0.3831092119216919, 0.34355828166007996, 0.0006225420511327684, 0.48307910561561584, 0.23180747032165527, 0.49768415093421936, 0.08932037651538849, 0.2529391646385193, 0.2107997089624405, 0.03673900291323662, 0.2649419903755188, 0.2459663301706314, 0.19436627626419067, 0.1417030245065689, 0.08156432211399078, 0.14917759597301483, 0.13578684628009796, 0.13442137837409973, 0.20252518355846405, 0.3300899863243103, 0.4627249836921692, 0.34880387783050537, 0.4598919153213501], dtype='float32').reshape([30]),
            paddle.to_tensor([0.014852501451969147, 0.03150090202689171, 0.4271894693374634, 0.49809566140174866, 0.3355916142463684, 0.3899807929992676, 0.45428037643432617, 0.1635909378528595, 0.09100683033466339, 0.2601015567779541, 0.21527144312858582, 0.03422962874174118, 0.3615928888320923, 0.2388879656791687, 0.3336990475654602, 0.4955972731113434, 0.3735051155090332, 0.11392024159431458, 0.24049445986747742, 0.02572222240269184, 0.21817654371261597, 0.1410747766494751, 0.044494375586509705, 0.3142273426055908, 0.0068282438442111015, 0.15888191759586334, 0.10883499681949615, 0.42967405915260315, 0.31219449639320374, 0.34726372361183167], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_abc15bd94e55776fdbcb840279865c01(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6d281309572beef9ae66d7d93ea99599(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_abc15bd94e55776fdbcb840279865c01
    def get_inputs(self):
        return [
            paddle.uniform([196, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ddcae80447f3a1f853c0fc94a9fe5a16(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9a1deb43b50b64b34c5e3a282e6901b2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 64, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bb12de59fcb63d32987a7595ff8791a6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7b399430f7a42aab902c8e52cd3f967d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 640, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8b81736d86f0a9b81d42dce102f6dbbc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9d784bd09a7c2757fe830c3afd34aa89(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_71adf679806cab2a22b147a9b47d0cbc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a8183ce602bcf98bedbf213cdea451de(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_71adf679806cab2a22b147a9b47d0cbc
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aaf8f811d98b3569d56d2a36683944f9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 256, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_77573abe0722800d42562be2995aef39(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aa44b40e6952793979f65cc61ccff3d9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cd592a707cc99e2c7e8ca7861d5e9afe(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 1120, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1120], dtype='float32', min=0, max=0.5),
            paddle.uniform([1120], dtype='float32', min=0, max=0.5),
            paddle.uniform([1120], dtype='float32', min=0, max=0.5),
            paddle.uniform([1120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8ba98f6bf19f07b7384a8347ed6f6d29(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_0336412036bb12c89542873a9d0cc312
    def get_inputs(self):
        return [
            paddle.uniform([1, 1536, 10, 10], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e7bf210cb701f8428febf6017b5c768d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_abc15bd94e55776fdbcb840279865c01
    def get_inputs(self):
        return [
            paddle.uniform([49, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_13025b164297d9226f9641b7f0715c76(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5cce777fca70c36ee9743d27a3eb49bd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_13025b164297d9226f9641b7f0715c76
    def get_inputs(self):
        return [
            paddle.uniform([1, 728, 19, 19], dtype='float16', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_548b85b8858d71a3befdace270f51131(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_94d5381cbacce11942247c418f3815e1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4009912312030792, 0.37617290019989014, 0.014402667060494423, 0.40215447545051575, 0.25075653195381165, 0.2421955168247223, 0.28637054562568665, 0.1329742819070816, 0.44706428050994873, 0.3822038471698761, 0.2319452315568924, 0.0949869379401207, 0.4470282793045044, 0.2505302429199219, 0.480585515499115, 0.26049134135246277, 0.36541062593460083, 0.37038636207580566, 0.23104989528656006, 0.1436995416879654, 0.09627600014209747, 0.033574625849723816, 0.49091222882270813, 0.2078864425420761, 0.4606236517429352, 0.12021178752183914, 0.4618273675441742, 0.03846101462841034, 0.4119422435760498, 0.2121843695640564], dtype='float32').reshape([30]),
            paddle.to_tensor([0.14954404532909393, 0.45759183168411255, 0.34352993965148926, 0.22776947915554047, 0.44726744294166565, 0.3965056240558624, 0.24360297620296478, 0.13450762629508972, 0.4237346947193146, 0.38935863971710205, 0.08441381901502609, 0.35692450404167175, 0.470897912979126, 0.06576534360647202, 0.14912977814674377, 0.18080690503120422, 0.14402182400226593, 0.18470807373523712, 0.14264561235904694, 0.09386442601680756, 0.09430341422557831, 0.15741732716560364, 0.12318994104862213, 0.3970627784729004, 0.2938072681427002, 0.034525059163570404, 0.2875553071498871, 0.34797388315200806, 0.11594606190919876, 0.186179518699646], dtype='float32').reshape([30]),
            paddle.to_tensor([0.47019246220588684, 0.1271834820508957, 0.22324684262275696, 0.3072981536388397, 0.30232343077659607, 0.31559666991233826, 0.33091166615486145, 0.2820672392845154, 0.41187021136283875, 0.2684237062931061, 0.10119571536779404, 0.03744782134890556, 0.40445476770401, 0.36125895380973816, 0.330868661403656, 0.025416245684027672, 0.08091343194246292, 0.02993237040936947, 0.2494908571243286, 0.393409788608551, 0.11580687761306763, 0.001787073677405715, 0.042858969420194626, 0.10874079167842865, 0.2569397985935211, 0.12184023857116699, 0.47100716829299927, 0.11438305675983429, 0.35244807600975037, 0.33257487416267395], dtype='float32').reshape([30]),
            paddle.to_tensor([0.20098654925823212, 0.4341227412223816, 0.3947579264640808, 0.4945065975189209, 0.08515088260173798, 0.20277002453804016, 0.29586562514305115, 0.12947474420070648, 0.04324159771203995, 0.3224252462387085, 0.3884146809577942, 0.3129787743091583, 0.14708496630191803, 0.18632125854492188, 0.24559101462364197, 0.40497589111328125, 0.4464520514011383, 0.4470001459121704, 0.3999560475349426, 0.20132377743721008, 0.45573654770851135, 0.22641560435295105, 0.1481114625930786, 0.3807436227798462, 0.47115805745124817, 0.3263740837574005, 0.37149348855018616, 0.1890612691640854, 0.0013897831086069345, 0.20014168322086334], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c8eb2c5457fdc7d58c0283a829df7b53(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 38, 38], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_da3d7e970fb8e163cc4456c463d7dbdb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f59a1f7aba49460d810dfcbe282f0605(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_136bc18b276f0d4051503a1cf4b91fee(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 32, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2af11aea63805c1a4f58f579092335f3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 8, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_97909d3f01b80d28dd022de766d486dd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 200, 336], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3a77f566e5e9608316ad3af26facf7bc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_31150122fea62d793af3563ac0c08a39(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 175, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_37fa9e087d01a9a8538e2f9ef96d98d3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2854004204273224, 0.18828964233398438, 0.1637091487646103, 0.47349727153778076, 0.39209914207458496, 0.45975202322006226, 0.34511882066726685, 0.2838236689567566, 0.12028523534536362, 0.34847623109817505, 0.4398897588253021, 0.18453732132911682, 0.17998194694519043, 0.16099156439304352, 0.4324599504470825, 0.18891142308712006, 0.3907780051231384, 0.2982591688632965, 0.40256237983703613, 0.12644711136817932, 0.23959766328334808, 0.11414358764886856, 0.46599912643432617, 0.13464315235614777], dtype='float32').reshape([24]),
            paddle.to_tensor([0.27807146310806274, 0.026066835969686508, 0.3119122087955475, 0.36412590742111206, 0.2749992609024048, 0.05526860058307648, 0.19253934919834137, 0.4772399961948395, 0.07461585849523544, 0.4850483238697052, 0.418695867061615, 0.4246477782726288, 0.23661550879478455, 0.25843173265457153, 0.1404682844877243, 0.4894144535064697, 0.2903245985507965, 0.48116201162338257, 0.31017574667930603, 0.2740022540092468, 0.395796537399292, 0.06737420707941055, 0.42271313071250916, 0.16494520008563995], dtype='float32').reshape([24]),
            paddle.to_tensor([0.26784712076187134, 0.2869563102722168, 0.2522556185722351, 0.05661337450146675, 0.18933093547821045, 0.027637658640742302, 0.11906326562166214, 0.41951969265937805, 0.3637144863605499, 0.17098909616470337, 0.10480748862028122, 0.10773088037967682, 0.2911556661128998, 0.12405931204557419, 0.14432696998119354, 0.1677810102701187, 0.29059502482414246, 0.4742397665977478, 0.4020801782608032, 0.29623258113861084, 0.4656302034854889, 0.2554573118686676, 0.39741405844688416, 0.16154466569423676], dtype='float32').reshape([24]),
            paddle.to_tensor([0.21778729557991028, 0.09977909922599792, 0.39460620284080505, 0.46380847692489624, 0.2025262415409088, 0.2586837410926819, 0.17509301006793976, 0.012738010846078396, 0.397941529750824, 0.2208000272512436, 0.2523929476737976, 0.40117183327674866, 0.339544415473938, 0.4618389904499054, 0.2670344412326813, 0.3482533395290375, 0.008868166245520115, 0.3527756929397583, 0.05651400610804558, 0.14240342378616333, 0.008844864554703236, 0.4510621130466461, 0.40566086769104004, 0.2466210573911667], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_00f56ff231e83d320db78e810b26dffb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 38, 38], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f647585e55080fdced14fce77419e352(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_33c44343315cbd1d8cf684dd1bfeb8c1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7ba1457e3712f1490fc95714566a4840(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_13025b164297d9226f9641b7f0715c76
    def get_inputs(self):
        return [
            paddle.uniform([1, 1152, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d71cbd6f4a71059a5205fde8e60c27ce(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.11773978173732758, 0.45219656825065613, 0.3724818527698517, 0.3814125061035156, 0.3043578863143921, 0.178776815533638, 0.3371961712837219, 0.19321593642234802, 0.2476882040500641, 0.2162647545337677, 0.14056287705898285, 0.47451573610305786, 0.16890530288219452, 0.06954766809940338, 0.28556200861930847, 0.22570674121379852, 0.09931743890047073, 0.3967396318912506, 0.31169724464416504, 0.09481292963027954, 0.4613901674747467, 0.09698588401079178, 0.3611709475517273, 0.39038076996803284, 0.22427305579185486, 0.2992705702781677, 0.44460099935531616, 0.4005754292011261, 0.054491713643074036, 0.2261076718568802], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2714458405971527, 0.396857887506485, 0.23747782409191132, 0.24371805787086487, 0.22765713930130005, 0.09643419086933136, 0.05566910654306412, 0.36287152767181396, 0.36698296666145325, 0.2631871700286865, 0.03732944279909134, 0.05333629623055458, 0.06294161081314087, 0.22856776416301727, 0.4415123164653778, 0.42051365971565247, 0.360446959733963, 0.03356404975056648, 0.3562668561935425, 0.2885308265686035, 0.49539706110954285, 0.04201491177082062, 0.369399756193161, 0.04079466313123703, 0.23214265704154968, 0.1659097969532013, 0.22822976112365723, 0.45369818806648254, 0.4291858375072479, 0.4781855642795563], dtype='float32').reshape([30]),
            paddle.to_tensor([0.030841108411550522, 0.30912813544273376, 0.49211204051971436, 0.11793528497219086, 0.38131844997406006, 0.2556415796279907, 0.15113875269889832, 0.07226497679948807, 0.012385506182909012, 0.009633056819438934, 0.1515335738658905, 0.22713671624660492, 0.32700690627098083, 0.23032402992248535, 0.032506924122571945, 0.06892775744199753, 0.16744862496852875, 0.4780232012271881, 0.07901642471551895, 0.12423332780599594, 0.04911862686276436, 0.28376978635787964, 0.4718223214149475, 0.33902081847190857, 0.08912298083305359, 0.005845804698765278, 0.1986878514289856, 0.054195333272218704, 0.3439200818538666, 0.19437970221042633], dtype='float32').reshape([30]),
            paddle.to_tensor([0.1418883055448532, 0.4235123097896576, 0.4984837472438812, 0.49197009205818176, 0.28835663199424744, 0.23220670223236084, 0.3045744001865387, 0.40423980355262756, 0.10138314217329025, 0.31392911076545715, 0.4343814551830292, 0.1564614474773407, 0.3139078915119171, 0.24269172549247742, 0.053639769554138184, 0.3091413378715515, 0.24995127320289612, 0.039432890713214874, 0.35996830463409424, 0.25843551754951477, 0.3627026081085205, 0.477413535118103, 0.05357472598552704, 0.15846416354179382, 0.09661407023668289, 0.34496238827705383, 0.4579167068004608, 0.334669828414917, 0.34799811244010925, 0.08211660385131836], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4e802446edc4c3306a0bc25177f2476c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_011da317767d7d878442e64f528baf6f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 4, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9028b679fc286d097d52e63f82dc11fa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 31, 31], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_186eae946956756b13f3654169c2ef9a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_33edf907248be594994c2b0b3b45a1f2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_186eae946956756b13f3654169c2ef9a
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3d345dd61fa342a04a0fe790e8a47713(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_13025b164297d9226f9641b7f0715c76
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c61fe325a82d6fa513da5a324b7f9242(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 122, 122], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c460a515b3dcb81a15fd275e4599971a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bef821972e0c0b9f2864e21282082fe4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 2, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ace0885333cf71cf8a41fdd305ecf8c1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 26], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c1d891cac13a3741a7483895b0eb25aa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3911367952823639, 0.1965111792087555, 0.15140864253044128, 0.1268540620803833, 0.33134907484054565, 0.11597070097923279, 0.11627296358346939, 0.389244943857193, 0.09681526571512222, 0.41784441471099854, 0.4240811765193939, 0.1285683512687683, 0.41459912061691284, 0.22248761355876923, 0.3640151917934418, 0.05959701165556908, 0.31770965456962585, 0.44694358110427856, 0.14823122322559357, 0.4216688871383667, 0.2709929049015045, 0.3359881043434143, 0.3329317271709442, 0.3282211422920227], dtype='float32').reshape([24]),
            paddle.to_tensor([0.37069961428642273, 0.2975713312625885, 0.2677811086177826, 0.3242771327495575, 0.14992476999759674, 0.07209287583827972, 0.3351374566555023, 0.2263539582490921, 0.4502807855606079, 0.15593969821929932, 0.08306926488876343, 0.1154647096991539, 0.13974492251873016, 0.1839856505393982, 0.14510783553123474, 0.1699952632188797, 0.4496850073337555, 0.05479730665683746, 0.2977023124694824, 0.08064787089824677, 0.20525677502155304, 0.29786279797554016, 0.1265459507703781, 0.47131362557411194], dtype='float32').reshape([24]),
            paddle.to_tensor([0.006815141066908836, 0.07862517982721329, 0.46674802899360657, 0.3453272581100464, 0.36831429600715637, 0.3398631811141968, 0.22333897650241852, 0.30501794815063477, 0.3037267029285431, 0.1135629266500473, 0.01037339586764574, 0.4456331133842468, 0.3010467290878296, 0.016302388161420822, 0.054502733051776886, 0.08460333943367004, 0.34389528632164, 0.45624542236328125, 0.15129809081554413, 0.07595139741897583, 0.18821173906326294, 0.12125913798809052, 0.16474977135658264, 0.3115333020687103], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2861105799674988, 0.2695261538028717, 0.17278225719928741, 0.06973353773355484, 0.4174291491508484, 0.06804826855659485, 0.004840441048145294, 0.30404290556907654, 0.09546920657157898, 0.21170823276042938, 0.04248928651213646, 0.4094401001930237, 0.3222547471523285, 0.4720083475112915, 0.1217862218618393, 0.17090637981891632, 0.05136411264538765, 0.038067735731601715, 0.3764837980270386, 0.016122298315167427, 0.38272762298583984, 0.08734012395143509, 0.08571020513772964, 0.44568032026290894], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b8e0b76671fb6c514b12f9038acc224d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.11044556647539139, 0.38542723655700684, 0.20021696388721466, 0.07415679097175598, 0.009702719748020172, 0.47063368558883667, 0.13902267813682556, 0.14174319803714752, 0.12545743584632874, 0.26535797119140625, 0.4511420428752899, 0.16208751499652863, 0.16732436418533325, 0.31990912556648254, 0.12274371087551117, 0.4407392740249634, 0.31387555599212646, 0.2103782743215561, 0.10072088986635208, 0.16630735993385315, 0.4173157811164856, 0.3791913390159607, 0.17324793338775635, 0.0460093654692173, 0.11568647623062134, 0.05194632336497307, 0.45802783966064453, 0.27612030506134033, 0.14019697904586792, 0.4238722026348114], dtype='float32').reshape([30]),
            paddle.to_tensor([0.28552305698394775, 0.32896995544433594, 0.24503570795059204, 0.4491429030895233, 0.11658010631799698, 0.20718668401241302, 0.17339786887168884, 0.06843890994787216, 0.45273369550704956, 0.29674232006073, 0.4691162407398224, 0.1287052184343338, 0.179533451795578, 0.06826796382665634, 0.4640454649925232, 0.44794541597366333, 0.2520490884780884, 0.39734897017478943, 0.20090733468532562, 0.17867937684059143, 0.3700239062309265, 0.35433459281921387, 0.025651840493083, 0.28390786051750183, 0.060081999748945236, 0.3249567449092865, 0.20799334347248077, 0.10019903630018234, 0.44200411438941956, 0.3838551342487335], dtype='float32').reshape([30]),
            paddle.to_tensor([0.40595364570617676, 0.2893979847431183, 0.3216296136379242, 0.36246398091316223, 0.42584508657455444, 0.2631599009037018, 8.356041507795453e-05, 0.19925883412361145, 0.4290390908718109, 0.0097207585349679, 0.14670872688293457, 0.4978121519088745, 0.11594593524932861, 0.35101771354675293, 0.02961897663772106, 0.4975041151046753, 0.04295702278614044, 0.2964697480201721, 0.28283414244651794, 0.17233791947364807, 0.09710648655891418, 0.2192731499671936, 0.30871936678886414, 0.35070765018463135, 0.005463873967528343, 0.35507580637931824, 0.4171978235244751, 0.3082747757434845, 0.15491244196891785, 0.1674768328666687], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2382194697856903, 0.1975332796573639, 0.37628602981567383, 0.31568703055381775, 0.35163015127182007, 0.273114949464798, 0.22690285742282867, 0.34679552912712097, 0.21904516220092773, 0.367571085691452, 0.10526791214942932, 0.28835681080818176, 0.02301407977938652, 0.013439211063086987, 0.41529861092567444, 0.34582942724227905, 0.3546859622001648, 0.48403874039649963, 0.3582666516304016, 0.008960608392953873, 0.19265815615653992, 0.03590688481926918, 0.39629456400871277, 0.152740016579628, 0.022577926516532898, 0.11287537962198257, 0.19205105304718018, 0.1715579330921173, 0.2819284498691559, 0.2294635772705078], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_046adcb4dc2a1a2679f3a54fed2b8fb1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 972, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([972], dtype='float32', min=0, max=0.5),
            paddle.uniform([972], dtype='float32', min=0, max=0.5),
            paddle.uniform([972], dtype='float32', min=0, max=0.5),
            paddle.uniform([972], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6c4f042716df8555ae170e673ad80f1c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b488c2455bc024f36cc61686d3ad804c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 416, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4da62d934b5cac392e4eab69e227d216(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aed24c479bdded8e879a49030cd544b6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 800, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([800], dtype='float32', min=0, max=0.5),
            paddle.uniform([800], dtype='float32', min=0, max=0.5),
            paddle.uniform([800], dtype='float32', min=0, max=0.5),
            paddle.uniform([800], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_42341e72cdffe2e1abc9eca49f4cada2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_eca8d58b55061bc837f77a5117cf84eb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_190f5489288e3db92fdca6b9490f8160
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 4, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_eca2aea431ef78de1b611fed2e7d6f79(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.0867689698934555, 0.263782262802124, 0.0024293786846101284, 0.30059120059013367, 0.07478311657905579, 0.036859944462776184, 0.04947369545698166, 0.14042851328849792, 0.1769578903913498, 0.41994428634643555, 0.369582861661911, 0.2241259515285492, 0.28195852041244507, 0.01758921518921852, 0.36847561597824097, 0.27123749256134033, 0.2964341342449188, 0.14901894330978394, 0.21844139695167542, 0.33804410696029663], dtype='float32').reshape([20]),
            paddle.to_tensor([0.30557411909103394, 0.3717119097709656, 0.24306577444076538, 0.33762216567993164, 0.23457014560699463, 0.296254962682724, 0.18123772740364075, 0.16261664032936096, 0.2238103151321411, 0.22405001521110535, 0.3386358916759491, 0.4254676401615143, 0.15625165402889252, 0.4989171326160431, 0.000511910708155483, 0.048191819339990616, 0.3859730064868927, 0.19386853277683258, 0.4767901599407196, 0.4728297293186188], dtype='float32').reshape([20]),
            paddle.to_tensor([0.2087090015411377, 0.34822583198547363, 0.46605053544044495, 0.44679969549179077, 0.16044142842292786, 0.21840400993824005, 0.47840428352355957, 0.4053443968296051, 0.1784474104642868, 0.23462346196174622, 0.11321539431810379, 0.42093193531036377, 0.23099744319915771, 0.465829998254776, 0.48514315485954285, 0.4434916377067566, 0.06251054257154465, 0.11778563261032104, 0.36249902844429016, 0.47652366757392883], dtype='float32').reshape([20]),
            paddle.to_tensor([0.457986056804657, 0.19257819652557373, 0.004063892178237438, 0.28525829315185547, 0.041682321578264236, 0.30759698152542114, 0.39001384377479553, 0.14756424725055695, 0.3430522084236145, 0.45872417092323303, 0.3230314552783966, 0.24310711026191711, 0.2674834430217743, 0.29999855160713196, 0.04940848425030708, 0.4121844172477722, 0.046536196023225784, 0.013379726558923721, 0.21197816729545593, 0.2786138355731964], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_20e900f81bd9bc7e8a50a8dc42d01324(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 2, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8b77c6e60291dee950d0c5e451b5f265(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_248d344f15093e3cdbb35dbce2dfc2e9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([8, 256, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6918df52e78aa75b9935503cc0b2df09(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_13025b164297d9226f9641b7f0715c76
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ab61d74670f8d72af17badf613bc4ee9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 432, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e9e8b0bd3d37284c45fcab2de3275af1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_13025b164297d9226f9641b7f0715c76
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 75, 75], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9617229d3c75d62a44e2494c8d1020a2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_94fa055316c5f245bf65e23e0d448e4d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c74d8052d1d30d04ab62bb42e90478c6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 25, 42], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8851128c889ea09ca7cd527c76f31249(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 1632, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1632], dtype='float32', min=0, max=0.5),
            paddle.uniform([1632], dtype='float32', min=0, max=0.5),
            paddle.uniform([1632], dtype='float32', min=0, max=0.5),
            paddle.uniform([1632], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4368d61955ec210d007894063c98e07c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d9395d8b3372641e328dff44dcd42e05(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.350501149892807, 0.11547993123531342, 0.3456163704395294, 0.42091965675354004, 0.2552458345890045, 0.4582546651363373, 0.36797523498535156, 0.4237874448299408, 0.34651482105255127, 0.002812352031469345, 0.43812885880470276, 0.15552306175231934, 0.035169780254364014, 0.028706660494208336, 0.17875052988529205, 0.4692661464214325, 0.24221964180469513, 0.351133793592453, 0.11628562957048416, 0.27043187618255615, 0.2428886443376541, 0.3017314374446869, 0.12328634411096573, 0.43975555896759033, 0.03274824470281601, 0.43242353200912476, 0.19643917679786682, 0.3005152642726898, 0.056857917457818985, 0.36188194155693054], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4576278328895569, 0.4148518741130829, 0.4572694003582001, 0.16680021584033966, 0.4935934543609619, 0.32498589158058167, 0.28103697299957275, 0.3310156464576721, 0.3117212653160095, 0.2844379246234894, 0.3713410496711731, 0.08133772015571594, 0.13227578997612, 0.4400239884853363, 0.04807592183351517, 0.3259448707103729, 0.49134713411331177, 0.31847506761550903, 0.27251115441322327, 0.4677774906158447, 0.49152249097824097, 0.49143728613853455, 0.21744270622730255, 0.2571260631084442, 0.45702704787254333, 0.09577728807926178, 0.3206145763397217, 0.26633453369140625, 0.2509974241256714, 0.31583288311958313], dtype='float32').reshape([30]),
            paddle.to_tensor([0.44629207253456116, 0.3577226996421814, 0.12361541390419006, 0.16378509998321533, 0.4672880172729492, 0.459896057844162, 0.13277974724769592, 0.13151223957538605, 0.32097408175468445, 0.050638917833566666, 0.4684419631958008, 0.2120719701051712, 0.030584363266825676, 0.12338784337043762, 0.3436943590641022, 0.11559461802244186, 0.18806354701519012, 0.175562784075737, 0.0685274600982666, 0.4744751751422882, 0.009893382899463177, 0.20395952463150024, 0.3054858148097992, 0.4038833975791931, 0.35479456186294556, 0.23721136152744293, 0.26674684882164, 0.11208155006170273, 0.3258809447288513, 0.47017747163772583], dtype='float32').reshape([30]),
            paddle.to_tensor([0.43735742568969727, 0.4081907868385315, 0.29852011799812317, 0.044696688652038574, 0.2974678874015808, 0.06688983738422394, 0.4437485635280609, 0.19901958107948303, 0.22657470405101776, 0.18091759085655212, 0.14946384727954865, 0.22594408690929413, 0.38727661967277527, 0.46453559398651123, 0.19138851761817932, 0.08616228401660919, 0.3945731520652771, 0.16536729037761688, 0.17385903000831604, 0.36339375376701355, 0.1277574896812439, 0.0777372494339943, 0.3617907762527466, 0.05184026062488556, 0.017809627577662468, 0.18314005434513092, 0.28446507453918457, 0.4449096918106079, 0.4307384192943573, 0.42131394147872925], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b5caef0ebab73ae42891df17a5700f91(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.46791690587997437, 0.42518919706344604, 0.283853143453598, 0.18531900644302368, 0.10298466682434082, 0.40644705295562744, 0.020151203498244286, 0.45823004841804504, 0.36171430349349976, 0.42176127433776855, 0.3586062788963318, 0.4050736129283905, 0.4730387032032013, 0.10375220328569412, 0.2104099988937378, 0.4936061203479767], dtype='float32').reshape([16]),
            paddle.to_tensor([0.38564610481262207, 0.3106781244277954, 0.3018033504486084, 0.44522324204444885, 0.004751322790980339, 0.3349073827266693, 0.09690143167972565, 0.1561702936887741, 0.49409496784210205, 0.28356149792671204, 0.35139185190200806, 0.44424518942832947, 0.45092296600341797, 0.05941836163401604, 0.2100413590669632, 0.19021789729595184], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3128986656665802, 0.11530844867229462, 0.09572000801563263, 0.2823304533958435, 0.3533079922199249, 0.19982971251010895, 0.48489052057266235, 0.2122286707162857, 0.36648258566856384, 0.4351740777492523, 0.28426867723464966, 0.390857994556427, 0.387899249792099, 0.4643559455871582, 0.2136058658361435, 0.010489190928637981], dtype='float32').reshape([16]),
            paddle.to_tensor([0.18821494281291962, 0.26564860343933105, 0.3299156129360199, 0.39971137046813965, 0.38938188552856445, 0.3135489821434021, 0.12401048094034195, 0.33556675910949707, 0.3319908678531647, 0.017151132225990295, 0.09484287351369858, 0.17288409173488617, 0.254898339509964, 0.43326398730278015, 0.21916909515857697, 0.37783488631248474], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ee37de5ae1584ab231b857080e072513(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d005c643207948e6866bd0201b76118f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 88, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9b7156be2bf8301410d55ea627542ff5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 8, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_33b510466ee40f13e697ae6f40034874(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cac9512033e39f20b765e901cecac173(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 4, 512, 1024], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.08177782595157623, 0.4929604232311249, 0.3617161214351654, 0.4073362350463867], dtype='float32').reshape([4]),
            paddle.to_tensor([0.35357698798179626, 0.3623207211494446, 0.22713810205459595, 0.08543817698955536], dtype='float32').reshape([4]),
            paddle.to_tensor([0.3941805064678192, 0.37798717617988586, 0.2386365830898285, 0.2933065891265869], dtype='float32').reshape([4]),
            paddle.to_tensor([0.3624834418296814, 0.48081913590431213, 0.08604811131954193, 0.39365285634994507], dtype='float32').reshape([4]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_51946bfdbffbc02ce48e42e36570b4b6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d8d05dc4b8e515add8ea8c4499f4b586(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_71adf679806cab2a22b147a9b47d0cbc
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_14a0b23eceea4bce008315624e9e8092(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_79cbb20cefc9bfea31c462d17884aad5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3f48d7f7f1ad5042218138460c879235(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_13025b164297d9226f9641b7f0715c76
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d91b3bb77e864a233a245d7ca88a2849(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 17, 17], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_76fafac12abd94bc4248461e9522e7d7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3509295880794525, 0.13257892429828644, 0.334819495677948, 0.03175622969865799, 0.3108804523944855, 0.3444502651691437, 0.02757306396961212, 0.26872655749320984, 0.15321281552314758, 0.4520026743412018, 0.13659149408340454, 0.4466424584388733, 0.25931045413017273, 0.1643560826778412, 0.3030281960964203, 0.23713389039039612, 0.43074488639831543, 0.20052437484264374, 0.22977952659130096, 0.06264027208089828, 0.27555498480796814, 0.17660124599933624, 0.17324848473072052, 0.19450822472572327, 0.4322766363620758, 0.06517180055379868, 0.01702982746064663, 0.0907268300652504, 0.05391859635710716, 0.0024019444826990366], dtype='float32').reshape([30]),
            paddle.to_tensor([0.16447553038597107, 0.4809444546699524, 0.12821166217327118, 0.3147328794002533, 0.010546828620135784, 0.45225730538368225, 0.45840200781822205, 0.13486015796661377, 0.23686756193637848, 0.4765041172504425, 0.04041581600904465, 0.1836993545293808, 0.3501952290534973, 0.28235870599746704, 0.21927666664123535, 0.31145864725112915, 0.030498405918478966, 0.23352116346359253, 0.07460762560367584, 0.031027821823954582, 0.1371898651123047, 0.010908491909503937, 0.3990439772605896, 0.02093818224966526, 0.014910301193594933, 0.40024587512016296, 0.18035711348056793, 0.25394922494888306, 0.25104519724845886, 0.472616046667099], dtype='float32').reshape([30]),
            paddle.to_tensor([0.41225117444992065, 0.3112136721611023, 0.19377899169921875, 0.1106235533952713, 0.43213123083114624, 0.22236387431621552, 0.4370879530906677, 0.27298563718795776, 0.29655733704566956, 0.3486902415752411, 0.24284771084785461, 0.24133001267910004, 0.01142787840217352, 0.3678855895996094, 0.22631406784057617, 0.19555656611919403, 0.10904530435800552, 0.38518354296684265, 0.33519378304481506, 0.3070562779903412, 0.3285343647003174, 0.0327836275100708, 0.2997418940067291, 0.34752175211906433, 0.262317419052124, 0.023517929017543793, 0.4650430679321289, 0.16886787116527557, 0.3073676824569702, 0.19651363790035248], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4174928665161133, 0.07947343587875366, 0.40980175137519836, 0.2711547911167145, 0.3489481210708618, 0.3854246735572815, 0.4032283425331116, 0.38856616616249084, 0.0821535512804985, 0.2257111370563507, 0.24802784621715546, 0.48483872413635254, 0.20984555780887604, 0.3485463559627533, 0.38063889741897583, 0.34399333596229553, 0.38804513216018677, 0.1383611559867859, 0.051604244858026505, 0.405643105506897, 0.42760562896728516, 0.3090231716632843, 0.13901618123054504, 0.09652788192033768, 0.055063169449567795, 0.19958989322185516, 0.026220688596367836, 0.1539030373096466, 0.0689769983291626, 0.08152738213539124], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7e5555abcc8d3f9b6a2fa54976a30725(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 61, 61], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_46efa02d868f745f4264e3b286e0be9c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 640, 640], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.09446853399276733, 0.05570997670292854, 0.46258556842803955, 0.49542519450187683, 0.4368799924850464, 0.07684466987848282, 0.26280108094215393, 0.3384602963924408, 0.3897107243537903, 0.10497565567493439, 0.27433332800865173, 0.2304472029209137], dtype='float32').reshape([12]),
            paddle.to_tensor([0.44057705998420715, 0.43984559178352356, 0.3922331929206848, 0.017491135746240616, 0.24887976050376892, 0.12717965245246887, 0.23303619027137756, 0.09831079095602036, 0.428014874458313, 0.009928863495588303, 0.41834163665771484, 0.41995736956596375], dtype='float32').reshape([12]),
            paddle.to_tensor([0.13465829193592072, 0.460126131772995, 0.08848533779382706, 0.39933133125305176, 0.13097557425498962, 0.04170502349734306, 0.06782533973455429, 0.1201956644654274, 0.11455992609262466, 0.01686019077897072, 0.3027969300746918, 0.3059382438659668], dtype='float32').reshape([12]),
            paddle.to_tensor([0.17788758873939514, 0.49888965487480164, 0.3979722559452057, 0.08577946573495865, 0.3620993196964264, 0.4587302505970001, 0.10063417255878448, 0.1550135463476181, 0.3244674503803253, 0.39513784646987915, 0.4447031319141388, 0.0873313620686531], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3412636f9cc8d5a6fe3f916a7682ef40(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_97d73871b4ab954213bb307c0715d490(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_99d2349d5a3ed70046a646950f560aed(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 320, 320], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4459611475467682, 0.1711605340242386, 0.47586342692375183, 0.24157895147800446, 0.48486775159835815, 0.042203642427921295, 0.3338377773761749, 0.25314661860466003, 0.12339060753583908, 0.03100219927728176, 0.29716283082962036, 0.441384494304657, 0.43609365820884705, 0.19945695996284485, 0.4665638506412506, 0.2525874376296997], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3665907680988312, 0.16327707469463348, 0.1875540167093277, 0.12818583846092224, 0.00519880186766386, 0.4922688901424408, 0.45126619935035706, 0.1676250547170639, 0.034755200147628784, 0.07888855040073395, 0.34221431612968445, 0.37483206391334534, 0.48362019658088684, 0.12953269481658936, 0.39460545778274536, 0.3624388575553894], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4566061496734619, 0.1868327409029007, 0.2312353402376175, 0.3955964148044586, 0.2965630292892456, 0.06556756794452667, 0.16231554746627808, 0.48123615980148315, 0.43984872102737427, 0.33848559856414795, 0.06760238111019135, 0.05130967125296593, 0.039596833288669586, 0.06086201220750809, 0.383510559797287, 0.2595975697040558], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2302812933921814, 0.2882026433944702, 0.079243965446949, 0.41670548915863037, 0.3086181879043579, 0.010777948424220085, 0.40279561281204224, 0.42990803718566895, 0.3999595046043396, 0.14243155717849731, 0.16809195280075073, 0.3845745027065277, 0.08246614038944244, 0.2154066413640976, 0.25662684440612793, 0.0727994292974472], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_161f00f11c95f43425b39801522b4043(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1317d3ac2114634514d00dbd072f500f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_161f00f11c95f43425b39801522b4043
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bf5224178617ed7ab8d6266173f44361(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.13107089698314667, 0.44250309467315674, 0.17749358713626862, 0.20103999972343445, 0.30100196599960327, 0.33981677889823914, 0.4812527596950531, 0.31450459361076355, 0.09286084771156311, 0.1136881485581398, 0.08656454086303711, 0.20195256173610687, 0.07879095524549484, 0.18526485562324524, 0.21344856917858124, 0.3454402685165405, 0.30814456939697266, 0.15504206717014313, 0.22266465425491333, 0.47562655806541443, 0.00933320727199316, 0.49828624725341797, 0.4765017032623291, 0.4280132055282593], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4976855218410492, 0.292985737323761, 0.001019241288304329, 0.2346542924642563, 0.17289431393146515, 0.2740548551082611, 0.09529296308755875, 0.23757319152355194, 0.2703506350517273, 0.28587621450424194, 0.16292481124401093, 0.07336889207363129, 0.19152460992336273, 0.19505661725997925, 0.29964321851730347, 0.18638518452644348, 0.35068798065185547, 0.42861562967300415, 0.21798665821552277, 0.2902575731277466, 0.12458857893943787, 0.10478456318378448, 0.4268544018268585, 0.37033116817474365], dtype='float32').reshape([24]),
            paddle.to_tensor([0.014736190438270569, 0.1964513212442398, 0.08471904695034027, 0.49256443977355957, 0.4568740725517273, 0.3098975419998169, 0.3629843592643738, 0.4459758400917053, 0.21741126477718353, 0.18739376962184906, 0.07087387144565582, 0.45830488204956055, 0.013760975562036037, 0.1840589940547943, 0.03587569668889046, 0.28526344895362854, 0.2919447720050812, 0.2752399146556854, 0.2068580538034439, 0.4892725646495819, 0.4342101514339447, 0.2806907892227173, 0.4081212282180786, 0.035919155925512314], dtype='float32').reshape([24]),
            paddle.to_tensor([0.10885833948850632, 0.12419538199901581, 0.25512099266052246, 0.4306502342224121, 0.14426909387111664, 0.4116104245185852, 0.01315295323729515, 0.2579682469367981, 0.3114013373851776, 0.18922753632068634, 0.31616052985191345, 0.053589195013046265, 0.10393238812685013, 0.29609382152557373, 0.3175983726978302, 0.3818701505661011, 0.13247621059417725, 0.06384545564651489, 0.3561759889125824, 0.03385018929839134, 0.4978446364402771, 0.4233914315700531, 0.2812916040420532, 0.383105993270874], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_44e50ae01fd702b75b132ddef2a875a2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 4, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_93362a546e190aa1e100938efae8f82c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_16cd09bcccd72ca509cc914551ff7a5d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4c26ef55beaa98bc688f31588db5ed41(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_159ed3b05bcbd6df42c65d883f869b3d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 61, 61], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_d176968eb532ab7742141be9f49773ee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9b11c0c70b12bc50a8544940f3f8c0bf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_d176968eb532ab7742141be9f49773ee
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b137920b39c9cf58269bcf809981ff7b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6f3219ed0b0feaaaadf8eefe39abe491(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 350, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_99fa9adf86252800dc8499c6f1d30008(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6d51dcc99b2d7727e1af1de9751ff4a4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_43d333d5344efe91d7e92b7d6f6e91ca(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 24, 24], dtype='float16', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4d819b16ec3bd5912c95daa811c52a7b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ee088f1836588dbc1961926df25c22a5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.1541016697883606, 0.4790455400943756, 0.4149005711078644, 0.34849467873573303, 0.3481000065803528, 0.061124030500650406, 0.1370122730731964, 0.18773910403251648, 0.32782554626464844, 0.14333458244800568, 0.44360214471817017, 0.16059871017932892, 0.4168979525566101, 0.00648929737508297, 0.16128148138523102, 0.365530788898468, 0.15365023910999298, 0.4046735465526581], dtype='float32').reshape([18]),
            paddle.to_tensor([0.30717819929122925, 0.18501505255699158, 0.0209847129881382, 0.4670516848564148, 0.04355526342988014, 0.05098944902420044, 0.4064110815525055, 0.049429237842559814, 0.07544425129890442, 0.23499000072479248, 0.22399860620498657, 0.3185025453567505, 0.342404842376709, 0.22212882339954376, 0.08132591843605042, 0.38069403171539307, 0.11054453253746033, 0.007333752233535051], dtype='float32').reshape([18]),
            paddle.to_tensor([0.03968328237533569, 0.23502761125564575, 0.18331310153007507, 0.18853144347667694, 0.2754374146461487, 0.4068150520324707, 0.4848400354385376, 0.3607823848724365, 0.11114992201328278, 0.16013577580451965, 0.05023505911231041, 0.0701243206858635, 0.396144837141037, 0.21808622777462006, 0.39074039459228516, 0.3386062979698181, 0.04296818748116493, 0.4623790383338928], dtype='float32').reshape([18]),
            paddle.to_tensor([0.005262364167720079, 0.10163582861423492, 0.4014609754085541, 0.2461850643157959, 0.15498115122318268, 0.19580969214439392, 0.42447441816329956, 0.011087032034993172, 0.10666163265705109, 0.45044785737991333, 0.3126083016395569, 0.41485944390296936, 0.4104344844818115, 0.39292111992836, 0.29220643639564514, 0.46849820017814636, 0.0854385495185852, 0.16817180812358856], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2e3580481c7c774a1ee0e4185348cd5d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_033f7fe4c81c57e4dfba6bc5c6e19584(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_05297f10ef37f174e0727280e8ccec9e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.25504177808761597, 0.4110138714313507, 0.45345234870910645, 0.024657143279910088, 0.0676889419555664, 0.4431328773498535, 0.154115229845047, 0.11978098750114441, 0.06542248278856277, 0.4250452220439911, 0.02490038424730301, 0.2750764787197113, 0.3010740876197815, 0.4747260808944702, 0.30731239914894104, 0.3342880308628082, 0.02426080033183098, 0.3218211233615875, 0.231814444065094, 0.045502256602048874, 0.43921810388565063, 0.053095679730176926, 0.4188224971294403, 0.2863624095916748], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2760331630706787, 0.39892736077308655, 0.38209328055381775, 0.008891148492693901, 0.4871581196784973, 0.28857147693634033, 0.4316452443599701, 0.15911222994327545, 0.44503188133239746, 0.44814226031303406, 0.1030961200594902, 0.012877315282821655, 0.4122832417488098, 0.14434514939785004, 0.41327181458473206, 0.2662707567214966, 0.25704601407051086, 0.19303403794765472, 0.018473170697689056, 0.31131628155708313, 0.2223893254995346, 0.09400613605976105, 0.0038409349508583546, 0.18445797264575958], dtype='float32').reshape([24]),
            paddle.to_tensor([0.42259228229522705, 0.15151220560073853, 0.09649064391851425, 0.03645414486527443, 0.2910933196544647, 0.07189039140939713, 0.2239110916852951, 0.16571606695652008, 0.21257856488227844, 0.14031928777694702, 0.23836757242679596, 0.043501004576683044, 0.2644689381122589, 0.059969350695610046, 0.31965965032577515, 0.07003835588693619, 0.06288674473762512, 0.2346203625202179, 0.23388175666332245, 0.1794656217098236, 0.4774172008037567, 0.1976400762796402, 0.11743780970573425, 0.4278930723667145], dtype='float32').reshape([24]),
            paddle.to_tensor([0.417415052652359, 0.03654336929321289, 0.20041070878505707, 0.46831682324409485, 0.44935616850852966, 0.4734780788421631, 0.22609947621822357, 0.305745929479599, 0.1738726943731308, 0.4461401700973511, 0.027080774307250977, 0.33410605788230896, 0.4890035390853882, 0.04907334968447685, 0.39465776085853577, 0.020100915804505348, 0.044876694679260254, 0.234508216381073, 0.4186379015445709, 0.1055406928062439, 0.44320759177207947, 0.014767009764909744, 0.20709529519081116, 0.3789803087711334], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a9a77193086ddcb5bae42f69c80ec01d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 320, 320], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8c686fe49cb33be38414bfcad9691b59(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_186eae946956756b13f3654169c2ef9a
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 32, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.22342564165592194, 0.05266697704792023, 0.278504878282547, 0.39925965666770935, 0.39775991439819336, 0.4968000054359436, 0.1982782483100891, 0.4787493348121643, 0.29986390471458435, 0.09128008037805557, 0.3281673789024353, 0.13294300436973572, 0.2968483865261078, 0.44678428769111633, 0.4486144781112671, 0.3237954378128052], dtype='float32').reshape([16]),
            paddle.to_tensor([0.38421595096588135, 0.3399111330509186, 0.13780632615089417, 0.2758950889110565, 0.32238373160362244, 0.16202545166015625, 0.4509813189506531, 0.37224751710891724, 0.13215316832065582, 0.4075804352760315, 0.2433677762746811, 0.14233995974063873, 0.41063788533210754, 0.2090403288602829, 0.39706695079803467, 0.02703184261918068], dtype='float32').reshape([16]),
            paddle.to_tensor([0.46530136466026306, 0.3883226811885834, 0.263029009103775, 0.33940836787223816, 0.033643390983343124, 0.44849443435668945, 0.4639875292778015, 0.12753061950206757, 0.307628333568573, 0.35012659430503845, 0.37548789381980896, 0.4146667718887329, 0.07124080508947372, 0.3904839754104614, 0.06549330800771713, 0.0005751678836531937], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3487672805786133, 0.23647655546665192, 0.40277108550071716, 0.16488319635391235, 0.1780153214931488, 0.10892912745475769, 0.3278331756591797, 0.4331061542034149, 0.27328819036483765, 0.05930840224027634, 0.4069588780403137, 0.1778518408536911, 0.1546967774629593, 0.45919978618621826, 0.32979169487953186, 0.49894365668296814], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5ea20c99e2438fd2413a229ea3bc6a29(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 240, 240], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_628713dc0fb5c5c6ba2b5081b76f04d3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fa23f344f3e281fe63b6196c78e929b8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_abc15bd94e55776fdbcb840279865c01
    def get_inputs(self):
        return [
            paddle.uniform([16, 384], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8f158a08f239858c1a8343c9d946f0b8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 16, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_646a91e93d31f462dc3531b8cb603859(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_519264f658962ce80f2a6a4ca2f22f04(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_13025b164297d9226f9641b7f0715c76
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 38, 38], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1c37c83010ed5ceb1a4b7d4b24be6f03(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bf0344cffbd006ceb18a53a0279c2614(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2249520868062973, 0.3223801851272583, 0.3780757784843445, 0.06623204797506332, 0.1449548453092575, 0.3563615083694458, 0.3513590395450592, 0.33570143580436707, 0.4516802132129669, 0.1437147855758667, 0.14870524406433105, 0.38108310103416443, 0.14896000921726227, 0.14787785708904266, 0.09282636642456055, 0.3707093894481659, 0.21688522398471832, 0.12211792916059494, 0.354798287153244, 0.06741246581077576], dtype='float32').reshape([20]),
            paddle.to_tensor([0.33898240327835083, 0.05352599173784256, 0.3632284104824066, 0.4208592474460602, 0.3036438226699829, 0.16182082891464233, 0.29818227887153625, 0.04843934252858162, 0.2807370722293854, 0.013487473130226135, 0.42972826957702637, 0.36100268363952637, 0.21476195752620697, 0.24786986410617828, 0.42153653502464294, 0.08224356919527054, 0.24628883600234985, 0.2565893530845642, 0.1867389976978302, 0.04559263214468956], dtype='float32').reshape([20]),
            paddle.to_tensor([0.2251928299665451, 0.07823988795280457, 0.2841613292694092, 0.4019782841205597, 0.049768827855587006, 0.19460847973823547, 0.09367184340953827, 0.4206433892250061, 0.36532357335090637, 0.3845869302749634, 0.24400067329406738, 0.1644522249698639, 0.04242024943232536, 0.1567649394273758, 0.11292173713445663, 0.23268316686153412, 0.43337807059288025, 0.288149893283844, 0.13032855093479156, 0.317025363445282], dtype='float32').reshape([20]),
            paddle.to_tensor([0.07681673020124435, 0.18930862843990326, 0.48043447732925415, 0.09980285912752151, 0.11290542781352997, 0.20153847336769104, 0.4493850767612457, 0.0688384622335434, 0.4946011006832123, 0.38595396280288696, 0.18899551033973694, 0.24393658339977264, 0.30214282870292664, 0.44059672951698303, 0.4374919533729553, 0.2158319503068924, 0.4059707224369049, 0.41654473543167114, 0.3846858739852905, 0.3396300673484802], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_09735178b0e8d286aa993a5b2cb08eb4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 76, 76], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b0e8f6ac7085313fbac280f182741dfe(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 120, 120], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b52a1fb7878453718b5de515af3ccf02(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 88, 25], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0f6356c2aa0a7d919884f1f1dc7d5041(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 60, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ed427fd3c37da16fa810b5c2bacf933c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_161f00f11c95f43425b39801522b4043
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_58558e40447ab43297f2e62859bdfa90(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_186eae946956756b13f3654169c2ef9a
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 32, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.02253717929124832, 0.3739713132381439, 0.031755827367305756, 0.3531123697757721, 0.4061458706855774, 0.3963210880756378, 0.12976346909999847, 0.426050066947937], dtype='float32').reshape([8]),
            paddle.to_tensor([0.3514131009578705, 0.45983269810676575, 0.3717820942401886, 0.36633890867233276, 0.3810426890850067, 0.44499310851097107, 0.151933953166008, 0.4595675766468048], dtype='float32').reshape([8]),
            paddle.to_tensor([0.35245779156684875, 0.12354035675525665, 0.3333646059036255, 0.14740219712257385, 0.15520505607128143, 0.16051778197288513, 0.49947166442871094, 0.33424076437950134], dtype='float32').reshape([8]),
            paddle.to_tensor([0.10581932216882706, 0.2914049029350281, 0.024353336542844772, 0.38898929953575134, 0.11280658096075058, 0.08784684538841248, 0.07338666170835495, 0.43067851662635803], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_97aefbaae11fc0fb8adb2b714d5bf301(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 10, 10], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ca39dcf446089d2eca9de53d3a8f07a0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_53f5204446a6687395b19b72729c2927(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4170352518558502, 0.20911268889904022, 0.02902226336300373, 0.17759378254413605, 0.44460704922676086, 0.12002193182706833, 0.3459607660770416, 0.07760331779718399, 0.12654049694538116, 0.2506880462169647, 0.44976991415023804, 0.13172826170921326, 0.45018619298934937, 0.31466639041900635, 0.34217631816864014, 0.0728529840707779, 0.21092385053634644, 0.34942787885665894], dtype='float32').reshape([18]),
            paddle.to_tensor([0.49842244386672974, 0.04175245389342308, 0.30872511863708496, 0.16252084076404572, 0.44747763872146606, 0.2178022265434265, 0.37065431475639343, 0.14823758602142334, 0.3483079671859741, 0.119754858314991, 0.05874338746070862, 0.3173305094242096, 0.13067270815372467, 0.29696235060691833, 0.37064558267593384, 0.4522359371185303, 0.40099698305130005, 0.284452348947525], dtype='float32').reshape([18]),
            paddle.to_tensor([0.41692501306533813, 0.21193639934062958, 0.46519985795021057, 0.11929170042276382, 0.27981579303741455, 0.1800684630870819, 0.16142600774765015, 0.07412372529506683, 0.36914896965026855, 0.4820345640182495, 0.18082274496555328, 0.36679312586784363, 0.2922963798046112, 0.07548550516366959, 0.47544142603874207, 0.3929617702960968, 0.2640610635280609, 0.4392552077770233], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4902115762233734, 0.0001914259628392756, 0.29051652550697327, 0.10946162045001984, 0.1550828069448471, 0.03301144018769264, 0.3166135549545288, 0.4695022404193878, 0.12114184349775314, 0.4449195861816406, 0.32475167512893677, 0.4533005952835083, 0.2778073847293854, 0.4780188202857971, 0.02155192941427231, 0.2517256736755371, 0.2484966367483139, 0.36942651867866516], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_666a20f2fbffa12d300ddfb90eedce99(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 768, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_45b5baa903280473d2c0997e133bfdd7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 244, 244], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.08733619004487991, 0.3239569067955017, 0.24421875178813934, 0.17679788172245026, 0.18739968538284302, 0.19944266974925995, 0.0640265941619873, 0.08962536603212357, 0.08281692117452621, 0.08611846715211868, 0.3474888801574707, 0.3829869031906128, 0.17374703288078308, 0.08018862456083298, 0.2818179428577423, 0.07668008655309677], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4574089050292969, 0.3621880114078522, 0.2493942230939865, 0.14382128417491913, 0.39068707823753357, 0.22403354942798615, 0.161472350358963, 0.013186575844883919, 0.0575237050652504, 0.12165175378322601, 0.08350071310997009, 0.26019087433815, 0.19846391677856445, 0.4851181209087372, 0.49685904383659363, 0.37691017985343933], dtype='float32').reshape([16]),
            paddle.to_tensor([0.1662362813949585, 0.37889915704727173, 0.3551270365715027, 0.36118945479393005, 0.007723164279013872, 0.01969269849359989, 0.12056714296340942, 0.40556982159614563, 0.1776849329471588, 0.039190638810396194, 0.32754087448120117, 0.3116658926010132, 0.2835372984409332, 0.17336417734622955, 0.3440204858779907, 0.44129055738449097], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3926730751991272, 0.1499868780374527, 0.0936325415968895, 0.4526810646057129, 0.30120304226875305, 0.22448593378067017, 0.009621335193514824, 0.347201406955719, 0.34126394987106323, 0.24291634559631348, 0.015776561573147774, 0.10869932174682617, 0.21911205351352692, 0.12679679691791534, 0.10191476345062256, 0.4033397138118744], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9a704e2b86a63a59464bce6f56ef3428(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4413067102432251, 0.2964634895324707, 0.09239115566015244, 0.4329897463321686, 0.2480694055557251, 0.1821596622467041, 0.060796309262514114, 0.0901530310511589, 0.3954739272594452, 0.486925333738327, 0.1687360256910324, 0.18538744747638702, 0.23507677018642426, 0.042272746562957764, 0.40779122710227966, 0.15392416715621948], dtype='float32').reshape([16]),
            paddle.to_tensor([0.49471530318260193, 0.32350778579711914, 0.07322490960359573, 0.04637699946761131, 0.3325192630290985, 0.47422873973846436, 0.02921057678759098, 0.04252775385975838, 0.38074013590812683, 0.22045928239822388, 0.3574044406414032, 0.09325618296861649, 0.21648111939430237, 0.35297447443008423, 0.1277683526277542, 0.29021862149238586], dtype='float32').reshape([16]),
            paddle.to_tensor([0.31732797622680664, 0.10665149241685867, 0.33618223667144775, 0.08699721843004227, 0.21425583958625793, 0.2379080206155777, 0.3739159405231476, 0.42795854806900024, 0.13665831089019775, 0.014994733035564423, 0.3937638998031616, 0.46126604080200195, 0.3652370870113373, 0.12535715103149414, 0.30347180366516113, 0.3698582947254181], dtype='float32').reshape([16]),
            paddle.to_tensor([0.25242123007774353, 0.4622608721256256, 0.4188007414340973, 0.3680109679698944, 0.44939661026000977, 0.38554495573043823, 0.21508018672466278, 0.25609859824180603, 0.188524067401886, 0.026090869680047035, 0.057197801768779755, 0.20827805995941162, 0.08954617381095886, 0.029858270660042763, 0.013937447220087051, 0.4635967016220093], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_914a0760ede547292630f80d7a391477(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 768, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2c557423c93b474713d30c4f8401e29c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 61, 61], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6ab6e09c4f5c3d10bd070f07a574fa73(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 64, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d4d2195e0a839816a018fdc2c37338fe(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 30, 30], dtype='float16', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_105a7d1223acff8f153661fea741349d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3043479323387146, 0.3928532600402832, 0.3567635118961334, 0.45525744557380676, 0.49138563871383667, 0.14831532537937164, 0.24716831743717194, 0.41701167821884155], dtype='float32').reshape([8]),
            paddle.to_tensor([0.35770732164382935, 0.49454253911972046, 0.1267930120229721, 0.4939921796321869, 0.06929615139961243, 0.1305542290210724, 0.2980739176273346, 0.321111798286438], dtype='float32').reshape([8]),
            paddle.to_tensor([0.1606498509645462, 0.3732074499130249, 0.26139500737190247, 0.12849724292755127, 0.4800335764884949, 0.3129163980484009, 0.38482701778411865, 0.140788733959198], dtype='float32').reshape([8]),
            paddle.to_tensor([0.3352012634277344, 0.09756212681531906, 0.15806928277015686, 0.18681974709033966, 0.19270744919776917, 0.4629390835762024, 0.17110322415828705, 0.07640755921602249], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cca0f41d2ac375a143eaea9dbe253b35(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.1517322063446045, 0.2323104590177536, 0.2555454969406128, 0.18645763397216797, 0.40504372119903564, 0.39670419692993164, 0.21677473187446594, 0.017404954880475998, 0.04436180368065834, 0.08446741849184036, 0.04438728094100952, 0.3645259141921997, 0.24616101384162903, 0.22761447727680206, 0.14629751443862915, 0.49246329069137573, 0.24087314307689667, 0.38257381319999695], dtype='float32').reshape([18]),
            paddle.to_tensor([0.26782917976379395, 0.007974301464855671, 0.3863489031791687, 0.12586557865142822, 0.45677393674850464, 0.18339650332927704, 0.4711231291294098, 0.33272692561149597, 0.05595513805747032, 0.013525082729756832, 0.11837711930274963, 0.00782803725451231, 0.018536560237407684, 0.2859826683998108, 0.4289437532424927, 0.06279604882001877, 0.45432940125465393, 0.18152457475662231], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4609600305557251, 0.4355179965496063, 0.4192005693912506, 0.2569550573825836, 0.4151737689971924, 0.1579117774963379, 0.14129644632339478, 0.1040615364909172, 0.16338005661964417, 0.43984243273735046, 0.08619214594364166, 0.011551782488822937, 0.40613311529159546, 0.3313307762145996, 0.4064880311489105, 0.4255768954753876, 0.4332973062992096, 0.4569582939147949], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3351415991783142, 0.18420632183551788, 0.3167549967765808, 0.24876391887664795, 0.10659585148096085, 0.06806744635105133, 0.0155783137306571, 0.1543026864528656, 0.4079337418079376, 0.27246639132499695, 0.42134755849838257, 0.29207780957221985, 0.32493770122528076, 0.30576521158218384, 0.0711170881986618, 0.32375016808509827, 0.33384475111961365, 0.2621229290962219], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a58d457976e102205cb2b1d391e5332f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 8, 25], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a2c6502f36c52ce1315374fca2b0d6ec(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 4, 3], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4daa9bf4c1a02646f40a7d1dbd135093(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 120, 120], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aab54ca6c93cc0e70d1f70f727c7c437(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 16, 12], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_78b7240e85893e763742115719bd3380(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 32, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a5337f5c8bd3b624d938f136bae6eea4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0e51392ff5d1fb7156fe7e1460e19225(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5812f64d755961232207cd617cbc56fb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_121320f8f53e53fcc8395cb2b91efefd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_39e346ae1d082cad3d3ecb525ac707ec(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1ea9354753469fe4a26dfa6ac5fd109c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 8, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8b8b7160a6009f79905d0a0d0eb3a7f8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ffd3fc27b945a2a7d800ac671d1ab81b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_71adf679806cab2a22b147a9b47d0cbc
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d51d3b7cc365db7da5b7e2beb6550a47(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cd13c7aaec11b092d6dc55e5c4b8b60a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 200, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7db49f00ba3b2f9328a922c0193a8e67(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 256, 128], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.1648780107498169, 0.45711252093315125, 0.37652772665023804, 0.140256866812706, 0.005488045979291201, 0.4646975100040436, 0.20623373985290527, 0.23649227619171143, 0.22603808343410492, 0.2863084077835083, 0.21018484234809875, 0.2167963683605194, 0.33334892988204956, 0.3124854266643524, 0.47620874643325806, 0.42950141429901123, 0.36412933468818665, 0.1842498928308487, 0.39838168025016785, 0.39257752895355225, 0.1833525002002716, 0.42434972524642944, 0.08640415966510773, 0.0930996984243393], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4044874906539917, 0.11816610395908356, 0.47529956698417664, 0.3666866421699524, 0.40360698103904724, 0.09798771142959595, 0.1579853743314743, 0.23345273733139038, 0.38471630215644836, 0.3239022195339203, 0.11722223460674286, 0.072657510638237, 0.49100619554519653, 0.08665963262319565, 0.19499005377292633, 0.14106331765651703, 0.12237472087144852, 0.03107292205095291, 0.3524426519870758, 0.21170637011528015, 0.2064504474401474, 0.0127237718552351, 0.3579181730747223, 0.10827112197875977], dtype='float32').reshape([24]),
            paddle.to_tensor([0.1466229408979416, 0.16073329746723175, 0.4658757746219635, 0.1856369525194168, 0.21954458951950073, 0.016738710924983025, 0.29109659790992737, 0.273040235042572, 0.34475451707839966, 0.3900500237941742, 0.4113042652606964, 0.026066163554787636, 0.3414578139781952, 0.2510187327861786, 0.3661590814590454, 0.16395749151706696, 0.016947539523243904, 0.08316794037818909, 0.15525168180465698, 0.1317850947380066, 0.1962071657180786, 0.2732810974121094, 0.3609008491039276, 0.005431986879557371], dtype='float32').reshape([24]),
            paddle.to_tensor([0.47371241450309753, 0.33813777565956116, 0.4415341019630432, 0.2731763422489166, 0.3216632902622223, 0.08532316237688065, 0.3065106272697449, 0.3260706067085266, 0.03024519793689251, 0.25111064314842224, 0.11845994740724564, 0.4591449797153473, 0.2631469666957855, 0.33004945516586304, 0.4988301694393158, 0.033787231892347336, 0.4646962881088257, 0.30022546648979187, 0.18282125890254974, 0.4252871572971344, 0.25155186653137207, 0.09907233715057373, 0.24999317526817322, 0.4288601279258728], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e41c5b40bd8da1ea137b416332323a8d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_0336412036bb12c89542873a9d0cc312
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3718cc91b6314bdd8b0195f9b098aba0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4756677746772766, 0.1724395453929901, 0.21287213265895844, 0.0015595299191772938, 0.1665508896112442, 0.030079025775194168, 0.10765443742275238, 0.10566727072000504, 0.01832525245845318, 0.3038787543773651, 0.3139740228652954, 0.19398681819438934, 0.18762460350990295, 0.4326283633708954, 0.005280999466776848, 0.37367841601371765, 0.40469229221343994, 0.006551546044647694, 0.22379843890666962, 0.3610111176967621, 0.44300055503845215, 0.19509845972061157, 0.1659812480211258, 0.38772496581077576], dtype='float32').reshape([24]),
            paddle.to_tensor([0.11554297804832458, 0.4285138249397278, 0.06574180722236633, 0.06267087906599045, 0.1830563098192215, 0.4208203852176666, 0.3731807768344879, 0.4512549638748169, 0.3121984899044037, 0.34690070152282715, 0.10067764669656754, 0.01028318889439106, 0.20288307964801788, 0.013114847242832184, 0.15791238844394684, 0.2707860767841339, 0.2604575455188751, 0.1735227108001709, 0.2545278072357178, 0.24929343163967133, 0.3120848536491394, 0.31258124113082886, 0.28755730390548706, 0.048908889293670654], dtype='float32').reshape([24]),
            paddle.to_tensor([0.14791423082351685, 0.042662255465984344, 0.3695630431175232, 0.2380928248167038, 0.20105811953544617, 0.24604929983615875, 0.2639319598674774, 0.4292905330657959, 0.3045284152030945, 0.044937700033187866, 0.15571551024913788, 0.4856624901294708, 0.3816254436969757, 0.4205263555049896, 0.34057894349098206, 0.17618785798549652, 0.10466185957193375, 0.16226348280906677, 0.24677987396717072, 0.11392612010240555, 0.3701188266277313, 0.22407805919647217, 0.028212739154696465, 0.4889870285987854], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3651377260684967, 0.13897264003753662, 0.4505676031112671, 0.036655329167842865, 0.4723168611526489, 0.43671348690986633, 0.26800376176834106, 0.3726195991039276, 0.03536864370107651, 0.19324198365211487, 0.3844032287597656, 0.36382368206977844, 0.0655578225851059, 0.3271372318267822, 0.2581240236759186, 0.026433659717440605, 0.23223251104354858, 0.06833211332559586, 0.4932920038700104, 0.0495666079223156, 0.4180496633052826, 0.28349465131759644, 0.020205385982990265, 0.4018867313861847], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cac48f1f948feea38c6ccba3567971b2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.09546172618865967, 0.27340978384017944, 0.1164926066994667, 0.2989979684352875, 0.3562309741973877, 0.3390488028526306, 0.45927342772483826, 0.3646329939365387, 0.449148952960968, 0.27166423201560974, 0.1064569428563118, 0.07575420290231705, 0.1050247997045517, 0.23991073668003082, 0.31335175037384033, 0.03952399268746376, 0.32422950863838196, 0.4635382294654846, 0.1600172072649002, 0.13150054216384888, 0.24540261924266815, 0.34697550535202026, 0.3750007152557373, 0.4931861162185669, 0.3675272762775421, 0.3793931007385254, 0.18296554684638977, 0.09861957281827927], dtype='float32').reshape([28]),
            paddle.to_tensor([0.13973604142665863, 0.15072055160999298, 0.26338279247283936, 0.16424576938152313, 0.3818857967853546, 0.3621755540370941, 0.4488184154033661, 0.35995757579803467, 0.27496305108070374, 0.1720600575208664, 0.4007311165332794, 0.43745505809783936, 0.33828893303871155, 0.1748659461736679, 0.11959042400121689, 0.4445609748363495, 0.3336970806121826, 0.33158165216445923, 0.21361598372459412, 0.2613237500190735, 0.1229340210556984, 0.054848216474056244, 0.450783371925354, 0.2877049744129181, 0.026992928236722946, 0.3285404145717621, 0.48892533779144287, 0.40818971395492554], dtype='float32').reshape([28]),
            paddle.to_tensor([0.24008621275424957, 0.2652614712715149, 0.01144121028482914, 0.1263219714164734, 0.46972841024398804, 0.2859640419483185, 0.08257101476192474, 0.4176137447357178, 0.14737559854984283, 0.3827206492424011, 0.42261844873428345, 0.4469578266143799, 0.3286185562610626, 0.47372376918792725, 0.09101946651935577, 0.44145432114601135, 0.4267478287220001, 0.4523506760597229, 0.36065244674682617, 0.08370739221572876, 0.13178583979606628, 0.2942020297050476, 0.2677766680717468, 0.34471791982650757, 0.2845214307308197, 0.23885005712509155, 0.3886480927467346, 0.13954199850559235], dtype='float32').reshape([28]),
            paddle.to_tensor([0.04551688954234123, 0.08615610748529434, 0.154649555683136, 0.23798438906669617, 0.29322466254234314, 0.13151094317436218, 0.1522844135761261, 0.27358919382095337, 0.49425503611564636, 0.4561573266983032, 0.02643289975821972, 0.49605897068977356, 0.37068310379981995, 0.4668460190296173, 0.375527560710907, 0.3987565338611603, 0.2354312539100647, 0.26413610577583313, 0.4499346911907196, 0.34490033984184265, 0.46715474128723145, 0.1067756935954094, 0.265696257352829, 0.007861162535846233, 0.4973199963569641, 0.028425583615899086, 0.1896405667066574, 0.3568684458732605], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_45d0fde5fbe1e6f50270baa851c1384f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 24, 24], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_38b7992e3b3f28f921c59b01ff82c9ca(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 8, 25], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e45050131dfa4d33b7fd838c6bf001a4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b52911afd35b8fa095e93b415e39f604(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 320, 320], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4494883120059967, 0.3284679353237152, 0.49101313948631287, 0.12162681668996811, 0.18401949107646942, 0.12022657692432404, 0.3289133310317993, 0.021484116092324257, 0.0018558583687990904, 0.334695428609848, 0.24804461002349854, 0.20650477707386017, 0.24231305718421936, 0.21883615851402283, 0.1047176718711853, 0.10588929057121277, 0.3933853507041931, 0.3357760012149811, 0.3495701253414154, 0.30663371086120605, 0.2600036561489105, 0.14095266163349152, 0.015982504934072495, 0.445645809173584], dtype='float32').reshape([24]),
            paddle.to_tensor([0.08801011741161346, 0.301108717918396, 0.2083333432674408, 0.15969301760196686, 0.2235211879014969, 0.4547964036464691, 0.33886125683784485, 0.10391698032617569, 0.4621741771697998, 0.036642711609601974, 0.26132676005363464, 0.30588722229003906, 0.4981608986854553, 0.3931330442428589, 0.24716265499591827, 0.0003924893098883331, 0.10177023708820343, 0.3481284976005554, 0.09165254980325699, 0.42106905579566956, 0.17796766757965088, 0.11348742991685867, 0.005859571509063244, 0.16435113549232483], dtype='float32').reshape([24]),
            paddle.to_tensor([0.28832170367240906, 0.3869573175907135, 0.3595585525035858, 0.09604544937610626, 0.23128676414489746, 0.21928472816944122, 0.1362098753452301, 0.2673179507255554, 0.3993685245513916, 0.48752182722091675, 0.3593544065952301, 0.092015340924263, 0.21035747230052948, 0.3577609956264496, 0.05182996019721031, 0.19554144144058228, 0.23344233632087708, 0.31739988923072815, 0.49909862875938416, 0.051473017781972885, 0.11097104102373123, 0.040374018251895905, 0.11879626661539078, 0.2683113217353821], dtype='float32').reshape([24]),
            paddle.to_tensor([0.19608469307422638, 0.2890477180480957, 0.3096833825111389, 0.4521532654762268, 0.16674353182315826, 0.3895159661769867, 0.15631084144115448, 0.12304722517728806, 0.13078084588050842, 0.31190025806427, 0.3163364827632904, 0.015754852443933487, 0.0867621898651123, 0.015529314987361431, 0.011636392213404179, 0.05077445134520531, 0.09979411959648132, 0.10862208902835846, 0.35785847902297974, 0.24926075339317322, 0.04928209260106087, 0.271770715713501, 0.20867137610912323, 0.3285549283027649], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c7e5ae90096e77229b4e801e3814034e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_a2fd76ef6d10c288900c0fe38e7a817a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ef6eb3147115c3d82527885651ed721e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_a2fd76ef6d10c288900c0fe38e7a817a
    def get_inputs(self):
        return [
            paddle.uniform([16, 768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bc3b5709dc17d9e999a33e9201c4f91d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5397a2812ea465f4fde3f4d83cd6f684(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 64, 128], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.13344652950763702, 0.4252508282661438, 0.4633430540561676, 0.1768721342086792, 0.4861290156841278, 0.12618517875671387, 0.023930422961711884, 0.21509182453155518, 0.040660422295331955, 0.11809200793504715, 0.25400498509407043, 0.04527797922492027, 0.3596619963645935, 0.17287211120128632, 0.23839640617370605, 0.28619927167892456, 0.12055923789739609, 0.2840757369995117], dtype='float32').reshape([18]),
            paddle.to_tensor([0.12736696004867554, 0.261738657951355, 0.10012220591306686, 0.06500867009162903, 0.4542534053325653, 0.22171589732170105, 0.4584847688674927, 0.46255582571029663, 0.2041403204202652, 0.12413055449724197, 0.3245523273944855, 0.42143791913986206, 0.17090532183647156, 0.12015040963888168, 0.3871471583843231, 0.3378082513809204, 0.13869041204452515, 0.28852578997612], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3692542612552643, 0.4003998637199402, 0.3057512640953064, 0.17296536266803741, 0.24721086025238037, 0.26753899455070496, 0.47566768527030945, 0.3273662328720093, 0.17319929599761963, 0.198903426527977, 0.3925250470638275, 0.4814966320991516, 0.06384992599487305, 0.07528822124004364, 0.4158863425254822, 0.11741551011800766, 0.1587211638689041, 0.006407173816114664], dtype='float32').reshape([18]),
            paddle.to_tensor([0.39818549156188965, 0.3999907970428467, 0.3099365830421448, 0.0921553373336792, 0.23551863431930542, 0.4851545989513397, 0.3905855417251587, 0.17005567252635956, 0.3139403462409973, 0.42514026165008545, 0.33631816506385803, 0.3671928942203522, 0.037565238773822784, 0.03187474235892296, 0.027012459933757782, 0.4164927899837494, 0.10851998627185822, 0.31436100602149963], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fa1bfc8f3bee0a94d2ee257d11c144c2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 10, 10], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_70baa2cd451328bc13ab50bb40a85874(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6c1ca6c8fce8f4328af9ceff70df9de0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.17423374950885773, 0.21329329907894135, 0.08648812770843506, 0.39275509119033813, 0.16427473723888397, 0.12087471038103104, 0.12134876102209091, 0.4046820104122162, 0.0923936665058136, 0.42713865637779236, 0.29915037751197815, 0.016855334863066673, 0.08808716386556625, 0.1399592012166977, 0.11137884110212326, 0.0034190514124929905, 0.23476016521453857, 0.4728332459926605, 0.3036319315433502, 0.08234090358018875, 0.26239830255508423, 0.09609879553318024, 0.162388876080513, 0.48343273997306824], dtype='float32').reshape([24]),
            paddle.to_tensor([0.30138644576072693, 0.2911202311515808, 0.32369837164878845, 0.27105841040611267, 0.3218200206756592, 0.1756325513124466, 0.031790364533662796, 0.133049875497818, 0.3759283423423767, 0.40247756242752075, 0.3428418040275574, 0.035631146281957626, 0.4099007844924927, 0.42248737812042236, 0.12714321911334991, 0.39881381392478943, 0.32127633690834045, 0.0004646203597076237, 0.2855747938156128, 0.16985727846622467, 0.4682386517524719, 0.23769846558570862, 0.36907926201820374, 0.42395922541618347], dtype='float32').reshape([24]),
            paddle.to_tensor([0.10272200405597687, 0.39354822039604187, 0.2009076029062271, 0.281826913356781, 0.35624465346336365, 0.1065157800912857, 0.47179052233695984, 0.0917140319943428, 0.013331293128430843, 0.4461151361465454, 0.2781255543231964, 0.11440811306238174, 0.38195085525512695, 0.15836842358112335, 0.41007816791534424, 0.4961288869380951, 0.47176364064216614, 0.285934716463089, 0.10232067853212357, 0.39996349811553955, 0.1263623684644699, 0.25272679328918457, 0.22642678022384644, 0.33040258288383484], dtype='float32').reshape([24]),
            paddle.to_tensor([0.19631287455558777, 0.22002600133419037, 0.4592902660369873, 0.44669297337532043, 0.414950430393219, 0.19700703024864197, 0.11498285830020905, 0.4939894676208496, 0.3417675793170929, 0.056319303810596466, 0.07094904780387878, 0.30292564630508423, 0.32601332664489746, 0.2660031318664551, 0.3107839822769165, 0.26821303367614746, 0.48840856552124023, 0.3351230323314667, 0.038302093744277954, 0.17974327504634857, 0.10055708885192871, 0.19172804057598114, 0.24576424062252045, 0.25098708271980286], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_56011244a5350f0214b8ba4b491fa951(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 12, 12], dtype='float16', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_54d6fec0b2b60eced6fe567fce669d1e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5432ea661a40dede47facf0e17cdaeb3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_13025b164297d9226f9641b7f0715c76
    def get_inputs(self):
        return [
            paddle.uniform([1, 112, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_238ad0c317f1127b5aaa0badee7130d7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 1344, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1344], dtype='float32', min=0, max=0.5),
            paddle.uniform([1344], dtype='float32', min=0, max=0.5),
            paddle.uniform([1344], dtype='float32', min=0, max=0.5),
            paddle.uniform([1344], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dee34c47abab97a403b0dbcc75ba9f79(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_186eae946956756b13f3654169c2ef9a
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 4, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7604e4c8a8256eb11e04f237c71894ec(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 35, 35], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3c3f73714eb8c5315e074b1d5a357ee8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 240, 240], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.1340763419866562, 0.34638527035713196, 0.46409618854522705, 0.37523847818374634, 0.2817813754081726, 0.3755131661891937, 0.1019979938864708, 0.4415130913257599], dtype='float32').reshape([8]),
            paddle.to_tensor([0.1976177841424942, 0.2487218976020813, 0.42398494482040405, 0.17462743818759918, 0.3651624321937561, 0.3529103994369507, 0.2313307821750641, 0.08947482705116272], dtype='float32').reshape([8]),
            paddle.to_tensor([0.3635404407978058, 0.4749946594238281, 0.2909347414970398, 0.4460679292678833, 0.27201300859451294, 0.3150060474872589, 0.11461541801691055, 0.3350023031234741], dtype='float32').reshape([8]),
            paddle.to_tensor([0.26951053738594055, 0.2025785595178604, 0.4549993574619293, 0.008032753132283688, 0.1397523730993271, 0.21552857756614685, 0.44805169105529785, 0.08512768894433975], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_780a6029d995a09ef3a25964b1735a01(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_36e1608a680cef49139e81494925dc59(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.279964417219162, 0.02068757452070713, 0.009723187424242496, 0.23430757224559784, 0.3990369141101837, 0.36066827178001404, 0.17275206744670868, 0.2995491325855255, 0.30414584279060364, 0.24630510807037354, 0.05062836408615112, 0.062156662344932556, 0.09247970581054688, 0.30511361360549927, 0.2911446988582611, 0.1393979787826538, 0.03657160699367523, 0.02302224189043045], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2590388059616089, 0.18393874168395996, 0.2464103102684021, 0.4507589340209961, 0.16819050908088684, 0.2298605740070343, 0.2806991636753082, 0.15458530187606812, 0.19961680471897125, 0.3028940260410309, 0.2947876453399658, 0.12406392395496368, 0.19382192194461823, 0.3734801113605499, 0.10414119809865952, 0.3123321831226349, 0.24114255607128143, 0.18794167041778564], dtype='float32').reshape([18]),
            paddle.to_tensor([0.43746045231819153, 0.16610047221183777, 0.27840179204940796, 0.43039563298225403, 0.08951988816261292, 0.3428032100200653, 0.21019601821899414, 0.13074852526187897, 0.05632524937391281, 0.3011937141418457, 0.06353545188903809, 0.32398486137390137, 0.28029051423072815, 0.09636934101581573, 0.17866580188274384, 0.2463359832763672, 0.07700285315513611, 0.4833797514438629], dtype='float32').reshape([18]),
            paddle.to_tensor([0.09447168558835983, 0.19978152215480804, 0.102137990295887, 0.22096003592014313, 0.14846640825271606, 0.3368673026561737, 0.44029945135116577, 0.3917536735534668, 0.0228089839220047, 0.07191513478755951, 0.18027165532112122, 0.05687350779771805, 0.16145610809326172, 0.022835206240415573, 0.4166673421859741, 0.076496422290802, 0.18433591723442078, 0.44816258549690247], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_562797741695237dd89239aca7efe6d6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_186eae946956756b13f3654169c2ef9a
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 4, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_67003c3d6a013de935fa40b3dc6b6088(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2e3e062fdecf450fbd8462a366531ff5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f5a647380a2be17d47e98cd91f21dc46(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.06777318567037582, 0.35385167598724365, 0.4214196801185608, 0.2322915494441986, 0.14812104403972626, 0.38593924045562744, 0.4687599837779999, 0.462760329246521, 0.06698489189147949, 0.3058202862739563, 0.1258922964334488, 0.3127962052822113, 0.4213080406188965, 0.3747255802154541, 0.07690471410751343, 0.03453690558671951, 0.48869258165359497, 0.36665642261505127, 0.15000905096530914, 0.24084481596946716, 0.39404210448265076, 0.012194957584142685, 0.335986852645874, 0.4999820590019226], dtype='float32').reshape([24]),
            paddle.to_tensor([0.23425419628620148, 0.3896716237068176, 0.27453288435935974, 0.3893747925758362, 0.06794437766075134, 0.49197709560394287, 0.10801076889038086, 0.3734659254550934, 0.3615691065788269, 0.09098115563392639, 0.44024455547332764, 0.22674383223056793, 0.30123409628868103, 0.4523032009601593, 0.2406296283006668, 0.3686322867870331, 0.37846824526786804, 0.255069375038147, 0.42230895161628723, 0.26674994826316833, 0.28042152523994446, 0.2859461307525635, 0.12605415284633636, 0.012322744354605675], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4710479974746704, 0.35929280519485474, 0.14114433526992798, 0.2655605971813202, 0.05116220563650131, 0.2882092595100403, 0.41684776544570923, 0.11793183535337448, 0.37191471457481384, 0.09300367534160614, 0.3255148231983185, 0.1712949424982071, 0.22012488543987274, 0.2196803241968155, 0.3470928370952606, 0.27325689792633057, 0.07785986363887787, 0.1078326404094696, 0.25684529542922974, 0.3161504864692688, 0.07910396158695221, 0.33423367142677307, 0.18314293026924133, 0.2227533906698227], dtype='float32').reshape([24]),
            paddle.to_tensor([0.1902388036251068, 0.15821020305156708, 0.06031027063727379, 0.10922625660896301, 0.40755337476730347, 0.4683202803134918, 0.1817658543586731, 0.4477655589580536, 0.30986762046813965, 0.10044030845165253, 0.26298287510871887, 0.0538841113448143, 0.3142419457435608, 0.003394641913473606, 0.0792502760887146, 0.3816709816455841, 0.3216284215450287, 0.46349960565567017, 0.4620932638645172, 0.18875622749328613, 0.38001275062561035, 0.025918928906321526, 0.22643592953681946, 0.20476515591144562], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b64a27943843b260242f250bae0fde73(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 8, 6], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_534e67b0372d1bd7920f7d2728f484f0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0cd6b8cf01c4427e5c393735b55745c8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_abc15bd94e55776fdbcb840279865c01
    def get_inputs(self):
        return [
            paddle.uniform([196, 640], dtype='float16', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dbf40bbba7e79d9803c47c5c28bfb435(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6afd411effdb9b271314ab7e526c4051(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 4, 3], dtype='float16', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2ea44c6037dfeb64051cb2c2b117a951(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 144, 16, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a5c43bf8baee371afad0959b8caba92b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_38552873dff7f7ddd3dc8d01de3f4014(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_72d2520099c96e13bebb347a99a6dcf6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_197c1bf1ae399e1859fb303b29fef335(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1a50897ff2769f5b6c9675f29f1681ce(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 61, 61], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.1645323932170868, 0.2966400682926178, 0.483605295419693, 0.24932625889778137, 0.32578325271606445, 0.02636580727994442, 0.3899519145488739, 0.3458669185638428, 0.3136230707168579, 0.4005230963230133, 0.13729123771190643, 0.13521656394004822, 0.4348995089530945, 0.41992196440696716, 0.09335463494062424, 0.29431989789009094, 0.06995999068021774, 0.41973716020584106, 0.27156564593315125, 0.11588042229413986, 0.4012902081012726, 0.1843518167734146, 0.09246212244033813, 0.19817198812961578], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4023168385028839, 0.49433207511901855, 0.339283287525177, 0.3516569435596466, 0.32701537013053894, 0.07130955904722214, 0.13785654306411743, 0.36780479550361633, 0.11576791852712631, 0.19815175235271454, 0.2191346436738968, 0.12715795636177063, 0.37721267342567444, 0.37430164217948914, 0.13490669429302216, 0.13695649802684784, 0.4047693610191345, 0.3777874708175659, 0.3617033064365387, 0.09435001760721207, 0.21545006334781647, 0.0925493985414505, 0.15181532502174377, 0.31100308895111084], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2725006639957428, 0.326723575592041, 0.2291249930858612, 0.2969558537006378, 0.13327087461948395, 0.213419571518898, 0.060402512550354004, 0.3268815577030182, 0.47146329283714294, 0.26591312885284424, 0.21019946038722992, 0.08918030560016632, 0.008843228220939636, 0.4045948386192322, 0.214107483625412, 0.3118003010749817, 0.37347543239593506, 0.15406614542007446, 0.3094249367713928, 0.3644295036792755, 0.2865891754627228, 0.48096099495887756, 0.3396012485027313, 0.2635771334171295], dtype='float32').reshape([24]),
            paddle.to_tensor([0.15637558698654175, 0.04686834663152695, 0.02515534497797489, 0.36527150869369507, 0.1896996647119522, 0.1834624856710434, 0.24988439679145813, 0.4738658368587494, 0.10484842211008072, 0.06902021169662476, 0.28561165928840637, 0.4309330880641937, 0.3778615891933441, 0.26451992988586426, 0.18835166096687317, 0.4461297392845154, 0.2480713129043579, 0.1570938527584076, 0.3299860954284668, 0.018361125141382217, 0.00349561613984406, 0.09587951004505157, 0.2483622282743454, 0.2692435383796692], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_17609b187e1d0c2306a6c02114ecf755(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.15117838978767395, 0.4143960773944855, 0.10966368019580841, 0.3051413595676422, 0.09542915970087051, 0.2533899247646332, 0.45807796716690063, 0.0705857053399086, 0.3750361502170563, 0.24475634098052979, 0.010809985920786858, 0.2076047956943512, 0.3373273015022278, 0.05536814033985138, 0.4344834089279175, 0.24417631328105927, 0.019749650731682777, 0.46081089973449707, 0.3993205428123474, 0.2570950984954834], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3225487172603607, 0.15043897926807404, 0.4377061426639557, 0.29449695348739624, 0.3864825665950775, 0.49045050144195557, 0.18962925672531128, 0.3007242679595947, 0.07158327847719193, 0.01533450186252594, 0.3249281048774719, 0.04392660781741142, 0.3011418879032135, 0.4593496024608612, 0.15568506717681885, 0.33860230445861816, 0.14059270918369293, 0.1578928530216217, 0.28593477606773376, 0.3807156980037689], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4550013244152069, 0.44278696179389954, 0.07680562883615494, 0.3717986047267914, 0.36682093143463135, 0.3896644115447998, 0.10812986642122269, 0.460114061832428, 0.4823351502418518, 0.016038335859775543, 0.47736719250679016, 0.12613661587238312, 0.272971510887146, 0.31866785883903503, 0.05827956646680832, 0.4866660237312317, 0.07118764519691467, 0.34003815054893494, 0.1424296349287033, 0.02033633179962635], dtype='float32').reshape([20]),
            paddle.to_tensor([0.391781210899353, 0.24030788242816925, 0.18040348589420319, 0.1762012094259262, 0.1873386651277542, 0.24279680848121643, 0.25528663396835327, 0.19400286674499512, 0.4283149838447571, 0.20310157537460327, 0.042219627648591995, 0.14299137890338898, 0.22047463059425354, 0.20527859032154083, 0.072211854159832, 0.06781157106161118, 0.14314110577106476, 0.21527111530303955, 0.3728708326816559, 0.10113377869129181], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_db96d0d9ad8d4a0a379261e5a15366d1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 30, 30], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_319c5e6cde4eea0125627c491c9fbd0e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 6, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_44d6217db395e8f55b30479706ea52da(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_a2fd76ef6d10c288900c0fe38e7a817a
    def get_inputs(self):
        return [
            paddle.uniform([16, 512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bedc092adc80cf51378158a39f4c45fe(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 52, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ad3b2a059b4717fe6d3f68ce20515b16(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.1678209900856018, 0.43497177958488464, 0.3305284380912781, 0.46146059036254883, 0.06058613583445549, 0.3272162079811096, 0.062053799629211426, 0.20249436795711517, 0.26465943455696106, 0.1559143364429474, 0.1450517773628235, 0.07769891619682312, 0.0009693262982182205, 0.49362805485725403, 0.2515558898448944, 0.3526344895362854, 0.4500240683555603, 0.4283817708492279, 0.03897014632821083, 0.29369089007377625, 0.32621651887893677, 0.47513118386268616, 0.15791058540344238, 0.3083517253398895, 0.4452316164970398, 0.020094744861125946, 0.34923887252807617, 0.42952027916908264, 0.0013149904552847147, 0.24316807091236115], dtype='float32').reshape([30]),
            paddle.to_tensor([0.49483048915863037, 0.09674187749624252, 0.0837256982922554, 0.039702072739601135, 0.4866773188114166, 0.16425347328186035, 0.3997992277145386, 0.32201239466667175, 0.04581788182258606, 0.3594111502170563, 0.20116205513477325, 0.4379723370075226, 0.22999557852745056, 0.2573215067386627, 0.23817074298858643, 0.3076179623603821, 0.13773790001869202, 0.40267759561538696, 0.10252857208251953, 0.020426949486136436, 0.23836113512516022, 0.04860755428671837, 0.31833499670028687, 0.08254729956388474, 0.009519405663013458, 0.11432306468486786, 0.3399020731449127, 0.27094995975494385, 0.49009937047958374, 0.10703928768634796], dtype='float32').reshape([30]),
            paddle.to_tensor([0.35821717977523804, 0.09807076305150986, 0.16364286839962006, 0.304315447807312, 0.2710953950881958, 0.3554212152957916, 0.4506894052028656, 0.3391837775707245, 0.293569952249527, 0.18139438331127167, 0.34530091285705566, 0.13636384904384613, 0.412343293428421, 0.19748884439468384, 0.07091943174600601, 0.035416096448898315, 0.4319179952144623, 0.20723707973957062, 0.49443233013153076, 0.32738545536994934, 0.4364430606365204, 0.259503036737442, 0.31452980637550354, 0.20707301795482635, 0.2126968950033188, 0.1583290994167328, 0.4760943651199341, 0.22113747894763947, 0.497013121843338, 0.46098679304122925], dtype='float32').reshape([30]),
            paddle.to_tensor([0.08964709937572479, 0.025205153971910477, 0.3192145824432373, 0.3321404755115509, 0.2091563642024994, 0.18008175492286682, 0.3345956802368164, 0.21954061090946198, 0.3493695557117462, 0.42882442474365234, 0.027129948139190674, 0.1905977874994278, 0.08512213081121445, 0.22282588481903076, 0.09634789079427719, 0.26320555806159973, 0.34376344084739685, 0.023650089278817177, 0.3723297417163849, 0.11727015674114227, 0.446654349565506, 0.18216679990291595, 0.27366694808006287, 0.20577828586101532, 0.33130601048469543, 0.27900052070617676, 0.09396237134933472, 0.14927467703819275, 0.4636051654815674, 0.4974159002304077], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_71abf36f5310e9f593eda7ef4c6d91a5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2ee31a39a01f51c082ce73b73b905014(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_551b7b803f5be7f435279fa2aa263ba7
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cd2ce51ee70ce8790d8d77c30f3d5c9d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4467075765132904, 0.08187971264123917, 0.04262814298272133, 0.18712620437145233, 0.44091951847076416, 0.3358621299266815, 0.2823732793331146, 0.3030357360839844, 0.4981211721897125, 0.03533525392413139, 0.06885778903961182, 0.0564197339117527], dtype='float32').reshape([12]),
            paddle.to_tensor([0.2837500274181366, 0.23310230672359467, 0.46007412672042847, 0.39144787192344666, 0.005646729376167059, 0.028173040598630905, 0.11922792345285416, 0.10796201974153519, 0.1175256073474884, 0.11532612144947052, 0.11841936409473419, 0.09777802228927612], dtype='float32').reshape([12]),
            paddle.to_tensor([0.48986881971359253, 0.3951551020145416, 0.4720197916030884, 0.05650504305958748, 0.0994977056980133, 0.38293325901031494, 0.15246209502220154, 0.25726088881492615, 0.21468664705753326, 0.2969905436038971, 0.26946938037872314, 0.06550391763448715], dtype='float32').reshape([12]),
            paddle.to_tensor([0.2926086485385895, 0.23500211536884308, 0.30305173993110657, 0.37713873386383057, 0.37708309292793274, 0.39858320355415344, 0.08339937776327133, 0.0028162004891783, 0.06460021436214447, 0.38861337304115295, 0.03654494509100914, 0.2453441619873047], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_12680cd2ebc8fec01319e5d46183030c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 1536, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aba67eb655ef539c892c4e5aee67bc1a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6189f57e657c3274e5b2cd2cfa1abb1c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 512, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.41334035992622375, 0.4374687075614929, 0.08180412650108337, 0.19742774963378906, 0.047095589339733124, 0.007882081903517246, 0.39541566371917725, 0.40303707122802734, 0.1552194505929947, 0.49430808424949646, 0.17393942177295685, 0.2548926770687103, 0.009109807200729847, 0.2494952231645584, 0.41202637553215027, 0.26392799615859985], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4765668213367462, 0.09785100817680359, 0.1715855449438095, 0.07605813443660736, 0.17682409286499023, 0.20579780638217926, 0.2713804841041565, 0.08150774985551834, 0.07314113527536392, 0.4397720694541931, 0.06562776118516922, 0.10906363278627396, 0.3519017994403839, 0.10685461014509201, 0.42148226499557495, 0.02544715628027916], dtype='float32').reshape([16]),
            paddle.to_tensor([0.24801914393901825, 0.27157920598983765, 0.2196718454360962, 0.12358777225017548, 0.3476331830024719, 0.4061090350151062, 0.06360785663127899, 0.29247182607650757, 0.4863199293613434, 0.34580761194229126, 0.417805552482605, 0.07871726900339127, 0.21051301062107086, 0.3276929557323456, 0.21718598902225494, 0.3574477434158325], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3381407558917999, 0.19616511464118958, 0.41710343956947327, 0.41535836458206177, 0.2590107321739197, 0.3743027448654175, 0.1840442419052124, 0.4485427141189575, 0.4140240550041199, 0.26816803216934204, 0.3447421193122864, 0.49197497963905334, 0.05670788884162903, 0.081504225730896, 0.3231194317340851, 0.36754274368286133], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2147c0bd230343bd5b39e61f63ccb5aa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4a74dff69810584603eaa2115634d13b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 240, 240], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3e5f95aa0c66bb10fe2416f87dd6a4f0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 4, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8b6593ceae352421b68eab8e7d50c486(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.02912207692861557, 0.3304753005504608, 0.3735743463039398, 0.4861410856246948, 0.06737253814935684, 0.3709265887737274, 0.11870334297418594, 0.16431519389152527, 0.3075755834579468, 0.44476941227912903, 0.43706178665161133, 0.340932160615921, 0.47313642501831055, 0.04143358767032623, 0.48597148060798645, 0.007347870618104935, 0.055563490837812424, 0.0041485680267214775, 0.1293713003396988, 0.2894936501979828], dtype='float32').reshape([20]),
            paddle.to_tensor([0.03824140504002571, 0.027623405680060387, 0.09783481061458588, 0.13226372003555298, 0.49993669986724854, 0.06586004793643951, 0.12227804958820343, 0.45896416902542114, 0.40873023867607117, 0.4638637602329254, 0.3275032937526703, 0.41646820306777954, 0.2367662787437439, 0.3419901430606842, 0.29466310143470764, 0.40208080410957336, 0.10397123545408249, 0.44747963547706604, 0.10157012939453125, 0.2543886601924896], dtype='float32').reshape([20]),
            paddle.to_tensor([0.37108710408210754, 0.37169793248176575, 0.03475566953420639, 0.13131596148014069, 0.10549840331077576, 0.15001286566257477, 0.0578853003680706, 0.31823617219924927, 0.1224762350320816, 0.23190899193286896, 0.10802975296974182, 0.23153051733970642, 0.30020564794540405, 0.18308281898498535, 0.2732173204421997, 0.1518210470676422, 0.11720700562000275, 0.3441813290119171, 0.15838202834129333, 0.04359842836856842], dtype='float32').reshape([20]),
            paddle.to_tensor([0.25240394473075867, 0.45196396112442017, 0.4952865242958069, 0.33504873514175415, 0.052961211651563644, 0.1615004688501358, 0.13484224677085876, 0.3145725131034851, 0.09925248473882675, 0.06549014151096344, 0.2682183086872101, 0.4975821077823639, 0.2375830113887787, 0.0616176463663578, 0.3197343349456787, 0.44216856360435486, 0.3172157406806946, 0.3120475709438324, 0.08170963078737259, 0.44843801856040955], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ddfd260b742eee16b3976ea2afb96a4b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2ec35d012daa8c9bd3e9e43e67e3a8bc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.30254584550857544, 0.23588691651821136, 0.15032321214675903, 0.1477462500333786, 0.13225959241390228, 0.08664547652006149, 0.1351948082447052, 0.1231326088309288, 0.19104428589344025, 0.17280779778957367, 0.25415492057800293, 0.3199962377548218, 0.4335920810699463, 0.3303641378879547, 0.4048132598400116, 0.3018522560596466, 0.030357519164681435, 0.2531677782535553], dtype='float32').reshape([18]),
            paddle.to_tensor([0.07858257740736008, 0.32817384600639343, 0.32869696617126465, 0.008606264367699623, 0.20630528032779694, 0.1299886405467987, 0.07307501137256622, 0.011521898210048676, 0.18180088698863983, 0.22099746763706207, 0.34600576758384705, 0.4701019525527954, 0.07164724171161652, 0.08713985979557037, 0.11235424876213074, 0.0005052654887549579, 0.2975764274597168, 0.08948806673288345], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2780972719192505, 0.28359389305114746, 0.04506814852356911, 0.41694140434265137, 0.301605224609375, 0.20708715915679932, 0.2549738883972168, 0.1412269026041031, 0.0736943930387497, 0.19381017982959747, 0.03186782822012901, 0.12254953384399414, 0.0592363178730011, 0.14694905281066895, 0.4670146703720093, 0.009267314337193966, 0.26862049102783203, 0.4234354794025421], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2083299458026886, 0.3777104914188385, 0.1429218351840973, 0.07078274339437485, 0.22319015860557556, 0.22045083343982697, 0.3983916640281677, 0.11151833832263947, 0.35641518235206604, 0.21276485919952393, 0.036930330097675323, 0.4876510202884674, 0.16078045964241028, 0.25982582569122314, 0.12127801030874252, 0.27914124727249146, 0.32673293352127075, 0.4824838936328888], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ed478c498e9f14b73fc88e48be92caed(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.13069242238998413, 0.4838069677352905, 0.3208092451095581, 0.03706599399447441, 0.21202389895915985, 0.28031864762306213, 0.1400383859872818, 0.19479380548000336, 0.08674086630344391, 0.40617072582244873, 0.23011143505573273, 0.06704646348953247, 0.3037903308868408, 0.21285739541053772, 0.012730881571769714, 0.3350847661495209, 0.37328097224235535, 0.08216480910778046, 0.2630881369113922, 0.07254995405673981], dtype='float32').reshape([20]),
            paddle.to_tensor([0.05001983791589737, 0.03089253231883049, 0.4137263000011444, 0.030445631593465805, 0.20842419564723969, 0.2537655532360077, 0.06748257577419281, 0.385488897562027, 0.16445936262607574, 0.09409687668085098, 0.3963354527950287, 0.3699575364589691, 0.06523340940475464, 0.06717352569103241, 0.1963588148355484, 0.48640018701553345, 0.26005542278289795, 0.247501939535141, 0.1517622023820877, 0.11759061366319656], dtype='float32').reshape([20]),
            paddle.to_tensor([0.09943585842847824, 0.2829946279525757, 0.12221254408359528, 0.37872058153152466, 0.12633031606674194, 0.31752780079841614, 0.29762816429138184, 0.1949542611837387, 0.49562400579452515, 0.4825846254825592, 0.21234574913978577, 0.29391875863075256, 0.4768352806568146, 0.30150434374809265, 0.04216647148132324, 0.04676564410328865, 0.3212769329547882, 0.12168875336647034, 0.4649867117404938, 0.34303703904151917], dtype='float32').reshape([20]),
            paddle.to_tensor([0.04973487928509712, 0.039067454636096954, 0.07725831121206284, 0.4849775731563568, 0.018761618062853813, 0.10684900730848312, 0.4307531714439392, 0.2970156669616699, 0.28688758611679077, 0.1482541710138321, 0.23655542731285095, 0.47743847966194153, 0.20860503613948822, 0.12005216628313065, 0.26373639702796936, 0.43300509452819824, 0.16011328995227814, 0.30515339970588684, 0.43371137976646423, 0.028639117255806923], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cbff8a4d27f9159e215e347f5f8f1adc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1820859e88d1a2abbe052dba73658463(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.0011195330880582333, 0.29716992378234863, 0.005297466646879911, 0.3244757354259491, 0.36724552512168884, 0.34641069173812866, 0.11034964770078659, 0.3298044204711914], dtype='float32').reshape([8]),
            paddle.to_tensor([0.2732956111431122, 0.1460752636194229, 0.13253077864646912, 0.09341907501220703, 0.4112154245376587, 0.22860363125801086, 0.0946347787976265, 0.15773633122444153], dtype='float32').reshape([8]),
            paddle.to_tensor([0.3837874233722687, 0.23665158450603485, 0.008758489973843098, 0.09054970741271973, 0.04523699730634689, 0.17795538902282715, 0.14397844672203064, 0.08735889941453934], dtype='float32').reshape([8]),
            paddle.to_tensor([0.0614933967590332, 0.08024647831916809, 0.17780254781246185, 0.23449774086475372, 0.4988485872745514, 0.35645022988319397, 0.48628613352775574, 0.3989776372909546], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bd08273ac869e7179ec43dbb5d14c69a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 240, 240], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a9b234cb7945eb827d526254fa0c4f3e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 350, 25], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_95f6248087ec797736201b62e020a20a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 1760, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1760], dtype='float32', min=0, max=0.5),
            paddle.uniform([1760], dtype='float32', min=0, max=0.5),
            paddle.uniform([1760], dtype='float32', min=0, max=0.5),
            paddle.uniform([1760], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_185387c7b957e6cfb43fbc1338f3b6e1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f335b6007a02f86085f03c0f87612c03(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 8, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_f563867af680a9ee1502aac2665534b4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_98ce785e60627bd10114ca9e3a050ac0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_f563867af680a9ee1502aac2665534b4
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_60a513c4eee4d7f771136c5a8274c5b1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7bc8ae2853681bc18462327323bfa24c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 100, 168], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6da49a622d8937fabdff52fc6483df4a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_068af54a237d9c819368f60ba21d774a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_190f5489288e3db92fdca6b9490f8160
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 32, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3641260266304016, 0.19249972701072693, 0.19466768205165863, 0.15472951531410217, 0.12469509243965149, 0.39042556285858154, 0.052427034825086594, 0.36152759194374084], dtype='float32').reshape([8]),
            paddle.to_tensor([0.12311949580907822, 0.40941357612609863, 0.4060513377189636, 0.03472413867712021, 0.4072152078151703, 0.13139432668685913, 0.3336934447288513, 0.14690938591957092], dtype='float32').reshape([8]),
            paddle.to_tensor([0.25737929344177246, 0.3817337155342102, 0.001689286669716239, 0.12475665658712387, 0.12882617115974426, 0.34923943877220154, 0.40520867705345154, 0.2901664972305298], dtype='float32').reshape([8]),
            paddle.to_tensor([0.05120386555790901, 0.06303932517766953, 0.16067685186862946, 0.3462037742137909, 0.3652280271053314, 0.07869458198547363, 0.48941072821617126, 0.4994611144065857], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bd43511055d885a67bfa5095b4667703(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 8, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e032a32c5a2af020de1266b86245d2da(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 512, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.25233012437820435, 0.4198077321052551, 0.3959590792655945, 0.2092735767364502, 0.1194838434457779, 0.22187922894954681, 0.14358671009540558, 0.4534744322299957, 0.399495929479599, 0.4287090599536896, 0.34976425766944885, 0.4078899323940277, 0.4360284209251404, 0.09981326013803482, 0.4289954900741577, 0.19061578810214996], dtype='float32').reshape([16]),
            paddle.to_tensor([0.32358068227767944, 0.11171483248472214, 0.05362957715988159, 0.1289963722229004, 0.04567268118262291, 0.028050635010004044, 0.3830483853816986, 0.10014020651578903, 0.021042462438344955, 0.06053590774536133, 0.1155712902545929, 0.29503554105758667, 0.19583941996097565, 0.20396268367767334, 0.026773380115628242, 0.19825465977191925], dtype='float32').reshape([16]),
            paddle.to_tensor([0.17290690541267395, 0.08578368276357651, 0.3350052833557129, 0.07109420746564865, 0.4013918340206146, 0.22143733501434326, 0.004720100201666355, 0.10854731500148773, 0.017748836427927017, 0.2631535530090332, 0.11904153227806091, 0.4660208225250244, 0.4205915331840515, 0.3919002413749695, 0.28451764583587646, 0.3930169343948364], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3844844400882721, 0.17885087430477142, 0.45821306109428406, 0.3588950037956238, 0.4736619293689728, 0.1533336043357849, 0.35469603538513184, 0.23473989963531494, 0.47361600399017334, 0.10580760985612869, 0.41039830446243286, 0.23381033539772034, 0.33510759472846985, 0.13568559288978577, 0.0889672040939331, 0.4920141100883484], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d59ac17ea78fe2656c1c728344f41bf1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2041008472442627, 0.07205162942409515, 0.21118389070034027, 0.3082756996154785, 0.040882427245378494, 0.3027464747428894, 0.31552720069885254, 0.14996902644634247, 0.20631495118141174, 0.03886311128735542, 0.13701578974723816, 0.32906374335289, 0.029432443901896477, 0.46190863847732544, 0.16301240026950836, 0.27606940269470215, 0.4040454924106598, 0.2533743381500244, 0.14009502530097961, 0.3368102014064789, 0.07401911169290543, 0.08291378617286682, 0.22608031332492828, 0.11589042097330093, 0.29584819078445435, 0.1181475967168808, 0.18652859330177307, 0.23734207451343536, 0.24162757396697998, 0.31285780668258667], dtype='float32').reshape([30]),
            paddle.to_tensor([0.1600860059261322, 0.002662563929334283, 0.2483144998550415, 0.46944743394851685, 0.21711821854114532, 0.06854236871004105, 0.39721763134002686, 0.09209954738616943, 0.08433517068624496, 0.22577643394470215, 0.4127228856086731, 0.3103910982608795, 0.3481689393520355, 0.18348819017410278, 0.46531054377555847, 0.26190561056137085, 0.2873459756374359, 0.24097028374671936, 0.41238638758659363, 0.3606323003768921, 0.09636656939983368, 0.20043838024139404, 0.3548068702220917, 0.49634456634521484, 0.0942002609372139, 0.07154086977243423, 0.3985694944858551, 0.2002885341644287, 0.01071174442768097, 0.14210155606269836], dtype='float32').reshape([30]),
            paddle.to_tensor([0.38158029317855835, 0.32364311814308167, 0.24142487347126007, 0.35080891847610474, 0.20724427700042725, 0.14196911454200745, 0.15676335990428925, 0.0008750632987357676, 0.45778337121009827, 0.4310651123523712, 0.145315483212471, 0.21834300458431244, 0.27279552817344666, 0.04784134775400162, 0.47833338379859924, 0.04044413939118385, 0.13941241800785065, 0.28441786766052246, 0.31517481803894043, 0.18207064270973206, 0.2107447385787964, 0.12735222280025482, 0.09715444594621658, 0.13159678876399994, 0.3968736231327057, 0.21177281439304352, 0.27692416310310364, 0.13474825024604797, 0.2785647511482239, 0.42010948061943054], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4398273825645447, 0.2380935251712799, 0.2679588198661804, 0.42402106523513794, 0.1827983856201172, 0.1496143788099289, 0.3334345519542694, 0.3690054416656494, 0.10033349692821503, 0.4527677297592163, 0.20562449097633362, 0.2772658169269562, 0.134047269821167, 0.3058672845363617, 0.3667132556438446, 0.08301898837089539, 0.33215591311454773, 0.3815048336982727, 0.4206307828426361, 0.11120342463254929, 0.09486711770296097, 0.20967717468738556, 0.4761378765106201, 0.41403883695602417, 0.028683843091130257, 0.08288513869047165, 0.15134303271770477, 0.4864990711212158, 0.2182757705450058, 0.158333420753479], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f9f92055e6aec4fd75221c0d14a78e5d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 100, 168], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cfd177f77552592a670e71c29469ed62(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 64, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_80268cf65e0826b7ec9964581e347804(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 416, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_31093ad3b8cdd8155b603feba42a0672(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.0971294417977333, 0.09861535578966141, 0.05974068492650986, 0.41250061988830566, 0.47504541277885437, 0.11519303917884827, 0.1397763192653656, 0.348631888628006, 0.32286718487739563, 0.41421738266944885, 0.19212476909160614, 0.20285604894161224], dtype='float32').reshape([12]),
            paddle.to_tensor([0.20670227706432343, 0.36951711773872375, 0.31650668382644653, 0.40484246611595154, 0.45024600625038147, 0.15589651465415955, 0.11501728743314743, 0.1710096299648285, 0.42632347345352173, 0.17739562690258026, 0.007944791577756405, 0.400633305311203], dtype='float32').reshape([12]),
            paddle.to_tensor([0.3216763734817505, 0.10049289464950562, 0.40222862362861633, 0.009785639122128487, 0.0972255989909172, 0.21228770911693573, 0.03759726881980896, 0.2686753273010254, 0.3462105393409729, 0.016567593440413475, 0.23972158133983612, 0.40386295318603516], dtype='float32').reshape([12]),
            paddle.to_tensor([0.2281198799610138, 0.1381605565547943, 0.4293656349182129, 0.018021777272224426, 0.2604086101055145, 0.4807657301425934, 0.4027722179889679, 0.318206787109375, 0.38120490312576294, 0.22555050253868103, 0.2601485848426819, 0.11952383071184158], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5caf144537dbce7cba3ec14bf281200f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5333b5296ad52324fe9ae5c4d824f6a6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.1013190820813179, 0.2928595244884491, 0.47029659152030945, 0.4348531663417816, 0.025637060403823853, 0.2814491093158722, 0.4963650405406952, 0.44682618975639343, 0.34062036871910095, 0.3481162488460541, 0.05729445070028305, 0.4888003468513489, 0.14975033700466156, 0.3286519944667816, 0.15151162445545197, 0.049458641558885574, 0.2538132965564728, 0.2953023314476013, 0.19471652805805206, 0.07640077918767929], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4075900912284851, 0.4793543219566345, 0.25942927598953247, 0.49222472310066223, 0.02256389521062374, 0.378609299659729, 0.013669147156178951, 0.1705968677997589, 0.2858932316303253, 0.2611381411552429, 0.3032208979129791, 0.27884960174560547, 0.46073949337005615, 0.37120720744132996, 0.32335934042930603, 0.09768017381429672, 0.24237453937530518, 0.4902743697166443, 0.14890311658382416, 0.40208420157432556], dtype='float32').reshape([20]),
            paddle.to_tensor([0.2088938057422638, 0.4306691884994507, 0.3298862874507904, 0.3191153407096863, 0.17125649750232697, 0.2846381962299347, 0.4878503680229187, 0.1784907877445221, 0.09046042710542679, 0.4616627097129822, 0.30279776453971863, 0.19689345359802246, 0.09619182348251343, 0.43094536662101746, 0.22536231577396393, 0.41675615310668945, 0.14169146120548248, 0.29608142375946045, 0.31328701972961426, 0.27606335282325745], dtype='float32').reshape([20]),
            paddle.to_tensor([0.2213907241821289, 0.017542406916618347, 0.4609695374965668, 0.06853214651346207, 0.43437957763671875, 0.16778279840946198, 0.4843980371952057, 0.40677300095558167, 0.4969418942928314, 0.3812783658504486, 0.10852787643671036, 0.08402430266141891, 0.264816015958786, 0.006138400174677372, 0.43952158093452454, 0.29746460914611816, 0.3738044202327728, 0.4230389893054962, 0.35671743750572205, 0.2622223496437073], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_122d24fdf25ea527d33c339051957f9b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 320, 320], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c41a749715c444edccd0f873d1d4bfda(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2249c9307af91607f3056a732f7dff5a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_abc15bd94e55776fdbcb840279865c01
    def get_inputs(self):
        return [
            paddle.uniform([16, 512], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_080fdc09dd38ee3d6318c0d60bb96441(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 4, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cddf590ca93443d91f89dbebfc454738(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 2, 27], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b0832588f11239b9b980fabc093e5b3d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1d9a5ba4ce177c8e5055c449e46298db(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4c42f756858dcc386ca7f0e30776d01a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7d3706774e5ac477dbd041a43be0c369(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_161f00f11c95f43425b39801522b4043
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_835a054054ec0985d20d5d05e109cb3c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2b980a7a57dbda86f8577100e8687bf4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_56f019968001ae30e11c75eafcbd30b8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7f09cae41f4a94b9700d7dee591b17eb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_56b296d165c75766a00630afd96a7b42(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_186eae946956756b13f3654169c2ef9a
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 4, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4da000a56e29638c89d4dddff9305c27(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2206522673368454, 0.19652514159679413, 0.39867570996284485, 0.16767281293869019, 0.13068494200706482, 0.20574268698692322, 0.2713800072669983, 0.142775759100914, 0.41338828206062317, 0.3912314772605896, 0.32821816205978394, 0.21165147423744202, 0.011761282570660114, 0.15009914338588715, 0.1806395798921585, 0.3422676622867584, 0.26598313450813293, 0.16347739100456238, 0.020044144243001938, 0.1658831536769867, 0.44785964488983154, 0.44401484727859497, 0.36113235354423523, 0.46765273809432983, 0.41990822553634644, 0.30045729875564575, 0.049749091267585754, 0.35244014859199524, 0.1665138602256775, 0.16769146919250488], dtype='float32').reshape([30]),
            paddle.to_tensor([0.02117019332945347, 0.3154711425304413, 0.04968280717730522, 0.2080373913049698, 0.3744973838329315, 0.14198479056358337, 0.4485662579536438, 0.49335867166519165, 0.1820654720067978, 0.13715383410453796, 0.35619258880615234, 0.044191353023052216, 0.09561558812856674, 0.14208492636680603, 0.016463857144117355, 0.08773813396692276, 0.028074439615011215, 0.24364827573299408, 0.46612349152565, 0.3395776152610779, 0.4547059237957001, 0.20575398206710815, 0.16284596920013428, 0.368028849363327, 0.3145334720611572, 0.09997417777776718, 0.06384504586458206, 0.11661314219236374, 0.4349590241909027, 0.4433248043060303], dtype='float32').reshape([30]),
            paddle.to_tensor([0.38643866777420044, 0.028869742527604103, 0.3124414384365082, 0.2835981845855713, 0.13088053464889526, 0.46714770793914795, 0.15198208391666412, 0.07670492678880692, 0.35721102356910706, 0.43438708782196045, 0.41920432448387146, 0.3347148299217224, 0.36149659752845764, 0.3552853465080261, 0.0890006422996521, 0.21841071546077728, 0.42743977904319763, 0.21745705604553223, 0.25587210059165955, 0.08307164162397385, 0.1693021059036255, 0.4570120573043823, 0.0694642961025238, 0.11796334385871887, 0.3423939645290375, 0.4368877112865448, 0.45357826352119446, 0.1682879626750946, 0.1878446489572525, 0.23246242105960846], dtype='float32').reshape([30]),
            paddle.to_tensor([0.26256847381591797, 0.3172338902950287, 0.03988572955131531, 0.11350458115339279, 0.1656767576932907, 0.4451427459716797, 0.2699887752532959, 0.4986937344074249, 0.050523094832897186, 0.35572323203086853, 0.4209563434123993, 0.19961601495742798, 0.16353967785835266, 0.03817065432667732, 0.09943386912345886, 0.3314470052719116, 0.029673587530851364, 0.2612173557281494, 0.034221526235342026, 0.43501603603363037, 0.37866276502609253, 0.4094543159008026, 0.286957710981369, 0.36145564913749695, 0.2144189327955246, 0.05330668389797211, 0.36926817893981934, 0.2733144760131836, 0.3970237374305725, 0.20628227293491364], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_403cfba87df1d80b4433fd7ec85ae046(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 100], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6a9fa3f1fd56b7d44821cf06b8ad599a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7ef01f2225a330695c0090d29798e565(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_190f5489288e3db92fdca6b9490f8160
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 32, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_409a8604eb04241e21fe3c9e4f0df20c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e95ad13d39e8c0737a812d115fbe743d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 816, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([816], dtype='float32', min=0, max=0.5),
            paddle.uniform([816], dtype='float32', min=0, max=0.5),
            paddle.uniform([816], dtype='float32', min=0, max=0.5),
            paddle.uniform([816], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8648f7d588dfd23cbffd845f835af4bc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6b1d0379f783ed806e3e525563109ff0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b82f38237a5ac2a14c1794433f31e685(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5c1b98125cae9f834989aaa16aa8f9ff(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 128, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_df6ae9258b0f23592791b45c179f2ee4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 17, 17], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7e0035572b1313dfd60ee4172f7cedb4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 30, 30], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_98982ac1668ec458bffac80a1c4ec227(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 1280, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_15d2c1cd975306006b890cb415f54753(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9a7f05262d204811a8b396a424871ded(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 2, 4], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_03fe913e69f684e93aa09f0dbe629900(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.14986979961395264, 0.10433904081583023, 0.091023288667202, 0.06423071026802063, 0.46794575452804565, 0.404985249042511, 0.474265456199646, 0.4675609767436981, 0.005098896566778421, 0.3053031861782074, 0.23140521347522736, 0.2454606592655182, 0.17575913667678833, 0.1416168510913849, 0.3300567865371704, 0.0830918699502945, 0.24677273631095886, 0.041493263095617294, 0.19727495312690735, 0.38986751437187195, 0.262055903673172, 0.4019075930118561, 0.09315312653779984, 0.07740083336830139, 0.12849266827106476, 0.31331923604011536, 0.14894631505012512, 0.26236608624458313, 0.4024422764778137, 0.15893353521823883], dtype='float32').reshape([30]),
            paddle.to_tensor([0.299417108297348, 0.41544753313064575, 0.47295892238616943, 0.48668333888053894, 0.25625309348106384, 0.062587670981884, 0.3160380721092224, 0.25925174355506897, 0.3827807903289795, 0.214381605386734, 0.48820656538009644, 0.04331185296177864, 0.2290002405643463, 0.49656763672828674, 0.26197901368141174, 0.032350119203329086, 0.14597992599010468, 0.0359613299369812, 0.30959388613700867, 0.3103322386741638, 0.2743358612060547, 0.10746176540851593, 0.0700303465127945, 0.46060964465141296, 0.4284970760345459, 0.27434736490249634, 0.1259765774011612, 0.4577281177043915, 0.02190205454826355, 0.48454564809799194], dtype='float32').reshape([30]),
            paddle.to_tensor([0.38538658618927, 0.369075745344162, 0.23831906914710999, 0.22347646951675415, 0.18104389309883118, 0.1970195323228836, 0.3495767414569855, 0.3319634199142456, 0.2884425222873688, 0.27157193422317505, 0.059984639286994934, 0.489068865776062, 0.17571435868740082, 0.46005502343177795, 0.3511095345020294, 0.1080380529165268, 0.03765299916267395, 0.3683095872402191, 0.00117021263577044, 0.00010980007937178016, 0.45894819498062134, 0.46364039182662964, 0.4908508062362671, 0.3462427258491516, 0.27575236558914185, 0.4946011006832123, 0.3668467700481415, 0.4946099519729614, 0.17881928384304047, 0.3228582441806793], dtype='float32').reshape([30]),
            paddle.to_tensor([0.0848027691245079, 0.04533624276518822, 0.21510611474514008, 0.0537952296435833, 0.006665346212685108, 0.2827301025390625, 0.2329172044992447, 0.14652250707149506, 0.3003425896167755, 0.1696365475654602, 0.4591623544692993, 0.24182932078838348, 0.06728607416152954, 0.1653769314289093, 0.1944465935230255, 0.2768995463848114, 0.24255132675170898, 0.11907334625720978, 0.45427513122558594, 0.19389551877975464, 0.1810244470834732, 0.08641814440488815, 0.13343115150928497, 0.4149682819843292, 0.021114103496074677, 0.11042863130569458, 0.4096379578113556, 0.22094659507274628, 0.38974109292030334, 0.4206741154193878], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c74988f3650e620f9386853b412e94cc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 112, 64, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e01fb74ed6804ccce375f2e3558c154b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a939ac64bd0447795d43876310eb995a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_0336412036bb12c89542873a9d0cc312
    def get_inputs(self):
        return [
            paddle.uniform([1, 728, 19, 19], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8589cb457a365b65b234232f2e6b22cf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 1680, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1680], dtype='float32', min=0, max=0.5),
            paddle.uniform([1680], dtype='float32', min=0, max=0.5),
            paddle.uniform([1680], dtype='float32', min=0, max=0.5),
            paddle.uniform([1680], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9bcbed6755a662bc016614bf9195c6f3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_254661e843a59d5ff3bd08684467e576(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 50, 84], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2f227fe36e5c9bb9a7c35a6b8b4ae602(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4ffed3832bea93c9af7616ebe6b989c2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_17fe866e53256d3178b99c4397a5c525(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_83e29bb588455d3a1e2a8566f5ec8ef0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_21541860bce322189aad845ee6550fe7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3820870816707611, 0.12778344750404358, 0.11547937244176865, 0.3094707131385803, 0.12086375802755356, 0.12087995558977127, 0.42085233330726624, 0.02211889624595642, 0.2561190128326416, 0.3058108687400818, 0.4257018268108368, 0.23622222244739532, 0.3425258994102478, 0.011713971383869648, 0.19783084094524384, 0.4215892553329468, 0.017174340784549713, 0.04258691146969795, 0.1297958791255951, 0.06338024139404297, 0.3971998691558838, 0.43299874663352966, 0.0795646384358406, 0.4628938138484955, 0.3478909432888031, 0.1758233904838562, 0.43071573972702026, 0.30905452370643616, 0.11827711760997772, 0.07314193248748779], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4978731870651245, 0.27460622787475586, 0.36468505859375, 0.054695501923561096, 0.227700874209404, 0.07006651908159256, 0.21302714943885803, 0.0015401686541736126, 0.48655274510383606, 0.15664488077163696, 0.3779195547103882, 0.2591204047203064, 0.2920592725276947, 0.1678052693605423, 0.06598026305437088, 0.064415343105793, 0.298667311668396, 0.27671706676483154, 0.2935265600681305, 0.46030810475349426, 0.4141347110271454, 0.38925591111183167, 0.39503344893455505, 0.33209100365638733, 0.2501835823059082, 0.2874090373516083, 0.23789846897125244, 0.2760452926158905, 0.11710076779127121, 0.47423234581947327], dtype='float32').reshape([30]),
            paddle.to_tensor([0.12461378425359726, 0.07685711979866028, 0.23973903059959412, 0.2422332912683487, 0.20672470331192017, 0.35974735021591187, 0.22088269889354706, 0.29405686259269714, 0.1450355350971222, 0.047463491559028625, 0.4293578565120697, 0.05741313844919205, 0.035114191472530365, 0.12087031453847885, 0.165639266371727, 0.34614098072052, 0.17340470850467682, 0.16563378274440765, 0.18254294991493225, 0.3814714252948761, 0.21350599825382233, 0.045023445039987564, 0.2814432978630066, 0.36406153440475464, 0.18290650844573975, 0.023002706468105316, 0.08668425679206848, 0.38933801651000977, 0.232034832239151, 0.030553650110960007], dtype='float32').reshape([30]),
            paddle.to_tensor([0.32249847054481506, 0.022547608241438866, 0.18912458419799805, 0.22017507255077362, 0.008093921467661858, 0.4305223822593689, 0.38674554228782654, 0.3619052767753601, 0.23039206862449646, 0.4903857409954071, 0.24782036244869232, 0.3504332900047302, 0.1758124679327011, 0.36737707257270813, 0.49729499220848083, 0.04718111827969551, 0.1978757083415985, 0.43832874298095703, 0.1709507554769516, 0.3387649357318878, 0.1039789468050003, 0.2809353768825531, 0.12163002043962479, 0.49769631028175354, 0.31782153248786926, 0.34383368492126465, 0.1918579638004303, 0.31130895018577576, 0.06226430833339691, 0.2663041055202484], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3183226a591724947315d01e9c996112(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dbb433c1461d1ee9c988cbbdbd3d5bcc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 8, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2d9ba4376c15a881d69a580090a2ce67(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a135c5f6dd0bb85f360bb6f3c24b5c8c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.40691065788269043, 0.09006012976169586, 0.17042478919029236, 0.3141004741191864, 0.3945181667804718, 0.12305555492639542, 0.3987584114074707, 0.06638704240322113, 0.47508612275123596, 0.42269691824913025, 0.25223734974861145, 0.49117496609687805, 0.32646408677101135, 0.20018723607063293, 0.31277209520339966, 0.4861329197883606, 0.2154523730278015, 0.350018709897995, 0.12819884717464447, 0.020441586151719093, 0.1608341783285141, 0.31401580572128296, 0.12094096094369888, 0.23518221080303192, 0.11812901496887207, 0.4937785863876343, 0.07174921780824661, 0.13362640142440796, 0.4533114433288574, 0.10315771400928497], dtype='float32').reshape([30]),
            paddle.to_tensor([0.09179387241601944, 0.1680879443883896, 0.15652959048748016, 0.05340667814016342, 0.01462707668542862, 0.060200341045856476, 0.3686809539794922, 0.18863722681999207, 0.08899440616369247, 0.03549231216311455, 0.3064831495285034, 0.07212958484888077, 0.45623505115509033, 0.23786906898021698, 0.18708127737045288, 0.06167086586356163, 0.3798772990703583, 0.34893035888671875, 0.1871429979801178, 0.34084752202033997, 0.1546514481306076, 0.04588857293128967, 0.013716374523937702, 0.223603293299675, 0.25967755913734436, 0.047465983778238297, 0.3725956082344055, 0.19569188356399536, 0.18120259046554565, 0.22370857000350952], dtype='float32').reshape([30]),
            paddle.to_tensor([0.235163614153862, 0.49336954951286316, 0.0416036993265152, 0.1287687122821808, 0.2978602647781372, 0.34645089507102966, 0.4218985140323639, 0.45831072330474854, 0.06586360186338425, 0.13753902912139893, 0.02725580707192421, 0.3212619721889496, 0.4363114535808563, 0.28773680329322815, 0.4960440695285797, 0.24123910069465637, 0.062052834779024124, 0.39543411135673523, 0.017812585458159447, 0.0630154013633728, 0.25614625215530396, 0.21392081677913666, 0.2256462275981903, 0.09235816448926926, 0.3220592141151428, 0.4856158196926117, 0.08219914883375168, 0.16971801221370697, 0.07174088060855865, 0.0035973284393548965], dtype='float32').reshape([30]),
            paddle.to_tensor([0.1750784069299698, 0.19362503290176392, 0.11565272510051727, 0.35442695021629333, 0.03387463465332985, 0.3027600347995758, 0.002980569377541542, 0.13141806423664093, 0.2590087950229645, 0.3382472097873688, 0.3782104551792145, 0.3312678933143616, 0.1370418816804886, 0.1580197662115097, 0.038976896554231644, 0.3056235909461975, 0.4541966915130615, 0.05372226983308792, 0.07514649629592896, 0.41104885935783386, 0.4067819118499756, 0.0926143079996109, 0.2822953760623932, 0.038930077105760574, 0.05235084146261215, 0.005669318605214357, 0.05958382785320282, 0.41718003153800964, 0.3141004741191864, 0.4464481472969055], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7ef7eb959a6ce19040b707d9fe71a3f1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a4a46651a86e0f5057c7172824b0b309(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4aa9589539ecdffe584edcc110a947fe(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 4, 3], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fd1a95e4e27d3ab441dc1e9fb937cce1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3667117655277252, 0.048817094415426254, 0.2742544710636139, 0.2384796440601349, 0.10466022789478302, 0.4902833104133606, 0.4392566978931427, 0.008631027303636074, 0.029117759317159653, 0.20383092761039734, 0.47998976707458496, 0.4178122580051422, 0.43838027119636536, 0.15919166803359985, 0.24195586144924164, 0.4707447588443756, 0.3081722557544708, 0.14122119545936584, 0.4193495810031891, 0.20769473910331726, 0.08099757879972458, 0.10550078004598618, 0.43784093856811523, 0.22790533304214478, 0.38107696175575256, 0.33596622943878174, 0.08519905805587769, 0.4893810749053955, 0.49832597374916077, 0.3303556740283966], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3047560751438141, 0.44935232400894165, 0.07016658782958984, 0.27388429641723633, 0.16222821176052094, 0.12343139946460724, 0.44773730635643005, 0.19532081484794617, 0.46658632159233093, 0.24168255925178528, 0.015293761156499386, 0.43166229128837585, 0.0587778314948082, 0.40780025720596313, 0.05142832547426224, 0.17271322011947632, 0.46030858159065247, 0.22352857887744904, 0.030833976343274117, 0.335493803024292, 0.35612982511520386, 0.4620436728000641, 0.16195861995220184, 0.41771379113197327, 0.2709850072860718, 0.2383313626050949, 0.09132472425699234, 0.027043096721172333, 0.05679210275411606, 0.13294512033462524], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3514946699142456, 0.16769903898239136, 0.2611423432826996, 0.496507853269577, 0.45450514554977417, 0.4050714075565338, 0.286663681268692, 0.17455579340457916, 0.48104697465896606, 0.0783802717924118, 0.036176715046167374, 0.027463899925351143, 0.2795507311820984, 0.09450581669807434, 0.04247572273015976, 0.38437575101852417, 0.2652565836906433, 0.07743257284164429, 0.19187474250793457, 0.06614793092012405, 0.3631935119628906, 0.07664967328310013, 0.03826802968978882, 0.2842666208744049, 0.11659418046474457, 0.26281917095184326, 0.3575023412704468, 0.04376855492591858, 0.14530238509178162, 0.36104774475097656], dtype='float32').reshape([30]),
            paddle.to_tensor([0.49104970693588257, 0.10985513031482697, 0.32356688380241394, 0.1488349437713623, 0.489477276802063, 0.22378917038440704, 0.40665122866630554, 0.16326715052127838, 0.3225630223751068, 0.30433693528175354, 0.3827572464942932, 0.28379350900650024, 0.13984575867652893, 0.037731315940618515, 0.36000436544418335, 0.006777013652026653, 0.2959434688091278, 0.06386876106262207, 0.21380622684955597, 0.08720449358224869, 0.14089839160442352, 0.19316673278808594, 0.15160973370075226, 0.1719641089439392, 0.4191668629646301, 0.36656561493873596, 0.05442861467599869, 0.29903534054756165, 0.05797233805060387, 0.001513193128630519], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3f588befc00fb20622feeb5621bc842b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.31652072072029114, 0.26511630415916443, 0.47914913296699524, 0.04545082896947861, 0.19410958886146545, 0.21083413064479828, 0.44578972458839417, 0.07342328876256943, 0.15499117970466614, 0.21989303827285767, 0.07350119948387146, 0.364078164100647, 0.2076035588979721, 0.15527261793613434, 0.48632684350013733, 0.22948600351810455, 0.038645390421152115, 0.23855386674404144], dtype='float32').reshape([18]),
            paddle.to_tensor([0.20309685170650482, 0.4721415638923645, 0.23118682205677032, 0.12411347031593323, 0.06922771036624908, 0.26835736632347107, 0.13214047253131866, 0.36119651794433594, 0.33198270201683044, 0.27453428506851196, 0.2078004628419876, 0.007742546498775482, 0.4290705919265747, 0.060737479478120804, 0.048812154680490494, 0.3972422778606415, 0.22943434119224548, 0.21780399978160858], dtype='float32').reshape([18]),
            paddle.to_tensor([0.1739397644996643, 0.34140169620513916, 0.4654507637023926, 0.23264572024345398, 0.13074693083763123, 0.11412248015403748, 0.3094141483306885, 0.44597381353378296, 0.3575630187988281, 0.12517111003398895, 0.22186170518398285, 0.31107810139656067, 0.31935033202171326, 0.30534929037094116, 0.3760182559490204, 0.4618510603904724, 0.024795055389404297, 0.24334663152694702], dtype='float32').reshape([18]),
            paddle.to_tensor([0.35768526792526245, 0.3015865981578827, 0.26621007919311523, 0.036334533244371414, 0.17753830552101135, 0.20565354824066162, 0.29877185821533203, 0.28900325298309326, 0.023430824279785156, 0.20586638152599335, 0.13006184995174408, 0.2515009045600891, 0.4971450865268707, 0.05817099288105965, 0.4804757833480835, 0.24877703189849854, 0.08743549138307571, 0.4304092526435852], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c09ebce8b6bac568d7f4ccfd4e75ef6c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_35415932508bbea2bd8b45fa1df15bdd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f47a8b0c1468e3992b19a691f11bc1ce(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 8, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5b698aabca50dea6d5aafde9331124dc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d058af465f6720cce0530e08f39c3540(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.43539959192276, 0.4676257371902466, 0.28088781237602234, 0.05801296979188919, 0.1414814293384552, 0.25439855456352234, 0.17853961884975433, 0.22479777038097382, 0.4092589020729065, 0.2125178724527359, 0.4185883104801178, 0.05361469089984894, 0.009248277172446251, 0.4979872703552246, 0.4699816107749939, 0.4918723404407501, 0.4059756398200989, 0.42580446600914, 0.03502217307686806, 0.3983031213283539], dtype='float32').reshape([20]),
            paddle.to_tensor([0.09093043208122253, 0.49558860063552856, 0.3200298845767975, 0.4234687089920044, 0.21904747188091278, 0.03574128821492195, 0.4106382131576538, 0.08024477958679199, 0.2878115475177765, 0.06875816732645035, 0.3415088355541229, 0.027989357709884644, 0.31722718477249146, 0.45199376344680786, 0.32843250036239624, 0.06679309159517288, 0.35568931698799133, 0.4156579077243805, 0.44765138626098633, 0.21893325448036194], dtype='float32').reshape([20]),
            paddle.to_tensor([0.004634777083992958, 0.19342108070850372, 0.04395177960395813, 0.29416123032569885, 0.44756340980529785, 0.2572595179080963, 0.356380432844162, 0.21450376510620117, 0.04368303343653679, 0.28656667470932007, 0.42781832814216614, 0.2974831461906433, 0.19744864106178284, 0.2501789927482605, 0.4705536663532257, 0.12468503415584564, 0.27869296073913574, 0.4877375364303589, 0.23574739694595337, 0.06981069594621658], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3534533679485321, 0.04502394050359726, 0.118975430727005, 0.22207877039909363, 0.2508416473865509, 0.3010261356830597, 0.45159003138542175, 0.4662209451198578, 0.06875935941934586, 0.2872810661792755, 0.07993895560503006, 0.2512832581996918, 0.19297777116298676, 0.2365928441286087, 0.2762051522731781, 0.04829014837741852, 0.44777631759643555, 0.30428048968315125, 0.2745291292667389, 0.03087526746094227], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_78889ccedf1ffba6e134c6e113315504(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cda3edf8e4317f943227fa261e4987f1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ea1f94aa856dd851f0f47e86599ef2c7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 256, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0afdb7612e1e8abfd393244cf07794de(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 10, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.25614678859710693, 0.36365291476249695, 0.3736727833747864, 0.4681761860847473, 0.40539851784706116, 0.29564914107322693, 0.09741079062223434, 0.13236495852470398, 0.10026287287473679, 0.1255384236574173], dtype='float32').reshape([10]),
            paddle.to_tensor([0.21308965981006622, 0.270720511674881, 0.06528127938508987, 0.07184562087059021, 0.090150386095047, 0.4036979079246521, 0.1276272088289261, 0.16093230247497559, 0.27087920904159546, 0.1815568208694458], dtype='float32').reshape([10]),
            paddle.to_tensor([0.38457900285720825, 0.3012534976005554, 0.2344057559967041, 0.38745617866516113, 0.2569827437400818, 0.33235543966293335, 0.20572490990161896, 0.33387044072151184, 0.010547209531068802, 0.27680596709251404], dtype='float32').reshape([10]),
            paddle.to_tensor([0.18128931522369385, 0.3400970995426178, 0.3805246651172638, 0.13820093870162964, 0.4038410782814026, 0.20512841641902924, 0.4968043863773346, 0.052680354565382004, 0.4105563759803772, 0.29278749227523804], dtype='float32').reshape([10]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bd571d2f4383ec3c74a74e8f49766a23(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_abc15bd94e55776fdbcb840279865c01
    def get_inputs(self):
        return [
            paddle.uniform([49, 1280], dtype='float16', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aa54e6aa2a4803426a1d7087865ac02b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_81a51590b9a96f7118ba6ea55e0036c9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 8, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6b36c6fcf166563eedd1adbe7cd44f09(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 4, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fa93d866e46288a1cd8f7286cebea9ad(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 32, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cad2863e93dac8b37aa9e2bef923b077(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_24559f96edc4aa05e9a3e7c018c66b40(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_d176968eb532ab7742141be9f49773ee
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 256, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8eb79e24a182e2a4f5961e598352fcfb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 17, 17], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f4db00cf85e0f58854363709882b242b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 300, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([300], dtype='float32', min=0, max=0.5),
            paddle.uniform([300], dtype='float32', min=0, max=0.5),
            paddle.uniform([300], dtype='float32', min=0, max=0.5),
            paddle.uniform([300], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aac0d020e6ff9f944009213325c90d1a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 26], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3ac8696433c5f3c475d2d274a57b7675(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_44ddbb84aaecaaf399de90c671ebcb71(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_295acd5c00f9a95927309f9daa202d58(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 640, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_85e19cadda9dacd66aca3a12099e7c44(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 528, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([528], dtype='float32', min=0, max=0.5),
            paddle.uniform([528], dtype='float32', min=0, max=0.5),
            paddle.uniform([528], dtype='float32', min=0, max=0.5),
            paddle.uniform([528], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_552f8dc9549f59abae1f796899aac1aa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7e4f017e250439096a6b3762344bd6f3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 175, 25], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_15d208d6b1d63686c8c761c0ed0bfd22(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_60c0e59e996f461ed7a7b642127fc003(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_15d208d6b1d63686c8c761c0ed0bfd22
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 49], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_45366f920f6b8a9740ea8f74c908bced(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_660c70898a5d86d6bb98bcf8a0d6399f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 31, 31], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f1f51d8d65d7d90b506e3d8e55969a10(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_a2fd76ef6d10c288900c0fe38e7a817a
    def get_inputs(self):
        return [
            paddle.uniform([16, 384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_760cc2b83e56567d5233a46de3c971f7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 10, 10], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e8bb45376ca5e7b90ede3cce63248d07(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3580a8475738ad9aaa86eb9b516870fd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 52, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d46422db5bfda42d8de07289b70c9ee5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b1108b92164e42e26f84fe8ed9591058(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_655c3b6d553085df72a9ee44d4d7e20e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 8, 6], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_eea7a5ee779e37482a08c21bba5d78ce(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([8, 512, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a385c8210f0907a1fe04f7c0e83949c8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 720, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a07101d91ec8bc80c09a713c7910af47(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 32, 64], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.10975776612758636, 0.4184778034687042, 0.4951764643192291, 0.44483527541160583, 0.14659206569194794, 0.32105711102485657, 0.12781167030334473, 0.11699218302965164, 0.020395858213305473, 0.40889185667037964, 0.037397146224975586, 0.1592552661895752, 0.3224446773529053, 0.46606311202049255, 0.27019739151000977, 0.4598022401332855, 0.13196802139282227, 0.3794037699699402], dtype='float32').reshape([18]),
            paddle.to_tensor([0.46134886145591736, 0.428850382566452, 0.27469319105148315, 0.1910398006439209, 0.16109664738178253, 0.4953792989253998, 0.2753244936466217, 0.2511668801307678, 0.14446866512298584, 0.1903054416179657, 0.40380993485450745, 0.4654881954193115, 0.204682856798172, 0.33767780661582947, 0.010078677907586098, 0.4183063805103302, 0.1714605689048767, 0.4833314120769501], dtype='float32').reshape([18]),
            paddle.to_tensor([0.0224243625998497, 0.2900558412075043, 0.06019086763262749, 0.16181066632270813, 0.3361230194568634, 0.38984325528144836, 0.40239760279655457, 0.1681811511516571, 0.3844480812549591, 0.30197957158088684, 0.34900176525115967, 0.2989369332790375, 0.3920721113681793, 0.2994302213191986, 0.06114959716796875, 0.4161347448825836, 0.05834970995783806, 0.2895062267780304], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2678831219673157, 0.05132513865828514, 0.28014862537384033, 0.3540476858615875, 0.08059459924697876, 0.31991833448410034, 0.031376227736473083, 0.030236339196562767, 0.08635202795267105, 0.11114473640918732, 0.02193281054496765, 0.4211057424545288, 0.03770394250750542, 0.028834065422415733, 0.371351033449173, 0.13441573083400726, 0.400128036737442, 0.03047122061252594], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3559f5a262b8ebf62e26e65c2648c1af(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 2, 4], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3b1e3668a8516a82373050857a6609d3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4500826299190521, 0.29898661375045776, 0.17478622496128082, 0.0643262267112732, 0.40052422881126404, 0.09473664313554764, 0.2728365659713745, 0.23146921396255493, 0.38432565331459045, 0.016438642516732216, 0.040145162492990494, 0.469796359539032], dtype='float32').reshape([12]),
            paddle.to_tensor([0.4441717863082886, 0.22273671627044678, 0.48588958382606506, 0.3408101797103882, 0.16281718015670776, 0.38092440366744995, 0.0601641871035099, 0.06500305980443954, 0.005795621313154697, 0.4455072283744812, 0.051018789410591125, 0.022545790299773216], dtype='float32').reshape([12]),
            paddle.to_tensor([0.3893119990825653, 0.1608218103647232, 0.21167601644992828, 0.15952050685882568, 0.26236283779144287, 0.39609643816947937, 0.021268147975206375, 0.28688177466392517, 0.01723962463438511, 0.09437444806098938, 0.36436450481414795, 0.05044713616371155], dtype='float32').reshape([12]),
            paddle.to_tensor([0.2161296010017395, 0.3337599039077759, 0.01369041483849287, 0.48971956968307495, 0.3294229805469513, 0.39553067088127136, 0.3678251802921295, 0.01507926732301712, 0.03543580323457718, 0.40979477763175964, 0.42865580320358276, 0.17816172540187836], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1651d9aef1c316c7f7604bd99f035efd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4548962116241455, 0.02378443069756031, 0.33825749158859253, 0.22751635313034058, 0.4111498296260834, 0.44609954953193665, 0.10768809914588928, 0.012196400202810764, 0.2650805115699768, 0.26786383986473083, 0.37586092948913574, 0.4023688733577728, 0.23696602880954742, 0.3383936583995819, 0.2970152199268341, 0.16554132103919983], dtype='float32').reshape([16]),
            paddle.to_tensor([0.06047526374459267, 0.37362945079803467, 0.35671281814575195, 0.29724493622779846, 0.33294326066970825, 0.32768747210502625, 0.4582906663417816, 0.1834084391593933, 0.09155555069446564, 0.35022926330566406, 0.09669613093137741, 0.2711905539035797, 0.2998281419277191, 0.4439045488834381, 0.05236873775720596, 0.47200408577919006], dtype='float32').reshape([16]),
            paddle.to_tensor([0.03468332439661026, 0.23741766810417175, 0.1035400927066803, 0.01447984017431736, 0.1439676135778427, 0.39133986830711365, 0.2568381726741791, 0.19740459322929382, 0.22145581245422363, 0.26668575406074524, 0.2522555887699127, 0.4998347759246826, 0.4647095799446106, 0.25717613101005554, 0.10706233978271484, 0.21376824378967285], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4253321886062622, 0.07103018462657928, 0.33127298951148987, 0.22449345886707306, 0.20205195248126984, 0.28177934885025024, 0.3305737376213074, 0.33585813641548157, 0.1932939887046814, 0.46800699830055237, 0.3541000187397003, 0.451806902885437, 0.23865440487861633, 0.45041623711586, 0.4037363529205322, 0.361949622631073], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ce99e7e720dc0ea61bf087833d80da0a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_a2fd76ef6d10c288900c0fe38e7a817a
    def get_inputs(self):
        return [
            paddle.uniform([196, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f07a6ce5e3c361ea64f182cbf7e122d2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.18423976004123688, 0.21351595222949982, 0.13020071387290955, 0.496939092874527, 0.24006706476211548, 0.10982109606266022, 0.22600556910037994, 0.45395728945732117, 0.34859761595726013, 0.12100667506456375, 0.05792655050754547, 0.2235472947359085, 0.24507197737693787, 0.22712811827659607, 0.047638099640607834, 0.25197577476501465, 0.2653900682926178, 0.49250879883766174, 0.27753162384033203, 0.05184035375714302, 0.012364494614303112, 0.4708135426044464, 0.1437295526266098, 0.35119250416755676, 0.3052719235420227, 0.4776648283004761, 0.09516924619674683, 0.45223677158355713, 0.06405487656593323, 0.3800605237483978], dtype='float32').reshape([30]),
            paddle.to_tensor([0.38037407398223877, 0.4880896210670471, 0.04985174909234047, 0.302955687046051, 0.17366304993629456, 0.3602786362171173, 0.44961950182914734, 0.248072549700737, 0.06555789709091187, 0.2196304053068161, 0.4051705598831177, 0.44814103841781616, 0.19595947861671448, 0.06980010867118835, 0.00533330999314785, 0.3593123257160187, 0.06789595633745193, 0.017142867669463158, 0.0937063917517662, 0.18006563186645508, 0.4077746868133545, 0.48407459259033203, 0.07109881937503815, 0.29864346981048584, 0.30032235383987427, 0.0070954179391264915, 0.4106603264808655, 0.3926559388637543, 0.44317132234573364, 0.15331454575061798], dtype='float32').reshape([30]),
            paddle.to_tensor([0.02278382144868374, 0.34287968277931213, 0.3187847435474396, 0.18378663063049316, 0.21930192410945892, 0.4585583209991455, 0.03621421381831169, 0.19426648318767548, 0.3255566954612732, 0.12980663776397705, 0.4317067265510559, 0.2706177532672882, 0.43900957703590393, 0.48364725708961487, 0.122555673122406, 0.2758019268512726, 0.4225198030471802, 0.12011443078517914, 0.3648371696472168, 0.4376462399959564, 0.16025175154209137, 0.40160486102104187, 0.20842377841472626, 0.11603143066167831, 0.05996285751461983, 0.2986759543418884, 0.21815769374370575, 0.48358529806137085, 0.12385676801204681, 0.18420542776584625], dtype='float32').reshape([30]),
            paddle.to_tensor([0.10443569719791412, 0.40242451429367065, 0.24479125440120697, 0.39984139800071716, 0.212088942527771, 0.44099491834640503, 0.38559073209762573, 0.38119909167289734, 0.22607488930225372, 0.2687850892543793, 0.46303030848503113, 0.32306474447250366, 0.34015023708343506, 0.004199279472231865, 0.295876681804657, 0.1327989250421524, 0.45562127232551575, 0.35949739813804626, 0.30877935886383057, 0.4535805284976959, 0.33964985609054565, 0.07681194692850113, 0.09206638485193253, 0.3440577983856201, 0.20432819426059723, 0.3480983078479767, 0.3330353796482086, 0.16881491243839264, 0.26021960377693176, 0.4029286801815033], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f4a85a303d3b856d8d25276e8f966d6f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_57e3aca8def7349af04bfe7c53e1a630(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 256, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_eb76ec34729d85d5e09fff4c5b8d2206(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 244, 244], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8d27b9032cc7d8a037186a28c232c005(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([8, 128, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_86af509da1f65740341f63e5830cb466(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 26], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d1201c3d03eae25344ad1965bc9bde3d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1b6c3858adaafdbc0bc411b009b7aa60(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_161f00f11c95f43425b39801522b4043
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_db765a4df41822884a88308c83b9c228(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3182450532913208, 0.4873952865600586, 0.15661443769931793, 0.19742055237293243, 0.2444695085287094, 0.08849851042032242, 0.053848203271627426, 0.10237620770931244, 0.34250059723854065, 0.4763694703578949, 0.05673322081565857, 0.32337474822998047, 0.34900227189064026, 0.10874417424201965, 0.27835822105407715, 0.37901419401168823, 0.30465617775917053, 0.30356913805007935, 0.30771785974502563, 0.4632449448108673, 0.31072065234184265, 0.21227188408374786, 0.2007150501012802, 0.26725780963897705], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3986889719963074, 0.19736714661121368, 0.4675854444503784, 0.42272236943244934, 0.29312533140182495, 0.36539915204048157, 0.10544940829277039, 0.0032276390120387077, 0.36533525586128235, 0.4850498139858246, 0.39406442642211914, 0.12541434168815613, 0.26669663190841675, 0.4705939292907715, 0.17264631390571594, 0.09320028871297836, 0.08819105476140976, 0.04970478266477585, 0.1784769743680954, 0.061865564435720444, 0.011930492706596851, 0.4989427626132965, 0.22592110931873322, 0.325776606798172], dtype='float32').reshape([24]),
            paddle.to_tensor([0.030326509848237038, 0.4300839602947235, 0.3816697597503662, 0.16243673861026764, 0.125918447971344, 0.43927517533302307, 0.2877426743507385, 0.21471726894378662, 0.3066110908985138, 0.14693988859653473, 0.20620949566364288, 0.09232679754495621, 0.41701972484588623, 0.3701663017272949, 0.07814831286668777, 0.41549187898635864, 0.1533805876970291, 0.49962183833122253, 0.36963075399398804, 0.09153775125741959, 0.26306045055389404, 0.3710462152957916, 0.07439648360013962, 0.292068749666214], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3868817389011383, 0.4607231914997101, 0.10751621425151825, 0.019646357744932175, 0.43964463472366333, 0.4418475031852722, 0.22890061140060425, 0.42596426606178284, 0.19597700238227844, 0.4939756691455841, 0.3307698667049408, 0.3926103711128235, 0.1552717238664627, 0.1938548982143402, 0.15655499696731567, 0.4917066693305969, 0.42656514048576355, 0.09869644790887833, 0.18247738480567932, 0.03624098002910614, 0.2564363479614258, 0.46151646971702576, 0.03531159833073616, 0.20678827166557312], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_8921d61813df679983676cdb4c46b3a7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_96fe1d541a1625b18f40a6d0b085b158(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_8921d61813df679983676cdb4c46b3a7
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_144010aaadc33206e0b3a3003729d786(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 88, 61, 61], dtype='float16', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4a0155b9685f4037fc197f9ecb63c805(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 152, 152], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f48c110d8bc6a1a0ae149a2331ae6269(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 76, 76], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4d98a57cfdc055d6c9122ade4defa160(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.16883234679698944, 0.21755921840667725, 0.40218064188957214, 0.16557680070400238, 0.2774640619754791, 0.4666329324245453, 0.464037150144577, 0.17132294178009033, 0.4231467843055725, 0.3836173415184021, 0.4112800657749176, 0.13192425668239594, 0.036156509071588516, 0.4295010268688202, 0.013747690245509148, 0.027616050094366074, 0.4615880846977234, 0.021442804485559464], dtype='float32').reshape([18]),
            paddle.to_tensor([0.11479631811380386, 0.037047695368528366, 0.26340436935424805, 0.3122694790363312, 0.44083574414253235, 0.005161696579307318, 0.37027424573898315, 0.001254888717085123, 0.33538419008255005, 0.09688074141740799, 0.34254810214042664, 0.22197097539901733, 0.38202768564224243, 0.38380756974220276, 0.36181050539016724, 0.3087419867515564, 0.010792652145028114, 0.3756890892982483], dtype='float32').reshape([18]),
            paddle.to_tensor([0.30408063530921936, 0.42915788292884827, 0.3105124831199646, 0.1099386215209961, 0.2988247275352478, 0.021201642230153084, 0.10119854658842087, 0.3627050220966339, 0.3684020936489105, 0.4862857460975647, 0.08999209105968475, 0.3886055648326874, 0.05702877417206764, 0.05289251729846001, 0.19062823057174683, 0.05346798524260521, 0.4605308771133423, 0.29015928506851196], dtype='float32').reshape([18]),
            paddle.to_tensor([0.28789639472961426, 0.3379826843738556, 0.07753878086805344, 0.23009897768497467, 0.22503633797168732, 0.13891969621181488, 0.4992651343345642, 0.3757018446922302, 0.42730093002319336, 0.43412137031555176, 0.06407325714826584, 0.45712095499038696, 0.059452857822179794, 0.13683468103408813, 0.41786032915115356, 0.1482677012681961, 0.12030625343322754, 0.006934723816812038], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_46ff3aa838ad37f2f501e6b1d89732bb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 636, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([636], dtype='float32', min=0, max=0.5),
            paddle.uniform([636], dtype='float32', min=0, max=0.5),
            paddle.uniform([636], dtype='float32', min=0, max=0.5),
            paddle.uniform([636], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b63f4878107dbc622ca3d189772d3da1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4090040326118469, 0.2939794659614563, 0.42589229345321655, 0.10020535439252853, 0.46299365162849426, 0.4367101788520813, 0.38614559173583984, 0.03914061188697815, 0.13243699073791504, 0.07366253435611725, 0.4722416400909424, 0.3687119483947754, 0.3123621344566345, 0.08874420821666718, 0.3478107750415802, 0.4038470983505249, 0.028865808621048927, 0.11149980872869492, 0.1309080272912979, 0.004648692440241575, 0.13543157279491425, 0.19580000638961792, 0.38171058893203735, 0.48342519998550415, 0.406567245721817, 0.36009952425956726, 0.09841692447662354, 0.4860353171825409], dtype='float32').reshape([28]),
            paddle.to_tensor([0.3012494146823883, 0.12798067927360535, 0.3464169204235077, 0.373947411775589, 0.0006085341447032988, 0.27421605587005615, 0.29128873348236084, 0.4004787802696228, 0.2237284779548645, 0.3187437951564789, 0.49118152260780334, 0.21357977390289307, 0.1301323026418686, 0.024311602115631104, 0.3885400891304016, 0.26518315076828003, 0.018867336213588715, 0.022284284234046936, 0.2199830263853073, 0.03989200294017792, 0.13832131028175354, 0.2471599131822586, 0.24818718433380127, 0.30599549412727356, 0.28596049547195435, 0.09221221506595612, 0.43527576327323914, 0.25650259852409363], dtype='float32').reshape([28]),
            paddle.to_tensor([0.1359727531671524, 0.4061409831047058, 0.15487562119960785, 0.4130313992500305, 0.3093487620353699, 0.1694260537624359, 0.21230089664459229, 0.11470603942871094, 0.12925587594509125, 0.33315509557724, 0.24340273439884186, 0.40947481989860535, 0.27112308144569397, 0.178958922624588, 0.05943300202488899, 0.46525540947914124, 0.37075117230415344, 0.4940292537212372, 0.059574004262685776, 0.01505843922495842, 0.12880022823810577, 0.4210871160030365, 0.12434536218643188, 0.002875426784157753, 0.3395463824272156, 0.41612815856933594, 0.19586187601089478, 0.38899755477905273], dtype='float32').reshape([28]),
            paddle.to_tensor([0.16821549832820892, 0.17432714998722076, 0.19499453902244568, 0.28341951966285706, 0.4901637136936188, 0.31354957818984985, 0.28407880663871765, 0.10283910483121872, 0.4069986641407013, 0.07185361534357071, 0.3632122278213501, 0.14442142844200134, 0.20822229981422424, 0.13984620571136475, 0.4260112941265106, 0.4254988133907318, 0.05249966308474541, 0.3785189092159271, 0.4749225974082947, 0.18815253674983978, 0.1524294763803482, 0.13583703339099884, 0.3684752583503723, 0.2993733286857605, 0.3221757113933563, 0.2330058217048645, 0.3413548171520233, 0.22123152017593384], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dd6b909687a8239ec428deb293b2c4a8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 32, 100], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_66b76b3fd3c698aab6739529f3108033(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 16, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_62d8c0742a15a262a41fe63e2bc8f434(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_13025b164297d9226f9641b7f0715c76
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4a0d53dbd5da710e6704553af278c182(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 28, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.11071257293224335, 0.028555581346154213, 0.11651821434497833, 0.10988195985555649, 0.477130651473999, 0.26172858476638794, 0.4864303469657898, 0.060099754482507706, 0.4833396375179291, 0.27848556637763977, 0.05123497545719147, 0.133828267455101, 0.33025214076042175, 0.054596152156591415, 0.17673684656620026, 0.23858174681663513], dtype='float32').reshape([16]),
            paddle.to_tensor([0.03679012507200241, 0.2015986144542694, 0.07891976833343506, 0.17220228910446167, 0.1267222762107849, 0.4211868643760681, 0.025689370930194855, 0.36678797006607056, 0.02965419739484787, 0.43835970759391785, 0.23402602970600128, 0.4964698851108551, 0.24936972558498383, 0.3164392411708832, 0.2629487216472626, 0.08327417075634003], dtype='float32').reshape([16]),
            paddle.to_tensor([0.0211823470890522, 0.13717292249202728, 0.15572622418403625, 0.03538505733013153, 0.21690857410430908, 0.05572722479701042, 0.4087638556957245, 0.219981849193573, 0.3986056447029114, 0.3630332946777344, 0.18137063086032867, 0.4384388327598572, 0.12360473722219467, 0.28307658433914185, 0.22256414592266083, 0.2408042997121811], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2638886570930481, 0.2530445158481598, 0.036087408661842346, 0.17467576265335083, 0.07722678780555725, 0.2327204793691635, 0.2486211210489273, 0.26040950417518616, 0.2896212637424469, 0.16683858633041382, 0.030701307579874992, 0.14289630949497223, 0.4916330575942993, 0.27892476320266724, 0.09885472059249878, 0.09627091139554977], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3af455894a9fcbc85c2657a4d2d35cf6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_db1dbcd3c3509067a0445c4e67191777(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a2bbe92b33c0278d372abcf0a524f860(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e144b700c18a9086783177195f867191(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a952ef3559ca4bba2c1dfc0ebeb9b34c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 17, 17], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_52fc59b43cb5ddc449890fcad95aaf6d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_23949454ffb186082ac795968d470335(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f8cd8541467d80b56a2faac719fa8345(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.12316232919692993, 0.23720106482505798, 0.2022686004638672, 0.3290823996067047, 0.4245462119579315, 0.14414779841899872, 0.46223878860473633, 0.37507566809654236, 0.45756658911705017, 0.3512982428073883, 0.16472752392292023, 0.058632515370845795, 0.029517631977796555, 0.28099533915519714, 0.3906143009662628, 0.3047703504562378, 0.0767163410782814, 0.3611312806606293, 0.17059344053268433, 0.30207541584968567, 0.39523497223854065, 0.18584813177585602, 0.2954536974430084, 0.03806249052286148], dtype='float32').reshape([24]),
            paddle.to_tensor([0.10935375839471817, 0.3529336750507355, 0.20431701838970184, 0.08000082522630692, 0.37443384528160095, 0.16145199537277222, 0.05955993011593819, 0.13647016882896423, 0.021007129922509193, 0.3038478493690491, 0.27847084403038025, 0.05935340374708176, 0.390642374753952, 0.13611209392547607, 0.08386445045471191, 0.22760751843452454, 0.3330545425415039, 0.05137573555111885, 0.030796874314546585, 0.35698699951171875, 0.31815996766090393, 0.2557958662509918, 0.17298142611980438, 0.2907204031944275], dtype='float32').reshape([24]),
            paddle.to_tensor([0.15213599801063538, 0.31396403908729553, 0.20521624386310577, 0.1555936634540558, 0.14651009440422058, 0.43344083428382874, 0.012373475357890129, 0.36303985118865967, 0.14839093387126923, 0.2390165477991104, 0.23089049756526947, 0.4434621334075928, 0.10850504785776138, 0.273019939661026, 0.3946050703525543, 0.12647564709186554, 0.10748495906591415, 0.06568086892366409, 0.3324180543422699, 0.2828785181045532, 0.31260454654693604, 0.30748334527015686, 0.027964316308498383, 0.21620088815689087], dtype='float32').reshape([24]),
            paddle.to_tensor([0.22570423781871796, 0.09912995249032974, 0.020853765308856964, 0.1886056363582611, 0.48273658752441406, 0.4383401870727539, 0.020601846277713776, 0.3728061318397522, 0.06005028262734413, 0.46437498927116394, 0.017970874905586243, 0.009594419039785862, 0.3236028552055359, 0.2388138771057129, 0.09337825328111649, 0.357793390750885, 0.4317243993282318, 0.14401236176490784, 0.3327144980430603, 0.0398256778717041, 0.22856442630290985, 0.48764193058013916, 0.17068231105804443, 0.2637113034725189], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3904822463b53f77feb5e056e8c26dd3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f47c917c1659a71dfaf430b026fbff86(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 128, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c431a9ce1693966f1bcd61bfc17ed371(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 16, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b29ea2b6222e058948b06e0d9f7568be(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 30, 30], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_988b561adb8a35dd3060063a4f575a69(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_47962a888ed9c9e4da9a56276d7c5a87(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 256, 512], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dc85c786005a73cc352e493e879a022d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 120, 120], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3806486427783966, 0.035763468593358994, 0.09687500447034836, 0.44736790657043457, 0.12468233704566956, 0.44144514203071594, 0.1633075475692749, 0.27629557251930237, 0.0045157563872635365, 0.37653619050979614, 0.026555804535746574, 0.06254518777132034, 0.22970491647720337, 0.285728394985199, 0.3875277638435364, 0.3496714234352112, 0.24884232878684998, 0.26406869292259216, 0.24219655990600586, 0.25968581438064575, 0.1685090959072113, 0.12287866324186325, 0.43964898586273193, 0.11284879595041275], dtype='float32').reshape([24]),
            paddle.to_tensor([0.09291230887174606, 0.4886852502822876, 0.26723381876945496, 0.4333195984363556, 0.30605292320251465, 0.2541435658931732, 0.06817983835935593, 0.22020506858825684, 0.37759384512901306, 0.27736178040504456, 0.099095419049263, 0.3769158124923706, 0.0030463363509625196, 0.42308321595191956, 0.11052343994379044, 0.2750775218009949, 0.04613883048295975, 0.3220624029636383, 0.03996923565864563, 0.1139918640255928, 0.29460614919662476, 0.34726738929748535, 0.46297019720077515, 0.1676826775074005], dtype='float32').reshape([24]),
            paddle.to_tensor([0.025826042518019676, 0.045583076775074005, 0.17880001664161682, 0.05326184257864952, 0.14821399748325348, 0.3299510180950165, 0.26422229409217834, 0.4617007076740265, 0.28641965985298157, 0.27292290329933167, 0.47429004311561584, 0.24183392524719238, 0.3349969685077667, 0.21029958128929138, 0.050797078758478165, 0.047345682978630066, 0.02358410693705082, 0.491783082485199, 0.06974667310714722, 0.1492234319448471, 0.06019771844148636, 0.3644930422306061, 0.16805380582809448, 0.27786949276924133], dtype='float32').reshape([24]),
            paddle.to_tensor([0.38843676447868347, 0.13620774447917938, 0.3642178177833557, 0.04148125648498535, 0.2869589328765869, 0.2848176956176758, 0.1140132024884224, 0.10490810126066208, 0.3734930753707886, 0.22204816341400146, 0.49100950360298157, 0.4186002314090729, 0.43522781133651733, 0.015193245373666286, 0.0010611072648316622, 0.41828760504722595, 0.12656274437904358, 0.282452255487442, 0.2709956765174866, 0.368102103471756, 0.013611029833555222, 0.01262175664305687, 0.42174232006073, 0.428712397813797], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ed46b045ee55de299c9c135129b64b39(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8acbbc38cc7733a8620765882fdf84f8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_08dd6f268893117bbedcc88c870d20ce(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 240, 240], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9f0737146fbd5240b505717225857659(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3186776041984558, 0.26247966289520264, 0.20377296209335327, 0.08720751106739044, 0.18510988354682922, 0.3816817104816437, 0.24075335264205933, 0.3475191295146942, 0.07432132214307785, 0.49240273237228394, 0.22108034789562225, 0.22596225142478943, 0.4908309280872345, 0.3728961944580078, 0.4626716673374176, 0.005694643594324589, 0.1772352010011673, 0.06611883640289307, 0.4324798882007599, 0.2970556318759918], dtype='float32').reshape([20]),
            paddle.to_tensor([0.01522422581911087, 0.03092225082218647, 0.0016113552264869213, 0.28488337993621826, 0.3339008688926697, 0.21403548121452332, 0.13461588323116302, 0.2878456115722656, 0.2156180739402771, 0.1994180530309677, 0.37597736716270447, 0.09051264822483063, 0.26700544357299805, 0.33554473519325256, 0.1213783323764801, 0.37941819429397583, 0.34381988644599915, 0.23901116847991943, 0.029744060710072517, 0.29566818475723267], dtype='float32').reshape([20]),
            paddle.to_tensor([0.05386960878968239, 0.43924474716186523, 0.18014656007289886, 0.07663712650537491, 0.4757179617881775, 0.3501715660095215, 0.19977253675460815, 0.31332358717918396, 0.29452770948410034, 0.41399329900741577, 0.3089030086994171, 0.3499290645122528, 0.09168264269828796, 0.3898768126964569, 0.2449720799922943, 0.17991195619106293, 0.15894122421741486, 0.13719895482063293, 0.3602083921432495, 0.20280960202217102], dtype='float32').reshape([20]),
            paddle.to_tensor([0.2686845362186432, 0.45897176861763, 0.19823916256427765, 0.14290334284305573, 0.3575899004936218, 0.29354217648506165, 0.37512966990470886, 0.31022214889526367, 0.40563008189201355, 0.14506325125694275, 0.2878458797931671, 0.2514529228210449, 0.47588256001472473, 0.45755788683891296, 0.21865035593509674, 0.06758655607700348, 0.23183484375476837, 0.09475602954626083, 0.1956833153963089, 0.3799203336238861], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3360ac0014a3d639f4ee7f85eccccf14(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_161f00f11c95f43425b39801522b4043
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_126bf522f75e4fe38558a53112c16653(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e9ff71d4fa08beb5b93ed1cbf22ebb13(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 1728, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1728], dtype='float32', min=0, max=0.5),
            paddle.uniform([1728], dtype='float32', min=0, max=0.5),
            paddle.uniform([1728], dtype='float32', min=0, max=0.5),
            paddle.uniform([1728], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b079cac104410bd899409d5e2d350712(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cc02c8c3efd5a35efde0ac74faa9909b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f10cc7a2ad3ca91e3e399bd523675a7c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_780ebb389ce66a7ec0cb3043aeaee0e4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 480, 480], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.17760048806667328, 0.16558881103992462, 0.014332612045109272, 0.0865204706788063, 0.02436278946697712, 0.2992015779018402, 0.15600769221782684, 0.07663706690073013], dtype='float32').reshape([8]),
            paddle.to_tensor([0.3180259168148041, 0.2912239730358124, 0.06499075144529343, 0.47715523838996887, 0.2630048394203186, 0.1481064110994339, 0.09710544347763062, 0.19247359037399292], dtype='float32').reshape([8]),
            paddle.to_tensor([0.2631453573703766, 0.024827174842357635, 0.27558326721191406, 0.3583705723285675, 0.23689740896224976, 0.3217276632785797, 0.17132015526294708, 0.090987429022789], dtype='float32').reshape([8]),
            paddle.to_tensor([0.2369135171175003, 0.07652095705270767, 0.22820819914340973, 0.2734222114086151, 0.08063525706529617, 0.46153146028518677, 0.06101979687809944, 0.1955530047416687], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_12d3e2c9e8e8cd6a6633b3ae3d793639(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b5492d07b0002ece97691ae48c54c091(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 512], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.021702105179429054, 0.15953606367111206, 0.02393326349556446, 0.33385804295539856, 0.22663956880569458, 0.4803045094013214, 0.27105268836021423, 0.2558581829071045, 0.24518975615501404, 0.1085892915725708, 0.32034948468208313, 0.07692644000053406, 0.1378457248210907, 0.33750078082084656, 0.31253790855407715, 0.3002321422100067], dtype='float32').reshape([16]),
            paddle.to_tensor([0.12102213501930237, 0.006692485883831978, 0.0985613614320755, 0.38619858026504517, 0.2129538357257843, 0.499460369348526, 0.10206157714128494, 0.027583537623286247, 0.0592033825814724, 0.058473050594329834, 0.018370285630226135, 0.04917367547750473, 0.443875789642334, 0.04882962256669998, 0.04693009331822395, 0.3505316972732544], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4435562193393707, 0.37526944279670715, 0.17933936417102814, 0.14035369455814362, 0.18850956857204437, 0.3422357439994812, 0.2980184555053711, 0.014590704813599586, 0.3472730815410614, 0.47739580273628235, 0.028299899771809578, 0.0811145082116127, 0.21115413308143616, 0.0920393094420433, 0.1354130208492279, 0.46991249918937683], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4206344187259674, 0.4761955738067627, 0.33422648906707764, 0.22748905420303345, 0.46617430448532104, 0.2276952713727951, 0.44207561016082764, 0.02925017848610878, 0.139873668551445, 0.22320210933685303, 0.3346427083015442, 0.19795341789722443, 0.44974568486213684, 0.2000378519296646, 0.32479944825172424, 0.2657388150691986], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_322a15fa896d43abcff09e60a7e49879(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bd9b2087f1cfcbd008c80257c0da767c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2679781913757324, 0.3228271007537842, 0.07531953603029251, 0.07024170458316803, 0.32507839798927307, 0.25462040305137634, 0.31905537843704224, 0.48334458470344543, 0.2269769310951233, 0.37739449739456177, 0.2967555820941925, 0.3414953052997589, 0.49138450622558594, 0.353643536567688, 0.028652938082814217, 0.2728622257709503, 0.053784098476171494, 0.0229191891849041, 0.13483580946922302, 0.1366039216518402, 0.18846876919269562, 0.4161904752254486, 0.3890421390533447, 0.17674891650676727], dtype='float32').reshape([24]),
            paddle.to_tensor([0.18838855624198914, 0.02041500248014927, 0.3584669232368469, 0.3657403886318207, 0.2511354386806488, 0.24652697145938873, 0.15802942216396332, 0.4264752268791199, 0.27576595544815063, 0.33580338954925537, 0.3043631315231323, 0.010746520943939686, 0.4580937623977661, 0.24227042496204376, 0.44928663969039917, 0.26589542627334595, 0.09842412173748016, 0.4867129325866699, 0.31185534596443176, 0.26068177819252014, 0.2796756625175476, 0.23178830742835999, 0.09322960674762726, 0.49427855014801025], dtype='float32').reshape([24]),
            paddle.to_tensor([0.46403273940086365, 0.08008072525262833, 0.18318359553813934, 0.14144152402877808, 0.412349671125412, 0.3689609467983246, 0.21041926741600037, 0.13928058743476868, 0.1531754583120346, 0.06589165329933167, 0.040946073830127716, 0.3879829943180084, 0.4636225700378418, 0.15732821822166443, 0.013949128799140453, 0.4493958055973053, 0.15697163343429565, 0.06828442960977554, 0.33883821964263916, 0.04744488745927811, 0.08225442469120026, 0.4739632308483124, 0.10368023812770844, 0.07012981176376343], dtype='float32').reshape([24]),
            paddle.to_tensor([0.32783883810043335, 0.01887565292418003, 0.15580545365810394, 0.20358748733997345, 0.31428197026252747, 0.23101867735385895, 0.11006869375705719, 0.28136730194091797, 0.21939732134342194, 0.4929039776325226, 0.034493640065193176, 0.13364392518997192, 0.23980802297592163, 0.38667333126068115, 0.23496030271053314, 0.3689151406288147, 0.4804844558238983, 0.11619863659143448, 0.23645497858524323, 0.05195761099457741, 0.0207035131752491, 0.11188924312591553, 0.01852991431951523, 0.30329984426498413], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_738861bafbad86a2b12f3a9af7a0a33e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 10, 10], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4efe1d87fbb8ec2202d5b19c94b0d54b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 1536, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a819be9316e0558d27e5e6f87aeb23e1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f321186e63628062642eda08cc0cf7cc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5020c08c130b7d935ddc3fe404cd5b0d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 14, 25], dtype='float16', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9321d76336ccaa2a23d16ed29307a6c6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.18053586781024933, 0.2711268365383148, 0.41032111644744873, 0.4504948556423187, 0.31382694840431213, 0.4759393036365509, 0.23605859279632568, 0.04341389238834381, 0.03240778669714928, 0.3240393102169037, 0.47580254077911377, 0.3587186932563782, 0.41918641328811646, 0.001991815399378538, 0.485141783952713, 0.2707047164440155, 0.2384471893310547, 0.2722436487674713, 0.4233212471008301, 0.352256715297699], dtype='float32').reshape([20]),
            paddle.to_tensor([0.08555074036121368, 0.4668639600276947, 0.26552653312683105, 0.4821494519710541, 0.23684290051460266, 0.34100431203842163, 0.2765006422996521, 0.08318771421909332, 0.3691909611225128, 0.050130248069763184, 0.19408217072486877, 0.08161944150924683, 0.11142763495445251, 0.007704844698309898, 0.3923785984516144, 0.0768800750374794, 0.3846435844898224, 0.3588591516017914, 0.3068687617778778, 0.3005125820636749], dtype='float32').reshape([20]),
            paddle.to_tensor([0.461387574672699, 0.4629506468772888, 0.228630930185318, 0.09069351851940155, 0.1581968069076538, 0.3153361976146698, 0.337420791387558, 0.343149870634079, 0.4604266881942749, 0.24109987914562225, 0.4697089195251465, 0.15573352575302124, 0.45754697918891907, 0.2900945246219635, 0.20668260753154755, 0.08907400816679001, 0.34788623452186584, 0.07274562120437622, 0.3129185438156128, 0.2352868765592575], dtype='float32').reshape([20]),
            paddle.to_tensor([0.13234995305538177, 0.06758160144090652, 0.4790821969509125, 0.06818579882383347, 0.15816333889961243, 0.4038059711456299, 0.1674276888370514, 0.12105729430913925, 0.28533655405044556, 0.42529088258743286, 0.3083578944206238, 0.4715566635131836, 0.43153101205825806, 0.27038872241973877, 0.23609833419322968, 0.1899702250957489, 0.3026716709136963, 0.49925607442855835, 0.40816420316696167, 0.4354361891746521], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_daf88bb992f01c54211e2c9d5b6b2a98(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 30, 30], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_72e0034e35f0322a670237bf61e4ee92(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7388f2107b10fc13b2c2aa8ea1516732(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_f563867af680a9ee1502aac2665534b4
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 4], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9dd5cc103fb6e11e232ad30359f03eed(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4272698163986206, 0.11042730510234833, 0.42183417081832886, 0.30107417702674866, 0.0045098187401890755, 0.3732704520225525, 0.4782586693763733, 0.41399112343788147, 0.3465716540813446, 0.3443501889705658, 0.24579519033432007, 0.495233416557312, 0.3016095459461212, 0.16848279535770416, 0.07009151577949524, 0.13819672167301178, 0.3531048893928528, 0.36271777749061584, 0.42327073216438293, 0.045459531247615814, 0.37494096159935, 0.007571365684270859, 0.1894688904285431, 0.19121624529361725, 0.267265260219574, 0.25642409920692444, 0.3847566246986389, 0.012603871524333954, 0.2571825683116913, 0.10673012584447861], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3037570118904114, 0.4612588584423065, 0.49199220538139343, 0.37572821974754333, 0.21369569003582, 0.0023910519666969776, 0.006216942798346281, 0.251067578792572, 0.06579868495464325, 0.10537435114383698, 0.13365672528743744, 0.41934147477149963, 0.04610578343272209, 0.026243407279253006, 0.3831390142440796, 0.050739336758852005, 0.32085034251213074, 0.2527316212654114, 0.07410170137882233, 0.11439131200313568, 0.3908839225769043, 0.47213199734687805, 0.09063132852315903, 0.49667689204216003, 0.3965282738208771, 0.3317103683948517, 0.36510565876960754, 0.17909900844097137, 0.11836244910955429, 0.46104761958122253], dtype='float32').reshape([30]),
            paddle.to_tensor([0.11778979748487473, 0.06890342384576797, 0.040542859584093094, 0.4056749641895294, 0.14050526916980743, 0.031216561794281006, 0.1380605846643448, 0.2315051257610321, 0.4061746597290039, 0.06337602436542511, 0.12852683663368225, 0.019631538540124893, 0.27811935544013977, 0.1847711205482483, 0.4279879927635193, 0.006293729413300753, 0.3132506012916565, 0.005200518295168877, 0.1576261967420578, 0.42755937576293945, 0.3893316984176636, 0.08188237249851227, 0.34583115577697754, 0.1475406438112259, 0.32051146030426025, 0.4014955461025238, 0.4352864921092987, 0.26900947093963623, 0.2598712146282196, 0.20740969479084015], dtype='float32').reshape([30]),
            paddle.to_tensor([0.17342431843280792, 0.2535570561885834, 0.423626571893692, 0.16720980405807495, 0.2903846502304077, 0.040900517255067825, 0.08390253037214279, 0.3943742513656616, 0.12835130095481873, 0.1254471093416214, 0.09803715348243713, 0.04107103869318962, 0.26945260167121887, 0.25989827513694763, 0.33219972252845764, 0.4643735885620117, 0.24710921943187714, 0.2583807408809662, 0.14917360246181488, 0.49866464734077454, 0.40016838908195496, 0.11665403097867966, 0.3672768771648407, 0.37790077924728394, 0.37891337275505066, 0.40618085861206055, 0.19526877999305725, 0.29635924100875854, 0.06002368777990341, 0.2972593903541565], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d621dd12990d63b9ffe3a714b00909ef(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.21185243129730225, 0.15754514932632446, 0.053071122616529465, 0.09702728688716888, 0.03133627772331238, 0.045245248824357986, 0.2539634108543396, 0.43387970328330994, 0.032155152410268784, 0.34542015194892883, 0.13615477085113525, 0.41869449615478516, 0.287560373544693, 0.40982282161712646, 0.24795903265476227, 0.023062966763973236, 0.47654223442077637, 0.20831646025180817, 0.3037184178829193, 0.33341944217681885], dtype='float32').reshape([20]),
            paddle.to_tensor([0.19747501611709595, 0.027591627091169357, 0.2114463448524475, 0.1622532606124878, 0.06219096481800079, 0.13810081779956818, 0.4738254249095917, 0.32298412919044495, 0.48505377769470215, 0.2618485987186432, 0.4641872048377991, 0.46856558322906494, 0.34404313564300537, 0.2672926187515259, 0.4098948836326599, 0.19242031872272491, 0.31805166602134705, 0.20603953301906586, 0.2641716003417969, 0.03896823897957802], dtype='float32').reshape([20]),
            paddle.to_tensor([0.042126234620809555, 0.11651702225208282, 0.027863383293151855, 0.2475440800189972, 0.3455018997192383, 0.023989476263523102, 0.3481467366218567, 0.17632119357585907, 0.487700492143631, 0.4850638806819916, 0.32188960909843445, 0.29655948281288147, 0.17744866013526917, 0.3448841869831085, 0.22093848884105682, 0.29265767335891724, 0.08462394028902054, 0.00482477992773056, 0.20665161311626434, 0.08515576273202896], dtype='float32').reshape([20]),
            paddle.to_tensor([0.37299829721450806, 0.3352191150188446, 0.4682852029800415, 0.2640881836414337, 0.031183185055851936, 0.3328147828578949, 0.28294456005096436, 0.33952197432518005, 0.3365558087825775, 0.13783961534500122, 0.17001314461231232, 0.03779838606715202, 0.4698904752731323, 0.4912704825401306, 0.4334421753883362, 0.24895349144935608, 0.4503500461578369, 0.30316075682640076, 0.3633083999156952, 0.34466466307640076], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_88c0081500c63b5fb230e4c4429548a9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.16598619520664215, 0.04392825439572334, 0.11208423227071762, 0.41495707631111145, 0.41831955313682556, 0.2948123514652252, 0.37550655007362366, 0.28453779220581055, 0.2240464836359024, 0.4522765874862671, 0.3215349018573761, 0.4816500246524811, 0.3553529679775238, 0.06496276706457138, 0.2599423825740814, 0.009208334609866142], dtype='float32').reshape([16]),
            paddle.to_tensor([0.09982825815677643, 0.22090521454811096, 0.48070335388183594, 0.10391530394554138, 0.10679344087839127, 0.3179246485233307, 0.33472365140914917, 0.2655676007270813, 0.27087798714637756, 0.25361618399620056, 0.3745121359825134, 0.4503074884414673, 0.4841802716255188, 0.37999415397644043, 0.49994125962257385, 0.4060586988925934], dtype='float32').reshape([16]),
            paddle.to_tensor([0.25188153982162476, 0.12147356569766998, 0.05747996270656586, 0.40818676352500916, 0.48252078890800476, 0.3975154459476471, 0.13138224184513092, 0.3578394055366516, 0.4481849670410156, 0.35190388560295105, 0.46783018112182617, 0.4804723858833313, 0.3188568353652954, 0.061978600919246674, 0.1036832183599472, 0.21222594380378723], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2564135491847992, 0.10827820003032684, 0.08072450011968613, 0.42332857847213745, 0.3489128351211548, 0.2990979552268982, 0.3712642788887024, 0.36148127913475037, 0.32303565740585327, 0.49208566546440125, 0.22361914813518524, 0.060446158051490784, 0.497230589389801, 0.22134846448898315, 0.2301419973373413, 0.1018645390868187], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4ba6f50e6f7a0c587f5912d2fdfdb1d6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_190f5489288e3db92fdca6b9490f8160
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 4, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bd5b7b56fc08dd0a17fe1b7b150b30f0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 128, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_21abc5585e1efdf6e269b55ee37bc75c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_106ea5efa5d786782be175dbe56c0184(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 118, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([118], dtype='float32', min=0, max=0.5),
            paddle.uniform([118], dtype='float32', min=0, max=0.5),
            paddle.uniform([118], dtype='float32', min=0, max=0.5),
            paddle.uniform([118], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ecab748cf1b74cc594f17655d6d5429e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 28, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7e620b51e03f384e556d0385df0ba57b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e4da1679983335f4940350b7dff61a70(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 8, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_274f1f30b4b66f8cd620e8196173ddf7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_13025b164297d9226f9641b7f0715c76
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 10, 10], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e23889cae6e8648e1c29aee6e1d6a94a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 12, 12], dtype='float16', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_47c6d8781bfb8090c972bb9419c4c60f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8bdbe95900e56db03bcd52d68da37a38(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b2e1676740b0997f88108be0583a79d4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 31, 31], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_54cb6ed8e8479a1b20265a961fb848e1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_71adf679806cab2a22b147a9b47d0cbc
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f19337e8cffe67237b40e94be7a3368c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.12468212842941284, 0.3659191429615021, 0.2640036344528198, 0.39326146245002747, 0.07061648368835449, 0.16785399615764618, 0.3558252453804016, 0.23369890451431274, 0.3936498761177063, 0.17216500639915466, 0.10969798266887665, 0.3981751799583435, 0.48817601799964905, 0.004619269166141748, 0.47745272517204285, 0.2824893891811371, 0.2829035818576813, 0.1490665078163147], dtype='float32').reshape([18]),
            paddle.to_tensor([0.21176834404468536, 0.06995651125907898, 0.30639997124671936, 0.03259749710559845, 0.4693900942802429, 0.32588478922843933, 0.3839183747768402, 0.05200249329209328, 0.027943521738052368, 0.3122197389602661, 0.23103195428848267, 0.26260969042778015, 0.42563849687576294, 0.4905800521373749, 0.12425584346055984, 0.12957863509655, 0.06886965036392212, 0.01874208077788353], dtype='float32').reshape([18]),
            paddle.to_tensor([0.42936739325523376, 0.29963192343711853, 0.2177073210477829, 0.0338701456785202, 0.4941456913948059, 0.18618452548980713, 0.06973797082901001, 0.10359442234039307, 0.41091716289520264, 0.2376152127981186, 0.4687356948852539, 0.06945102661848068, 0.38794058561325073, 0.09134119004011154, 0.42406848073005676, 0.36014220118522644, 0.3246407210826874, 0.19864359498023987], dtype='float32').reshape([18]),
            paddle.to_tensor([0.393131285905838, 0.3800446689128876, 0.1731112152338028, 0.4486076831817627, 0.4388609826564789, 0.10086648166179657, 0.4819706082344055, 0.4783812165260315, 0.055144693702459335, 0.45943671464920044, 0.36921772360801697, 0.044777993112802505, 0.17059051990509033, 0.015192716382443905, 0.2767358422279358, 0.4851576089859009, 0.4388939142227173, 0.3810936212539673], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ef549bceb0fbcbf9fe94cf8439f77d48(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_13025b164297d9226f9641b7f0715c76
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 75, 75], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7f25d2f03681a9dafc91ad3d4d26910c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5cf532d046e08b08bdcc84082345f93f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ec3137061f63e880721a620fa1606bb2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_d176968eb532ab7742141be9f49773ee
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2fc6749590ada870d90156de818e22c0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fb1dbbc3bc92ca303d799fa53ad6aa72(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 8, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e0527c986219dae722afec253f73b22c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 82, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e3a3debf33d191da9de58b283b03d349(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 17, 17], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_03228614cffd529dd0e1392d10663dda(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4f63e7d2b4111992275a65607bf8c070(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.36778122186660767, 0.2543427348136902, 0.03004700317978859, 0.3311799168586731, 0.0014156331308186054, 0.42354124784469604, 0.32007691264152527, 0.13397035002708435, 0.008487903513014317, 0.4334776997566223, 0.2640470862388611, 0.4401566684246063, 0.3182477056980133, 0.45755475759506226, 0.30963894724845886, 0.27920541167259216, 0.2208453118801117, 0.35619238018989563, 0.4444119930267334, 0.3952106535434723], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4945557415485382, 0.25795140862464905, 0.02889176644384861, 0.05337733402848244, 0.4727558195590973, 0.2412288784980774, 0.44407665729522705, 0.4087560176849365, 0.33280032873153687, 0.10162852704524994, 0.05595129355788231, 0.4949607253074646, 0.17364691197872162, 0.05857834219932556, 0.29472020268440247, 0.05353251099586487, 0.4903413951396942, 0.05818304046988487, 0.33816736936569214, 0.18586869537830353], dtype='float32').reshape([20]),
            paddle.to_tensor([0.2514496445655823, 0.24929368495941162, 0.4717809855937958, 0.4674318730831146, 0.42778679728507996, 0.2974417209625244, 0.1506505161523819, 0.40895482897758484, 0.29470330476760864, 0.47479134798049927, 0.052720170468091965, 0.13528728485107422, 0.30209288001060486, 0.171421080827713, 0.1255788952112198, 0.20489218831062317, 0.34677305817604065, 0.08916061371564865, 0.21099083125591278, 0.33989211916923523], dtype='float32').reshape([20]),
            paddle.to_tensor([0.31892579793930054, 0.13246385753154755, 0.40055206418037415, 0.30586162209510803, 0.4899570941925049, 0.048471514135599136, 0.28783899545669556, 0.29800915718078613, 0.492760568857193, 0.09659949690103531, 0.03236084431409836, 0.1666504442691803, 0.18620577454566956, 0.39325472712516785, 0.21117126941680908, 0.18883273005485535, 0.13887275755405426, 0.09734299778938293, 0.015004012733697891, 0.20402534306049347], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_63bd2b73502bacdcc019d97aa7c29fa0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 1248, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1248], dtype='float32', min=0, max=0.5),
            paddle.uniform([1248], dtype='float32', min=0, max=0.5),
            paddle.uniform([1248], dtype='float32', min=0, max=0.5),
            paddle.uniform([1248], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7782eda9f530c7b7469cbeec07631366(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b6cb930d969b75a142c760ba524f57de(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3251159191131592, 0.044733718037605286, 0.3065713942050934, 0.28659677505493164, 0.4080398380756378, 0.1513465791940689, 0.177688866853714, 0.2778531014919281, 0.496244877576828, 0.12881746888160706, 0.025487901642918587, 0.14786270260810852, 0.037782907485961914, 0.126045361161232, 0.3462452292442322, 0.044000573456287384, 0.1528850793838501, 0.35421645641326904, 0.284567654132843, 0.2508748471736908, 0.0958629921078682, 0.017535990104079247, 0.2875836491584778, 0.348987340927124, 0.3545544445514679, 0.2951711118221283, 0.1262914538383484, 0.2472284585237503, 0.3801446557044983, 0.4954836070537567], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2224946916103363, 0.44492897391319275, 0.0978798121213913, 0.44309425354003906, 0.31735658645629883, 0.32887881994247437, 0.379914790391922, 0.13518258929252625, 0.49479979276657104, 0.2503494918346405, 0.4704859256744385, 0.40299031138420105, 0.08363011479377747, 0.3364420533180237, 0.06791798025369644, 0.41290032863616943, 0.07790640741586685, 0.22410419583320618, 0.2227293998003006, 0.07371918112039566, 0.044987037777900696, 0.02483939751982689, 0.19990229606628418, 0.3021213710308075, 0.4626353979110718, 0.0385078601539135, 0.33166199922561646, 0.1296851485967636, 0.47356268763542175, 0.4151732623577118], dtype='float32').reshape([30]),
            paddle.to_tensor([0.16626079380512238, 0.3150506317615509, 0.4585789740085602, 0.43888017535209656, 0.11367934942245483, 0.27137550711631775, 0.2960670292377472, 0.1326693743467331, 0.04756029695272446, 0.3454110026359558, 0.2138461321592331, 0.002796455752104521, 0.0115982536226511, 0.31097352504730225, 0.3026123344898224, 0.14960026741027832, 0.31538069248199463, 0.40801113843917847, 0.04621358588337898, 0.2808460295200348, 0.2106931060552597, 0.1384984403848648, 0.17651933431625366, 0.2839842140674591, 0.3735434114933014, 0.31725940108299255, 0.4196540117263794, 0.15105487406253815, 0.32169127464294434, 0.040086597204208374], dtype='float32').reshape([30]),
            paddle.to_tensor([0.17268696427345276, 0.3281269371509552, 0.16815148293972015, 0.3109768033027649, 0.019003689289093018, 0.24160826206207275, 0.25991418957710266, 0.23876751959323883, 0.47504398226737976, 0.311753511428833, 0.10867849737405777, 0.1767490953207016, 0.1446678638458252, 0.09301766753196716, 0.27234408259391785, 0.3906380534172058, 0.2916807532310486, 0.4446617066860199, 0.29882535338401794, 0.4608089327812195, 0.32306167483329773, 0.05074260011315346, 0.11934109032154083, 0.41729018092155457, 0.1702033132314682, 0.21286191046237946, 0.15811589360237122, 0.02392132394015789, 0.32504042983055115, 0.2278834581375122], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4e67711fe75e51e7d4741c0827dcc780(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 128, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0b640e125161897709646377529eea35(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.35119566321372986, 0.04077789932489395, 0.46548810601234436, 0.23948338627815247, 0.31676724553108215, 0.01620413362979889, 0.3484744727611542, 0.43974432349205017, 0.10963902622461319, 0.42839744687080383, 0.11622386425733566, 0.11994631588459015, 0.005570366978645325, 0.08340761065483093, 0.2417035549879074, 0.2904212176799774, 0.10180321335792542, 0.38881754875183105, 0.3860135078430176, 0.38306570053100586], dtype='float32').reshape([20]),
            paddle.to_tensor([0.33569297194480896, 0.40943267941474915, 0.2356373518705368, 0.4892774820327759, 0.44525977969169617, 0.09252933412790298, 0.18896400928497314, 0.3728419542312622, 0.24098242819309235, 0.3990333676338196, 0.22264477610588074, 0.27913928031921387, 0.4445667266845703, 0.30672961473464966, 0.4017661511898041, 0.40025341510772705, 0.4118990898132324, 0.3836119472980499, 0.4316333532333374, 0.2449481338262558], dtype='float32').reshape([20]),
            paddle.to_tensor([0.40315547585487366, 0.18306700885295868, 0.4165183901786804, 0.12469534575939178, 0.18721745908260345, 0.37863287329673767, 0.31454983353614807, 0.3563874065876007, 0.32937535643577576, 0.13830210268497467, 0.24914811551570892, 0.47240665555000305, 0.28195300698280334, 0.36164671182632446, 0.15200990438461304, 0.48898568749427795, 0.40862134099006653, 0.10869961231946945, 0.21346043050289154, 0.3568553924560547], dtype='float32').reshape([20]),
            paddle.to_tensor([0.05444267764687538, 0.20366103947162628, 0.3730103671550751, 0.07603328675031662, 0.30153506994247437, 0.33343103528022766, 0.3627329170703888, 0.23786410689353943, 0.37321221828460693, 0.23843885958194733, 0.031040344387292862, 0.4697454273700714, 0.32292765378952026, 0.2546060085296631, 0.10084207355976105, 0.3897371292114258, 0.22001945972442627, 0.09261675924062729, 0.41812554001808167, 0.494372695684433], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_005c4968d864f6025a6aafbd80a359ad(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.13732337951660156, 0.3936956822872162, 0.2919529676437378, 0.05987349897623062, 0.27553820610046387, 0.48162898421287537, 0.2501356303691864, 0.4926702678203583, 0.18690799176692963, 0.20493152737617493, 0.12154962867498398, 0.23083868622779846, 0.08742614090442657, 0.37261664867401123, 0.2580137550830841, 0.09872078150510788, 0.18250587582588196, 0.30941715836524963, 0.23210978507995605, 0.4670671820640564, 0.22835738956928253, 0.46912819147109985, 0.43923643231391907, 0.1708884835243225, 0.11476892232894897, 0.2148391157388687, 0.013402396813035011, 0.11742059141397476, 0.055805761367082596, 0.11990999430418015], dtype='float32').reshape([30]),
            paddle.to_tensor([0.28464263677597046, 0.2999962568283081, 0.19720257818698883, 0.011776512488722801, 0.4525684118270874, 0.39422714710235596, 0.26724928617477417, 0.16925708949565887, 0.47172343730926514, 0.10739873349666595, 0.2461390346288681, 0.4284989535808563, 0.08756878972053528, 0.44287270307540894, 0.1872584968805313, 0.01577725075185299, 0.18534217774868011, 0.17324189841747284, 0.39854878187179565, 0.13939964771270752, 0.3658403754234314, 0.3109774589538574, 0.4629514515399933, 0.10143040120601654, 0.4276840090751648, 0.3585819602012634, 0.1988488882780075, 0.43369677662849426, 0.4795473515987396, 0.37121108174324036], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2310730665922165, 0.23463192582130432, 0.06125863268971443, 0.06400610506534576, 0.00932965986430645, 0.31706252694129944, 0.39697256684303284, 0.49628111720085144, 0.13311254978179932, 0.13695406913757324, 0.06125916168093681, 0.16900113224983215, 0.4741879999637604, 0.0685979500412941, 0.21251021325588226, 0.1623069792985916, 0.3622010350227356, 0.4669138193130493, 0.19183997809886932, 0.22274158895015717, 0.10525365173816681, 0.425596684217453, 0.4742248058319092, 0.004198831971734762, 0.4178279936313629, 0.37558335065841675, 0.07223936915397644, 0.18285205960273743, 0.0952305942773819, 0.07635164260864258], dtype='float32').reshape([30]),
            paddle.to_tensor([0.34804767370224, 0.03388707712292671, 0.03450607508420944, 0.011477826163172722, 0.4155445396900177, 0.4581313729286194, 0.0626172125339508, 0.29642757773399353, 0.4758254885673523, 0.4345848560333252, 0.3838306963443756, 0.11176134645938873, 0.34397611021995544, 0.40436869859695435, 0.39379146695137024, 0.13215599954128265, 0.23473283648490906, 0.1816590428352356, 0.26518258452415466, 0.06904280185699463, 0.020771030336618423, 0.22690795361995697, 0.23214928805828094, 0.4603258967399597, 0.1432323455810547, 0.14278708398342133, 0.2709932327270508, 0.43007802963256836, 0.19617842137813568, 0.09898019582033157], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_37ccfa395fdb5f0bdb162e68ac856bd7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.16316094994544983, 0.29734930396080017, 0.48048654198646545, 0.4200817346572876, 0.07707206904888153, 0.4045477509498596, 0.20081745088100433, 0.2888506352901459, 0.2876424491405487, 0.11518734693527222, 0.11016643792390823, 0.223116934299469, 0.3276385962963104, 0.25014689564704895, 0.4561595618724823, 0.1396932750940323, 0.038762591779232025, 0.1890871673822403, 0.3414885997772217, 0.18995030224323273, 0.3787901997566223, 0.34419071674346924, 0.37218165397644043, 0.44106000661849976], dtype='float32').reshape([24]),
            paddle.to_tensor([0.331996887922287, 0.3788010776042938, 0.22394371032714844, 0.08295591920614243, 0.31813758611679077, 0.22653111815452576, 0.005231021903455257, 0.42602190375328064, 0.4596017599105835, 0.09004092961549759, 0.26441001892089844, 0.4968629777431488, 0.0843629240989685, 0.1412523090839386, 0.1508951485157013, 0.29945534467697144, 0.40568578243255615, 0.07102224975824356, 0.019660379737615585, 0.13115990161895752, 0.3775499761104584, 0.46881574392318726, 0.24255242943763733, 0.18902285397052765], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4308832585811615, 0.1398068368434906, 0.06058292090892792, 0.01811443641781807, 0.46385303139686584, 0.11621254682540894, 0.4283115863800049, 0.3074212074279785, 0.34780335426330566, 0.08204558491706848, 0.315086305141449, 0.3917412757873535, 0.11794629693031311, 0.03393354266881943, 0.273290753364563, 0.22595633566379547, 0.27957674860954285, 0.016089659184217453, 0.4586077034473419, 0.3205999732017517, 0.10163984447717667, 0.30494967103004456, 0.17184513807296753, 0.4043320119380951], dtype='float32').reshape([24]),
            paddle.to_tensor([0.09432847797870636, 0.20407657325267792, 0.4745326638221741, 0.0013683014549314976, 0.12209625542163849, 0.46547892689704895, 0.34425419569015503, 0.3569582402706146, 0.30295222997665405, 0.41843727231025696, 0.1464552879333496, 0.4620498716831207, 0.4771196246147156, 0.46855518221855164, 0.09836865961551666, 0.013464831747114658, 0.27597588300704956, 0.3710341453552246, 0.13315479457378387, 0.4311448335647583, 0.06161663681268692, 0.3147031366825104, 0.24053820967674255, 0.2874942123889923], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_003f2f0358290fe1beae3435d92070e5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_13025b164297d9226f9641b7f0715c76
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d080dcc663c709e9ee86859a25b90907(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 240, 240], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9de6744bba2dabfc230dac851b8f3562(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 1, 26], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b8a00dbfc7ab104157c628fc27840566(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.35922759771347046, 0.0847187340259552, 0.3031674027442932, 0.08950483798980713, 0.2029666006565094, 0.013852633535861969, 0.0005880881217308342, 0.46677401661872864, 0.277660608291626, 0.1995021104812622, 0.34885361790657043, 0.4070892632007599, 0.01018683984875679, 0.012526474893093109, 0.2735103368759155, 0.28183436393737793, 0.14600040018558502, 0.0625329464673996, 0.4229930639266968, 0.4951404929161072, 0.30077287554740906, 0.10899671912193298, 0.1656768023967743, 0.2857661843299866], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4690901041030884, 0.14634524285793304, 0.2941364645957947, 0.016107428818941116, 0.48501279950141907, 0.2661013901233673, 0.4485693573951721, 0.09061197191476822, 0.15917839109897614, 0.07065179944038391, 0.35009777545928955, 0.07106982171535492, 0.05592206120491028, 0.2816942632198334, 0.07363126426935196, 0.495225191116333, 0.08853565901517868, 0.019867781549692154, 0.31004995107650757, 0.06284990906715393, 0.09563270211219788, 0.061056140810251236, 0.3597521483898163, 0.4420683979988098], dtype='float32').reshape([24]),
            paddle.to_tensor([0.414614200592041, 0.23612256348133087, 0.2581821382045746, 0.02431955188512802, 0.1958175003528595, 0.01338045671582222, 0.3086322546005249, 0.07299001514911652, 0.06477565318346024, 0.15606173872947693, 0.24459858238697052, 0.4644562602043152, 0.24562996625900269, 0.1642623096704483, 0.3309173583984375, 0.3241008520126343, 0.2520360052585602, 0.22220827639102936, 0.4984739422798157, 0.2509322464466095, 0.3343770503997803, 0.01757035218179226, 0.017784282565116882, 0.4216580390930176], dtype='float32').reshape([24]),
            paddle.to_tensor([0.27833789587020874, 0.2652271091938019, 0.31101691722869873, 0.27268362045288086, 0.08487294614315033, 0.40573352575302124, 0.23253118991851807, 0.3640115559101105, 0.2468777745962143, 0.2886181175708771, 0.4265492558479309, 0.16503022611141205, 0.02896394021809101, 0.44061169028282166, 0.22026842832565308, 0.0040125614032149315, 0.37581831216812134, 0.4918193221092224, 0.3400459885597229, 0.3201220631599426, 0.3742001950740814, 0.09313177317380905, 0.28352782130241394, 0.3373146951198578], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_225e4b0aafca45648a4181344bffa8e4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 8, 6], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0c80c243d8d59549762bf099bfdc4d5c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 132, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([132], dtype='float32', min=0, max=0.5),
            paddle.uniform([132], dtype='float32', min=0, max=0.5),
            paddle.uniform([132], dtype='float32', min=0, max=0.5),
            paddle.uniform([132], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_39ec77e94b81e98c76dd08ebfc60e770(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 88, 61, 61], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bbd48ead865a0fb895f2537aabed011b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0984336e4ed283da644c65a13fda1725(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 27, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.35700705647468567, 0.15656475722789764, 0.3245794475078583, 0.017956415191292763, 0.358944296836853, 0.3620586693286896, 0.16123521327972412, 0.43118923902511597, 0.15688800811767578, 0.10026875883340836, 0.03913355991244316, 0.28554272651672363, 0.13631509244441986, 0.012515652924776077, 0.22595594823360443, 0.25839492678642273, 0.25929465889930725, 0.37243887782096863, 0.3661503195762634, 0.03482012450695038, 0.279798299074173, 0.4662624001502991, 0.18862983584403992, 0.00437748059630394, 0.020527468994259834, 0.43247413635253906, 0.2416747361421585], dtype='float32').reshape([27]),
            paddle.to_tensor([0.150857612490654, 0.28668636083602905, 0.22517213225364685, 0.48730918765068054, 0.2806381285190582, 0.40316927433013916, 0.04753152281045914, 0.34680238366127014, 0.44704198837280273, 0.20430438220500946, 0.23844635486602783, 0.2433350384235382, 0.40597856044769287, 0.3825465440750122, 0.1770140677690506, 0.03178347647190094, 0.042912162840366364, 0.4454728066921234, 0.025828344747424126, 0.37122222781181335, 0.24018989503383636, 0.08542664349079132, 0.3105754554271698, 0.37055540084838867, 0.4938764274120331, 0.3785865902900696, 0.007746007293462753], dtype='float32').reshape([27]),
            paddle.to_tensor([0.48941725492477417, 0.19002386927604675, 0.35585901141166687, 0.40324217081069946, 0.19160792231559753, 0.035471111536026, 0.04219004139304161, 0.10140900313854218, 0.1747761368751526, 0.42322400212287903, 0.432923287153244, 0.21332888305187225, 0.09105008840560913, 4.0910381358116865e-05, 0.0403158999979496, 0.1980976164340973, 0.26337605714797974, 0.3414797782897949, 0.4137640595436096, 0.36654284596443176, 0.1278800219297409, 0.3932817280292511, 0.2096145749092102, 0.340465784072876, 0.3355647623538971, 0.3127899169921875, 0.14587630331516266], dtype='float32').reshape([27]),
            paddle.to_tensor([0.43568867444992065, 0.325041800737381, 0.09387095272541046, 0.13375972211360931, 0.20556005835533142, 0.05290258303284645, 0.20101256668567657, 0.23807573318481445, 0.010473910719156265, 0.20736508071422577, 0.18367300927639008, 0.14935100078582764, 0.1461312621831894, 0.13656505942344666, 0.420229971408844, 0.29304182529449463, 0.015712114050984383, 0.4576675593852997, 0.09882310777902603, 0.40782657265663147, 0.22826647758483887, 0.3992934823036194, 0.36072689294815063, 0.3588164150714874, 0.38044387102127075, 0.3955881893634796, 0.020369529724121094], dtype='float32').reshape([27]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b1dd8eda0a5f066e42f8c426b26c2e31(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3985e122044fceb9a45366e5c71aa114(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 144, 16, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1a13dfda2fc078c20a7bfcd198f3b873(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 256, 512], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_75abf43d8d8c880638ef4bfbc73bfbcd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 512, 1024], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.475342333316803, 0.013035351410508156, 0.4093072712421417, 0.2620989680290222, 0.17896735668182373, 0.4844193756580353, 0.04859365522861481, 0.36043781042099, 0.14855483174324036, 0.36464977264404297, 0.09030605107545853, 0.4704442322254181, 0.1686011105775833, 0.05736618861556053, 0.39656829833984375, 0.01659935899078846], dtype='float32').reshape([16]),
            paddle.to_tensor([0.40986302495002747, 0.40552929043769836, 0.04724440723657608, 0.49032410979270935, 0.36973705887794495, 0.23223333060741425, 0.4180510640144348, 0.3250437378883362, 0.4773520231246948, 0.3132112920284271, 0.1368769407272339, 0.24129743874073029, 0.09309516847133636, 0.3557901680469513, 0.44665056467056274, 0.12593893706798553], dtype='float32').reshape([16]),
            paddle.to_tensor([0.38624274730682373, 0.12297410517930984, 0.12202860414981842, 0.08668660372495651, 0.26165056228637695, 0.29231026768684387, 0.348996102809906, 0.14175574481487274, 0.11984319984912872, 0.030154993757605553, 0.22238358855247498, 0.07498611509799957, 0.4212714731693268, 0.3366958796977997, 0.27627941966056824, 0.27844229340553284], dtype='float32').reshape([16]),
            paddle.to_tensor([0.45788389444351196, 0.29609906673431396, 0.3172542154788971, 0.22868414223194122, 0.4411267936229706, 0.2577226758003235, 0.32533445954322815, 0.40261155366897583, 0.1599903106689453, 0.3604581952095032, 0.07916613668203354, 0.4172859489917755, 0.1484689563512802, 0.33597683906555176, 0.1312432587146759, 0.1922946572303772], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_828a4870be6ea8a788994203d56655bb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 256, 512], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0de02f501c38f1e060f02d842359968c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 720, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4e02f85c59f9c3c7c84743b77841fe08(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7b486c5a6def996cc2f04f5bf6f63240(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4053037464618683, 0.10145023465156555, 0.4805488586425781, 0.09928247332572937, 0.359730988740921, 0.14899000525474548, 0.433727890253067, 0.014844256453216076, 0.33052727580070496, 0.027865611016750336, 0.16676126420497894, 0.008081002160906792, 0.45997312664985657, 0.43527981638908386, 0.09646394103765488, 0.3237294852733612, 0.16077472269535065, 0.1167752742767334, 0.1003134697675705, 0.010031338781118393, 0.33252009749412537, 0.40494805574417114, 0.06489035487174988, 0.1425049901008606, 0.42525115609169006, 0.062267519533634186, 0.29796501994132996, 0.08529370278120041, 0.16473613679409027, 0.40384700894355774], dtype='float32').reshape([30]),
            paddle.to_tensor([0.17964044213294983, 0.3017699718475342, 0.32424062490463257, 0.08517094701528549, 0.2364865243434906, 0.42721179127693176, 0.40250423550605774, 0.3829353153705597, 0.2633380591869354, 0.3893356919288635, 0.10973905771970749, 0.12675738334655762, 0.17641109228134155, 0.44003090262413025, 0.16993626952171326, 0.4638884961605072, 0.1900993138551712, 0.031224379315972328, 0.2274620234966278, 0.14112043380737305, 0.19953131675720215, 0.4356243312358856, 0.3682679533958435, 0.38023892045021057, 0.3794247508049011, 0.17679248750209808, 0.16908429563045502, 0.03241655230522156, 0.0918206050992012, 0.27437153458595276], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2553008794784546, 0.1277354508638382, 0.48625850677490234, 0.41368675231933594, 0.2253105789422989, 0.31676164269447327, 0.3596919775009155, 0.09040645509958267, 0.3258277177810669, 0.2886217534542084, 0.4214566648006439, 0.4442393183708191, 0.1903180181980133, 0.2629418671131134, 0.3326827883720398, 0.017571954056620598, 0.12844954431056976, 0.49788305163383484, 0.024168044328689575, 0.2137468457221985, 0.18070733547210693, 0.2585768699645996, 0.13634845614433289, 0.26414918899536133, 0.24579721689224243, 0.306344598531723, 0.1816740781068802, 0.02037309855222702, 0.32913342118263245, 0.47867831587791443], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4686307907104492, 0.009142898954451084, 0.411418080329895, 0.4254520833492279, 0.4662517309188843, 0.475606232881546, 0.33973535895347595, 0.3408418893814087, 0.24410414695739746, 0.3211512267589569, 0.17798326909542084, 0.09150203317403793, 0.43308278918266296, 0.3897351026535034, 0.006013050675392151, 0.1789105236530304, 0.25177687406539917, 0.29216253757476807, 0.33647674322128296, 0.22960264980793, 0.39263632893562317, 0.04087943211197853, 0.1642877757549286, 0.32699400186538696, 0.01669297367334366, 0.1145673543214798, 0.42313918471336365, 0.15507033467292786, 0.00364583358168602, 0.3716716170310974], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a5ce381b4583e5d9b6a4a16861fc6a4c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.20523564517498016, 0.4426819682121277, 0.009887507185339928, 0.20431123673915863, 0.40269872546195984, 0.3242991268634796, 0.20351436734199524, 0.28587719798088074, 0.4694729745388031, 0.45752859115600586, 0.22778292000293732, 0.08718748390674591, 0.25365105271339417, 0.49321606755256653, 0.45541831851005554, 0.25243374705314636, 0.13559289276599884, 0.22624549269676208, 0.332432359457016, 0.40240660309791565, 0.06251838058233261, 0.4330860674381256, 0.09098910540342331, 0.20523548126220703, 0.43386319279670715, 0.20633098483085632, 0.0323980487883091, 0.4392319917678833], dtype='float32').reshape([28]),
            paddle.to_tensor([0.10071088373661041, 0.4905592203140259, 0.023668646812438965, 0.2779620587825775, 0.1532338708639145, 0.4542378783226013, 0.28745728731155396, 0.02935526706278324, 0.04968724027276039, 0.3811890780925751, 0.2978552579879761, 0.49349600076675415, 0.22416676580905914, 0.1675165593624115, 0.06947197020053864, 0.15426743030548096, 0.17685624957084656, 0.2521823048591614, 0.39179593324661255, 0.3293714225292206, 0.435068279504776, 0.05643780902028084, 0.23934820294380188, 0.33791452646255493, 0.39597463607788086, 0.3233834207057953, 0.28739526867866516, 0.1649392992258072], dtype='float32').reshape([28]),
            paddle.to_tensor([0.11374527961015701, 0.4070321321487427, 0.37715524435043335, 0.27806758880615234, 0.04954648017883301, 0.024863358587026596, 0.2342044562101364, 0.22337110340595245, 0.1160154715180397, 0.07247450947761536, 0.14947962760925293, 0.0351608470082283, 0.08127126097679138, 0.313663512468338, 0.4381113052368164, 0.4693659245967865, 0.09181660413742065, 0.24276015162467957, 0.09579387307167053, 0.20964516699314117, 0.3841034471988678, 0.13142290711402893, 0.16258440911769867, 0.4839833676815033, 0.13476432859897614, 0.33340904116630554, 0.20990464091300964, 0.012775321491062641], dtype='float32').reshape([28]),
            paddle.to_tensor([0.06867668777704239, 0.25583532452583313, 0.395504891872406, 0.26071998476982117, 0.27564218640327454, 0.08579307049512863, 0.3264594078063965, 0.32945331931114197, 0.263858824968338, 0.40560615062713623, 0.13774800300598145, 0.34300628304481506, 0.10460082441568375, 0.31109654903411865, 0.08495974540710449, 0.3135850727558136, 0.20684128999710083, 0.20043787360191345, 0.11067435145378113, 0.33282092213630676, 0.040625639259815216, 0.16495253145694733, 0.42100852727890015, 0.33761337399482727, 0.3101647198200226, 0.3340561091899872, 0.2370734065771103, 0.34660661220550537], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b74104c7e5b6e56bf33637eacf1fd7fc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 30, 30], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_62de5062a07f3b0f8561babdf0f58f90(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 12], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c539de2d501c5bdc1a5a250655f16260(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 112, 12, 12], dtype='float16', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_33bfc441dfa83d4bff8b6c37b90d5a1e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 32, 64], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2740885317325592, 0.2946349084377289, 0.14222358167171478, 0.4179168939590454, 0.1958773136138916, 0.08762100338935852, 0.0007308003841899335, 0.4169382154941559, 0.3454127907752991, 0.18876083195209503, 0.0941239595413208, 0.48338764905929565, 0.35134682059288025, 0.44410771131515503, 0.38381120562553406, 0.4421883523464203, 0.08446548879146576, 0.06373555958271027], dtype='float32').reshape([18]),
            paddle.to_tensor([0.25718703866004944, 0.2925228476524353, 0.46707284450531006, 0.4435407817363739, 0.20619864761829376, 0.1801687628030777, 0.2349979132413864, 0.03603887930512428, 0.13901345431804657, 0.15203852951526642, 0.3709079623222351, 0.00712589593604207, 0.1522655040025711, 0.2669880986213684, 0.31327083706855774, 0.2564639449119568, 0.19405584037303925, 0.37847545742988586], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4392859637737274, 0.011594451032578945, 0.14490142464637756, 0.4919896423816681, 0.1534498929977417, 0.2734872102737427, 0.32871681451797485, 0.052220966666936874, 0.04830402880907059, 0.005718008149415255, 0.38563910126686096, 0.4574580788612366, 0.355900377035141, 0.3195957839488983, 0.4406987130641937, 0.25296053290367126, 0.3209178149700165, 0.3972527086734772], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4098185896873474, 0.4002934396266937, 0.16435480117797852, 0.2921997010707855, 0.06040165200829506, 0.09338171780109406, 0.002154323272407055, 0.25187692046165466, 0.18907153606414795, 0.3907715082168579, 0.1514379233121872, 0.45796623826026917, 0.4143853783607483, 0.14627225697040558, 0.2352760136127472, 0.007593419402837753, 0.14843687415122986, 0.1666605919599533], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_380e3d0bb0ee77727edf5d57e36b0e90(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4075726568698883, 0.3692548871040344, 0.32725757360458374, 0.3371542692184448, 0.11926650255918503, 0.36661288142204285, 0.16441218554973602, 0.09941161423921585, 0.018991537392139435, 0.4422333240509033, 0.04647665470838547, 0.05809276923537254, 0.22933731973171234, 0.25268229842185974, 0.2540811002254486, 0.04538784921169281, 0.12846782803535461, 0.14351077377796173, 0.4342506527900696, 0.10224759578704834, 0.13496659696102142, 0.4587664008140564, 0.10303521156311035, 0.45227062702178955, 0.16724839806556702, 0.4590992033481598, 0.06842759251594543, 0.16575001180171967, 0.1001632809638977, 0.09078454971313477], dtype='float32').reshape([30]),
            paddle.to_tensor([0.1336848884820938, 0.07619116455316544, 0.2808154225349426, 0.3552854657173157, 0.07548843324184418, 0.08880798518657684, 0.14414702355861664, 0.3486228883266449, 0.34351637959480286, 0.3652445077896118, 0.13154307007789612, 0.022567037492990494, 0.2694193124771118, 0.33247455954551697, 0.4329288601875305, 0.442717045545578, 0.11919377744197845, 0.3449958264827728, 0.3113219738006592, 0.1427963674068451, 0.13625235855579376, 0.26625970005989075, 0.08854587376117706, 0.4614468514919281, 0.06742283701896667, 0.07727736234664917, 0.012455609627068043, 0.01909632608294487, 0.23550187051296234, 0.0941731408238411], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2588116526603699, 0.3432237207889557, 0.024868672713637352, 0.46628695726394653, 0.045955590903759, 0.4863170385360718, 0.11247432976961136, 0.4265551269054413, 0.15442398190498352, 0.4026404321193695, 0.09967181086540222, 0.3308964669704437, 0.4953496754169464, 0.22936972975730896, 0.47920313477516174, 0.12886252999305725, 0.425704687833786, 0.002899988554418087, 0.25281277298927307, 0.1851877123117447, 0.3848298192024231, 0.3297877311706543, 0.38706138730049133, 0.2963593602180481, 0.07469917088747025, 0.1718873232603073, 0.044236600399017334, 0.18135951459407806, 0.2995053231716156, 0.40007612109184265], dtype='float32').reshape([30]),
            paddle.to_tensor([0.10374356061220169, 0.32937490940093994, 0.0592171885073185, 0.3097052574157715, 0.49957332015037537, 0.29271697998046875, 0.25218504667282104, 0.22258242964744568, 0.1387978494167328, 0.1853916496038437, 0.008230865933001041, 0.06222347915172577, 0.0017517467495054007, 0.09397479146718979, 0.25822293758392334, 0.24850140511989594, 0.1480156034231186, 0.24207554757595062, 0.043796028941869736, 0.4736013114452362, 0.36093029379844666, 0.20746733248233795, 0.36984169483184814, 0.3528304100036621, 0.19832326471805573, 0.3875957727432251, 0.22633472084999084, 0.2501416504383087, 0.21263866126537323, 0.13248462975025177], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d5d2dd6720d2a8edfc71ef880db1300b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b7e7aba0c2b1f781b4be0d098013351e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_414699e4ca90c04bbdb40707c6f71e62(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 64, 128], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.03551861643791199, 0.37739652395248413, 0.13545997440814972, 0.4397885799407959, 0.39102432131767273, 0.44656285643577576, 0.31260716915130615, 0.22142238914966583, 0.2039344608783722, 0.09277985990047455, 0.019077012315392494, 0.317561537027359, 0.03297963738441467, 0.30374184250831604, 0.07578808814287186, 0.30780741572380066, 0.46272119879722595, 0.16210387647151947], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4627515971660614, 0.05520612746477127, 0.025400837883353233, 0.3336857259273529, 0.08480678498744965, 0.46043601632118225, 0.09006249904632568, 0.04894912987947464, 0.13170300424098969, 0.13744567334651947, 0.31673604249954224, 0.05250485613942146, 0.21025273203849792, 0.10742160677909851, 0.3472948670387268, 0.15689250826835632, 0.36483272910118103, 0.41990789771080017], dtype='float32').reshape([18]),
            paddle.to_tensor([0.001463703578338027, 0.19885031878948212, 0.06804826110601425, 0.44458457827568054, 0.3200911581516266, 0.36042410135269165, 0.07751254737377167, 0.4746260344982147, 0.36131182312965393, 0.20190522074699402, 0.2354172170162201, 0.28837019205093384, 0.3568204939365387, 0.21816369891166687, 0.3240044116973877, 0.33486801385879517, 0.13803724944591522, 0.09664293378591537], dtype='float32').reshape([18]),
            paddle.to_tensor([0.18536725640296936, 0.188695028424263, 0.3598833680152893, 0.2955321669578552, 0.48385441303253174, 0.13151830434799194, 0.15385030210018158, 0.48833003640174866, 0.0716041550040245, 0.4228646457195282, 0.44799938797950745, 0.4050482511520386, 0.25586631894111633, 0.21004796028137207, 0.22817441821098328, 0.069869764149189, 0.014997308142483234, 0.24355293810367584], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3e6a8c53519b62b408ee175fc0e03640(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 576, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d231f8cdffe39386ef614d48232807f7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1454f23aa2f039ba386ecd121f605e0d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 304, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([304], dtype='float32', min=0, max=0.5),
            paddle.uniform([304], dtype='float32', min=0, max=0.5),
            paddle.uniform([304], dtype='float32', min=0, max=0.5),
            paddle.uniform([304], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9b5e8eafdacf2280207dd42ff4657490(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 30, 30], dtype='float16', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6dfe2fdef2f47d0488b7ac4d158b3622(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_186eae946956756b13f3654169c2ef9a
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 8, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6d9a6fe955fc2d2129d31e20128383d3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 25, 42], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_842e7b94d95a617fe9f62b1e565c4347(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.022744115442037582, 0.11355732381343842, 0.22825443744659424, 0.20835307240486145, 0.44157877564430237, 0.08793213963508606, 0.20208674669265747, 0.20294907689094543, 0.38920706510543823, 0.22062203288078308, 0.4665409326553345, 0.14127308130264282, 0.1064055785536766, 0.4481145739555359, 0.35019785165786743, 0.089622363448143, 0.4091990292072296, 0.19320161640644073, 0.2754295766353607, 0.20086455345153809], dtype='float32').reshape([20]),
            paddle.to_tensor([0.32228633761405945, 0.2904178500175476, 0.301287442445755, 0.2355465292930603, 0.3465299606323242, 0.43042120337486267, 0.3774159252643585, 0.1808013617992401, 0.16772878170013428, 0.030517075210809708, 0.3399929106235504, 0.3933759927749634, 0.3063826262950897, 0.49530941247940063, 0.20009009540081024, 0.16916345059871674, 0.3824380934238434, 0.26132407784461975, 0.4393634498119354, 0.050015877932310104], dtype='float32').reshape([20]),
            paddle.to_tensor([0.47206446528434753, 0.3273538053035736, 0.29024001955986023, 0.17020511627197266, 0.3876417577266693, 0.4821666181087494, 0.472717821598053, 0.2408868372440338, 0.1780201941728592, 0.26908570528030396, 0.42139941453933716, 0.45557907223701477, 0.07683577388525009, 0.25690755248069763, 0.4436873495578766, 0.3257308900356293, 0.3304123282432556, 0.04706988111138344, 0.03755832090973854, 0.00517472717911005], dtype='float32').reshape([20]),
            paddle.to_tensor([0.0017305663786828518, 0.08655041456222534, 0.06901200115680695, 0.008557381108403206, 0.3625018894672394, 0.4309677481651306, 0.46386075019836426, 0.42679867148399353, 0.4412331283092499, 0.3358955681324005, 0.06053914502263069, 0.4422900080680847, 0.37545597553253174, 0.10004157572984695, 0.31809988617897034, 0.47856131196022034, 0.12305795401334763, 0.07271220535039902, 0.049808062613010406, 0.2178644835948944], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_49d23971ec9828015a94330665057bcf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f70f6bea3916ebb6a87d1a21518cd2b7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 320, 320], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3afb78776ed263169c62e618456f18c7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 112, 64, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2192fb9c40e3db352a42dc91b76daf07(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 480, 480], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_54a45b31b2dcdd438d67ab537372123d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.005836358293890953, 0.43213796615600586, 0.3488873541355133, 0.17483289539813995, 0.2149866819381714, 0.4228822588920593, 0.01984878070652485, 0.12659355998039246, 0.45502641797065735, 0.4403129816055298, 0.29718270897865295, 0.4296182692050934, 0.1203538179397583, 0.1273990124464035, 0.4817652702331543, 0.332605242729187, 0.3541191816329956, 0.4173358082771301, 0.047849178314208984, 0.3178785443305969], dtype='float32').reshape([20]),
            paddle.to_tensor([0.320673406124115, 0.3963756561279297, 0.2338039129972458, 0.07100857049226761, 0.15474776923656464, 0.37833818793296814, 0.1493080109357834, 0.3762783408164978, 0.4244025647640228, 0.3513113260269165, 0.00649507250636816, 0.3797454535961151, 0.26919007301330566, 0.21692673861980438, 0.4939001500606537, 0.23103520274162292, 0.1556565910577774, 0.09059110283851624, 0.48135554790496826, 0.19282303750514984], dtype='float32').reshape([20]),
            paddle.to_tensor([0.300552099943161, 0.491688072681427, 0.001818730728700757, 0.4822978377342224, 0.3751741945743561, 0.39091944694519043, 0.4079504609107971, 0.33541983366012573, 0.3989103436470032, 0.09509951621294022, 0.12632252275943756, 0.21315371990203857, 0.07887706160545349, 0.3869228661060333, 0.02072974108159542, 0.467472106218338, 0.4206165373325348, 0.43509441614151, 0.03089696727693081, 0.40341636538505554], dtype='float32').reshape([20]),
            paddle.to_tensor([0.07911138981580734, 0.009338448755443096, 0.4818592667579651, 0.39854782819747925, 0.33535587787628174, 0.30765944719314575, 0.406288743019104, 0.19374659657478333, 0.24796071648597717, 0.3885287344455719, 0.44118544459342957, 0.20342154800891876, 0.16276077926158905, 0.10268501192331314, 0.06856577098369598, 0.050763536244630814, 0.03427468240261078, 0.1943216770887375, 0.18758785724639893, 0.3252633512020111], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_faddcc865ab9d928ef5ebfc66dadec01(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 224, 224], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.008680556900799274, 0.348122775554657, 0.22036926448345184, 0.20373786985874176, 0.1404336392879486, 0.1533655971288681, 0.47613292932510376, 0.4086172580718994, 0.10304800420999527, 0.2430763691663742, 0.3710278570652008, 0.40184035897254944, 0.15087924897670746, 0.2434312254190445, 0.06144662946462631, 0.06740064918994904], dtype='float32').reshape([16]),
            paddle.to_tensor([0.14074411988258362, 0.3569350242614746, 0.1060052290558815, 0.2657613754272461, 0.3456861078739166, 0.052556827664375305, 0.45029863715171814, 0.4062696695327759, 0.1861843317747116, 0.09660191833972931, 0.2857630252838135, 0.4438928961753845, 0.09790794551372528, 0.20451325178146362, 0.1787445992231369, 0.3461536765098572], dtype='float32').reshape([16]),
            paddle.to_tensor([0.21468298137187958, 0.1186622679233551, 0.20204266905784607, 0.24063844978809357, 0.25511467456817627, 0.06693873554468155, 0.4636377692222595, 0.18101908266544342, 0.09418985247612, 0.3090687394142151, 0.2602042257785797, 0.038301218301057816, 0.1924002319574356, 0.2849644422531128, 0.2673795521259308, 0.01850917749106884], dtype='float32').reshape([16]),
            paddle.to_tensor([0.05018128454685211, 0.15671177208423615, 0.17538756132125854, 0.35178813338279724, 0.1402353048324585, 0.31668293476104736, 0.17818062007427216, 0.18648777902126312, 0.012130136601626873, 0.0402078777551651, 0.30293720960617065, 0.3693760633468628, 0.3704788386821747, 0.23508700728416443, 0.41495636105537415, 0.3602302074432373], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e367eadcab4f3a9b2a4f0fea1fcf026a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.07638490945100784, 0.025418654084205627, 0.4084394574165344, 0.33692044019699097, 0.15643195807933807, 0.40810057520866394, 0.2146066576242447, 0.2630190849304199, 0.24994418025016785, 0.3390684723854065, 0.16807299852371216, 0.08453532308340073, 0.048319000750780106, 0.09808596968650818, 0.413765013217926, 0.18459852039813995, 0.23861053586006165, 0.3025548756122589], dtype='float32').reshape([18]),
            paddle.to_tensor([0.019146181643009186, 0.3131754994392395, 0.3269864618778229, 0.15867890417575836, 0.1662178933620453, 0.09349469095468521, 0.25182631611824036, 0.41616854071617126, 0.41624194383621216, 0.253489226102829, 0.3294849395751953, 0.41838791966438293, 0.46670839190483093, 0.007766628637909889, 0.4048942029476166, 0.17509813606739044, 0.1811528205871582, 0.3394913077354431], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3033326268196106, 0.17370040714740753, 0.21940678358078003, 0.07078468054533005, 0.48099225759506226, 0.3456161320209503, 0.3749849200248718, 0.11159879714250565, 0.3523190915584564, 0.12121498584747314, 0.41654279828071594, 0.4424894154071808, 0.489825040102005, 0.42404237389564514, 0.45599982142448425, 0.07323814928531647, 0.07892195880413055, 0.44417664408683777], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3782443404197693, 0.18309427797794342, 0.45053884387016296, 0.13616785407066345, 0.42856019735336304, 0.09798604249954224, 0.36848148703575134, 0.1602753847837448, 0.12196476757526398, 0.30245542526245117, 0.22207224369049072, 0.3052179217338562, 0.10600122809410095, 0.4253428876399994, 0.26025402545928955, 0.3301231563091278, 0.4155137836933136, 0.3166324496269226], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e2a9acc06f8c7ee8955a6bc43635ecd8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 208, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f0d503fcba0f28bbf0f2ba93a1a8ed5c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 200, 64, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0cc907b5c0a343e0c8ae44dcc60714bc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.1131582111120224, 0.2441028207540512, 0.22631463408470154, 0.4008181095123291, 0.2937982380390167, 0.03920742869377136, 0.21581611037254333, 0.014338887296617031, 0.09920436888933182, 0.45857471227645874, 0.07041015475988388, 0.21198830008506775, 0.042564891278743744, 0.038432054221630096, 0.19956736266613007, 0.3589237928390503], dtype='float32').reshape([16]),
            paddle.to_tensor([0.06391175836324692, 0.12913815677165985, 0.223911315202713, 0.3169861435890198, 0.38109081983566284, 0.3165961802005768, 0.40576452016830444, 0.14588496088981628, 0.2646368145942688, 0.36729979515075684, 0.34483399987220764, 0.4246814250946045, 0.1607171893119812, 0.15895386040210724, 0.2705535292625427, 0.018736790865659714], dtype='float32').reshape([16]),
            paddle.to_tensor([0.10222902148962021, 0.40504196286201477, 0.44817712903022766, 0.48898687958717346, 0.32432952523231506, 0.14171327650547028, 0.13103856146335602, 0.40804678201675415, 0.34854596853256226, 0.11063738912343979, 0.11230577528476715, 0.391513466835022, 0.16116921603679657, 0.45045235753059387, 0.17983125150203705, 0.19399549067020416], dtype='float32').reshape([16]),
            paddle.to_tensor([0.0022692978382110596, 0.1156822070479393, 0.1411675661802292, 0.32213571667671204, 0.37127870321273804, 0.08233761787414551, 0.18208684027194977, 0.3090396821498871, 0.24380262196063995, 0.0758771300315857, 0.15688900649547577, 0.25301599502563477, 0.3718641698360443, 0.3711448907852173, 0.2653200626373291, 0.0565495528280735], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_50c29a56dd1282da38789650ec7ed432(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.18423894047737122, 0.11172455549240112, 0.42113229632377625, 0.22247126698493958, 0.4022683799266815, 0.054757118225097656, 0.1329709142446518, 0.3900403380393982, 0.37867194414138794, 0.4328165054321289, 0.04389062896370888, 0.39663153886795044, 0.3713778853416443, 0.015578688122332096, 0.34472551941871643, 0.3124266266822815, 0.42959344387054443, 0.26441705226898193, 0.017608409747481346, 0.15937823057174683, 0.42030438780784607, 0.11238723993301392, 0.4432079493999481, 0.016409194096922874], dtype='float32').reshape([24]),
            paddle.to_tensor([0.44723957777023315, 0.37358802556991577, 0.16932235658168793, 0.48618775606155396, 0.3537033200263977, 0.08159752190113068, 0.06082412600517273, 0.13442447781562805, 0.33422237634658813, 0.40823063254356384, 0.3007858991622925, 0.28760385513305664, 0.3120502233505249, 0.1209048330783844, 0.13156506419181824, 0.23749324679374695, 0.004826227203011513, 0.0729563981294632, 0.23031529784202576, 0.4904583692550659, 0.18799012899398804, 0.4290744364261627, 0.11356960237026215, 0.32393231987953186], dtype='float32').reshape([24]),
            paddle.to_tensor([0.28154435753822327, 0.21481113135814667, 0.23013776540756226, 0.41760632395744324, 0.336833119392395, 0.033520009368658066, 0.18923144042491913, 0.2223716378211975, 0.48739293217658997, 0.3963979184627533, 0.407617449760437, 0.2561015784740448, 0.3799203932285309, 0.4699765741825104, 0.49045228958129883, 0.017777686938643456, 0.10423553735017776, 0.29751282930374146, 0.08422184735536575, 0.47116971015930176, 0.2888636887073517, 0.36800500750541687, 0.34657928347587585, 0.06984491646289825], dtype='float32').reshape([24]),
            paddle.to_tensor([0.0921480655670166, 0.45533379912376404, 0.37959498167037964, 0.1546211689710617, 0.07290353626012802, 0.008771928958594799, 0.2932797074317932, 0.43884390592575073, 0.4579116702079773, 0.18269352614879608, 0.30324676632881165, 0.201070636510849, 0.04403502494096756, 0.3890981674194336, 0.1823786348104477, 0.45928797125816345, 0.14055778086185455, 0.48198202252388, 0.18797454237937927, 0.34588706493377686, 0.1963266283273697, 0.011192427016794682, 0.2097957879304886, 0.43751874566078186], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ee1cf86269a0555e47c5c09ebb6567d2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_17c6bbe6ae39c08eb15dec335bad8dc3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 16, 50], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.1508961170911789, 0.4615309238433838, 0.45560336112976074, 0.05461272969841957, 0.4781385660171509, 0.4990628659725189, 0.19383768737316132, 0.44576001167297363], dtype='float32').reshape([8]),
            paddle.to_tensor([0.4016944169998169, 0.30637219548225403, 0.2546408474445343, 0.4242829382419586, 0.24973250925540924, 0.36231526732444763, 0.05504481494426727, 0.2629179060459137], dtype='float32').reshape([8]),
            paddle.to_tensor([0.24543242156505585, 0.21017219126224518, 0.1408173143863678, 0.3889671862125397, 0.3422984778881073, 0.45030835270881653, 0.25357040762901306, 0.2706555128097534], dtype='float32').reshape([8]),
            paddle.to_tensor([0.413952112197876, 0.3872644901275635, 0.33629876375198364, 0.37798166275024414, 0.29520103335380554, 0.4924792945384979, 0.25158676505088806, 0.4476582705974579], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_55941a8b9c440fc7850ebfedfe51ebcf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 112, 64, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5702d8533a9fc74f7f966b21e4a81d6b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 16, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f3a0a11a4dd6f3c56b34ac5133713c68(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 960, 32, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d0b23a7f154ab81ef620d1a9a85bdfea(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 768, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ee21d95b785d250c5d7f32889e68a9ac(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6fc7711ab44ae6ddc0043bb4ae23beee(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.24523089826107025, 0.24183791875839233, 0.32848021388053894, 0.4376978874206543, 0.30589303374290466, 0.10434779524803162, 0.11020629107952118, 0.44663527607917786, 0.05549949035048485, 0.47663742303848267, 0.09380681812763214, 0.49458664655685425, 0.3384993076324463, 0.15692389011383057, 0.37605082988739014, 0.4050328731536865, 0.15449729561805725, 0.1661994904279709, 0.32062646746635437, 0.2031572163105011, 0.13731105625629425, 0.04546239227056503, 0.35454094409942627, 0.32201889157295227, 0.3566301167011261, 0.41566240787506104, 0.2503727674484253, 0.2465733289718628, 0.030572839081287384, 0.024773865938186646], dtype='float32').reshape([30]),
            paddle.to_tensor([0.37989717721939087, 0.4480263292789459, 0.016979433596134186, 0.2757590115070343, 0.01466304250061512, 0.28760236501693726, 0.26255497336387634, 0.20242367684841156, 0.33600112795829773, 0.2720063626766205, 0.12203112244606018, 0.3693491816520691, 0.4694415032863617, 0.11210315674543381, 0.47577542066574097, 0.49187296628952026, 0.3977168798446655, 0.31462347507476807, 0.10929824411869049, 0.45285648107528687, 0.33384785056114197, 0.22363941371440887, 0.002860595937818289, 0.11123092472553253, 0.22661951184272766, 0.43419280648231506, 0.24843744933605194, 0.3442573547363281, 0.2525666356086731, 0.062078818678855896], dtype='float32').reshape([30]),
            paddle.to_tensor([0.11726648360490799, 0.01777259260416031, 0.387628436088562, 0.27494382858276367, 0.28245094418525696, 0.376266747713089, 0.006700893398374319, 0.009458712302148342, 0.3308563530445099, 0.1573186069726944, 0.07817819714546204, 0.020992496982216835, 0.14199265837669373, 0.09726230800151825, 0.3935173749923706, 0.44466108083724976, 0.27670571208000183, 0.4107869565486908, 0.033642254769802094, 0.12419365346431732, 0.3504902422428131, 0.24651196599006653, 0.45074543356895447, 0.25282901525497437, 0.23449525237083435, 0.47414255142211914, 0.3222934603691101, 0.3368901312351227, 0.40675225853919983, 0.06838354468345642], dtype='float32').reshape([30]),
            paddle.to_tensor([0.05974609777331352, 0.20103885233402252, 0.10540582239627838, 0.4152834117412567, 0.03169618174433708, 0.13315147161483765, 0.10592164099216461, 0.20502832531929016, 0.18236735463142395, 0.3833565413951874, 0.3974614143371582, 0.4687483608722687, 0.2392607480287552, 0.10443591326475143, 0.3441665768623352, 0.08430960029363632, 0.3577195405960083, 0.3710969388484955, 0.3036910593509674, 0.3749294877052307, 0.13285212218761444, 0.015403848141431808, 0.4287356734275818, 0.47184401750564575, 0.37874099612236023, 0.050671737641096115, 0.1677769273519516, 0.3103996515274048, 0.425559401512146, 0.4379010796546936], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_743e1a2ea8b051d63a173bc7d989367a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 144, 48, 48], dtype='float16', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3dbc25d4b66f9f554d0853c35ece4f8a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([8, 1024, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f18fb9f16106ac4237030e9719890249(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d95b349d12a180432b9da8292b8a8c30(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 8, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2c464f1b8ab53f105cd5a628848a2552(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c914ec450f08f860a7b61bee0bee412e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 10, 10], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9951ab5480b0b57c6e09483b0eae12b8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_612cf3b8f70b28e85e0ecd2d86a6324e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_144b94a00a566030e09382d3ece6ae47(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_659001d114fcbe864e02b6a6113d4d88(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_94f00c434a03bb851e428e8dd7aa9bb1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3533512353897095, 0.48740285634994507, 0.004865249618887901, 0.30737993121147156, 0.044030167162418365, 0.4312095642089844, 0.43744343519210815, 0.38204285502433777, 0.17349600791931152, 0.4546245336532593, 0.041286759078502655, 0.3395642638206482, 0.37144964933395386, 0.0827748030424118, 0.0686507523059845, 0.24011358618736267, 0.22223640978336334, 0.24100469052791595, 0.4824925363063812, 0.13663813471794128, 0.022460777312517166, 0.2802353799343109, 0.2819092571735382, 0.01999964378774166], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4039037823677063, 0.35002222657203674, 0.2011391818523407, 0.3666988015174866, 0.11302967369556427, 0.17235292494297028, 0.36002117395401, 0.09853603690862656, 0.11812136322259903, 0.19606851041316986, 0.058142758905887604, 0.36762064695358276, 0.4840600788593292, 0.41574540734291077, 0.26021188497543335, 0.3221185505390167, 0.1453786939382553, 0.30223193764686584, 0.29167991876602173, 0.16338509321212769, 0.46002987027168274, 0.29046952724456787, 0.4260942041873932, 0.2712703049182892], dtype='float32').reshape([24]),
            paddle.to_tensor([0.27513545751571655, 0.10910017788410187, 0.34809887409210205, 0.33900806307792664, 0.096459299325943, 0.47540518641471863, 0.31259575486183167, 0.12301541119813919, 0.3127801716327667, 0.03522568568587303, 0.27428242564201355, 0.3717224895954132, 0.01146091427654028, 0.18934190273284912, 0.31011268496513367, 0.09461261332035065, 0.24736611545085907, 0.4123292863368988, 0.2765577435493469, 0.1617608219385147, 0.33426108956336975, 0.15634013712406158, 0.18208278715610504, 0.43492382764816284], dtype='float32').reshape([24]),
            paddle.to_tensor([0.34663355350494385, 0.1906454712152481, 0.4294598698616028, 0.01753815822303295, 0.08885490149259567, 0.40885642170906067, 0.15157975256443024, 0.07101637870073318, 0.005424996372312307, 0.2835574746131897, 0.44898876547813416, 0.005647942423820496, 0.0018209237605333328, 0.1491699516773224, 0.3640054762363434, 0.3525598645210266, 0.2535097897052765, 0.4751819968223572, 0.46215566992759705, 0.1941460818052292, 0.27630317211151123, 0.10890523344278336, 0.48906517028808594, 0.08592712134122849], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_74e5475cb149a48f8a6e632431ae1747(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 10, 10], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f82d792c7b7b6ad7fc0810e8f2d1b8f1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_13025b164297d9226f9641b7f0715c76
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 150, 150], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_69329c0bb461b245a7d83dd994803976(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 4, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_003785c47f42ef4f692920329789eb11(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 832, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([832], dtype='float32', min=0, max=0.5),
            paddle.uniform([832], dtype='float32', min=0, max=0.5),
            paddle.uniform([832], dtype='float32', min=0, max=0.5),
            paddle.uniform([832], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_db7a577b46caab94f20da637f9c71575(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 512], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2180834710597992, 0.20464687049388885, 0.35323983430862427, 0.4067040979862213, 0.021503573283553123, 0.3288695514202118, 0.18814964592456818, 0.31780683994293213, 0.2325124740600586, 0.38773050904273987, 0.2695414423942566, 0.4102957248687744, 0.4488239586353302, 0.23412594199180603, 0.15567925572395325, 0.10007841885089874], dtype='float32').reshape([16]),
            paddle.to_tensor([0.33837515115737915, 0.07541175186634064, 0.423000693321228, 0.04614248499274254, 0.27426931262016296, 0.4828551709651947, 0.20384520292282104, 0.4589787721633911, 0.12640932202339172, 0.4665690064430237, 0.04324731603264809, 0.3854750394821167, 0.03855406492948532, 0.41257932782173157, 0.20771363377571106, 0.447908878326416], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4878745973110199, 0.4582851529121399, 0.43830764293670654, 0.488290935754776, 0.04244694113731384, 0.37834277749061584, 0.4252052307128906, 0.4046477973461151, 0.4852153956890106, 0.4278702437877655, 0.15506474673748016, 0.1035836935043335, 0.06781740486621857, 0.027199028059840202, 0.038463689386844635, 0.256635457277298], dtype='float32').reshape([16]),
            paddle.to_tensor([0.30993008613586426, 0.4979071021080017, 0.4599195420742035, 0.45713678002357483, 0.13714097440242767, 0.40271589159965515, 0.34944793581962585, 0.21099095046520233, 0.01500771101564169, 0.23326753079891205, 0.0014777765609323978, 0.22067391872406006, 0.14385567605495453, 0.3030358552932739, 0.3577073812484741, 0.04649863764643669], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dd3d293c0b5ca42eef074c1682c4a1b4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 240, 240], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a70984717f23f1c99b0715bdec808843(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 400, 672], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d3e1b81b141527a74c643db7d9300220(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1882dd39d400600c80d032aacde98c39(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_551b7b803f5be7f435279fa2aa263ba7
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_193f9ddeed411e5f6d86959c55c5094b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 2208, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([2208], dtype='float32', min=0, max=0.5),
            paddle.uniform([2208], dtype='float32', min=0, max=0.5),
            paddle.uniform([2208], dtype='float32', min=0, max=0.5),
            paddle.uniform([2208], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b57667201343c05830ba7280227ace2d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.148032084107399, 0.33856773376464844, 0.4020281732082367, 0.4577556252479553, 0.05694995075464249, 0.20478378236293793, 0.48886340856552124, 0.1470417082309723, 0.18092094361782074, 0.385590523481369, 0.06675376743078232, 0.34475070238113403, 0.24023206532001495, 0.39356136322021484, 0.2882128357887268, 0.13526499271392822, 0.011359933763742447, 0.36930525302886963, 0.3934783339500427, 0.20796136558055878, 0.13518615067005157, 0.20853956043720245, 0.4566044211387634, 0.3247390389442444], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2925514876842499, 0.019291801378130913, 0.02114858850836754, 0.14750604331493378, 0.062222812324762344, 0.21435756981372833, 0.28898218274116516, 0.2460496872663498, 0.3693249523639679, 0.42334118485450745, 0.40723174810409546, 0.39541009068489075, 0.2989652156829834, 0.12457974255084991, 0.08963318169116974, 0.16449153423309326, 0.3270340859889984, 0.041956618428230286, 0.4244972765445709, 0.2104542851448059, 0.10193954408168793, 0.40597471594810486, 0.3373752236366272, 0.10381928831338882], dtype='float32').reshape([24]),
            paddle.to_tensor([0.010552224703133106, 0.3939388394355774, 0.19258378446102142, 0.2872898578643799, 0.4911717474460602, 0.08804642409086227, 0.48022136092185974, 0.037236537784338, 0.37521904706954956, 0.4113715887069702, 0.20086172223091125, 0.1565580666065216, 0.499164342880249, 0.27147698402404785, 0.2631712555885315, 0.3737183213233948, 0.461447536945343, 0.4984187185764313, 0.03698243945837021, 0.34261301159858704, 0.11335047334432602, 0.44398611783981323, 0.008263399824500084, 0.11733569949865341], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2798551619052887, 0.2103511095046997, 0.13263891637325287, 0.0594220906496048, 0.1763366013765335, 0.3595898151397705, 0.2807226777076721, 0.09149312227964401, 0.03731424733996391, 0.3410188555717468, 0.46522584557533264, 0.34525182843208313, 0.48336344957351685, 0.33242443203926086, 0.04227152094244957, 0.4565121531486511, 0.1560637354850769, 0.458391010761261, 0.2433166205883026, 0.17228034138679504, 0.07544247061014175, 0.3626278042793274, 0.0723830908536911, 0.431562215089798], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7444a3e6ac1e7fb520dd1d1225ace570(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 100, 168], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0e6e6141d165b4ad15994fa0dde046df(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_60ac4b248222a53fe362c073494b2839(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ec5a63d37f1900a14c084682bf73f435(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 16, 50], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.49291813373565674, 0.3518295884132385, 0.373944491147995, 0.4144655764102936, 0.10248830169439316, 0.3377249836921692, 0.30567505955696106, 0.24692052602767944], dtype='float32').reshape([8]),
            paddle.to_tensor([0.09928520023822784, 0.2282657027244568, 0.11835620552301407, 0.05726465955376625, 0.48918861150741577, 0.2860201299190521, 0.2825014293193817, 0.03767079859972], dtype='float32').reshape([8]),
            paddle.to_tensor([0.12263370305299759, 0.3564383387565613, 0.3110109865665436, 0.1287813037633896, 0.46347516775131226, 0.318218857049942, 0.13695752620697021, 0.355124831199646], dtype='float32').reshape([8]),
            paddle.to_tensor([0.4968148469924927, 0.08124049007892609, 0.4002050459384918, 0.4279220998287201, 0.07235563546419144, 0.46644800901412964, 0.38585996627807617, 0.17504280805587769], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9c7ca1acf8917688291f44d65eb14e5e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_422f0ec9508d16cd82839b51c0370bb5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 4, 512, 1024], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.10602299869060516, 0.4757339358329773, 0.39792969822883606, 0.3911449611186981], dtype='float32').reshape([4]),
            paddle.to_tensor([0.29896485805511475, 0.4560219347476959, 0.31410905718803406, 0.23400714993476868], dtype='float32').reshape([4]),
            paddle.to_tensor([0.1488274335861206, 0.1467278152704239, 0.4215374290943146, 0.3206620216369629], dtype='float32').reshape([4]),
            paddle.to_tensor([0.2701847553253174, 0.33735528588294983, 0.24167563021183014, 0.033497367054224014], dtype='float32').reshape([4]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c7fc4ab2da3e60fd4bd5f28dfd47d13c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 224, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6b655a5c19cbf9ea209104b1b777e850(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 1200, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5ba1a78484707016358af4a15db11707(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 320, 320], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.45560574531555176, 0.11176196485757828, 0.23053525388240814, 0.35333600640296936, 0.3891109526157379, 0.22341319918632507, 0.4086264371871948, 0.48125892877578735, 0.1963760256767273, 0.18021446466445923, 0.398040235042572, 0.4326294958591461, 0.2937089204788208, 0.07978580892086029, 0.2382284551858902, 0.03865577653050423], dtype='float32').reshape([16]),
            paddle.to_tensor([0.047353826463222504, 0.20998020470142365, 0.2097318172454834, 0.10045219957828522, 0.40863698720932007, 0.21877172589302063, 0.062044233083724976, 0.4350493848323822, 0.18859902024269104, 0.4784887135028839, 0.34669119119644165, 0.31707996129989624, 0.0833483338356018, 0.06330916285514832, 0.004107224754989147, 0.3559228181838989], dtype='float32').reshape([16]),
            paddle.to_tensor([0.22011229395866394, 0.2737288177013397, 0.14429351687431335, 0.3376181423664093, 0.4398023188114166, 0.4110814034938812, 0.03262761980295181, 0.41932785511016846, 0.24926835298538208, 0.015649255365133286, 0.47711560130119324, 0.18714210391044617, 0.46209612488746643, 0.04253441467881203, 0.30584293603897095, 0.23237313330173492], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3524637818336487, 0.27097219228744507, 0.3711303174495697, 0.2709904909133911, 0.16912660002708435, 0.19663676619529724, 0.372124046087265, 0.11557014286518097, 0.18315422534942627, 0.22968976199626923, 0.36172589659690857, 0.06727693974971771, 0.089671291410923, 0.33558011054992676, 0.4480047821998596, 0.23726089298725128], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_36d03336d5e5b099747b8e8e7e8cad1e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.15138545632362366, 0.13546398282051086, 0.4130065441131592, 0.48427340388298035, 0.0633607879281044, 0.2696723937988281, 0.27651387453079224, 0.39991962909698486, 0.10020347684621811, 0.01617002673447132, 0.00903611071407795, 0.08797646313905716, 0.14611642062664032, 0.32250526547431946, 0.14900001883506775, 0.28796878457069397, 0.4721461832523346, 0.05186479166150093], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3939515948295593, 0.31963157653808594, 0.40475699305534363, 0.36759430170059204, 0.06439058482646942, 0.4080677926540375, 0.296786904335022, 0.3333602249622345, 0.285904198884964, 0.07136962562799454, 0.09539582580327988, 0.48448312282562256, 0.21158677339553833, 0.14408791065216064, 0.4438488483428955, 0.21964479982852936, 0.11829329282045364, 0.473459929227829], dtype='float32').reshape([18]),
            paddle.to_tensor([0.41340091824531555, 0.34404346346855164, 0.10958535969257355, 0.16495533287525177, 0.38543465733528137, 0.427528440952301, 0.3564807176589966, 0.36809980869293213, 0.08327861875295639, 0.13036733865737915, 0.19639906287193298, 0.006642288528382778, 0.22147329151630402, 0.06751163303852081, 0.1722615659236908, 0.07035315781831741, 0.44494569301605225, 0.3282506763935089], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4036770164966583, 0.2592884600162506, 0.275314599275589, 0.2988310158252716, 0.4075409173965454, 0.4906750023365021, 0.24776680767536163, 0.4779801666736603, 0.4046466052532196, 0.11128267645835876, 0.14964336156845093, 0.04032799229025841, 0.39247414469718933, 0.3248269855976105, 0.04043031856417656, 0.15349385142326355, 0.186472088098526, 0.07060536742210388], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8461ce5842dd60327d86954fb6657497(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 1104, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1104], dtype='float32', min=0, max=0.5),
            paddle.uniform([1104], dtype='float32', min=0, max=0.5),
            paddle.uniform([1104], dtype='float32', min=0, max=0.5),
            paddle.uniform([1104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_befeb83f21ffae39ec7089c6443779e6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 208, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d73306961c3d220900da8f238333aa27(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_17fe0954b724a3944a7eb9bd273a3897(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.14546138048171997, 0.31612879037857056, 0.047790370881557465, 0.16686905920505524, 0.1810610443353653, 0.2776558995246887, 0.2915889024734497, 0.27222713828086853, 0.40250200033187866, 0.2442094385623932, 0.43250298500061035, 0.3756241500377655, 0.33469676971435547, 0.04890412837266922, 0.2621222734451294, 0.0671716257929802, 0.41898319125175476, 0.20809535682201385, 0.3968232572078705, 0.379779189825058, 0.43698829412460327, 0.03590956702828407, 0.10764720290899277, 0.33653444051742554, 0.04111948609352112, 0.07068689912557602, 0.3932201564311981, 0.4500427544116974, 0.45090413093566895, 0.28823021054267883], dtype='float32').reshape([30]),
            paddle.to_tensor([0.0056714992970228195, 0.0906287282705307, 0.3753158152103424, 0.35118529200553894, 0.04794679582118988, 0.20443010330200195, 0.2419189214706421, 0.26214149594306946, 0.3076992928981781, 0.029539158567786217, 0.2514982223510742, 0.34819671511650085, 0.23640982806682587, 0.33435913920402527, 0.027373291552066803, 0.3232675790786743, 0.08640384674072266, 0.15832918882369995, 0.4418087303638458, 0.01987459883093834, 0.3841298222541809, 0.27891966700553894, 0.21936051547527313, 0.3782585859298706, 0.053520917892456055, 0.32869473099708557, 0.3275797665119171, 0.3070680499076843, 0.2707492411136627, 0.34944406151771545], dtype='float32').reshape([30]),
            paddle.to_tensor([0.19969743490219116, 0.2590426504611969, 0.14827188849449158, 0.4806121289730072, 0.22005011141300201, 0.4589380621910095, 0.37876102328300476, 0.27040615677833557, 0.469865083694458, 0.24367877840995789, 0.1925014853477478, 0.47926634550094604, 0.06857559084892273, 0.39427047967910767, 0.06504876911640167, 0.2794498801231384, 0.27514299750328064, 0.2510640323162079, 0.37981781363487244, 0.4891168773174286, 0.25098615884780884, 0.2578732371330261, 0.3073441982269287, 0.17435641586780548, 0.2648399770259857, 0.18451562523841858, 0.17964933812618256, 0.38609781861305237, 0.21179820597171783, 0.39252257347106934], dtype='float32').reshape([30]),
            paddle.to_tensor([0.02691897377371788, 0.42131757736206055, 0.11707395315170288, 0.4427196681499481, 0.17734093964099884, 0.1177918016910553, 0.03462966904044151, 0.1296049803495407, 0.35341012477874756, 0.4027259349822998, 0.44710689783096313, 0.2480306625366211, 0.002516822423785925, 0.021200934424996376, 0.3809060752391815, 0.14573296904563904, 0.017466697841882706, 0.32544219493865967, 0.3676910102367401, 0.09913672506809235, 0.12497304379940033, 0.4907895624637604, 0.41514500975608826, 0.1933000683784485, 0.41958358883857727, 0.06249653920531273, 0.46704286336898804, 0.23378026485443115, 0.25918570160865784, 0.10114261507987976], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f8b342c585f3543dab4a709c2ab08ccb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.013839483261108398, 0.11023656278848648, 0.19199058413505554, 0.4690311849117279, 0.35645171999931335, 0.342123806476593, 0.12177947163581848, 0.49707284569740295, 0.14105333387851715, 0.1713590770959854, 0.43001240491867065, 0.22814801335334778, 0.4998511075973511, 0.33806782960891724, 0.04524315893650055, 0.3951587677001953, 0.32183021306991577, 0.24546536803245544, 0.0302700437605381, 0.16915711760520935, 0.4689449369907379, 0.2409590482711792, 0.27512943744659424, 0.17375849187374115, 0.4420104920864105, 0.3756902515888214, 0.3438292443752289, 0.4198892116546631, 0.24776990711688995, 0.2666245400905609], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4814158082008362, 0.2720497250556946, 0.07977288961410522, 0.24480989575386047, 0.42559555172920227, 0.15306733548641205, 0.3932790458202362, 0.01715392805635929, 0.2528358995914459, 0.23348824679851532, 0.3309863209724426, 0.41233083605766296, 0.466421514749527, 0.2090729922056198, 0.07230952382087708, 0.16827590763568878, 0.05712355673313141, 0.08078678697347641, 0.18145358562469482, 0.3572651743888855, 0.08545979857444763, 0.3092324137687683, 0.44642651081085205, 0.4748148024082184, 0.33709773421287537, 0.4035044312477112, 0.269713819026947, 0.22641445696353912, 0.29441171884536743, 0.4266332983970642], dtype='float32').reshape([30]),
            paddle.to_tensor([0.007978104054927826, 0.38261908292770386, 0.3057301938533783, 0.21560706198215485, 0.3786048889160156, 0.40342050790786743, 0.18441390991210938, 0.22859029471874237, 0.28143444657325745, 0.10580829530954361, 0.4865188002586365, 0.07284525036811829, 0.19203807413578033, 0.3348606824874878, 0.2588627338409424, 0.14297686517238617, 0.012076690793037415, 0.41068235039711, 0.025532620027661324, 0.4883270859718323, 0.30760622024536133, 0.41688403487205505, 0.49039459228515625, 0.07439851760864258, 0.19291867315769196, 0.023577479645609856, 0.024216724559664726, 0.2577161192893982, 0.19554151594638824, 0.15516497194766998], dtype='float32').reshape([30]),
            paddle.to_tensor([0.43307510018348694, 0.17241300642490387, 0.30593541264533997, 0.2172301858663559, 0.49801185727119446, 0.4143105149269104, 0.21488790214061737, 0.04231348633766174, 0.14419251680374146, 0.09387265890836716, 0.3713349401950836, 0.3638269305229187, 0.40970396995544434, 0.3686836361885071, 0.43025997281074524, 0.055757734924554825, 0.02741967886686325, 0.20706400275230408, 0.23474346101284027, 0.34234094619750977, 0.3810363709926605, 0.49342480301856995, 0.4307945668697357, 0.2211393415927887, 0.2700640559196472, 0.3987504839897156, 0.33798685669898987, 0.2499404102563858, 0.2533017098903656, 0.20014071464538574], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f8fe8b09acad642601bb0047b693dd3a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_805c0f59c0624394cbd423c4ea077374(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 1872, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1872], dtype='float32', min=0, max=0.5),
            paddle.uniform([1872], dtype='float32', min=0, max=0.5),
            paddle.uniform([1872], dtype='float32', min=0, max=0.5),
            paddle.uniform([1872], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5c725a07a4d97fc88a1e817188bb46c7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3261462450027466, 0.1612156182527542, 0.3867325782775879, 0.18181338906288147, 0.020162303000688553, 0.27890607714653015, 0.3807852268218994, 0.37952494621276855, 0.05383026972413063, 0.14238794147968292, 0.4606415033340454, 0.057945478707551956, 0.270212322473526, 0.25505220890045166, 0.3894635736942291, 0.11654341965913773, 0.3083970248699188, 0.08748811483383179, 0.17521601915359497, 0.252258837223053, 0.08474014699459076, 0.43853870034217834, 0.37010014057159424, 0.4814014732837677, 0.2254542112350464, 0.3262951970100403, 0.15890006721019745, 0.43894511461257935, 0.19780582189559937, 0.28801655769348145], dtype='float32').reshape([30]),
            paddle.to_tensor([0.448954701423645, 0.023029569536447525, 0.14141437411308289, 0.28351467847824097, 0.14167901873588562, 0.11381974071264267, 0.32059282064437866, 0.041587069630622864, 0.2170834094285965, 0.11732038110494614, 0.21058997511863708, 0.1535552442073822, 0.04328867793083191, 0.41317111253738403, 0.12042875587940216, 0.4348510503768921, 0.44452524185180664, 0.39230263233184814, 0.37929901480674744, 0.24957074224948883, 0.38507330417633057, 0.040314048528671265, 0.1462288498878479, 0.4358327090740204, 0.17658603191375732, 0.24162648618221283, 0.3246363699436188, 0.20608572661876678, 0.48797866702079773, 0.3821265995502472], dtype='float32').reshape([30]),
            paddle.to_tensor([0.40106767416000366, 0.10331235826015472, 0.3649585545063019, 0.2616981267929077, 0.3996867835521698, 0.06466524302959442, 0.029819970950484276, 0.09180445969104767, 0.32966405153274536, 0.40178877115249634, 0.029308613389730453, 0.06767963618040085, 0.37182942032814026, 0.46708258986473083, 0.3990817964076996, 0.4712066054344177, 0.12309832125902176, 0.0774875208735466, 0.22563588619232178, 0.39247003197669983, 0.007308224216103554, 0.2781006693840027, 0.14238935708999634, 0.3064476251602173, 0.06310991197824478, 0.2722763121128082, 0.15601055324077606, 0.18511678278446198, 0.06567244231700897, 0.4749406576156616], dtype='float32').reshape([30]),
            paddle.to_tensor([0.24974925816059113, 0.058038584887981415, 0.47960716485977173, 0.28579211235046387, 0.17460645735263824, 0.39997416734695435, 0.442191481590271, 0.4680020213127136, 0.16617214679718018, 0.2823851704597473, 0.4852907657623291, 0.4632781147956848, 0.19059468805789948, 0.34802713990211487, 0.39245492219924927, 0.19690315425395966, 0.40686821937561035, 0.02628351002931595, 0.073040671646595, 0.22776059806346893, 0.03668917715549469, 0.30278992652893066, 0.17308346927165985, 0.30350548028945923, 0.08589381724596024, 0.23864544928073883, 0.1756003051996231, 0.4387890100479126, 0.020438024774193764, 0.4972749650478363], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_68fe4194ba8c9522cdb174aa34991fed(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_32ca100fe28d863513f198491980d310(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 10, 10], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3ce7b82c8793cc48db55f1829afa9fab(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4153004586696625, 0.27898108959198, 0.33354178071022034, 0.2712521553039551, 0.2823652923107147, 0.2331008017063141, 0.37507936358451843, 0.39545154571533203], dtype='float32').reshape([8]),
            paddle.to_tensor([0.2564330995082855, 0.45171815156936646, 0.2650136947631836, 0.2651011347770691, 0.4425996243953705, 0.41095149517059326, 0.20849595963954926, 0.2539786100387573], dtype='float32').reshape([8]),
            paddle.to_tensor([0.2534286379814148, 0.4781047999858856, 0.1724180430173874, 0.0031340601854026318, 0.37535518407821655, 0.37696215510368347, 0.36133459210395813, 0.35386863350868225], dtype='float32').reshape([8]),
            paddle.to_tensor([0.06429338455200195, 0.47749558091163635, 0.0886954516172409, 0.3519575595855713, 0.2059379369020462, 0.003950865939259529, 0.43434011936187744, 0.27117976546287537], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_26a9083aa3e4499ecf4991978cf40a8d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.30360281467437744, 0.440216988325119, 0.17746677994728088, 0.2806521952152252, 0.2983764410018921, 0.36957651376724243, 0.155715674161911, 0.08399659395217896, 0.388194739818573, 0.0063703409396111965, 0.27424824237823486, 0.49182644486427307, 0.34936895966529846, 0.09067177027463913, 0.02210896462202072, 0.3626307547092438, 0.029796088114380836, 0.029492486268281937], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3555825352668762, 0.20369462668895721, 0.2746966779232025, 0.4239942133426666, 0.076015904545784, 0.3551502227783203, 0.21133175492286682, 0.26636072993278503, 0.08432503789663315, 0.289994478225708, 0.20077961683273315, 0.35727769136428833, 0.14398962259292603, 0.400728315114975, 0.1482422947883606, 0.3613673150539398, 0.49365487694740295, 0.08028508722782135], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4831150472164154, 0.1172480583190918, 0.4318172037601471, 0.23927362263202667, 0.414202481508255, 0.38741183280944824, 0.017038919031620026, 0.041259102523326874, 0.03977403789758682, 0.2688652276992798, 0.3325192332267761, 0.35699841380119324, 0.1392088383436203, 0.026162363588809967, 0.045613452792167664, 0.04319988563656807, 0.1790328174829483, 0.3622167408466339], dtype='float32').reshape([18]),
            paddle.to_tensor([0.13454319536685944, 0.23518481850624084, 0.41276758909225464, 0.49987590312957764, 0.1617707759141922, 0.4720308780670166, 0.15818829834461212, 0.36647331714630127, 0.13234513998031616, 0.28426992893218994, 0.3898060619831085, 0.40086761116981506, 0.11117612570524216, 0.10312464088201523, 0.43180611729621887, 0.155214324593544, 0.48136743903160095, 0.1793920397758484], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_088e8c3bf22cd720ecc61c26302550f1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 152, 152], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_40639c8898c4a9a6c8726feef93a57be(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b1defb8a13013605c7eae8fa75e0b38e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c1b5c8dc4a137fe7a2d4eb231a6b294f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.39263013005256653, 0.06428784877061844, 0.4199657738208771, 0.37987545132637024, 0.33308109641075134, 0.29317259788513184, 0.46924692392349243, 0.00861548725515604, 0.38715261220932007, 0.1912955641746521, 0.025087963789701462, 0.38961488008499146, 0.055582065135240555, 0.2837893068790436, 0.018304066732525826, 0.4984762370586395, 0.45627540349960327, 0.1392185389995575, 0.4047239422798157, 0.40065038204193115, 0.0427217036485672, 0.3209211528301239, 0.10266423970460892, 0.03207550570368767, 0.2748318314552307, 0.44048383831977844, 0.15290863811969757, 0.07741926610469818, 0.06077894568443298, 0.43545281887054443], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4426770806312561, 0.36145004630088806, 0.1009245291352272, 0.26272910833358765, 0.05317584052681923, 0.270346075296402, 0.063735231757164, 0.3180675208568573, 0.13761572539806366, 0.24428625404834747, 0.3549010157585144, 0.3062070906162262, 0.021869370713829994, 0.29538583755493164, 0.3086462914943695, 0.21792161464691162, 0.3640701174736023, 0.1475560963153839, 0.14940813183784485, 0.3766076862812042, 0.47042158246040344, 0.177782341837883, 0.16415119171142578, 0.08461430668830872, 0.33759739995002747, 0.47150611877441406, 0.02549121342599392, 0.33522024750709534, 0.3225476145744324, 0.23634915053844452], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3729441165924072, 0.03819316253066063, 0.03875860944390297, 0.23540548980236053, 0.004102788399904966, 0.09440438449382782, 0.4654962122440338, 0.4777255952358246, 0.33921658992767334, 0.2731693685054779, 0.36914271116256714, 0.33350124955177307, 0.4475255608558655, 0.3717983663082123, 0.025715358555316925, 0.006330952048301697, 0.4818085730075836, 0.46190398931503296, 0.37012481689453125, 0.10292984545230865, 0.16707094013690948, 0.22291819751262665, 0.3643803894519806, 0.1618821620941162, 0.427615761756897, 0.14673785865306854, 0.28145283460617065, 0.18114370107650757, 0.24647851288318634, 0.1979457288980484], dtype='float32').reshape([30]),
            paddle.to_tensor([0.29545506834983826, 0.0009722498361952603, 0.23470620810985565, 0.30483731627464294, 0.07925067096948624, 0.449590802192688, 0.2520422041416168, 0.44731491804122925, 0.34187063574790955, 0.26190486550331116, 0.341225802898407, 0.4267931580543518, 0.41436561942100525, 0.23660168051719666, 0.4212053418159485, 0.12833432853221893, 0.49704715609550476, 0.22156290709972382, 0.2613593637943268, 0.0041923788376152515, 0.39636480808258057, 0.44529202580451965, 0.28966188430786133, 0.42019158601760864, 0.21676966547966003, 0.11491423845291138, 0.15452364087104797, 0.3249296545982361, 0.2832091450691223, 0.32115957140922546], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_347c789a51ff29816e6da6f0ff0e3cc7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_08b1593269fc64e369258f63a0a0e0e5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.06100389361381531, 0.17151115834712982, 0.3166654706001282, 0.4929671287536621, 0.16879448294639587, 0.25647860765457153, 0.15804165601730347, 0.06141442060470581, 0.4786432981491089, 0.3087090253829956, 0.4993041753768921, 0.29513418674468994, 0.04903220385313034, 0.233463853597641, 0.1308140903711319, 0.21751976013183594, 0.2632414698600769, 0.42277976870536804, 0.45342379808425903, 0.4939366579055786, 0.3921503722667694, 0.4248935282230377, 0.49104639887809753, 0.2930837571620941, 0.4864009916782379, 0.3838137090206146, 0.23023366928100586, 0.12631021440029144, 0.2627389132976532, 0.3300711512565613], dtype='float32').reshape([30]),
            paddle.to_tensor([0.0870131254196167, 0.19018307328224182, 0.20508307218551636, 0.21096552908420563, 0.40040063858032227, 0.017239857465028763, 0.19833296537399292, 0.2742234170436859, 0.25885555148124695, 0.3244304656982422, 0.2681864798069, 0.2859663963317871, 0.48107919096946716, 0.10477569699287415, 0.188946932554245, 0.4758313000202179, 0.2992691397666931, 0.46644753217697144, 0.24359571933746338, 0.4004323482513428, 0.12816722691059113, 0.12162182480096817, 0.027037156745791435, 0.013605596497654915, 0.13097117841243744, 0.10676144063472748, 0.4826611280441284, 0.144741028547287, 0.4516623020172119, 0.18601414561271667], dtype='float32').reshape([30]),
            paddle.to_tensor([0.47266048192977905, 0.2425144761800766, 0.3507820963859558, 0.08941210061311722, 0.17695151269435883, 0.2837155759334564, 0.11644930392503738, 0.47746413946151733, 0.14918197691440582, 0.17167790234088898, 0.18478234112262726, 0.3250448703765869, 0.2800889313220978, 0.07986846566200256, 0.16585977375507355, 0.1834835708141327, 0.018920274451375008, 0.4110969305038452, 0.2980922758579254, 0.38087818026542664, 0.4404503405094147, 0.31900691986083984, 0.280266672372818, 0.30637070536613464, 0.022449148818850517, 0.2672046720981598, 0.2525104582309723, 0.26527658104896545, 0.1085294634103775, 0.3844616413116455], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2844138741493225, 0.4232133626937866, 0.414511501789093, 0.06202414259314537, 0.020018944516777992, 0.20173437893390656, 0.37765440344810486, 0.08928634971380234, 0.47568434476852417, 0.07507435977458954, 0.1734497845172882, 0.4312260150909424, 0.3672281503677368, 0.043627046048641205, 0.2900548577308655, 0.41310471296310425, 0.04708592966198921, 0.28802213072776794, 0.4437906742095947, 0.4709728956222534, 0.30630722641944885, 0.18416278064250946, 0.3532120883464813, 0.08412858098745346, 0.19897954165935516, 0.31244638562202454, 0.18212518095970154, 0.035573314875364304, 0.08377736806869507, 0.23137469589710236], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5a95600310213d9cef0c304e170f98a1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 30, 30], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5d8d3a8d0e33f9074386083575fe00a5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 1584, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1584], dtype='float32', min=0, max=0.5),
            paddle.uniform([1584], dtype='float32', min=0, max=0.5),
            paddle.uniform([1584], dtype='float32', min=0, max=0.5),
            paddle.uniform([1584], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5c49f22f95dbed21b611d13d4b2d94a8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 640, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_43e1504f7b98d573467d5ff5a4119963(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 2560, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([2560], dtype='float32', min=0, max=0.5),
            paddle.uniform([2560], dtype='float32', min=0, max=0.5),
            paddle.uniform([2560], dtype='float32', min=0, max=0.5),
            paddle.uniform([2560], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_09170a8353bb707a288b7c34dd4b2b43(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.08472128957509995, 0.4900910258293152, 0.30657362937927246, 0.14665015041828156, 0.31763559579849243, 0.1958678662776947, 0.44076600670814514, 0.4379170835018158, 0.38292306661605835, 0.036918189376592636, 0.18803128600120544, 0.4694959223270416, 0.07370724529027939, 0.4839821457862854, 0.2621202766895294, 0.0007307696505449712], dtype='float32').reshape([16]),
            paddle.to_tensor([0.014039709232747555, 0.12352067977190018, 0.27667638659477234, 0.19978110492229462, 0.08444174379110336, 0.4977010190486908, 0.25723761320114136, 0.22810913622379303, 0.32274022698402405, 0.07084789872169495, 0.35292163491249084, 0.11487329751253128, 0.4509691596031189, 0.28028327226638794, 0.066947802901268, 0.3485567569732666], dtype='float32').reshape([16]),
            paddle.to_tensor([0.24227012693881989, 0.36139434576034546, 0.02817707695066929, 0.21848921477794647, 0.4805435538291931, 0.41952046751976013, 0.46698272228240967, 0.04870116338133812, 0.4838521480560303, 0.021855074912309647, 0.04407837241888046, 0.09843044728040695, 0.32048991322517395, 0.47380685806274414, 0.22610807418823242, 0.16519488394260406], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4565092921257019, 0.021445676684379578, 0.4565924406051636, 0.09839913249015808, 0.3347511291503906, 0.19684581458568573, 0.2709236443042755, 0.39757269620895386, 0.43426966667175293, 0.35118788480758667, 0.12460266053676605, 0.34853634238243103, 0.2007274031639099, 0.17348523437976837, 0.3590750992298126, 0.34462618827819824], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5fe4d50871eb116620499c019a84b57d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5489a690b7df86fed8d0da300cb82683(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.21195711195468903, 0.18111322820186615, 0.009485065005719662, 0.1663541942834854, 0.4942689538002014, 0.008299121633172035, 0.43502360582351685, 0.21991747617721558, 0.17780661582946777, 0.1654822677373886, 0.3059452772140503, 0.004616006277501583, 0.45653849840164185, 0.4080204367637634, 0.1692325919866562, 0.4597778916358948, 0.48474469780921936, 0.019748587161302567, 0.05530361086130142, 0.462897926568985, 0.18080435693264008, 0.4057922661304474, 0.47917988896369934, 0.19799813628196716], dtype='float32').reshape([24]),
            paddle.to_tensor([0.13493983447551727, 0.29268109798431396, 0.35751137137413025, 0.2418900430202484, 0.12779080867767334, 0.23087438941001892, 0.2784402370452881, 0.3750450313091278, 0.020914467051625252, 0.19129657745361328, 0.4406765401363373, 0.424765944480896, 0.4588049054145813, 0.288688987493515, 0.35659152269363403, 0.21857087314128876, 0.1733863353729248, 0.43221214413642883, 0.13982710242271423, 0.38449615240097046, 0.18502289056777954, 0.38092324137687683, 0.1422814577817917, 0.49897462129592896], dtype='float32').reshape([24]),
            paddle.to_tensor([0.42099955677986145, 0.3467497229576111, 0.050844706594944, 0.3919687569141388, 0.25750330090522766, 0.4686153829097748, 0.412718802690506, 0.1820220947265625, 0.36509817838668823, 0.06728628277778625, 0.34063753485679626, 0.06681384146213531, 0.07179614156484604, 0.0718049556016922, 0.4471607506275177, 0.144729346036911, 0.39105433225631714, 0.15373294055461884, 0.25052186846733093, 0.05881864205002785, 0.13504111766815186, 0.42220282554626465, 0.4513743221759796, 0.060511644929647446], dtype='float32').reshape([24]),
            paddle.to_tensor([0.06056147813796997, 0.036054883152246475, 0.04577288031578064, 0.3169229328632355, 0.42636650800704956, 0.29957109689712524, 0.08740881085395813, 0.43394535779953003, 0.3886737525463104, 0.1774396449327469, 0.1702091097831726, 0.3868110477924347, 0.44161853194236755, 0.4766238331794739, 0.385311484336853, 0.43310925364494324, 0.25965747237205505, 0.025207560509443283, 0.31302621960639954, 0.44004157185554504, 0.40374964475631714, 0.42758914828300476, 0.24078235030174255, 0.08585862070322037], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aed341c19c9780503d3273e379588b62(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_abc15bd94e55776fdbcb840279865c01
    def get_inputs(self):
        return [
            paddle.uniform([49, 512], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_280a0fcf46f9fafbb19e161d39835ce2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 1296, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1296], dtype='float32', min=0, max=0.5),
            paddle.uniform([1296], dtype='float32', min=0, max=0.5),
            paddle.uniform([1296], dtype='float32', min=0, max=0.5),
            paddle.uniform([1296], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3e3680e934909640ba4bfa326386a5ae(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_190f5489288e3db92fdca6b9490f8160
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d8c5e2c58f2ddb8ee7bd1e67f7ec06d3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 96, 96], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3390946686267853, 0.48418083786964417, 0.1861966848373413, 0.4119059443473816, 0.04607453569769859, 0.4380255937576294, 0.3543047606945038, 0.12417297065258026, 0.05665064975619316, 0.4566282629966736, 0.3531724810600281, 0.27528026700019836, 0.19635623693466187, 0.28544896841049194, 0.1394350528717041, 0.0929810106754303], dtype='float32').reshape([16]),
            paddle.to_tensor([0.009635505266487598, 0.37920263409614563, 0.24384453892707825, 0.36575645208358765, 0.027092140167951584, 0.009958647191524506, 0.30697986483573914, 0.051062360405921936, 0.16605231165885925, 0.095807746052742, 0.16733810305595398, 0.2138768583536148, 0.2223229855298996, 0.07975295186042786, 0.17800800502300262, 0.4462372064590454], dtype='float32').reshape([16]),
            paddle.to_tensor([0.14574608206748962, 0.054977353662252426, 0.338347464799881, 0.04416753724217415, 0.04198095202445984, 0.3342800736427307, 0.40158116817474365, 0.48142874240875244, 0.3775792121887207, 0.2691571116447449, 0.48826783895492554, 0.0892278179526329, 0.4346211552619934, 0.21425899863243103, 0.38044366240501404, 0.2788597047328949], dtype='float32').reshape([16]),
            paddle.to_tensor([0.30449312925338745, 0.11608770489692688, 0.0875747874379158, 0.3348095417022705, 0.19208486378192902, 0.3274959623813629, 0.277310848236084, 0.028086181730031967, 0.26718831062316895, 0.4812157452106476, 0.46099281311035156, 0.2359052151441574, 0.17079776525497437, 0.37304434180259705, 0.4773770272731781, 0.4065381586551666], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_69c0cbaa9e690f9682233077084ed07c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9cf068ca57710238e131e4303b518646(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_190f5489288e3db92fdca6b9490f8160
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 4, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c7274b308a03a5c7ffadc143cd0ee45c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f4b0f90408d7d2d5eeb0e1c5440cb39c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 256, 512], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a83f236fdae0ed476c57566ff6e13963(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4940708577632904, 0.3126141428947449, 0.2945505976676941, 0.2934877872467041, 0.13540011644363403, 0.18751005828380585, 0.20768435299396515, 0.38041266798973083, 0.3974311947822571, 0.04516473412513733, 0.36021268367767334, 0.04215725511312485, 0.2826055884361267, 0.029989928007125854, 0.17779958248138428, 0.12371079623699188, 0.2088773399591446, 0.1613120287656784, 0.02617960423231125, 0.17587776482105255, 0.06178323179483414, 0.27068212628364563, 0.4742787182331085, 0.008340580388903618, 0.26609355211257935, 0.33264413475990295, 0.1261967569589615, 0.4834485948085785, 0.17592217028141022, 0.3396080732345581], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4315936863422394, 0.43031057715415955, 0.20507924258708954, 0.17745479941368103, 0.4615541696548462, 0.3057343363761902, 0.12337325513362885, 0.36712440848350525, 0.36028963327407837, 0.20780718326568604, 0.18961188197135925, 0.4962827265262604, 0.2742822468280792, 0.2114616483449936, 0.49365347623825073, 0.06842691451311111, 0.40834951400756836, 0.30206161737442017, 0.32420557737350464, 0.2696823179721832, 0.4549886882305145, 0.036523137241601944, 0.3269031047821045, 0.2969520390033722, 0.17941054701805115, 0.3433779776096344, 0.31414926052093506, 0.21595604717731476, 0.06300316751003265, 0.45669978857040405], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4811457693576813, 0.0465245395898819, 0.037401046603918076, 0.264100044965744, 0.20264288783073425, 0.43355193734169006, 0.28472158312797546, 0.05245402082800865, 0.4424992799758911, 0.3729780912399292, 0.3962555229663849, 0.46373483538627625, 0.29386094212532043, 0.14264130592346191, 0.09795472025871277, 0.31807029247283936, 0.1573382169008255, 0.22565671801567078, 0.4904884397983551, 0.3870457112789154, 0.16708317399024963, 0.02437737211585045, 0.4808324873447418, 0.19839096069335938, 0.43374520540237427, 0.007968166843056679, 0.22882795333862305, 0.18878817558288574, 0.39246034622192383, 0.44981649518013], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4272633194923401, 0.2462128847837448, 0.0653102844953537, 0.19120238721370697, 0.18390822410583496, 0.4962614178657532, 0.057647302746772766, 0.20805519819259644, 0.411813348531723, 0.30936166644096375, 0.4455847144126892, 0.3841199278831482, 0.1423860341310501, 0.3353794813156128, 0.1929810494184494, 0.03816034272313118, 0.10275478661060333, 0.09548665583133698, 0.019651882350444794, 0.07930415123701096, 0.1952114701271057, 0.3612549901008606, 0.2505890429019928, 0.2975568175315857, 0.02822338417172432, 0.25285640358924866, 0.46629101037979126, 0.16496969759464264, 0.41881677508354187, 0.46857014298439026], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ae391d9b1eb5f011c624f6fc4c4124d4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 288, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([288], dtype='float32', min=0, max=0.5),
            paddle.uniform([288], dtype='float32', min=0, max=0.5),
            paddle.uniform([288], dtype='float32', min=0, max=0.5),
            paddle.uniform([288], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_204f054ad4f49edac41dfd8cd91afc16(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 64, 128], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.015401007607579231, 0.25944873690605164, 0.485954225063324, 0.448882520198822, 0.30626872181892395, 0.49789631366729736, 0.23574991524219513, 0.15273864567279816, 0.43441885709762573, 0.1447143703699112, 0.040249649435281754, 0.4565587341785431, 0.018550114706158638, 0.09147863835096359, 0.22877782583236694, 0.30948954820632935, 0.30578815937042236, 0.2879602909088135], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2004641890525818, 0.09874369204044342, 0.10997095704078674, 0.3048836886882782, 0.42552363872528076, 0.2525544762611389, 0.4363483786582947, 0.46090757846832275, 0.44086700677871704, 0.22699280083179474, 0.2008104771375656, 0.19326309859752655, 0.06050172820687294, 0.20540392398834229, 0.33408796787261963, 0.4912738800048828, 0.21742618083953857, 0.1846499741077423], dtype='float32').reshape([18]),
            paddle.to_tensor([0.19413945078849792, 0.2255202978849411, 0.029865896329283714, 0.12501174211502075, 0.06963421404361725, 0.17401394248008728, 0.07256973534822464, 0.28368765115737915, 0.4961317777633667, 0.24283716082572937, 0.39081576466560364, 0.45165619254112244, 0.07436572015285492, 0.28465044498443604, 0.3083958029747009, 0.35836881399154663, 0.33400070667266846, 0.2453167736530304], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3842291235923767, 0.09917557239532471, 0.1397269070148468, 0.3632550537586212, 0.26300540566444397, 0.2021404653787613, 0.0977841317653656, 0.4447374939918518, 0.07118240743875504, 0.04544501006603241, 0.06447581946849823, 0.1625855565071106, 0.2827392816543579, 0.24088697135448456, 0.14847372472286224, 0.014856349676847458, 0.42619457840919495, 0.45736417174339294], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aaeda74fda0482ded1e7af8560dbfcf6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_186eae946956756b13f3654169c2ef9a
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 32, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.011559154838323593, 0.124541275203228, 0.21655058860778809, 0.28941774368286133, 0.05595901235938072, 0.14404349029064178, 0.10395690053701401, 0.0042781950905919075, 0.06909405440092087, 0.3809187710285187, 0.18653932213783264, 0.26572880148887634, 0.4921606481075287, 0.09862140566110611, 0.34054887294769287, 0.2186095416545868], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3211120665073395, 0.34744518995285034, 0.060150183737277985, 0.3054143786430359, 0.04311736673116684, 0.1372971087694168, 0.31760329008102417, 0.2476769983768463, 0.15451018512248993, 0.41001975536346436, 0.0731263980269432, 0.13742735981941223, 0.19616270065307617, 0.3008551597595215, 0.315640926361084, 0.40317827463150024], dtype='float32').reshape([16]),
            paddle.to_tensor([0.39791765809059143, 0.33059293031692505, 0.18647418916225433, 0.16249796748161316, 0.18123449385166168, 0.09396179765462875, 0.46357426047325134, 0.2943837344646454, 0.3224897086620331, 0.071182481944561, 0.2750115692615509, 0.3216184675693512, 0.228185772895813, 0.14753960072994232, 0.19461508095264435, 0.21660266816616058], dtype='float32').reshape([16]),
            paddle.to_tensor([0.15873217582702637, 0.41765451431274414, 0.14209015667438507, 0.4483386278152466, 0.021412277594208717, 0.27543243765830994, 0.03835824131965637, 0.19689832627773285, 0.1485716700553894, 0.390574187040329, 0.3927048444747925, 0.4335731267929077, 0.05922849103808403, 0.2716876268386841, 0.4846254587173462, 0.4773024320602417], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_02b6fc30f35702cece9e8cec4517702a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 1152, 6, 6], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fb662e7b2c269ca32044fc4573f02037(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.41041865944862366, 0.03556537628173828, 0.36011651158332825, 0.3799631893634796, 0.36257749795913696, 0.011895520612597466, 0.41169488430023193, 0.4587922990322113, 0.288282185792923, 0.034163810312747955, 0.4234016239643097, 0.15946435928344727, 0.25852280855178833, 0.2834390103816986, 0.08468683063983917, 0.26481786370277405, 0.0004519840585999191, 0.4757150113582611, 0.42783722281455994, 0.2407224178314209], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3437062203884125, 0.04883609712123871, 0.3276449739933014, 0.09835998713970184, 0.26564717292785645, 0.44431671500205994, 0.29569342732429504, 0.27984893321990967, 0.34092846512794495, 0.3166652023792267, 0.2123669534921646, 0.3493444323539734, 0.4417209327220917, 0.11735580861568451, 0.4758804440498352, 0.3217777907848358, 0.3978559076786041, 0.3873864710330963, 0.05823184549808502, 0.01098632626235485], dtype='float32').reshape([20]),
            paddle.to_tensor([0.37047475576400757, 0.04708309844136238, 0.4636844992637634, 0.45735999941825867, 0.3629785478115082, 0.38031908869743347, 0.3320232033729553, 0.3637561798095703, 0.3894207179546356, 0.028289807960391045, 0.37860748171806335, 0.018960537388920784, 0.476291298866272, 0.4891432225704193, 0.30002251267433167, 0.20397540926933289, 0.33929139375686646, 0.4339388608932495, 0.3688740134239197, 0.254404217004776], dtype='float32').reshape([20]),
            paddle.to_tensor([0.34935808181762695, 0.2680143415927887, 0.3220333755016327, 0.11675561219453812, 0.24084673821926117, 0.3076634407043457, 0.4281001091003418, 0.073870450258255, 0.14618416130542755, 0.1959385871887207, 0.28314679861068726, 0.277394562959671, 0.20444589853286743, 0.03031720221042633, 0.340146541595459, 0.3570055365562439, 0.20736345648765564, 0.3036312758922577, 0.472013920545578, 0.16610991954803467], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_18e69ed1f46b8c5e22e126741ca90b45(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 61, 61], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_52e90e2268cec1d508c5eebee9a349a6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_13025b164297d9226f9641b7f0715c76
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.21873755753040314, 0.1369936615228653, 0.41277316212654114, 0.25952979922294617, 0.3970128893852234, 0.0012026338372379541, 0.44825369119644165, 0.03405817225575447, 0.3911667764186859, 0.23587848246097565, 0.3094791769981384, 0.22854691743850708, 0.00543967355042696, 0.49019181728363037, 0.11904488503932953, 0.4211347699165344], dtype='float32').reshape([16]),
            paddle.to_tensor([0.47116440534591675, 0.1463383585214615, 0.26743197441101074, 0.28083720803260803, 0.12042763084173203, 0.17665420472621918, 0.4664161801338196, 0.20922783017158508, 0.2892500162124634, 0.1469467729330063, 0.3997400104999542, 0.47400879859924316, 0.19571129977703094, 0.09080485999584198, 0.4807227551937103, 0.014933077618479729], dtype='float32').reshape([16]),
            paddle.to_tensor([0.37776753306388855, 0.24053999781608582, 0.18713393807411194, 0.16837769746780396, 0.050861526280641556, 0.44582244753837585, 0.2580665647983551, 0.4263980984687805, 0.0525459423661232, 0.3911420702934265, 0.05804755538702011, 0.4475651681423187, 0.08390729129314423, 0.11024675518274307, 0.14169876277446747, 0.47464045882225037], dtype='float32').reshape([16]),
            paddle.to_tensor([0.046508628875017166, 0.14801543951034546, 0.01913534477353096, 0.14417441189289093, 0.1192605271935463, 0.2350328117609024, 0.28184616565704346, 0.3498215675354004, 0.04288410022854805, 0.3333283066749573, 0.045919373631477356, 0.4277186095714569, 0.16971515119075775, 0.13039402663707733, 0.43921950459480286, 0.20691530406475067], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_58cf98b5697a036ae1234b3021b5f35f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 624, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([624], dtype='float32', min=0, max=0.5),
            paddle.uniform([624], dtype='float32', min=0, max=0.5),
            paddle.uniform([624], dtype='float32', min=0, max=0.5),
            paddle.uniform([624], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0ea2a67168dd79ef2eaa63b68a8405d6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 50, 84], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5c33aed88d76addb39b7a422d56f3eec(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 208, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0f118f9c877289e870e934bf0507c34c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 61, 61], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f1ae5f9c666ef11aed06e0aa37410002(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_99a143b802141762ada10063a92cba5f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0ded43a4c27acd7e18137059de3e3e97(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c8f96facba916c9b24760c1ceee0e68c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 256, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_df27ef2a751a030633ddc5d0c3043fda(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_44851024ee7c0e9a3514ad7373759305(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_77148934bef73b8ea87c25fced7c54ea(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1984283f8cca1e86fa132007125a1234(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9e39514258d0937bc3b5371feb67c672(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 228, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([228], dtype='float32', min=0, max=0.5),
            paddle.uniform([228], dtype='float32', min=0, max=0.5),
            paddle.uniform([228], dtype='float32', min=0, max=0.5),
            paddle.uniform([228], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_88d32ddfd70b0239bf07556e32b8e831(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 960, 960], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_23b02f493b69ea2f9ff0b624eb15859e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.08923085033893585, 0.29123547673225403, 0.23607929050922394, 0.21793808043003082, 0.2508123815059662, 0.11472377181053162, 0.19098903238773346, 0.0499892458319664, 0.10525113344192505, 0.3346845805644989, 0.41340315341949463, 0.48623180389404297, 0.14859342575073242, 0.0021834434010088444, 0.2940760850906372, 0.43247711658477783, 0.32666823267936707, 0.18157732486724854, 0.47478020191192627, 0.3785657584667206, 0.268364816904068, 0.23061075806617737, 0.3067884147167206, 0.2471724897623062], dtype='float32').reshape([24]),
            paddle.to_tensor([0.43925899267196655, 0.04969917982816696, 0.4686689078807831, 0.18507125973701477, 0.07343966513872147, 0.06510815024375916, 0.01985102705657482, 0.4254336655139923, 0.05255964398384094, 0.031910255551338196, 0.3637421727180481, 0.1420615166425705, 0.027394529432058334, 0.2711658477783203, 0.2936747968196869, 0.4564531445503235, 0.17468447983264923, 0.4351844787597656, 0.04974767938256264, 0.0452239066362381, 0.3285088837146759, 0.11161311715841293, 0.13057422637939453, 0.06925486028194427], dtype='float32').reshape([24]),
            paddle.to_tensor([0.310083270072937, 0.42195966839790344, 0.28858089447021484, 0.0904366597533226, 0.39207732677459717, 0.24486736953258514, 0.2564018964767456, 0.3865462839603424, 0.03339526429772377, 0.46667763590812683, 0.4011446237564087, 0.39679935574531555, 0.19446732103824615, 0.18130850791931152, 0.30483192205429077, 0.46667027473449707, 0.39092519879341125, 0.19886614382266998, 0.08540460467338562, 0.10815858095884323, 0.13018269836902618, 0.33994707465171814, 0.07443796843290329, 0.2893407344818115], dtype='float32').reshape([24]),
            paddle.to_tensor([0.37009161710739136, 0.19007089734077454, 0.34179210662841797, 0.4414931535720825, 0.47168901562690735, 0.4960800111293793, 0.3272779583930969, 0.19098159670829773, 0.0926775187253952, 0.3445790112018585, 0.166364386677742, 0.11350760608911514, 0.4045778810977936, 0.36766135692596436, 0.35587480664253235, 0.2926502227783203, 0.28526604175567627, 0.33790120482444763, 0.03604605421423912, 0.330072283744812, 0.18734760582447052, 0.2541566491127014, 0.026140639558434486, 0.058734141290187836], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_956963367071d250c7893cb11ee799ac(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 8, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b728ab24da62f191fcb17313234ee8fb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.09295497834682465, 0.025722548365592957, 0.06422818452119827, 0.03898372873663902, 0.32906895875930786, 0.4219232201576233, 0.31956803798675537, 0.20048783719539642, 0.06442268937826157, 0.1930951327085495, 0.22685863077640533, 0.23906907439231873, 0.4656972289085388, 0.24735644459724426, 0.29132142663002014, 0.21089132130146027, 0.0666373074054718, 0.36685633659362793], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3031371533870697, 0.09579239785671234, 0.0932476818561554, 0.33111366629600525, 0.3489837646484375, 0.02176978811621666, 0.47510188817977905, 0.044223204255104065, 0.25316157937049866, 0.2125585675239563, 0.1971932053565979, 0.1974344104528427, 0.20993001759052277, 0.06971094757318497, 0.32229912281036377, 0.4381139576435089, 0.150462806224823, 0.2691768407821655], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3109012842178345, 0.1134110763669014, 0.17224037647247314, 0.18357376754283905, 0.4528023600578308, 0.16150140762329102, 0.4152207374572754, 0.2368774265050888, 0.007859867997467518, 0.30276575684547424, 0.02677152119576931, 0.2254311740398407, 0.3919932246208191, 0.4250199496746063, 0.4071047306060791, 0.2067701518535614, 0.07729945331811905, 0.42972132563591003], dtype='float32').reshape([18]),
            paddle.to_tensor([0.16573317348957062, 0.003276535775512457, 0.3673282861709595, 0.4551817774772644, 0.4349232614040375, 0.24182935059070587, 0.42949485778808594, 0.20142096281051636, 0.05779968574643135, 0.026521315798163414, 0.002453960943967104, 0.01308456715196371, 0.1449110358953476, 0.07367560267448425, 0.4976945221424103, 0.2558709383010864, 0.05985128879547119, 0.035011887550354004], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_35dc18975a00f22c7e7702c45fb5af3b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 576, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1affdfae7004a2a4973569230a939606(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8af90367b69985df53c5123e1325d7eb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 100, 168], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_84ed38a13762b7bb9aa4a57ca42b7d31(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 64, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_90b2b75e6e2c574ed7b29b67a622d27a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cb1c75f38043c96dd9cb6a802765954a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.1097412258386612, 0.38912227749824524, 0.20352190732955933, 0.48031434416770935, 0.32545438408851624, 0.3096928298473358, 0.44845613837242126, 0.021092558279633522, 0.0814170241355896, 0.2118275910615921, 0.3340749442577362, 0.3141053020954132, 0.36967676877975464, 0.4026300609111786, 0.3495711386203766, 0.2505955696105957, 0.32134291529655457, 0.4717355966567993], dtype='float32').reshape([18]),
            paddle.to_tensor([0.24527625739574432, 0.18177193403244019, 0.04999072477221489, 0.29868051409721375, 0.3637673556804657, 0.431185245513916, 0.139091357588768, 0.02396211214363575, 0.2649718225002289, 0.36315497756004333, 0.3574827313423157, 0.2891542315483093, 0.4667566418647766, 0.053662266582250595, 0.16968640685081482, 0.4740963578224182, 0.24434815347194672, 0.2062014937400818], dtype='float32').reshape([18]),
            paddle.to_tensor([0.0771026685833931, 0.08854522556066513, 0.09157431125640869, 0.26731595396995544, 0.3427618443965912, 0.3271751403808594, 0.38670673966407776, 0.2666974663734436, 0.3146568238735199, 0.12350417673587799, 0.4684188961982727, 0.25573545694351196, 0.32395878434181213, 0.13238386809825897, 0.26790162920951843, 0.2979985475540161, 0.07930094748735428, 0.4003696143627167], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4462992548942566, 0.4253186583518982, 0.03804847225546837, 0.18923626840114594, 0.11230511218309402, 0.47651153802871704, 0.10045086592435837, 0.15489599108695984, 0.2904762625694275, 0.4983534514904022, 0.17404446005821228, 0.040419068187475204, 0.17743171751499176, 0.4999631643295288, 0.13071300089359283, 0.44351667165756226, 0.03395833447575569, 0.29697033762931824], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_01b623089e8f3b6194089c2f1e57998f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.20907403528690338, 0.40098094940185547, 0.04913853108882904, 0.13927872478961945, 0.4790906310081482, 0.48561805486679077, 0.3746221363544464, 0.23577554523944855, 0.4015968441963196, 0.25111445784568787, 0.2663329541683197, 0.4493800401687622, 0.48454317450523376, 0.4194555878639221, 0.0036014867946505547, 0.0899820327758789, 0.39219698309898376, 0.29190507531166077, 0.0055603086948394775, 0.20682393014431, 0.3765419125556946, 0.015949366614222527, 0.07506459951400757, 0.16369910538196564, 0.30475497245788574, 0.016943419352173805, 0.1672579050064087, 0.22149141132831573, 0.0024993321858346462, 0.11895152181386948], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2056029736995697, 0.3291911482810974, 0.09764304012060165, 0.3388054370880127, 0.03629693016409874, 0.1549648940563202, 0.25339484214782715, 0.05083363130688667, 0.4155140519142151, 0.1663934290409088, 0.27335554361343384, 0.011370338499546051, 0.10398231446743011, 0.4545817971229553, 0.2860167622566223, 0.47966185212135315, 0.3236047327518463, 0.2835494875907898, 0.47406116127967834, 0.099497951567173, 0.22835126519203186, 0.4106844961643219, 0.17082414031028748, 0.0729893296957016, 0.24749545753002167, 0.49417662620544434, 0.36623165011405945, 0.45066288113594055, 0.1268652081489563, 0.48688817024230957], dtype='float32').reshape([30]),
            paddle.to_tensor([0.44523635506629944, 0.49063581228256226, 0.007458699867129326, 0.2759232521057129, 0.38447725772857666, 0.22050800919532776, 0.4274255633354187, 0.48283904790878296, 0.2822662889957428, 0.4083654284477234, 0.20764382183551788, 0.16313663125038147, 0.3480912148952484, 0.018326906487345695, 0.22005417943000793, 0.21291671693325043, 0.2661254405975342, 0.20931608974933624, 0.39165353775024414, 0.3946807086467743, 0.022221354767680168, 0.28710225224494934, 0.23373360931873322, 0.3156733214855194, 0.13571277260780334, 0.2103451043367386, 0.014237314462661743, 0.23534654080867767, 0.1121847927570343, 0.19898274540901184], dtype='float32').reshape([30]),
            paddle.to_tensor([0.16890861093997955, 0.12989391386508942, 0.0907568633556366, 0.3420048952102661, 0.44069603085517883, 0.31848984956741333, 0.09851431101560593, 0.07749917358160019, 0.2572435140609741, 0.16528014838695526, 0.1570003181695938, 0.02988220937550068, 0.350873202085495, 0.4712710976600647, 0.33636903762817383, 0.08823796361684799, 0.401199072599411, 0.47383326292037964, 0.09503743052482605, 0.3325773775577545, 0.38818615674972534, 0.22110599279403687, 0.4464859068393707, 0.3441787660121918, 0.2053704857826233, 0.23897194862365723, 0.4252418875694275, 0.24520522356033325, 0.18706724047660828, 0.07709956169128418], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_31135cfbb99f8c9b96c253ef8f5d44d9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 2, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_23ee8d35a9a4fb46fddb69dcc20a3803(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_190f5489288e3db92fdca6b9490f8160
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_df402d7e2523a56d48521d597ad0e14c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 15, 15], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e54954f1de87e2092c0100e9c15fc9f2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_186eae946956756b13f3654169c2ef9a
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 32, 8, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cc3361564c8f1ccaae657849bf05a5af(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 512, 1024], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.17309962213039398, 0.12586693465709686, 0.051698822528123856, 0.0903661847114563, 0.21480132639408112, 0.2257397174835205, 0.054270610213279724, 0.45760247111320496, 0.40215229988098145, 0.3037509620189667, 0.15110884606838226, 0.09529615193605423, 0.15998293459415436, 0.26957711577415466, 0.10516266524791718, 0.005591997876763344], dtype='float32').reshape([16]),
            paddle.to_tensor([0.39745694398880005, 0.07504505664110184, 0.3088572025299072, 0.22471532225608826, 0.1725989133119583, 0.03189980983734131, 0.08871262520551682, 0.18016064167022705, 0.2527386546134949, 0.3911970853805542, 0.2870738208293915, 0.3561650514602661, 0.4980582594871521, 0.26666000485420227, 0.31089654564857483, 0.24358266592025757], dtype='float32').reshape([16]),
            paddle.to_tensor([0.16013263165950775, 0.3015562891960144, 0.10161962360143661, 0.38559621572494507, 0.1705847680568695, 0.4915679395198822, 0.4705629050731659, 0.1500975787639618, 0.03465097397565842, 0.33730751276016235, 0.10780379921197891, 0.2166300117969513, 0.35213735699653625, 0.1909899264574051, 0.4069804549217224, 0.26882460713386536], dtype='float32').reshape([16]),
            paddle.to_tensor([0.1172625869512558, 0.2400456666946411, 0.02695801481604576, 0.46902939677238464, 0.4909965693950653, 0.44583094120025635, 0.17804548144340515, 0.2434786558151245, 0.1632828563451767, 0.08550861477851868, 0.06628318876028061, 0.4363638460636139, 0.4984699487686157, 0.28057336807250977, 0.17565757036209106, 0.18884241580963135], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8cb467713ecf1fa6ca2b529255065733(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3392932415008545, 0.12941519916057587, 0.09855390340089798, 0.3055537939071655, 0.23153351247310638, 0.4967758059501648, 0.05091498792171478, 0.3803443908691406, 0.13750874996185303, 0.4589601457118988, 0.19874782860279083, 0.05899178609251976, 0.3718815743923187, 0.19680869579315186, 0.4410713016986847, 0.17681139707565308], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4060444235801697, 0.36206239461898804, 0.29666322469711304, 0.40222597122192383, 0.05191454663872719, 0.2565002739429474, 0.35324791073799133, 0.09810301661491394, 0.06236269697546959, 0.449317991733551, 0.4695891737937927, 0.44028621912002563, 0.04135480523109436, 0.19204024970531464, 0.4523034691810608, 0.46717673540115356], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4765007495880127, 0.3072095811367035, 0.017157748341560364, 0.39487794041633606, 0.48376938700675964, 0.2512962818145752, 0.1294187307357788, 0.026791932061314583, 0.11961095035076141, 0.08937426656484604, 0.00377816054970026, 0.36432933807373047, 0.19344380497932434, 0.4504242539405823, 0.3319967985153198, 0.3684542179107666], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2783433496952057, 0.43911293148994446, 0.34778815507888794, 0.49023669958114624, 0.46718335151672363, 0.3211107850074768, 0.23232589662075043, 0.41287490725517273, 0.18173979222774506, 0.3130493462085724, 0.4534425735473633, 0.2063133716583252, 0.476601779460907, 0.07524550706148148, 0.20841725170612335, 0.12001355737447739], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d33a1c1d08a8add0971e65c3b4f26016(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_0336412036bb12c89542873a9d0cc312
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_192bd8ebb26218fec1f92881a23512f3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_800ffb259188eec1dd681034bacba1ca(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d15e59f93e5f2907514a66f468a24957(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([8, 512, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f1ef01c49d998446495eacea76a362ae(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 512, 512], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3714825fd56d6eb7569cfcf5167761f9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_88ba07e189a7c5b7ff495e08b1b7a235(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_db211f821ab41d4705c6366c78c12d38(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_52b9364df711306ed88e0585ff24a28c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_332d90aada4b986a212a0fe54f1df1c8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e8abf7ab677e086269e2fb95df7cf7fa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 100], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0ccf8318c3148b2e6ff66f998f86970d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fd04b877a358e6410bab4f9d229075ba(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 8, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_09cab0d9b50ac52337ca03b753a647ce(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 256, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_75a8347a718510a6dee77c5ec79ee7ca(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2de978387639a874fcd4c198034ef71a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 96, 96], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ede97cddb180af3678376e2f021e0da5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3317728638648987, 0.07083127647638321, 0.338352233171463, 0.09960247576236725, 0.4959547519683838, 0.007649327628314495, 0.025693539530038834, 0.2737681269645691, 0.35108423233032227, 0.43998000025749207, 0.13643191754817963, 0.32581260800361633, 0.3245624303817749, 0.23052433133125305, 0.33014562726020813, 0.29866448044776917], dtype='float32').reshape([16]),
            paddle.to_tensor([0.34928983449935913, 0.49773865938186646, 0.10179922729730606, 0.1979302614927292, 0.31347766518592834, 0.2830885946750641, 0.2828364372253418, 0.15339496731758118, 0.3783782720565796, 0.22561822831630707, 0.1844668686389923, 0.44171062111854553, 0.1509658694267273, 0.3984713554382324, 0.2091366946697235, 0.050685733556747437], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2267502248287201, 0.38795146346092224, 0.445113867521286, 0.3044244647026062, 0.013503401540219784, 0.006209423299878836, 0.026375824585556984, 0.2062993049621582, 0.030861664563417435, 0.07413889467716217, 0.20399609208106995, 0.448574423789978, 0.13016870617866516, 0.1786210536956787, 0.24997256696224213, 0.47152239084243774], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3228156864643097, 0.45770537853240967, 0.3294861316680908, 0.12662281095981598, 0.31772106885910034, 0.41914939880371094, 7.281388388946652e-05, 0.1536945104598999, 0.4596751630306244, 0.20660844445228577, 0.08755796402692795, 0.02893858402967453, 0.3291875123977661, 0.358367919921875, 0.3264123797416687, 0.06298815459012985], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8b813bec3f0c19b26974fc0e42ce82f4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b2f98e6e543dc7c758ff35bfbe4e8bd6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 120, 120], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b459e872ad7a65e35cc906f02d2fba32(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fa7347bd8efe00fd58f1075f2b3fb4a2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 25, 42], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_25e2ac9e88439bbaa4e2f572788684ff(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7c95d3f2a46998882f66651ed45a86f2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 70, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([70], dtype='float32', min=0, max=0.5),
            paddle.uniform([70], dtype='float32', min=0, max=0.5),
            paddle.uniform([70], dtype='float32', min=0, max=0.5),
            paddle.uniform([70], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4cbf85ce5256fa6f3ff5a9df28f0b263(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0619b54eea39cce51b39b633d7f9106d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 30, 30], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7d6ae2c714742eb16d7d469454021b6d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3284372091293335, 0.3876607418060303, 0.01445606630295515, 0.4824941158294678, 0.02872198447585106, 0.39906933903694153, 0.026326216757297516, 0.0085687804967165], dtype='float32').reshape([8]),
            paddle.to_tensor([0.43287229537963867, 0.197017103433609, 0.06303902715444565, 0.2712986171245575, 0.3162893056869507, 0.06737510859966278, 0.03221432492136955, 0.357036292552948], dtype='float32').reshape([8]),
            paddle.to_tensor([0.07757894694805145, 0.34064123034477234, 0.13905233144760132, 0.48163649439811707, 0.4704464375972748, 0.23144018650054932, 0.4220276474952698, 0.285566121339798], dtype='float32').reshape([8]),
            paddle.to_tensor([0.4190084636211395, 0.362041175365448, 0.29075783491134644, 0.08997370302677155, 0.00998803973197937, 0.37568894028663635, 0.11744344979524612, 0.3755228817462921], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_abb768182a008dfc161075002751eed1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3821f3db8465fe5f8f96b472994c0046(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aa8302147314cd0f38685bb4a174b6ab(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 16, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6bd1a29c67308e46449e80e1f58acf65(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4cf693d623d4e32d54716b4f178d9ece(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 8, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0c11284738570c9bae7c708ea399ed9a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4418cca0027c41d05aca60f7dc3cc005(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.022515645250678062, 0.41474443674087524, 0.006285642273724079, 0.41433393955230713, 0.42068272829055786, 0.08978991955518723, 0.49607929587364197, 0.0010496582835912704, 0.14615754783153534, 0.28247740864753723, 0.22021973133087158, 0.05573200434446335, 0.04493992403149605, 0.22071127593517303, 0.4813658893108368, 0.24888113141059875, 0.2553737461566925, 0.10079675167798996, 0.11251527816057205, 0.33101150393486023, 0.4610235393047333, 0.3690270483493805, 0.1915832906961441, 0.4924270212650299, 0.20917899906635284, 0.4997445344924927, 0.12647870182991028, 0.19833491742610931], dtype='float32').reshape([28]),
            paddle.to_tensor([0.41165274381637573, 0.3423635959625244, 0.3829147517681122, 0.35550618171691895, 0.30985793471336365, 0.13668815791606903, 0.1893695741891861, 0.31838563084602356, 0.16965408623218536, 0.21821798384189606, 0.3219045400619507, 0.2997877895832062, 0.43882396817207336, 0.2583630681037903, 0.08831347525119781, 0.48446032404899597, 0.21567748486995697, 0.03896842896938324, 0.2895923852920532, 0.19117628037929535, 0.4072529673576355, 0.3760918974876404, 0.09502144157886505, 0.25424355268478394, 0.33839553594589233, 0.15630686283111572, 0.471312940120697, 0.2142484337091446], dtype='float32').reshape([28]),
            paddle.to_tensor([0.33748874068260193, 0.19766224920749664, 0.2815045416355133, 0.04514758661389351, 0.43397486209869385, 0.10283705592155457, 0.4045077860355377, 0.03748835250735283, 0.42107802629470825, 0.18571098148822784, 0.4259681701660156, 0.0487491637468338, 0.023625213652849197, 0.4448549151420593, 0.4434424340724945, 0.0015296624042093754, 0.4532475173473358, 0.005845528095960617, 0.44982942938804626, 0.42255741357803345, 0.40820032358169556, 0.16163502633571625, 0.2604832947254181, 0.29348599910736084, 0.39357784390449524, 0.08154359459877014, 0.23070961236953735, 0.26159197092056274], dtype='float32').reshape([28]),
            paddle.to_tensor([0.354844868183136, 0.15216313302516937, 0.3522353172302246, 0.05749402195215225, 0.09344029426574707, 0.1635911613702774, 0.08259731531143188, 0.19544491171836853, 0.4728376567363739, 0.18973661959171295, 0.00099219661206007, 0.3670472800731659, 0.2801734209060669, 0.11415380239486694, 0.35405299067497253, 0.39084693789482117, 0.16042308509349823, 0.39420539140701294, 0.3816518783569336, 0.06586522608995438, 0.09069609642028809, 0.4077693223953247, 0.4314635992050171, 0.24182698130607605, 0.16742323338985443, 0.3372974693775177, 0.41731929779052734, 0.4210146367549896], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_42b8ddf67405452e151827e69024d410(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2542347311973572, 0.04880398511886597, 0.2875441908836365, 0.208971306681633, 0.46377167105674744, 0.45163995027542114, 0.22979635000228882, 0.20152738690376282, 0.2970291078090668, 0.4237397313117981, 0.13477617502212524, 0.26671192049980164, 0.190313458442688, 0.0439022034406662, 0.44948387145996094, 0.481729656457901, 0.21271604299545288, 0.42640259861946106, 0.029492845758795738, 0.1802329272031784], dtype='float32').reshape([20]),
            paddle.to_tensor([0.12582986056804657, 0.3583904206752777, 0.4976862370967865, 0.26541581749916077, 0.32733750343322754, 0.07201425731182098, 0.44147610664367676, 0.12745116651058197, 0.06791932880878448, 0.23327754437923431, 0.3623807728290558, 0.18590016663074493, 0.3259177803993225, 0.03677685558795929, 0.43407517671585083, 0.4215570092201233, 0.1480596363544464, 0.40511584281921387, 0.30448925495147705, 0.23512467741966248], dtype='float32').reshape([20]),
            paddle.to_tensor([0.40145593881607056, 0.3848341405391693, 0.29328715801239014, 0.08649270981550217, 0.22557443380355835, 0.0011759370099753141, 0.3952970504760742, 0.49119049310684204, 0.36704495549201965, 0.2183811217546463, 0.3176499009132385, 0.13947279751300812, 0.09610678255558014, 0.09055010229349136, 0.030512144789099693, 0.012965152971446514, 0.44669702649116516, 0.10423378646373749, 0.37988409399986267, 0.09527136385440826], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4057486653327942, 0.28092288970947266, 0.12804536521434784, 0.44504314661026, 0.05679401382803917, 0.07257263362407684, 0.14559373259544373, 0.232568621635437, 0.35194018483161926, 0.38849329948425293, 0.09263832867145538, 0.1730017513036728, 0.48438042402267456, 0.47469407320022583, 0.053956471383571625, 0.07956085354089737, 0.21881824731826782, 0.092872753739357, 0.3017481863498688, 0.01796054095029831], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5ea038a1530c8821aab6e0a2171e3be8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 1280, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7235363adba033145027102f57a9a777(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([8, 128, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_136bce240ff5e5b06dd99bf0a374d031(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 17, 17], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a81804714341c5c7c10ce426a37c858d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_46b99f082aecf111d7440924c2e678da(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.14585500955581665, 0.017270896583795547, 0.4221425950527191, 0.2854897081851959, 0.36174219846725464, 0.28488045930862427, 0.38960713148117065, 0.37952354550361633, 0.20425191521644592, 0.269528329372406, 0.27779504656791687, 0.013325054198503494, 0.28086692094802856, 0.30283093452453613, 0.36604416370391846, 0.4166794419288635, 0.16322411596775055, 0.06121331453323364, 0.12668579816818237, 0.4224090278148651], dtype='float32').reshape([20]),
            paddle.to_tensor([0.04070345312356949, 0.16515587270259857, 0.022858919575810432, 0.1297449767589569, 0.14362235367298126, 0.3447956442832947, 0.2745751738548279, 0.1736283004283905, 0.11016788333654404, 0.18979763984680176, 0.4946822226047516, 0.15243230760097504, 0.1210179477930069, 0.1450933814048767, 0.17961126565933228, 0.4376625120639801, 0.22661837935447693, 0.27894487977027893, 0.07074464112520218, 0.1371316760778427], dtype='float32').reshape([20]),
            paddle.to_tensor([0.20808465778827667, 0.1562686413526535, 0.3178386390209198, 0.49543923139572144, 0.3526775538921356, 0.3610720932483673, 0.34087154269218445, 0.23303978145122528, 0.27757957577705383, 0.40104398131370544, 0.2505250573158264, 0.27562040090560913, 0.23276546597480774, 0.16197048127651215, 0.16038303077220917, 0.48074671626091003, 0.13901767134666443, 0.22839675843715668, 0.2591753304004669, 0.18108640611171722], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3792310059070587, 0.38060981035232544, 0.37930506467819214, 0.4955160915851593, 0.25685909390449524, 0.05344896763563156, 0.029693912714719772, 0.43002304434776306, 0.13110747933387756, 0.3137838542461395, 0.05874563008546829, 0.2754287123680115, 0.07475177198648453, 0.4961361587047577, 0.23953458666801453, 0.4596402049064636, 0.14212974905967712, 0.36454853415489197, 0.3314458429813385, 0.39545905590057373], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_344ed08b3abdcd38fbe80929ce6bd931(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.47642454504966736, 0.3843069076538086, 0.26128560304641724, 0.3967403471469879, 0.07627438753843307, 0.2688705027103424, 0.3105244040489197, 0.3194432854652405, 0.47744134068489075, 0.12844142317771912, 0.33645546436309814, 0.4274417757987976, 0.12344878911972046, 0.39287522435188293, 0.0675618126988411, 0.4698594808578491, 0.1914316713809967, 0.06930914521217346, 0.188674658536911, 0.33775222301483154, 0.4147154986858368, 0.06627783924341202, 0.4513569176197052, 0.48653995990753174, 0.06081810221076012, 0.15570658445358276, 0.03803786262869835, 0.04319063574075699, 0.2876277267932892, 0.10794652998447418], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3774990141391754, 0.18462178111076355, 0.22711050510406494, 0.19853579998016357, 0.0939556360244751, 0.13964426517486572, 0.32802850008010864, 0.03300559148192406, 0.3564373552799225, 0.26025497913360596, 0.37238168716430664, 0.27941814064979553, 0.01664802059531212, 0.2086709886789322, 0.24587255716323853, 0.3168933391571045, 0.46291500329971313, 0.492944598197937, 0.2198963165283203, 0.44388434290885925, 0.0959101989865303, 0.3214467465877533, 0.1622922420501709, 0.033184271305799484, 0.4515220820903778, 0.11618221551179886, 0.2768895626068115, 0.07947956770658493, 0.3142755329608917, 0.14602908492088318], dtype='float32').reshape([30]),
            paddle.to_tensor([0.053513940423727036, 0.27837806940078735, 0.2407517433166504, 0.22614628076553345, 0.28477126359939575, 0.45884037017822266, 0.4494498670101166, 0.3546009957790375, 0.4139555096626282, 0.3098005950450897, 0.33478042483329773, 0.3297714293003082, 0.49483761191368103, 0.4028536081314087, 0.1417466700077057, 0.42756983637809753, 0.28876134753227234, 0.17495819926261902, 0.25828662514686584, 0.41094329953193665, 0.46602192521095276, 0.16207174956798553, 0.43060174584388733, 0.3590244948863983, 0.13257959485054016, 0.41217824816703796, 0.28659749031066895, 0.28526216745376587, 0.2661205530166626, 0.3243154287338257], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4332844316959381, 0.1700391173362732, 0.21660028398036957, 0.43739399313926697, 0.407402902841568, 0.16651172935962677, 0.01239914633333683, 0.42259714007377625, 0.1359664350748062, 0.12734439969062805, 0.17780324816703796, 0.07204530388116837, 0.44559240341186523, 0.07528829574584961, 0.4033139944076538, 0.16188295185565948, 0.4000277817249298, 0.35957884788513184, 0.29201799631118774, 0.31629467010498047, 0.4580695629119873, 0.43724384903907776, 0.15580275654792786, 0.45662155747413635, 0.31239500641822815, 0.3783590495586395, 0.4635009467601776, 0.4467927813529968, 0.4341062307357788, 0.342555433511734], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2c1cbf343529ae2369aa76f8103fed9d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 50, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([50], dtype='float32', min=0, max=0.5),
            paddle.uniform([50], dtype='float32', min=0, max=0.5),
            paddle.uniform([50], dtype='float32', min=0, max=0.5),
            paddle.uniform([50], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_683b8c0e5993fbb649f82935f7fc429e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_551b7b803f5be7f435279fa2aa263ba7
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 4], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1e591e3ea1806b59c51a42eb8614c972(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.49545538425445557, 0.1879841685295105, 0.3301108479499817, 0.43485045433044434, 0.36258581280708313, 0.43075239658355713, 0.46671897172927856, 0.4536816477775574, 0.39384856820106506, 0.21889878809452057, 0.48394274711608887, 0.07040031254291534, 0.29117098450660706, 0.21238981187343597, 0.09560593962669373, 0.4676995575428009], dtype='float32').reshape([16]),
            paddle.to_tensor([0.22677867114543915, 0.41542643308639526, 0.2238338589668274, 0.15342622995376587, 0.03962181881070137, 0.21874892711639404, 0.10806606709957123, 0.0811639353632927, 0.44485098123550415, 0.12192057818174362, 0.4634605050086975, 0.4405481517314911, 0.2167171835899353, 0.06654486805200577, 0.26720908284187317, 0.021461108699440956], dtype='float32').reshape([16]),
            paddle.to_tensor([0.24462249875068665, 0.477321982383728, 0.06572746485471725, 0.06733056902885437, 0.2196885049343109, 0.14542056620121002, 0.02034091018140316, 0.27383357286453247, 0.18002651631832123, 0.3357079327106476, 0.2563454210758209, 0.19066955149173737, 0.3903244137763977, 0.19424790143966675, 0.2318670153617859, 0.4547523856163025], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4477929472923279, 0.47383370995521545, 0.09458969533443451, 0.11116643249988556, 0.030205126851797104, 0.324928879737854, 0.17260241508483887, 0.3176364004611969, 0.24915166199207306, 0.18782739341259003, 0.47602906823158264, 0.045448724180459976, 0.15149052441120148, 0.2604852616786957, 0.37185510993003845, 0.37106621265411377], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_77be89ae71dc99b9a36010d7226c885e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_190f5489288e3db92fdca6b9490f8160
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 4, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.24773913621902466, 0.2541961073875427, 0.3357478678226471, 0.2580878734588623, 0.2824518084526062, 0.249701589345932, 0.31013360619544983, 0.07913414388895035, 0.41971126198768616, 0.11782509088516235, 0.27758628129959106, 0.007846884429454803, 0.0408676378428936, 0.3363967835903168, 0.20289969444274902, 0.44581708312034607], dtype='float32').reshape([16]),
            paddle.to_tensor([0.1541474610567093, 0.05029517412185669, 0.357904314994812, 0.24438630044460297, 0.3691096305847168, 0.17033977806568146, 0.13382603228092194, 0.27486398816108704, 0.3868556618690491, 0.05536811426281929, 0.1684689223766327, 0.2706899642944336, 0.11827895790338516, 0.3522036671638489, 0.27585068345069885, 0.3033716082572937], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3105025291442871, 0.30428576469421387, 0.07321476936340332, 0.17795687913894653, 0.22061040997505188, 0.44356268644332886, 0.19144004583358765, 0.1898077130317688, 0.041901011019945145, 0.3199949264526367, 0.3766671419143677, 0.3842627704143524, 0.08146757632493973, 0.3673712909221649, 0.24293366074562073, 0.010598217137157917], dtype='float32').reshape([16]),
            paddle.to_tensor([0.37336501479148865, 0.14367914199829102, 0.06440648436546326, 0.45685192942619324, 0.39981743693351746, 0.11334318667650223, 0.41529765725135803, 0.18701496720314026, 0.1349126249551773, 0.4838411211967468, 0.06691933423280716, 0.0897865816950798, 0.16896174848079681, 0.37787488102912903, 0.3043615520000458, 0.23771554231643677], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_abaf09b2975555e7b72a15e711963e03(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1b36ce9d1007aa53a59bb5056b6df333(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 224, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_67dede728230e1f69f7be9fe5fc473ea(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_190f5489288e3db92fdca6b9490f8160
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 32, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.278938889503479, 0.09495284408330917, 0.279276579618454, 0.1358800232410431, 0.20992949604988098, 0.03191445767879486, 0.42640599608421326, 0.18765302002429962, 0.18309751152992249, 0.34885212779045105, 0.44979336857795715, 0.4200054407119751, 0.04203781858086586, 0.493866503238678, 0.09907495230436325, 0.2502048909664154], dtype='float32').reshape([16]),
            paddle.to_tensor([0.18811951577663422, 0.19987985491752625, 0.3089240491390228, 0.035943515598773956, 0.2576085031032562, 0.49608343839645386, 0.2733975052833557, 0.11297234892845154, 0.49722763895988464, 0.4378755986690521, 0.4305882155895233, 0.17448465526103973, 0.1919957399368286, 0.3768080174922943, 0.07291758805513382, 0.039236657321453094], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4611050486564636, 0.29978513717651367, 0.49597492814064026, 0.31782475113868713, 0.49264100193977356, 0.49341386556625366, 0.34258124232292175, 0.029194025322794914, 0.47815534472465515, 0.2652621865272522, 0.42871108651161194, 0.030613895505666733, 0.23642085492610931, 0.19088448584079742, 0.4351924955844879, 0.15764617919921875], dtype='float32').reshape([16]),
            paddle.to_tensor([0.21702849864959717, 0.46742114424705505, 0.2798335552215576, 0.14415830373764038, 0.4792823791503906, 0.422802597284317, 0.30055567622184753, 0.42826005816459656, 0.28777095675468445, 0.11297789961099625, 0.39499223232269287, 0.3584980368614197, 0.32114461064338684, 0.20743423700332642, 0.1618318259716034, 0.17030192911624908], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9da4e69d693e55cd43c508386f8e38f9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_13025b164297d9226f9641b7f0715c76
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6ac34005bcbe27be25374c96a21d319e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cfa274ce1d8bad216a9283bd8d02486f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 32, 64], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4133801758289337, 0.4481034278869629, 0.4994775056838989, 0.09194060415029526, 0.374860018491745, 0.05956760421395302, 0.11179043352603912, 0.0438460074365139, 0.3705469071865082, 0.16108176112174988, 0.4572555720806122, 0.013181626796722412, 0.46054038405418396, 0.48670122027397156, 0.15099762380123138, 0.22591938078403473, 0.4828958809375763, 0.3002287447452545], dtype='float32').reshape([18]),
            paddle.to_tensor([0.014070969074964523, 0.09089011698961258, 0.017355190590023994, 0.06709197908639908, 0.1551785171031952, 0.2733013331890106, 0.3692517876625061, 0.18210630118846893, 0.017599310725927353, 0.20871518552303314, 0.08390984684228897, 0.4714093804359436, 0.39494073390960693, 0.030976098030805588, 0.4411231577396393, 0.35100725293159485, 0.27218863368034363, 0.0524815209209919], dtype='float32').reshape([18]),
            paddle.to_tensor([0.27505144476890564, 0.2530656158924103, 0.137360081076622, 0.3898642361164093, 0.04366283491253853, 0.48516014218330383, 0.13704976439476013, 0.3910636305809021, 0.20106664299964905, 0.017011160030961037, 0.2583252787590027, 0.33874574303627014, 0.27345380187034607, 0.3880385756492615, 0.0032433639280498028, 0.01913653127849102, 0.4580090343952179, 0.12738505005836487], dtype='float32').reshape([18]),
            paddle.to_tensor([0.13818050920963287, 0.1499757617712021, 0.21654222905635834, 0.4534921646118164, 0.002862402703613043, 0.46983951330184937, 0.28632596135139465, 0.28056782484054565, 0.27641135454177856, 0.09913894534111023, 0.19469758868217468, 0.06388358026742935, 0.45791327953338623, 0.2845165431499481, 0.19298362731933594, 0.4964863061904907, 0.283949613571167, 0.34391430020332336], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2766102c7bc10547c36b1bbf1b1d8085(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 4, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3eb40de42d83a2561d47e32286254cf2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 8, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f513209b4a28c199c7ffcc150e959af7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.24972225725650787, 0.3173692226409912, 0.1938735544681549, 0.07870806753635406, 0.4792601466178894, 0.024501612409949303, 0.03597541153430939, 0.2995992600917816, 0.4923608899116516, 0.3466852605342865, 0.0852084532380104, 0.05455486848950386, 0.14430809020996094, 0.22248928248882294, 0.4354904294013977, 0.3990548253059387, 0.12112677097320557, 0.45327407121658325, 0.19947494566440582, 0.07132409512996674, 0.12739793956279755, 0.22680076956748962, 0.3585147559642792, 0.2094620168209076, 0.16504324972629547, 0.06809990853071213, 0.4007347524166107, 0.1446109414100647, 0.4690696895122528, 0.07874639332294464], dtype='float32').reshape([30]),
            paddle.to_tensor([0.42656049132347107, 0.10113905370235443, 0.10234314948320389, 0.08959880471229553, 0.23222026228904724, 0.12768307328224182, 0.2803114056587219, 0.05214821547269821, 0.29856544733047485, 0.1707925647497177, 0.2310323417186737, 0.29199740290641785, 0.4836265444755554, 0.1895906776189804, 0.3986218571662903, 0.34239670634269714, 0.2829921245574951, 0.16814862191677094, 0.43190544843673706, 0.2911933362483978, 0.35326117277145386, 0.31494471430778503, 0.08457793295383453, 0.3246065676212311, 0.4804886281490326, 0.04525337368249893, 0.009336001239717007, 0.34790655970573425, 0.37337592244148254, 0.4910610318183899], dtype='float32').reshape([30]),
            paddle.to_tensor([0.024447934702038765, 0.24693599343299866, 0.46152687072753906, 0.41141244769096375, 0.1430114507675171, 0.33518609404563904, 0.19584937393665314, 0.10541055351495743, 0.01806787960231304, 0.12296927720308304, 0.002097163815051317, 0.13973620533943176, 0.20293007791042328, 0.1560431271791458, 0.3893451988697052, 0.2679941654205322, 0.32059794664382935, 0.4088176190853119, 0.060364559292793274, 0.4340207576751709, 0.23965634405612946, 0.29275012016296387, 0.16242673993110657, 0.44400936365127563, 0.3029041290283203, 0.30990296602249146, 0.23226715624332428, 0.43538469076156616, 0.24124252796173096, 0.494605153799057], dtype='float32').reshape([30]),
            paddle.to_tensor([0.001365168485790491, 0.25264984369277954, 0.3978407680988312, 0.1900380700826645, 0.21038402616977692, 0.20974136888980865, 0.06797974556684494, 0.2572104334831238, 0.41550204157829285, 0.39200925827026367, 0.3346722424030304, 0.28499799966812134, 0.40206876397132874, 0.17681092023849487, 0.33235105872154236, 0.12359264492988586, 0.16340592503547668, 0.23615477979183197, 0.38250306248664856, 0.09574703127145767, 0.29967284202575684, 0.10914289951324463, 0.07572682946920395, 0.21436180174350739, 0.19292302429676056, 0.3134087324142456, 0.328278124332428, 0.015249879099428654, 0.04634473845362663, 0.16628573834896088], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c41cb1bdefe195a395df7566b35a392e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 15, 15], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d0f3cf21da2baeb039363a302cd2f0ee(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.02966301515698433, 0.22459247708320618, 0.3565881550312042, 0.4770725965499878, 0.4475955665111542, 0.10995011031627655, 0.3320066034793854, 0.05893366038799286, 0.4704984724521637, 0.09033191949129105, 0.19253045320510864, 0.3422570526599884, 0.2544122338294983, 0.16278651356697083, 0.24727720022201538, 0.43528038263320923, 0.139642134308815, 0.2753477990627289, 0.00666059460490942, 0.267896443605423, 0.43067941069602966, 0.3540067970752716, 0.20979836583137512, 0.061268921941518784, 0.4456091821193695, 0.2665586769580841, 0.01994350738823414, 0.011911761946976185, 0.1907617300748825, 0.2763518691062927], dtype='float32').reshape([30]),
            paddle.to_tensor([0.068086177110672, 0.32094550132751465, 0.04929479584097862, 0.48745009303092957, 0.08742770552635193, 0.33173081278800964, 0.2960360646247864, 0.059988319873809814, 0.3103015422821045, 0.4122704267501831, 0.4584014415740967, 0.3974609971046448, 0.4674035608768463, 0.003643798641860485, 0.4421410858631134, 0.36237579584121704, 0.2837136685848236, 0.047831665724515915, 0.43419522047042847, 0.4598950445652008, 0.25821417570114136, 0.4193730354309082, 0.22513124346733093, 0.24177862703800201, 0.11061186343431473, 0.11445756256580353, 0.23014627397060394, 0.11048710346221924, 0.10867315530776978, 0.21054419875144958], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3673112392425537, 0.47849586606025696, 0.45774325728416443, 0.10456400364637375, 0.1893732249736786, 0.454082727432251, 0.07311336696147919, 0.2779991328716278, 0.33096498250961304, 0.26811638474464417, 0.07759832590818405, 0.12647458910942078, 0.0906817838549614, 0.38808393478393555, 0.13291534781455994, 0.1636010855436325, 0.14996010065078735, 0.0703233927488327, 0.21247445046901703, 0.006537801586091518, 0.4031144976615906, 0.006595790386199951, 0.08305006474256516, 0.10559066385030746, 0.15443973243236542, 0.1279984414577484, 0.25374066829681396, 0.3174867331981659, 0.30768704414367676, 0.27619045972824097], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3607330024242401, 0.14872373640537262, 0.013693944551050663, 0.15371665358543396, 0.4801521599292755, 0.08499948680400848, 0.041559476405382156, 0.10327395051717758, 0.28019294142723083, 0.08426940441131592, 0.28980693221092224, 0.2270648330450058, 0.4695175588130951, 0.35393279790878296, 0.31597596406936646, 0.40466442704200745, 0.2306584119796753, 0.44438445568084717, 0.20615428686141968, 0.2538788914680481, 0.08902774006128311, 0.44431066513061523, 0.06931009888648987, 0.2209058254957199, 0.3707658350467682, 0.21963168680667877, 0.23885655403137207, 0.2130068689584732, 0.34620749950408936, 0.030422063544392586], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_11c7fc0dbfc231dd7399a2609e5c397b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 4, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.49894383549690247, 0.2091309279203415, 0.0943206176161766, 0.10822614282369614], dtype='float32').reshape([4]),
            paddle.to_tensor([0.1182134747505188, 0.16327513754367828, 0.1958860456943512, 0.019990554079413414], dtype='float32').reshape([4]),
            paddle.to_tensor([0.11604581028223038, 0.4408544898033142, 0.014010226354002953, 0.2932492196559906], dtype='float32').reshape([4]),
            paddle.to_tensor([0.012706547975540161, 0.009141886606812477, 0.46262040734291077, 0.042212676256895065], dtype='float32').reshape([4]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_04763e8968489f52eb2736b1e2c46d14(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f9ad666596271a398997d18f56d56553(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 128, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9127b2da790d8eb4fdc71e3729e9a762(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 122, 122], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_58807bca1f3c1a49c2924f43d350259f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.11444225907325745, 0.4842827320098877, 0.4601356089115143, 0.4319097101688385, 0.09901079535484314, 0.1545366793870926, 0.25059235095977783, 0.4262654185295105, 0.28908810019493103, 0.4470100700855255, 0.008404148742556572, 0.27603504061698914, 0.2813003361225128, 0.44150814414024353, 0.4898815453052521, 0.0581652894616127, 0.000550661759916693, 0.16462957859039307, 0.35999029874801636, 0.3117901682853699, 0.1528700590133667, 0.0006818242254666984, 0.49815133213996887, 0.4492977559566498], dtype='float32').reshape([24]),
            paddle.to_tensor([0.08752478659152985, 0.08424553275108337, 0.03436291590332985, 0.4749377965927124, 0.4316752851009369, 0.29127267003059387, 0.48316696286201477, 0.40121781826019287, 0.16531099379062653, 0.4057088792324066, 0.47802600264549255, 0.43858781456947327, 0.4338393211364746, 0.1925077736377716, 0.042312655597925186, 0.11151550710201263, 0.483980268239975, 0.38694658875465393, 0.015870682895183563, 0.07166942954063416, 0.476351797580719, 0.30365949869155884, 0.44468748569488525, 0.07289215177297592], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2751694321632385, 0.11601287871599197, 0.20305712521076202, 0.15506799519062042, 0.47026923298835754, 0.49479958415031433, 0.36659687757492065, 0.38109511137008667, 0.3224097192287445, 0.04679377004504204, 0.03601761534810066, 0.15093554556369781, 0.011300227604806423, 0.331344336271286, 0.03983946889638901, 0.3102809488773346, 0.4763883650302887, 0.4132062792778015, 0.4574149250984192, 0.20723649859428406, 0.25982382893562317, 0.2503879964351654, 0.30578818917274475, 0.3057955205440521], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3267480134963989, 0.24436385929584503, 0.29772675037384033, 0.07501168549060822, 0.1682552844285965, 0.4447777569293976, 0.45208463072776794, 0.4572731554508209, 0.02135569602251053, 0.396155446767807, 0.47149166464805603, 0.007534777279943228, 0.12621019780635834, 0.27536511421203613, 0.017022769898176193, 0.18360714614391327, 0.2944292724132538, 0.03528134524822235, 0.171573668718338, 0.3101004660129547, 0.09032566845417023, 0.2492591142654419, 0.43341997265815735, 0.4849635064601898], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ff232c330e761545a8b1801d3c975335(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_35987047e239940ff30857e948c42970(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5524af6c3e12a7788b26c52c424313c2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d746e674a4d00606e5fd12e6b7ea46d0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 2, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_03b72031ab8d8d553e23c661ec6d2067(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.41880103945732117, 0.09576191008090973, 0.42742910981178284, 0.22173336148262024, 0.3383047878742218, 0.15485051274299622, 0.4300995469093323, 0.302457720041275, 0.31538113951683044, 0.18551738560199738, 0.4209426939487457, 0.24619218707084656, 0.3956764340400696, 0.16891556978225708, 0.12687240540981293, 0.23383469879627228, 0.0919145941734314, 0.15694944560527802, 0.2185821235179901, 0.06868169456720352], dtype='float32').reshape([20]),
            paddle.to_tensor([0.25113222002983093, 0.20480485260486603, 0.15295279026031494, 0.2707430124282837, 0.3012969195842743, 0.24620717763900757, 0.4894756078720093, 0.4206382930278778, 0.38586676120758057, 0.21378213167190552, 0.018821721896529198, 0.3783438503742218, 0.1692076474428177, 0.3637998104095459, 0.3931027948856354, 0.36382195353507996, 0.1275254487991333, 0.0754682868719101, 0.15849320590496063, 0.4493331015110016], dtype='float32').reshape([20]),
            paddle.to_tensor([0.10459458827972412, 0.08403027802705765, 0.29320716857910156, 0.2626951336860657, 0.2690960764884949, 0.4138389527797699, 0.17824216187000275, 0.3013226091861725, 0.30573105812072754, 0.09399663656949997, 0.2529101073741913, 0.37859296798706055, 0.05409792438149452, 0.3536752760410309, 0.19405066967010498, 0.050383612513542175, 0.024037491530179977, 0.39913418889045715, 0.3792959749698639, 0.409330278635025], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3451229929924011, 0.14762736856937408, 0.24177220463752747, 0.051565103232860565, 0.15840236842632294, 0.14787283539772034, 0.3717951774597168, 0.37201303243637085, 0.1388901025056839, 0.43867531418800354, 0.05344583839178085, 0.38809409737586975, 0.16192930936813354, 0.4589875340461731, 0.21828986704349518, 0.007241025101393461, 0.34133923053741455, 0.36460912227630615, 0.20514287054538727, 0.11935519427061081], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e17af736adde7b46d29d2c65f32159c4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 162, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([162], dtype='float32', min=0, max=0.5),
            paddle.uniform([162], dtype='float32', min=0, max=0.5),
            paddle.uniform([162], dtype='float32', min=0, max=0.5),
            paddle.uniform([162], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_24c44013f4c96e4b8b39ac085640cb80(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 31, 31], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3cf22e4b1803fd1d0579a256c36329c2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.1343759447336197, 0.3763108253479004, 0.11333990097045898, 0.0382799357175827, 0.1059267446398735, 0.19431570172309875, 0.4069518446922302, 0.33316633105278015, 0.3897106945514679, 0.14175179600715637, 0.06727048009634018, 0.3328656852245331, 0.2909950613975525, 0.4040493965148926, 0.4375452399253845, 0.34439677000045776], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3229038119316101, 0.13543206453323364, 0.1605822741985321, 0.20766326785087585, 0.07209505885839462, 0.08051759749650955, 0.4018581211566925, 0.0633847638964653, 0.04329715296626091, 0.08008880913257599, 0.29480770230293274, 0.24716626107692719, 0.03965316712856293, 0.10415727645158768, 0.49465784430503845, 0.4492396116256714], dtype='float32').reshape([16]),
            paddle.to_tensor([0.11043892055749893, 0.45251891016960144, 0.3065403997898102, 0.39726659655570984, 0.32174649834632874, 0.3176628351211548, 0.2715083956718445, 0.17728489637374878, 0.172149658203125, 0.3826785087585449, 0.3085547983646393, 0.39771074056625366, 0.4628744423389435, 0.40824073553085327, 0.14121943712234497, 0.00513935973867774], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3256787657737732, 0.3871241807937622, 0.44429370760917664, 0.3494041860103607, 0.2036639004945755, 0.37131184339523315, 0.16903503239154816, 0.41364261507987976, 0.1069997102022171, 0.3603075444698334, 0.34620800614356995, 0.018087510019540787, 0.4369424283504486, 0.07536455988883972, 0.05810939148068428, 0.030432874336838722], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9102a728a3d677d92a7a337ce1442b54(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_190f5489288e3db92fdca6b9490f8160
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 4, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0609f162e70dd0992b2434366e466a05(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 320, 320], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_df2df247d8200ca5d21850c4702dde88(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.01243404671549797, 0.35000309348106384, 0.20701250433921814, 0.4709281623363495, 0.1383696049451828, 0.2545817792415619, 0.3753706216812134, 0.3536231815814972, 0.2139003872871399, 0.22755612432956696, 0.36502453684806824, 0.18608924746513367, 0.0390925370156765, 0.29210415482521057, 0.41678571701049805, 0.04584003612399101, 0.05635394901037216, 0.08746995031833649, 0.13834244012832642, 0.1779354363679886, 0.3774103820323944, 0.0005819645593874156, 0.19183655083179474, 0.41039296984672546, 0.0028468880336731672, 0.05573822930455208, 0.013409048318862915, 0.3908444046974182, 0.27008500695228577, 0.4042581617832184], dtype='float32').reshape([30]),
            paddle.to_tensor([0.059657059609889984, 0.35771849751472473, 0.415702223777771, 0.37664473056793213, 0.1957385390996933, 0.0657201036810875, 0.2626459300518036, 0.33574214577674866, 0.471846342086792, 0.4474181532859802, 0.16241124272346497, 0.0815553143620491, 0.008491245098412037, 0.3821854591369629, 0.1244945228099823, 0.28732603788375854, 0.09456921368837357, 0.12892882525920868, 0.17325763404369354, 0.01586492359638214, 0.35752591490745544, 0.18796990811824799, 0.40277498960494995, 0.011870833113789558, 0.323512464761734, 0.13289813697338104, 0.4483426809310913, 0.474520742893219, 0.2373504638671875, 0.2912437915802002], dtype='float32').reshape([30]),
            paddle.to_tensor([0.18566273152828217, 0.1494705229997635, 0.3597482740879059, 0.2605351209640503, 0.35357430577278137, 0.3520442843437195, 0.21798744797706604, 0.37068796157836914, 0.1823415905237198, 0.25416648387908936, 0.06349185109138489, 0.23513926565647125, 0.462258905172348, 0.20705744624137878, 0.06960494816303253, 0.3208121061325073, 0.3566509485244751, 0.37199825048446655, 0.33673909306526184, 0.027233941480517387, 0.09086235612630844, 0.31503933668136597, 0.01041490864008665, 0.4210854172706604, 0.1630508303642273, 0.19223760068416595, 0.3572791814804077, 0.07833348214626312, 0.4357755482196808, 0.30205944180488586], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2663472592830658, 0.13401281833648682, 0.1733398288488388, 0.008820971474051476, 0.13968828320503235, 0.18653467297554016, 0.20248118042945862, 0.02962699718773365, 0.2890737056732178, 0.4935310482978821, 0.19493576884269714, 0.3120967447757721, 0.14485982060432434, 0.04100770503282547, 0.0946536585688591, 0.2373022735118866, 0.2826592028141022, 0.047575753182172775, 0.0004710308858193457, 0.37537631392478943, 0.27167385816574097, 0.2148578017950058, 0.43124085664749146, 0.07561192661523819, 0.3691905438899994, 0.43672415614128113, 0.37736210227012634, 0.41225820779800415, 0.3011206090450287, 0.03021940030157566], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_18ecb560d0c44f875c5b015e2e4b49cc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_af7f596ff281e62acb40a71fe557234d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 4, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1669c578c69c53818ee7309f4e123560(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.19952426850795746, 0.12107980251312256, 0.12853166460990906, 0.24132126569747925, 0.46017205715179443, 0.18128903210163116, 0.4546276330947876, 0.1641058325767517, 0.2779361605644226, 0.43556147813796997, 0.24663861095905304, 0.34518977999687195, 0.45094266533851624, 0.3941717743873596, 0.49053844809532166, 0.0936553031206131, 0.055081866681575775, 0.4214352071285248, 0.14173829555511475, 0.267448753118515, 0.31393206119537354, 0.023571910336613655, 0.1718296855688095, 0.08388236165046692, 0.30927151441574097, 0.07673608511686325, 0.4098190665245056, 0.237867534160614, 0.4400331974029541, 0.14179903268814087], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3071207106113434, 0.15923982858657837, 0.44365423917770386, 0.25514480471611023, 0.042394500225782394, 0.4536207914352417, 0.18362589180469513, 0.4887830317020416, 0.35516220331192017, 0.26647940278053284, 0.13777397572994232, 0.022952862083911896, 0.10253250598907471, 0.19207239151000977, 0.47508639097213745, 0.16947230696678162, 0.24398326873779297, 0.4236581027507782, 0.0506172813475132, 0.08075950294733047, 0.13417181372642517, 0.4428701102733612, 0.06757982820272446, 0.26346126198768616, 0.27140405774116516, 0.47538214921951294, 0.07021676003932953, 0.47277480363845825, 0.45019325613975525, 0.10276241600513458], dtype='float32').reshape([30]),
            paddle.to_tensor([0.12639464437961578, 0.45668214559555054, 0.08034104853868484, 0.49685996770858765, 0.22770501673221588, 0.13376344740390778, 0.41837188601493835, 0.28155604004859924, 0.4850340485572815, 0.30251115560531616, 0.35471805930137634, 0.1347801685333252, 0.4903375506401062, 0.22249126434326172, 0.06166775897145271, 0.2874268889427185, 0.230694979429245, 0.1819559931755066, 0.2613762617111206, 0.397502064704895, 0.23620767891407013, 0.21032293140888214, 0.24107739329338074, 0.19692841172218323, 0.3142894506454468, 0.352682501077652, 0.20756502449512482, 0.43114522099494934, 0.08743135631084442, 0.1779230535030365], dtype='float32').reshape([30]),
            paddle.to_tensor([0.34676647186279297, 0.264173299074173, 0.07719989120960236, 0.44260913133621216, 0.003827926004305482, 0.425234854221344, 0.26150500774383545, 0.3816484808921814, 0.3332265615463257, 0.014099678955972195, 0.06195034086704254, 0.1868472546339035, 0.44687068462371826, 0.07019434124231339, 0.37793245911598206, 0.3403458297252655, 0.026040421798825264, 0.06581249833106995, 0.02752114087343216, 0.37500280141830444, 0.4577024579048157, 0.1861565113067627, 0.3179422616958618, 0.34334680438041687, 0.15382960438728333, 0.1630467176437378, 0.09134351462125778, 0.38913974165916443, 0.4460362493991852, 0.05862295627593994], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a8649ed54a5fd926237adcc7408c91e7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 12, 12], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_917cd1ea5e6cc49e2b032afb27cc47f0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 1200, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_86ccce72a9422e0f67d64f988cef37e7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_15d208d6b1d63686c8c761c0ed0bfd22
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 26], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d0dc6a956f410470d035d42ccfbbfc5d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3515755a1c15b00f53c49aefbed024ac(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 19, 19], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_592c7944efbffc985a87a90a562eaa79(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 31, 31], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fdd8ef1fb1f2370260ce875928eb2106(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.31781265139579773, 0.46042346954345703, 0.46815982460975647, 0.46753159165382385, 0.2696022391319275, 0.17651209235191345, 0.2596750855445862, 0.39742061495780945, 0.22993984818458557, 0.3448304235935211, 0.08477247506380081, 0.056120019406080246, 0.004266493022441864, 0.2293175756931305, 0.39809444546699524, 0.18207168579101562, 0.021074432879686356, 0.1744382679462433, 0.16864384710788727, 0.08452855050563812], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4689025282859802, 0.3976818025112152, 0.4392884075641632, 0.22443510591983795, 0.09521884471178055, 0.38827192783355713, 0.1393936425447464, 0.296610563993454, 0.3410671651363373, 0.484730988740921, 0.01544065959751606, 0.4835728108882904, 0.18586412072181702, 0.008707807399332523, 0.0550018809735775, 0.36373817920684814, 0.1836373507976532, 0.2848814129829407, 0.20966115593910217, 0.3773607611656189], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4162138104438782, 0.46346983313560486, 0.3493868112564087, 0.26085764169692993, 0.34044530987739563, 0.20909060537815094, 0.3095158338546753, 0.25898459553718567, 0.16951873898506165, 0.39552977681159973, 0.13245940208435059, 0.32339462637901306, 0.37595275044441223, 0.32774102687835693, 0.17722657322883606, 0.21042102575302124, 0.3610890209674835, 0.40275514125823975, 0.06688638031482697, 0.30423039197921753], dtype='float32').reshape([20]),
            paddle.to_tensor([0.32083824276924133, 0.13075245916843414, 0.2853200137615204, 0.23739533126354218, 0.2171306014060974, 0.2577517032623291, 0.04794187843799591, 0.26084256172180176, 0.43604063987731934, 0.1579410284757614, 0.1671709269285202, 0.2677322030067444, 0.49272245168685913, 0.2945636510848999, 0.2778500020503998, 0.12797877192497253, 0.20431381464004517, 0.34443730115890503, 0.2218340039253235, 0.40796002745628357], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2090b9b0e7a4c4ca6bb830df65e738ae(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d812345b227480fb9f73d09ab64a5aea(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_679af74735bd44a894a7b1511e31925d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 2688, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([2688], dtype='float32', min=0, max=0.5),
            paddle.uniform([2688], dtype='float32', min=0, max=0.5),
            paddle.uniform([2688], dtype='float32', min=0, max=0.5),
            paddle.uniform([2688], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a1a64b3dbff58817c2f53c758b8a62d2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 31, 31], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7a8797791b5053c38bbf0aec188606fd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 352, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([352], dtype='float32', min=0, max=0.5),
            paddle.uniform([352], dtype='float32', min=0, max=0.5),
            paddle.uniform([352], dtype='float32', min=0, max=0.5),
            paddle.uniform([352], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_26f624f49f927c905df0f32ce245ea04(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0ca1c746c6e747ebdbf92401537764a4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.08163179457187653, 0.062054865062236786, 0.24017924070358276, 0.17843693494796753, 0.4805122911930084, 0.13409626483917236, 0.3407646715641022, 0.3801126480102539, 0.059595949947834015, 0.3913625180721283, 0.2835290729999542, 0.394604355096817, 0.33763203024864197, 0.4770475924015045, 0.06372196972370148, 0.40121835470199585, 0.13952948153018951, 0.3220968246459961, 0.24475699663162231, 0.08626073598861694], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3894460201263428, 0.08419127017259598, 0.09242710471153259, 0.24424414336681366, 0.429202675819397, 0.1613050103187561, 0.04441134259104729, 0.20309129357337952, 0.2828477621078491, 0.1169576495885849, 0.4987022280693054, 0.1303386241197586, 0.319776326417923, 0.36666610836982727, 0.39315611124038696, 0.061414316296577454, 0.33778443932533264, 0.38211768865585327, 0.02987128496170044, 0.23694269359111786], dtype='float32').reshape([20]),
            paddle.to_tensor([0.49643513560295105, 0.3476029634475708, 0.11115239560604095, 0.38153862953186035, 0.43398964405059814, 0.4484887421131134, 0.43715688586235046, 0.3783356845378876, 0.45787182450294495, 0.001609000377357006, 0.04928261414170265, 0.2567521631717682, 0.014653527177870274, 0.4004875719547272, 0.09739655256271362, 0.13011378049850464, 0.2484348565340042, 0.07979466766119003, 0.23191875219345093, 0.38041022419929504], dtype='float32').reshape([20]),
            paddle.to_tensor([0.0834592878818512, 0.005333236418664455, 0.02901446260511875, 0.36663898825645447, 0.46975845098495483, 0.11617178469896317, 0.02622140198945999, 0.3047412633895874, 0.45036959648132324, 0.19436383247375488, 0.48672446608543396, 0.4517323970794678, 0.4662896394729614, 0.4508027136325836, 0.18836526572704315, 0.03333107382059097, 0.38540393114089966, 0.4850044846534729, 0.24215750396251678, 0.1786184310913086], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_505ae0b585e11ab6a3b09fb17b75224b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f709ca2d250e541c75695876cd04824d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 10, 10], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0e6e0ab3a35cb18c327f3acf4db923e3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_039500199384e06ba89b3ccd59e80867(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 138, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([138], dtype='float32', min=0, max=0.5),
            paddle.uniform([138], dtype='float32', min=0, max=0.5),
            paddle.uniform([138], dtype='float32', min=0, max=0.5),
            paddle.uniform([138], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_739cadac2bdf826e576110560114c5be(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.37042444944381714, 0.48798832297325134, 0.32646530866622925, 0.20391686260700226, 0.14580772817134857, 0.4739017188549042, 0.2964082360267639, 0.2762841582298279, 0.3429056406021118, 0.3511194586753845, 0.0638086125254631, 0.21319983899593353, 0.3584657609462738, 0.04318571463227272, 0.2514267861843109, 0.21491070091724396, 0.1596851348876953, 0.20467661321163177, 0.22926969826221466, 0.4827543795108795, 0.49958398938179016, 0.4941858649253845, 0.40606123208999634, 0.473943829536438], dtype='float32').reshape([24]),
            paddle.to_tensor([0.020246472209692, 0.3783518373966217, 0.45521438121795654, 0.004086692351847887, 0.33491379022598267, 0.014301822520792484, 0.3105586767196655, 0.4249754250049591, 0.014557849615812302, 0.3159695863723755, 0.1606801450252533, 0.03499724715948105, 0.21813057363033295, 0.3521457314491272, 0.4064209461212158, 0.08184552192687988, 0.2385028451681137, 0.1354418247938156, 0.2795407176017761, 0.35062268376350403, 0.39923980832099915, 0.43480899930000305, 0.38207781314849854, 0.020524214953184128], dtype='float32').reshape([24]),
            paddle.to_tensor([0.07560915499925613, 0.027230046689510345, 0.2320951372385025, 0.35634535551071167, 0.4311842620372772, 0.16665397584438324, 0.05112561583518982, 0.09599526226520538, 0.07500780373811722, 0.09127948433160782, 0.2044287770986557, 0.3301699459552765, 0.19268548488616943, 0.11702835559844971, 0.2622299790382385, 0.31177306175231934, 0.23643268644809723, 0.3429502844810486, 0.2962503433227539, 0.20706309378147125, 0.4299812912940979, 0.2669690251350403, 0.20591206848621368, 0.20035281777381897], dtype='float32').reshape([24]),
            paddle.to_tensor([0.12602050602436066, 0.3136933445930481, 0.0473819300532341, 0.07437631487846375, 0.08544343709945679, 0.44059816002845764, 0.49251100420951843, 0.030382269993424416, 0.3860922157764435, 0.2772596478462219, 0.4207225441932678, 0.44190752506256104, 0.2864910662174225, 0.008508149534463882, 0.35244423151016235, 0.07664868235588074, 0.31514668464660645, 0.3211328983306885, 0.14011147618293762, 0.013669216074049473, 0.09716223925352097, 0.23860204219818115, 0.37767520546913147, 0.43324014544487], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_82fc855e4743490d6dd8fd74957d1891(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.15155650675296783, 0.11211007088422775, 0.16756735742092133, 0.40911251306533813, 0.24885815382003784, 0.21493121981620789, 0.17286816239356995, 0.2486688792705536, 0.2625502049922943, 0.17200717329978943, 0.2623860538005829, 0.04466868191957474, 0.027455192059278488, 0.4311560094356537, 0.3606272041797638, 0.24980542063713074, 0.23260214924812317, 0.3519381880760193, 0.3173144459724426, 0.30199795961380005, 0.3920033872127533, 0.44611653685569763, 0.37219828367233276, 0.2118644118309021], dtype='float32').reshape([24]),
            paddle.to_tensor([0.16187790036201477, 0.427031934261322, 0.08347044140100479, 0.43007832765579224, 0.4855632781982422, 0.1247115284204483, 0.3383425772190094, 0.48811978101730347, 0.3401477336883545, 0.22122740745544434, 0.3068976402282715, 0.08233433961868286, 0.11603032797574997, 0.31093481183052063, 0.22835537791252136, 0.20795725286006927, 0.39896905422210693, 0.21887649595737457, 0.39985495805740356, 0.19139406085014343, 0.4062943458557129, 0.4072902500629425, 0.08285579085350037, 0.31500670313835144], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4064018726348877, 0.4928106367588043, 0.14201268553733826, 0.37369027733802795, 0.30254748463630676, 0.12130448967218399, 0.2799264192581177, 0.2699863910675049, 0.22071218490600586, 0.3615989089012146, 0.2869415283203125, 0.22036950290203094, 0.10722912847995758, 0.21828438341617584, 0.4915381371974945, 0.453863263130188, 0.46350958943367004, 0.17512819170951843, 0.3588877022266388, 0.08228997886180878, 0.3380906283855438, 0.10668426752090454, 0.16019050776958466, 0.2668628692626953], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2030375450849533, 0.11870219558477402, 0.10954809188842773, 0.30659714341163635, 0.400676429271698, 0.4146100580692291, 0.31695011258125305, 0.23301343619823456, 0.07482382655143738, 0.3004550039768219, 0.27500954270362854, 0.4469038248062134, 0.054663822054862976, 0.01950431428849697, 0.17472583055496216, 0.21765533089637756, 0.47174835205078125, 0.3298819959163666, 0.3774926960468292, 0.1836944967508316, 0.3549270033836365, 0.3629637658596039, 0.3272600471973419, 0.11153631657361984], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2e014855096911f24220620ef8dae97a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 320, 320], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.22372965514659882, 0.04488392174243927, 0.27429690957069397, 0.27138468623161316, 0.046703267842531204, 0.2249229997396469, 0.11554612219333649, 0.26377302408218384], dtype='float32').reshape([8]),
            paddle.to_tensor([0.15311065316200256, 0.16951143741607666, 0.34768903255462646, 0.4885316789150238, 0.3051687777042389, 0.2731002867221832, 0.035751502960920334, 0.4198663532733917], dtype='float32').reshape([8]),
            paddle.to_tensor([0.14331479370594025, 0.4789062738418579, 0.3268654942512512, 0.3106729984283447, 0.3053402006626129, 0.23534724116325378, 0.26547878980636597, 0.34702539443969727], dtype='float32').reshape([8]),
            paddle.to_tensor([0.4549734592437744, 0.04099833592772484, 0.14557242393493652, 0.479201078414917, 0.4801117777824402, 0.15770481526851654, 0.030642762780189514, 0.20871812105178833], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2b229de0a9077e98936396a358d2aff6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 1, 3], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1237c791735e4145211114279e566d67(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.40441668033599854, 0.398845374584198, 0.4106633961200714, 0.263620525598526, 0.3858949840068817, 0.07692448049783707, 0.10292773693799973, 0.32555726170539856, 0.3354714810848236, 0.35408028960227966, 0.46499496698379517, 0.17723983526229858, 0.18826842308044434, 0.2978716194629669, 0.42548272013664246, 0.46143701672554016], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3559139370918274, 0.20570656657218933, 0.152628555893898, 0.22693835198879242, 0.009778405539691448, 0.042845383286476135, 0.3132818937301636, 0.14816686511039734, 0.3485317528247833, 0.09967998415231705, 0.1899534910917282, 0.3211855888366699, 0.3884583115577698, 0.4789973795413971, 0.22496812045574188, 0.08910930156707764], dtype='float32').reshape([16]),
            paddle.to_tensor([0.14843469858169556, 0.11408241838216782, 0.3970803916454315, 0.060107968747615814, 0.1146661639213562, 0.39592254161834717, 0.06691396236419678, 0.23447643220424652, 0.40597671270370483, 0.4660438895225525, 0.2627789080142975, 0.29170241951942444, 0.3389207422733307, 0.4717433452606201, 0.0461844876408577, 0.373349130153656], dtype='float32').reshape([16]),
            paddle.to_tensor([0.34054458141326904, 0.013906915672123432, 0.31799691915512085, 0.047040462493896484, 0.4522518515586853, 0.12911035120487213, 0.08223053067922592, 0.01171992439776659, 0.03202517330646515, 0.1488899439573288, 0.4025743007659912, 0.35701045393943787, 0.32810264825820923, 0.0016638878732919693, 0.22721461951732635, 0.0993071049451828], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c9a5974cfb6f2ead31e84db6dd9c82fc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 31, 31], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bb7b538cd9b5caa5b3ecb88f9fc59bda(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 6, 6], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_386686268654016d417834b1d464e751(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_34a3295cb274ba2b46949ce788c4b80f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 1488, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1488], dtype='float32', min=0, max=0.5),
            paddle.uniform([1488], dtype='float32', min=0, max=0.5),
            paddle.uniform([1488], dtype='float32', min=0, max=0.5),
            paddle.uniform([1488], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5bd907fbd158b64a4277bc954b829b82(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4983646869659424, 0.12637916207313538, 0.40096673369407654, 0.1649663746356964, 0.028334246948361397, 0.1398371011018753, 0.09482584148645401, 0.4713517129421234, 0.4325371980667114, 0.14059484004974365, 0.4250545799732208, 0.3976173996925354], dtype='float32').reshape([12]),
            paddle.to_tensor([0.47409743070602417, 0.21764874458312988, 0.4992104768753052, 0.182302787899971, 0.11377633363008499, 0.4107855260372162, 0.3413905203342438, 0.23194727301597595, 0.2049805372953415, 0.008072957396507263, 0.27870285511016846, 0.46331650018692017], dtype='float32').reshape([12]),
            paddle.to_tensor([0.4774322807788849, 0.1111861914396286, 0.24607467651367188, 0.04428870603442192, 0.443768709897995, 0.49554693698883057, 0.40087980031967163, 0.15246836841106415, 0.0010128291323781013, 0.1619996726512909, 0.15233223140239716, 0.29345306754112244], dtype='float32').reshape([12]),
            paddle.to_tensor([0.1885748654603958, 0.23647114634513855, 0.20676098763942719, 0.035693276673555374, 0.40264439582824707, 0.3814716339111328, 0.012690765783190727, 0.10305574536323547, 0.3862132728099823, 0.4538438320159912, 0.35740265250205994, 0.43860629200935364], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9ed332977029fecfafba636e16b9abe5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 512, 512], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3a0632f0da16fb72914db6812d1c20e1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 32, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9d2ce3f698b064b926a4a43ce6d215b2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 100, 168], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_83d829092cf981c4c3d7be68bdde214d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.07715920358896255, 0.32132235169410706, 0.44909581542015076, 0.35913318395614624, 0.03349166363477707, 0.05414913222193718, 0.14957654476165771, 0.379743367433548, 0.10403349250555038, 0.046636633574962616, 0.21434827148914337, 0.2168452888727188, 0.15152761340141296, 0.2191687971353531, 0.10927152633666992, 0.37014612555503845, 0.45260950922966003, 0.236175999045372, 0.47949182987213135, 0.026048190891742706, 0.11533527076244354, 0.4551870822906494, 0.46709370613098145, 0.16885267198085785, 0.09750828891992569, 0.4481053948402405, 0.35354360938072205, 0.07154125720262527, 0.44078728556632996, 0.25124305486679077], dtype='float32').reshape([30]),
            paddle.to_tensor([0.21637488901615143, 0.47820860147476196, 0.13763464987277985, 0.07301373779773712, 0.41646263003349304, 0.22277355194091797, 0.10466202348470688, 0.42236825823783875, 0.2919217050075531, 0.12760894000530243, 0.21481317281723022, 0.28085556626319885, 0.15236489474773407, 0.42612841725349426, 0.17635324597358704, 0.034026842564344406, 0.4537180960178375, 0.2865210771560669, 0.2035595327615738, 0.04164283350110054, 0.3896462023258209, 0.3424316644668579, 0.3154524862766266, 0.1702185720205307, 0.0901327058672905, 0.33721423149108887, 0.3406026065349579, 0.3420998454093933, 0.05810321494936943, 0.13194841146469116], dtype='float32').reshape([30]),
            paddle.to_tensor([0.017394723370671272, 0.4976612627506256, 0.47584861516952515, 0.4164991080760956, 0.27453017234802246, 0.002074081916362047, 0.4868418872356415, 0.10122997313737869, 0.30334869027137756, 0.4960676431655884, 0.12033697217702866, 0.30795592069625854, 0.041255414485931396, 0.3778497874736786, 0.3727222681045532, 0.1070779338479042, 0.18215708434581757, 0.4843243360519409, 0.3000768721103668, 0.4001134932041168, 0.11870016902685165, 0.34274914860725403, 0.3343830108642578, 0.43343085050582886, 0.26224589347839355, 0.26193463802337646, 0.22454479336738586, 0.026606891304254532, 0.40603065490722656, 0.23634029924869537], dtype='float32').reshape([30]),
            paddle.to_tensor([0.10410045832395554, 0.17344646155834198, 0.22214439511299133, 0.4655958116054535, 0.1551417112350464, 0.33823469281196594, 0.36683905124664307, 0.14458225667476654, 0.3644997477531433, 0.1907576620578766, 0.034597013145685196, 0.42295750975608826, 0.1960187554359436, 0.31513240933418274, 0.35662853717803955, 0.037403207272291183, 0.45197775959968567, 0.02218715101480484, 0.09131326526403427, 0.025840897113084793, 0.17956462502479553, 0.391378253698349, 0.4866850674152374, 0.29095518589019775, 0.09087789058685303, 0.1156892478466034, 0.4242350459098816, 0.24336640536785126, 0.4712376892566681, 0.05699897184967995], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e9447529efc7c470db1e7ccd6e550b8f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.00098068593069911, 0.4092668294906616, 0.10141468793153763, 0.17141054570674896, 0.25084611773490906, 0.3950497508049011, 0.020544448867440224, 0.2850266695022583], dtype='float32').reshape([8]),
            paddle.to_tensor([0.15612474083900452, 0.3555327355861664, 0.31578129529953003, 0.4579356908798218, 0.19006742537021637, 0.4280349016189575, 0.3170168399810791, 0.06697539240121841], dtype='float32').reshape([8]),
            paddle.to_tensor([0.26271405816078186, 0.35085901618003845, 0.3596281409263611, 0.4391469657421112, 0.12213744223117828, 0.30649691820144653, 0.18125495314598083, 0.4134674370288849], dtype='float32').reshape([8]),
            paddle.to_tensor([0.20688600838184357, 0.19396933913230896, 0.3606705665588379, 0.17203041911125183, 0.35814234614372253, 0.44482308626174927, 0.0076183113269507885, 0.18918149173259735], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_11513026919f33fd69c71d115d2b2b33(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_186eae946956756b13f3654169c2ef9a
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 32, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2621912360191345, 0.01754346489906311, 0.31303369998931885, 0.09928038716316223, 0.3346082270145416, 0.09433530271053314, 0.3356491029262543, 0.07924806326627731], dtype='float32').reshape([8]),
            paddle.to_tensor([0.289323627948761, 0.381731241941452, 0.3825381398200989, 0.006606598850339651, 0.36759310960769653, 0.2567419409751892, 0.08639203757047653, 0.14179778099060059], dtype='float32').reshape([8]),
            paddle.to_tensor([0.1567632555961609, 0.0961330309510231, 0.3910174071788788, 0.14270472526550293, 0.01738250069320202, 0.2162315398454666, 0.000116082897875458, 0.14678144454956055], dtype='float32').reshape([8]),
            paddle.to_tensor([0.31254804134368896, 0.27394822239875793, 0.031004074960947037, 0.06550263613462448, 0.03523872047662735, 0.06143729388713837, 0.20163783431053162, 0.2307119518518448], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_73918a7e705cab201e1a3f90b31b3ba6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4001970887184143, 0.2651050388813019, 0.4487476348876953, 0.4340157210826874, 0.08954072743654251, 0.39463889598846436, 0.4156869053840637, 0.11443720757961273, 0.30476126074790955, 0.05820426344871521, 0.4526497423648834, 0.0461558997631073, 0.23451440036296844, 0.16106565296649933, 0.2066805511713028, 0.08007264882326126, 0.49336323142051697, 0.05606812238693237, 0.09132691472768784, 0.3966807723045349], dtype='float32').reshape([20]),
            paddle.to_tensor([0.38089388608932495, 0.0997450202703476, 0.37685176730155945, 0.2520486116409302, 0.23455262184143066, 0.43666017055511475, 0.04028160497546196, 0.2363409698009491, 0.4956589341163635, 0.3495956361293793, 0.25439462065696716, 0.33917683362960815, 0.449836790561676, 0.3553984761238098, 0.45715615153312683, 0.49651992321014404, 0.4554704427719116, 0.3790765702724457, 0.17459160089492798, 0.1786486804485321], dtype='float32').reshape([20]),
            paddle.to_tensor([0.40781688690185547, 0.14084488153457642, 0.20674802362918854, 0.0030287830159068108, 0.09481687098741531, 0.31566768884658813, 0.3119169771671295, 0.4410308301448822, 0.31115031242370605, 0.08889070898294449, 0.41739827394485474, 0.02357696369290352, 0.37075579166412354, 0.18409690260887146, 0.3795126974582672, 0.25017595291137695, 0.0373031310737133, 0.25922060012817383, 0.3874324858188629, 0.4714774191379547], dtype='float32').reshape([20]),
            paddle.to_tensor([0.07273253798484802, 0.2827845811843872, 0.4509400427341461, 0.09707769751548767, 0.23026253283023834, 0.31312572956085205, 0.34630000591278076, 0.005674294196069241, 0.19691531360149384, 0.49676164984703064, 0.1568659394979477, 0.1066369041800499, 0.1594209522008896, 0.33537936210632324, 0.1342872530221939, 0.09071628004312515, 0.41636303067207336, 0.23338302969932556, 0.46808409690856934, 0.48499587178230286], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_537742a2f7ada57a7af47efd1c263968(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3025965988636017, 0.3501416742801666, 0.00612650578841567, 0.27234363555908203, 0.3830361068248749, 0.46453431248664856, 0.3142048716545105, 0.07042550295591354, 0.005584091413766146, 0.17375344038009644, 0.1996820718050003, 0.3003094792366028, 0.4677070081233978, 0.19299273192882538, 0.4636601507663727, 0.0935322642326355, 0.42025312781333923, 0.40699976682662964, 0.06401042640209198, 0.2458171546459198], dtype='float32').reshape([20]),
            paddle.to_tensor([0.15666626393795013, 0.02734784036874771, 0.4829655885696411, 0.4690341055393219, 0.3438364565372467, 0.4299326241016388, 0.19492527842521667, 0.10184488445520401, 0.3377637267112732, 0.01790890097618103, 0.4525979459285736, 0.2521076798439026, 0.11443894356489182, 0.1940992921590805, 0.042190056294202805, 0.4712008535861969, 0.17149032652378082, 0.3492051661014557, 0.4795803129673004, 0.06301332265138626], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3558656573295593, 0.37024298310279846, 0.4016057252883911, 0.10293783247470856, 0.4832538366317749, 0.23606856167316437, 0.09325751662254333, 0.13798269629478455, 0.40636488795280457, 0.27774298191070557, 0.14115163683891296, 0.3094111979007721, 0.12362363189458847, 0.4804913401603699, 0.3018425703048706, 0.15840396285057068, 0.37546366453170776, 0.4849421977996826, 0.3455602526664734, 0.26368340849876404], dtype='float32').reshape([20]),
            paddle.to_tensor([0.25477105379104614, 0.391164094209671, 0.2060045301914215, 0.40588411688804626, 0.20098276436328888, 0.4508419632911682, 0.3863883316516876, 0.2148863524198532, 0.07883358746767044, 0.3446783125400543, 0.4370259642601013, 0.021404754370450974, 0.3406035006046295, 0.4656158983707428, 0.48191335797309875, 0.3611816465854645, 0.46983903646469116, 0.10543699562549591, 0.21501977741718292, 0.05413208529353142], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_834cfcc27288b05d1145ce70129ffb42(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 320, 320], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.47666504979133606, 0.220243439078331, 0.07494040578603745, 0.4137759208679199, 0.456644207239151, 0.12460008263587952, 0.3987274765968323, 0.34451228380203247, 0.3607765734195709, 0.2496763914823532, 0.10548671334981918, 0.1437617987394333, 0.26741844415664673, 0.12556785345077515, 0.09010392427444458, 0.4576927125453949, 0.3842138350009918, 0.2774650752544403, 0.2037622481584549, 0.10948603600263596, 0.3472098708152771, 0.23263384401798248, 0.4432082772254944, 0.014366930350661278], dtype='float32').reshape([24]),
            paddle.to_tensor([0.407464861869812, 0.30023184418678284, 0.42324119806289673, 0.2412669062614441, 0.3332243859767914, 0.36257943511009216, 0.12187977135181427, 0.08614557981491089, 0.18218569457530975, 0.345111221075058, 0.4552921950817108, 0.4955901801586151, 0.07801679521799088, 0.45396530628204346, 0.29572203755378723, 0.3798314929008484, 0.05489013344049454, 0.06259775161743164, 0.37144750356674194, 0.37168094515800476, 0.29004645347595215, 0.20666633546352386, 0.3357240855693817, 0.34079280495643616], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3050254285335541, 0.3816494941711426, 0.28385165333747864, 0.43475526571273804, 0.21075187623500824, 0.3345816135406494, 0.042357344180345535, 0.30687808990478516, 0.22306925058364868, 0.31253933906555176, 0.03679843991994858, 0.029163196682929993, 0.3898128569126129, 0.23716428875923157, 0.005816407967358828, 0.2756290137767792, 0.18577368557453156, 0.40830057859420776, 0.024542314931750298, 0.4626503884792328, 0.3929345905780792, 0.39447322487831116, 0.2992578446865082, 0.3571058511734009], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4952376186847687, 0.36889728903770447, 0.15696179866790771, 0.004141559824347496, 0.38539978861808777, 0.028807293623685837, 0.38829532265663147, 0.4838564693927765, 0.3827132284641266, 0.33015984296798706, 0.058416612446308136, 0.10548070073127747, 0.16049987077713013, 0.2391723245382309, 0.21525515615940094, 0.1635219007730484, 0.2980082929134369, 0.0748986303806305, 0.20423030853271484, 0.16684751212596893, 0.44815585017204285, 0.32694345712661743, 0.1601378321647644, 0.13968534767627716], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_333e4efebc44a9480d261948b8cac349(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 16, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cb83137d3c43f9081aa3e61d08497bb7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_36432eade3b81a6f609a4edd3f13da51(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.40530967712402344, 0.15759491920471191, 0.362642765045166, 0.4894177317619324, 0.2272687703371048, 0.2946797311306, 0.07839572429656982, 0.3608400821685791, 0.05169737711548805, 0.08700694888830185, 0.2896486222743988, 0.2331421673297882, 0.11720579117536545, 0.281890869140625, 0.4689858555793762, 0.4067533612251282, 0.40765783190727234, 0.2708682715892792, 0.24441905319690704, 0.33151954412460327, 0.4207228124141693, 0.3815229833126068, 0.33014196157455444, 0.38002702593803406], dtype='float32').reshape([24]),
            paddle.to_tensor([0.28637781739234924, 0.1654985398054123, 0.23309223353862762, 0.024447428062558174, 0.36303576827049255, 0.4101991057395935, 0.20088838040828705, 0.08815351128578186, 0.30949050188064575, 0.08637293428182602, 0.4339812099933624, 0.4880203604698181, 0.060222119092941284, 0.46031877398490906, 0.0117497518658638, 0.21543627977371216, 0.12192179262638092, 0.43318477272987366, 0.20757561922073364, 0.3496062159538269, 0.40646737813949585, 0.21076370775699615, 0.08872045576572418, 0.41754671931266785], dtype='float32').reshape([24]),
            paddle.to_tensor([0.21637052297592163, 0.48779675364494324, 0.49553510546684265, 0.3338158428668976, 0.434607595205307, 0.2623617649078369, 0.45492708683013916, 0.4687954783439636, 0.3119964599609375, 0.4566510021686554, 0.3158959746360779, 0.19488127529621124, 0.3716540038585663, 0.007492515724152327, 0.3022388517856598, 0.35055068135261536, 0.3178103566169739, 0.39998987317085266, 0.49393051862716675, 0.4641536772251129, 0.005904096178710461, 0.31937921047210693, 0.14111089706420898, 0.22800688445568085], dtype='float32').reshape([24]),
            paddle.to_tensor([0.08119852095842361, 0.16662265360355377, 0.39186882972717285, 0.11442850530147552, 0.4411783516407013, 0.2167796939611435, 0.3179394602775574, 0.10905671864748001, 0.2536802589893341, 0.4967714250087738, 0.11380425095558167, 0.19714698195457458, 0.4878780245780945, 0.37495356798171997, 0.48608124256134033, 0.3128107786178589, 0.07850612699985504, 0.039059240370988846, 0.30481722950935364, 0.10021859407424927, 0.022488998249173164, 0.4284578263759613, 0.45113512873649597, 0.17573235929012299], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_22de7e6d60c611b5164d0908f3d26e3b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 1728, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1728], dtype='float32', min=0, max=0.5),
            paddle.uniform([1728], dtype='float32', min=0, max=0.5),
            paddle.uniform([1728], dtype='float32', min=0, max=0.5),
            paddle.uniform([1728], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4940c4041301df321293b10165d4c0c4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 32, 64], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.03994951397180557, 0.2902117967605591, 0.06901482492685318, 0.12083443254232407, 0.024351803585886955, 0.4189302325248718, 0.4972124695777893, 0.20596571266651154, 0.29974594712257385, 0.06356348097324371, 0.27618715167045593, 0.10889218002557755, 0.44564688205718994, 0.18684636056423187, 0.24048158526420593, 0.3596365451812744, 0.046151962131261826, 0.49445927143096924], dtype='float32').reshape([18]),
            paddle.to_tensor([0.19910383224487305, 0.3277864158153534, 0.4856113791465759, 0.01057178620249033, 0.03581276163458824, 0.37127920985221863, 0.1673947125673294, 0.3704778552055359, 0.45503947138786316, 0.016447357833385468, 0.18102222681045532, 0.3207801580429077, 0.07700694352388382, 0.47163015604019165, 0.020984208211302757, 0.36070165038108826, 0.42285990715026855, 0.2643563449382782], dtype='float32').reshape([18]),
            paddle.to_tensor([0.01635424979031086, 0.2320690006017685, 0.34777340292930603, 0.4074742794036865, 0.40686294436454773, 0.07119432091712952, 0.23106342554092407, 0.47739890217781067, 0.13358278572559357, 0.23188862204551697, 0.4284246265888214, 0.2592198848724365, 0.3589380979537964, 0.12156364321708679, 0.3812914490699768, 0.12612798810005188, 0.05490867793560028, 0.2934868335723877], dtype='float32').reshape([18]),
            paddle.to_tensor([0.010224352590739727, 0.025988683104515076, 0.082767553627491, 0.019651250913739204, 0.27451208233833313, 0.48627883195877075, 0.30689024925231934, 0.48784881830215454, 0.15170811116695404, 0.09544384479522705, 0.3184471130371094, 0.45005002617836, 0.265285849571228, 0.0736357793211937, 0.39201778173446655, 0.2040860503911972, 0.4202445447444916, 0.05150780454277992], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_df8e1b7eaa7538d927f88291cbaa9dbe(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 1216, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1216], dtype='float32', min=0, max=0.5),
            paddle.uniform([1216], dtype='float32', min=0, max=0.5),
            paddle.uniform([1216], dtype='float32', min=0, max=0.5),
            paddle.uniform([1216], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_61e4ef3b21e21fe3fa90196fffe12ac9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 4, 3], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8e29fc02ea5de39956727ca7bdfc34ec(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.06403395533561707, 0.21723689138889313, 0.08578023314476013, 0.12703517079353333, 0.27916768193244934, 0.3962082862854004, 0.08874938637018204, 0.3230089247226715, 0.47801148891448975, 0.4192841947078705, 0.258847177028656, 0.1639229953289032, 0.31736060976982117, 0.34067749977111816, 0.30948030948638916, 0.03344759717583656, 0.14741143584251404, 0.11274900287389755, 0.0942053496837616, 0.43458712100982666, 0.41953060030937195, 0.01081764791160822, 0.13100938498973846, 0.10763625800609589, 0.45986029505729675, 0.18331186473369598, 0.29678481817245483, 0.22228167951107025, 0.29836636781692505, 0.21029122173786163], dtype='float32').reshape([30]),
            paddle.to_tensor([0.22288170456886292, 0.37457162141799927, 0.043421801179647446, 0.199832022190094, 0.06335633248090744, 0.29874181747436523, 0.23881416022777557, 0.022969640791416168, 0.11500941216945648, 0.3855063319206238, 0.18354706466197968, 0.45903050899505615, 0.36428722739219666, 0.29509642720222473, 0.1333979219198227, 0.10750117152929306, 0.3100173771381378, 0.2849089205265045, 0.05951889604330063, 0.2359757125377655, 0.3733755052089691, 0.31320708990097046, 0.4466901421546936, 0.2313084602355957, 0.47874394059181213, 0.043347280472517014, 0.17981921136379242, 0.28656405210494995, 0.4596683979034424, 0.2464103400707245], dtype='float32').reshape([30]),
            paddle.to_tensor([0.02888781577348709, 0.4429013431072235, 0.2899637818336487, 0.02815295197069645, 0.07787381857633591, 0.20584072172641754, 0.15943646430969238, 0.21547159552574158, 0.40324667096138, 0.1285547912120819, 0.1926584392786026, 0.05447383597493172, 0.04506300762295723, 0.4304562211036682, 0.3293783366680145, 0.46808505058288574, 0.27071648836135864, 0.39318975806236267, 0.2567654848098755, 3.878440475091338e-05, 0.23170208930969238, 0.002104089129716158, 0.17953357100486755, 0.2966737747192383, 0.02640330232679844, 0.1764092594385147, 0.06317955255508423, 0.3440818786621094, 0.2655598819255829, 0.19142505526542664], dtype='float32').reshape([30]),
            paddle.to_tensor([0.38734278082847595, 0.19529983401298523, 0.37072017788887024, 0.3451511859893799, 0.20649346709251404, 0.1141333281993866, 0.1937745213508606, 0.1699558049440384, 0.07345698028802872, 0.2771627604961395, 0.42468976974487305, 0.3830486238002777, 0.1088147908449173, 0.39838099479675293, 0.15945886075496674, 0.426995187997818, 0.21807444095611572, 0.4121449887752533, 0.020497839897871017, 0.04537102207541466, 0.484291136264801, 0.17069226503372192, 0.13949225842952728, 0.418612539768219, 0.0198855921626091, 0.15638437867164612, 0.4084956645965576, 0.158088818192482, 0.289507120847702, 0.14948147535324097], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e016dc7fcf15b34836c0d831a612361a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 4, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cff19279a8b084c58f8b14aac7c2f7e8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9c92876fb1f518af7ce67b0389550b41(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.34061238169670105, 0.1522592157125473, 0.07528525590896606, 0.32635998725891113, 0.17076261341571808, 0.4554414451122284, 0.3517439067363739, 0.02335870824754238, 0.05828804522752762, 0.3924742341041565, 0.2596585750579834, 0.38724035024642944, 0.4442652463912964, 0.1085098385810852, 0.4521369934082031, 0.4017132520675659, 0.4959157705307007, 0.18937833607196808, 0.17704610526561737, 0.004124912433326244, 0.13107842206954956, 0.19547457993030548, 0.45730096101760864, 0.3002707064151764, 0.37539148330688477, 0.25787147879600525, 0.43873098492622375, 0.20122087001800537], dtype='float32').reshape([28]),
            paddle.to_tensor([0.18680012226104736, 0.034927450120449066, 0.17792999744415283, 0.2927561104297638, 0.4469858407974243, 0.289810448884964, 0.3520065248012543, 0.47587651014328003, 0.4193165898323059, 0.29833558201789856, 0.3589138984680176, 0.005886786617338657, 0.012814182788133621, 0.3335980772972107, 0.11478076130151749, 0.34715914726257324, 0.2219875007867813, 0.2811327278614044, 0.38974660634994507, 0.09704149514436722, 0.16110782325267792, 0.32185259461402893, 0.4045924246311188, 0.0761370062828064, 0.08003444969654083, 0.3143979012966156, 0.21956506371498108, 0.21430325508117676], dtype='float32').reshape([28]),
            paddle.to_tensor([0.03987133875489235, 0.0916077122092247, 0.184051975607872, 0.1321585327386856, 0.38024425506591797, 0.1081564649939537, 0.4833524823188782, 0.3130475580692291, 0.032841697335243225, 0.15655452013015747, 0.4995022416114807, 0.2222484052181244, 0.1520719677209854, 0.26881611347198486, 0.3454367220401764, 0.2535886764526367, 0.1393679529428482, 0.21563291549682617, 0.39126935601234436, 0.39063072204589844, 0.11702170968055725, 0.41664138436317444, 0.12325920909643173, 0.3752218186855316, 0.21188874542713165, 0.012409409508109093, 0.24836556613445282, 0.41078609228134155], dtype='float32').reshape([28]),
            paddle.to_tensor([0.15501779317855835, 0.49210989475250244, 0.15092186629772186, 0.27323228120803833, 0.36399129033088684, 0.42167097330093384, 0.22723740339279175, 0.36245447397232056, 0.10321775078773499, 0.4804154634475708, 0.05476803332567215, 0.29777273535728455, 0.05686867982149124, 0.29633650183677673, 0.15231923758983612, 0.30853763222694397, 0.26141130924224854, 0.2511698007583618, 0.462663471698761, 0.37336912751197815, 0.2509402930736542, 0.2778709828853607, 0.0006919653969816864, 0.4028286933898926, 0.35420215129852295, 0.0477709136903286, 0.4494708478450775, 0.0070119318552315235], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5d9573388b51bb0f1fe22be45d4d9e3b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([8, 256, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d2c37dea409b6532e0d432ed2d7014c8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 162, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([162], dtype='float32', min=0, max=0.5),
            paddle.uniform([162], dtype='float32', min=0, max=0.5),
            paddle.uniform([162], dtype='float32', min=0, max=0.5),
            paddle.uniform([162], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b4d235183b08b4051571a4fce4db5596(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c82d8a90f7db08f8d26ac44efa85c1d8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.40849071741104126, 0.20005236566066742, 0.2967625558376312, 0.3105223476886749, 0.41430360078811646, 0.15736836194992065, 0.30544131994247437, 0.2006440907716751, 0.39339107275009155, 0.13917721807956696, 0.4424225687980652, 0.11711635440587997, 0.1564401388168335, 0.23693592846393585, 0.37477371096611023, 0.39174509048461914, 0.06956823915243149, 0.03277481347322464, 0.4311491549015045, 0.26741793751716614, 0.23269636929035187, 0.1310189664363861, 0.06283091753721237, 0.317880779504776], dtype='float32').reshape([24]),
            paddle.to_tensor([0.06551719456911087, 0.4022621214389801, 0.13594885170459747, 0.07733036577701569, 0.11122165620326996, 0.24797438085079193, 0.40676963329315186, 0.34209465980529785, 0.4149981141090393, 0.2794311046600342, 0.12373591214418411, 0.37111353874206543, 0.20712752640247345, 0.10140540450811386, 0.041656170040369034, 0.02751964144408703, 0.29859262704849243, 0.3411012291908264, 0.4821068048477173, 0.37538155913352966, 0.41259026527404785, 0.36738190054893494, 0.08298816531896591, 0.4163966774940491], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2976153492927551, 0.3724423348903656, 0.3163577914237976, 0.001534170238301158, 0.05662039294838905, 0.08355499058961868, 0.42458605766296387, 0.19826366007328033, 0.29462480545043945, 0.41755354404449463, 0.04698701202869415, 0.4121767282485962, 0.28843235969543457, 0.1386885792016983, 0.03974584862589836, 0.08799285441637039, 0.4010465145111084, 0.04802155867218971, 0.21345223486423492, 0.10143523663282394, 0.2617005407810211, 0.3870459496974945, 0.4722096621990204, 0.09486552327871323], dtype='float32').reshape([24]),
            paddle.to_tensor([0.38031262159347534, 0.13115225732326508, 0.14387832581996918, 0.44203656911849976, 0.09869396686553955, 0.4188726246356964, 0.004420838318765163, 0.029196931049227715, 0.18071861565113068, 0.03614803031086922, 0.43151596188545227, 0.08030115813016891, 0.41594693064689636, 0.43447282910346985, 0.2664889395236969, 0.49000000953674316, 0.3438970148563385, 0.11862435191869736, 0.20086245238780975, 0.38919124007225037, 0.0487244538962841, 0.043810516595840454, 0.3871317505836487, 0.05736711621284485], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_72b20683c46d681e29006adabd8f12f2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 320, 320], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.1917145997285843, 0.14525629580020905, 0.13491089642047882, 0.02650339901447296, 0.42740997672080994, 0.1492345929145813, 0.20086440443992615, 0.41201379895210266], dtype='float32').reshape([8]),
            paddle.to_tensor([0.14435134828090668, 0.17339584231376648, 0.4677729308605194, 0.2730685770511627, 0.08681339770555496, 0.3128717541694641, 0.1352986991405487, 0.04680488631129265], dtype='float32').reshape([8]),
            paddle.to_tensor([0.3480464518070221, 0.4266836941242218, 0.38405659794807434, 0.0542532354593277, 0.14432039856910706, 0.2232455462217331, 0.4846668243408203, 0.25867077708244324], dtype='float32').reshape([8]),
            paddle.to_tensor([0.424111545085907, 0.4748759865760803, 0.39174744486808777, 0.36445102095603943, 0.2721955478191376, 0.15050165355205536, 0.05421384796500206, 0.06090507283806801], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9360a394d37f1ce7ec0b89f476c149bd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.15973056852817535, 0.31942251324653625, 0.18832999467849731, 0.042286962270736694, 0.10689164698123932, 0.26356130838394165, 0.41919898986816406, 0.4123912751674652, 0.10066977143287659, 0.19749857485294342, 0.49881839752197266, 0.10871054232120514, 0.039591241627931595, 0.3866293132305145, 0.48622143268585205, 0.16103315353393555], dtype='float32').reshape([16]),
            paddle.to_tensor([0.18254707753658295, 0.29257798194885254, 0.4934844374656677, 0.061270102858543396, 0.2810033857822418, 0.17213459312915802, 0.29364344477653503, 0.47153741121292114, 0.3785509765148163, 0.40397220849990845, 0.46254900097846985, 0.21601052582263947, 0.2377849817276001, 0.09268296509981155, 0.4481440782546997, 0.48658204078674316], dtype='float32').reshape([16]),
            paddle.to_tensor([0.47228217124938965, 0.4160947799682617, 0.3519399166107178, 0.19449375569820404, 0.39180850982666016, 0.1640804260969162, 0.16994155943393707, 0.11454109102487564, 0.22789958119392395, 0.13806477189064026, 0.05213058739900589, 0.41734838485717773, 0.21796903014183044, 0.14962387084960938, 0.10476098209619522, 0.19835707545280457], dtype='float32').reshape([16]),
            paddle.to_tensor([0.35687482357025146, 0.04537075385451317, 0.11380960792303085, 0.16389793157577515, 0.37038564682006836, 0.0687849149107933, 0.26090264320373535, 0.1815759241580963, 0.08031347393989563, 0.09999387711286545, 0.36949095129966736, 0.4701825678348541, 0.2611653804779053, 0.40073367953300476, 0.07539372146129608, 0.3062756061553955], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_19a49db41f3f0b4ef4fe0b77791376af(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_679e5296adbbe42b9746ddeeb129d5f2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 4, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_76b5576438a53bbaf142dd428ab96d70(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_027b9482921622b192e8029a23d7f0aa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 2, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5b766572b7a37b675efd5aadeeb82c86(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cf269370a9704585c1ad2eabe3c3baec(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 240, 240], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.363189697265625, 0.25176647305488586, 0.32057517766952515, 0.3253726065158844, 0.04823780059814453, 0.007574168499559164, 0.4387446641921997, 0.04841047525405884, 0.18051069974899292, 0.26178836822509766, 0.034763891249895096, 0.4481073021888733, 0.4247792363166809, 0.16437533497810364, 0.3097483515739441, 0.3078221082687378, 0.24658651649951935, 0.1127740815281868, 0.13236355781555176, 0.15880343317985535, 0.3582329750061035, 0.4249383509159088, 0.12746146321296692, 0.461669921875], dtype='float32').reshape([24]),
            paddle.to_tensor([0.18332983553409576, 0.4765949547290802, 0.011533970944583416, 0.3848622441291809, 0.029766961932182312, 0.2405446171760559, 0.2957654595375061, 0.42349860072135925, 0.41790783405303955, 0.08873720467090607, 0.23109915852546692, 0.10825113952159882, 0.1255694478750229, 0.2773588299751282, 0.10089627653360367, 0.43337446451187134, 0.4663742780685425, 0.06473017483949661, 0.0700964406132698, 0.00885939784348011, 0.25227734446525574, 0.03395165875554085, 0.01870734803378582, 0.1741129606962204], dtype='float32').reshape([24]),
            paddle.to_tensor([0.12323717027902603, 0.2597314417362213, 0.2973880171775818, 0.2576245665550232, 0.4216356575489044, 0.2185320258140564, 0.17227588593959808, 0.15849897265434265, 0.09336751699447632, 0.12414819002151489, 0.3567335903644562, 0.01794065162539482, 0.18171773850917816, 0.21900714933872223, 0.343353807926178, 0.44248080253601074, 0.06962631642818451, 0.34946951270103455, 0.1175563782453537, 0.08505085855722427, 0.3857732117176056, 0.4361962378025055, 0.1934213936328888, 0.2699592411518097], dtype='float32').reshape([24]),
            paddle.to_tensor([0.08561639487743378, 0.43746882677078247, 0.36534440517425537, 0.2548723816871643, 0.22404707968235016, 0.06579054147005081, 0.34758007526397705, 0.44812169671058655, 0.25159865617752075, 0.08927749842405319, 0.4552820920944214, 0.2274472713470459, 0.27840951085090637, 0.24203379452228546, 0.39608973264694214, 0.06387542188167572, 0.13649694621562958, 0.2083539515733719, 0.25408756732940674, 0.0844537764787674, 0.46897193789482117, 0.41855382919311523, 0.2812015414237976, 0.3576538562774658], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6781dade49b9a6b8a7528805c33aaf48(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 1824, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1824], dtype='float32', min=0, max=0.5),
            paddle.uniform([1824], dtype='float32', min=0, max=0.5),
            paddle.uniform([1824], dtype='float32', min=0, max=0.5),
            paddle.uniform([1824], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_12af1429d3f90b5b58795194e40a2523(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_512e251ff92adbc41052f75ccafa6230(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 60, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8bca481964096cfe97198963594fdc00(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 60, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_23f9f920438f92f89186c8b979d4f36d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_769245cd95c3be03a9760d5ad1e233c6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([8, 256, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_008fe4f4b4abc7f2e06c804ef731f385(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.02282620221376419, 0.4110115170478821, 0.3984547257423401, 0.4521254301071167, 0.11789185553789139, 0.026973094791173935, 0.2649022936820984, 0.3049532473087311, 0.3016934394836426, 0.08322235941886902, 0.4172348082065582, 0.2207144945859909, 0.23924089968204498, 0.4685601592063904, 0.355208158493042, 0.0036671413108706474, 0.3424859344959259, 0.34364816546440125, 0.31950312852859497, 0.39277905225753784, 0.20151928067207336, 0.0820511132478714, 0.11223870515823364, 0.4761347472667694, 0.11781390756368637, 0.05644778162240982, 0.4942314624786377, 0.34813711047172546], dtype='float32').reshape([28]),
            paddle.to_tensor([0.1525900512933731, 0.06678345799446106, 0.1995798796415329, 0.2586028277873993, 0.2430124282836914, 0.37194615602493286, 0.005434482358396053, 0.42663466930389404, 0.25294390320777893, 0.02519744262099266, 0.12683872878551483, 0.4170732796192169, 0.2933899760246277, 0.027452101930975914, 0.005117018241435289, 0.013850662857294083, 0.2805650532245636, 0.2106216847896576, 0.12623444199562073, 0.3030568063259125, 0.3425103724002838, 0.23499585688114166, 0.12921510636806488, 0.47554630041122437, 0.16995510458946228, 0.061073023825883865, 0.21042561531066895, 0.2057444453239441], dtype='float32').reshape([28]),
            paddle.to_tensor([0.20257742702960968, 0.3929566442966461, 0.10292086750268936, 0.22729796171188354, 0.4328130781650543, 0.3789111077785492, 0.29661157727241516, 0.2627553939819336, 0.33902671933174133, 0.33201342821121216, 0.38672202825546265, 0.20112790167331696, 0.33631524443626404, 0.2126658707857132, 0.3322421908378601, 0.2685594856739044, 0.14593641459941864, 0.14028361439704895, 0.437996506690979, 0.07550954073667526, 0.4465808868408203, 0.4876677095890045, 0.2607434391975403, 0.430660605430603, 0.3062579035758972, 0.300289511680603, 0.17064763605594635, 0.056804489344358444], dtype='float32').reshape([28]),
            paddle.to_tensor([0.30255618691444397, 0.36403006315231323, 0.3323858976364136, 0.3073616921901703, 0.1551816463470459, 0.26380687952041626, 0.21202987432479858, 0.42190808057785034, 0.0798085555434227, 0.41048532724380493, 0.10858891904354095, 0.4671378433704376, 0.1269838958978653, 0.40973493456840515, 0.10944080352783203, 0.31271126866340637, 0.04926803708076477, 0.4257911145687103, 0.24139419198036194, 0.19837719202041626, 0.4881667494773865, 0.0016097386833280325, 0.018716828897595406, 0.47152969241142273, 0.3683614134788513, 0.24553009867668152, 0.07750678062438965, 0.20013836026191711], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1de2a46790dd4f111e07be4505132d4d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9b2362ccfe4513a4409f87c30939869b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 50, 84], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9bc246b08499c278f02b17dd6a551369(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 61, 61], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_02061664183fd2771b2265426b8b58b8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.1487082839012146, 0.28744226694107056, 0.3985540270805359, 0.39891713857650757, 0.06996918469667435, 0.42699792981147766, 0.2846561074256897, 0.05502753704786301, 0.3112058937549591, 0.31594496965408325, 0.13102540373802185, 0.15918543934822083, 0.4677768051624298, 0.4092761278152466, 0.18936368823051453, 0.4188545346260071], dtype='float32').reshape([16]),
            paddle.to_tensor([0.26724767684936523, 0.39469489455223083, 0.0024270396679639816, 0.4995783865451813, 0.22611193358898163, 0.38022157549858093, 0.1339706927537918, 0.06019645184278488, 0.3113468289375305, 0.11193784326314926, 0.47505271434783936, 0.3560868501663208, 0.2807295620441437, 0.07250324636697769, 0.3906102180480957, 0.23738715052604675], dtype='float32').reshape([16]),
            paddle.to_tensor([0.1129026785492897, 0.3197409510612488, 0.4986647367477417, 0.11095434427261353, 0.27405786514282227, 0.17749622464179993, 0.00613306974992156, 0.2749677896499634, 0.1736123263835907, 0.31408846378326416, 0.3932825028896332, 0.4428180754184723, 0.0872575044631958, 0.20464490354061127, 0.2950572669506073, 0.47536858916282654], dtype='float32').reshape([16]),
            paddle.to_tensor([0.01403580978512764, 0.46772077679634094, 0.03255927190184593, 0.37033694982528687, 0.3632969558238983, 0.06373070925474167, 0.03473494574427605, 0.3151133060455322, 0.02830534428358078, 0.14909519255161285, 0.30465301871299744, 0.45060083270072937, 0.27701330184936523, 0.17052188515663147, 0.4343344271183014, 0.3565841019153595], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_07046e21cff5311b88c5a6a92b2215b5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6aaef8cbca7a6687bf01a32360045ee9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3f1664e0fce2fca9873401bfaa44df0e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.37774285674095154, 0.33413946628570557, 0.028739478439092636, 0.2944481670856476, 0.22810059785842896, 0.2915196716785431, 0.24569609761238098, 0.03435782715678215], dtype='float32').reshape([8]),
            paddle.to_tensor([0.0029480753000825644, 0.31065720319747925, 0.16197402775287628, 0.26149097084999084, 0.4325714409351349, 0.16312195360660553, 0.4407558739185333, 0.034578897058963776], dtype='float32').reshape([8]),
            paddle.to_tensor([0.12693974375724792, 0.05941732972860336, 0.48063117265701294, 0.08734845370054245, 0.23219329118728638, 0.17617712914943695, 0.23206409811973572, 0.4703582525253296], dtype='float32').reshape([8]),
            paddle.to_tensor([0.42655596137046814, 0.2799667716026306, 0.0008633194374851882, 0.18669256567955017, 0.2640725076198578, 0.15346460044384003, 0.4475107192993164, 0.11575261503458023], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_85a69f130f4ca31dda731afab59e5626(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 432, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_75365261511f811c7db8163f9a062571(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 19, 19], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2e402810ea4b5a45d3dc756423c2d830(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.38197919726371765, 0.42853546142578125, 0.32204172015190125, 0.1839013248682022, 0.25836268067359924, 0.4833086133003235, 0.19163110852241516, 0.2920694053173065, 0.13349051773548126, 0.40860965847969055, 0.11787392944097519, 0.4274855852127075, 0.4776051640510559, 0.28895479440689087, 0.06882933527231216, 0.023346811532974243, 0.4565816819667816, 0.4587003290653229], dtype='float32').reshape([18]),
            paddle.to_tensor([0.44811421632766724, 0.35609903931617737, 0.1782498061656952, 0.039876796305179596, 0.17656570672988892, 0.40396368503570557, 0.3692021369934082, 0.3217504024505615, 0.2575661838054657, 0.38550764322280884, 0.326505571603775, 0.2684803903102875, 0.10317496210336685, 0.04275358095765114, 0.11116781085729599, 0.022396177053451538, 0.2211483269929886, 0.2323569357395172], dtype='float32').reshape([18]),
            paddle.to_tensor([0.09923109412193298, 0.14974987506866455, 0.202860489487648, 0.06923066824674606, 0.36571380496025085, 0.24950209259986877, 0.3629670739173889, 0.3157504200935364, 0.35662323236465454, 0.2885669469833374, 0.1765749454498291, 0.24915175139904022, 0.4665757715702057, 0.0926164910197258, 0.11875344812870026, 0.0929281935095787, 0.46765798330307007, 0.016479890793561935], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3615719676017761, 0.23276498913764954, 0.20786388218402863, 0.4497385621070862, 0.4023052155971527, 0.28007832169532776, 0.21968401968479156, 0.021479276940226555, 0.1327212154865265, 0.32837575674057007, 0.4195932447910309, 0.08181717991828918, 0.49228036403656006, 0.49880537390708923, 0.3237418532371521, 0.1884862184524536, 0.2488740086555481, 0.052032142877578735], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7133bc824c9c02fa8032f07061894cae(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 1344, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1344], dtype='float32', min=0, max=0.5),
            paddle.uniform([1344], dtype='float32', min=0, max=0.5),
            paddle.uniform([1344], dtype='float32', min=0, max=0.5),
            paddle.uniform([1344], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4e2d7da326e43e836fe68cf56bc13049(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_48013035b087a8f64b352a2b112a689d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a59d09e8d5a8d7e62bdfaa05e2ebb8a0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2dc8ea2648b89251d3a14bd48bbbed25(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_d176968eb532ab7742141be9f49773ee
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6158196a0b7d48025ecf2b14c2f2ac15(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 96, 96], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1fd116a588d94248f6ee80aef151db30(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 1472, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1472], dtype='float32', min=0, max=0.5),
            paddle.uniform([1472], dtype='float32', min=0, max=0.5),
            paddle.uniform([1472], dtype='float32', min=0, max=0.5),
            paddle.uniform([1472], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e6c78a2b4c4e78cc9960223aecf7f8b9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 200, 336], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_07cd2cbbd3303d2c29f1d31c1478c9a7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_abc15bd94e55776fdbcb840279865c01
    def get_inputs(self):
        return [
            paddle.uniform([49, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_24c3a8d9893b52cdd26cc1c7da47e9b1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 8, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_da9971f4b60c063cf6714d715bad2ff9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 122, 122], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0e1d8fdc83dbcbb893bf443a9b191643(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3187565803527832, 0.04627612233161926, 0.35038554668426514, 0.041412197053432465, 0.12739133834838867, 0.04333037510514259, 0.3439205288887024, 0.4161439836025238, 0.3058464527130127, 0.37632253766059875, 0.46881288290023804, 0.46648654341697693, 0.4470750689506531, 0.16411897540092468, 0.35032808780670166, 0.412823885679245, 0.4082989990711212, 0.3669586181640625], dtype='float32').reshape([18]),
            paddle.to_tensor([0.43291783332824707, 0.15380823612213135, 0.3583782911300659, 0.08453279733657837, 0.35802873969078064, 0.38476476073265076, 0.07539607584476471, 0.4488184154033661, 0.37970107793807983, 0.2895526587963104, 0.04369359090924263, 0.09239720553159714, 0.13788558542728424, 0.13339293003082275, 0.027533013373613358, 0.34177303314208984, 0.001764993416145444, 0.44976481795310974], dtype='float32').reshape([18]),
            paddle.to_tensor([0.07686329632997513, 0.30529746413230896, 0.31018704175949097, 0.04238242655992508, 0.28775754570961, 0.425171434879303, 0.0038596331141889095, 0.14494958519935608, 0.4276674687862396, 0.1989736109972, 0.39390748739242554, 0.30635741353034973, 0.3119378387928009, 0.4665589928627014, 0.3034593462944031, 0.420471727848053, 0.19590018689632416, 0.2039225697517395], dtype='float32').reshape([18]),
            paddle.to_tensor([0.14187583327293396, 0.2509976029396057, 0.4605376124382019, 0.18275661766529083, 0.48217159509658813, 0.0564441904425621, 0.26415687799453735, 0.39773067831993103, 0.3415140211582184, 0.0849887877702713, 0.3627387285232544, 0.35921230912208557, 0.24678675830364227, 0.29575085639953613, 0.442641943693161, 0.04229871928691864, 0.033285439014434814, 0.3264000415802002], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1976286ea66d1503ab9e0191c55fa94e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_60c4802114724eea16bddb9458713787(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c6d28e94d042c0cdfc56b9820fd628a1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_71adf679806cab2a22b147a9b47d0cbc
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 320, 320], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3ec066adcb20ac0536d622bdebf5214c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 8, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9453095f29954fc859aec8db1f91c282(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 132, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([132], dtype='float32', min=0, max=0.5),
            paddle.uniform([132], dtype='float32', min=0, max=0.5),
            paddle.uniform([132], dtype='float32', min=0, max=0.5),
            paddle.uniform([132], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_28c7852fae1ab3ebdb7066c2bb81889b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 768, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_84fdb1a0bfb897b8e3679a855112c12a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_13025b164297d9226f9641b7f0715c76
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_258d44cd4291d2dba7dcbdd4144e0072(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 31, 31], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_408818b702ca8af392c0be145408963f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 26, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3276645541191101, 0.41176748275756836, 0.0052064587362110615, 0.33807772397994995, 0.03587736561894417, 0.4064595401287079, 0.48488789796829224, 0.03641723096370697, 0.2821633219718933, 0.46520793437957764, 0.0703548938035965, 0.363079309463501, 0.034293416887521744, 0.05839680880308151, 0.18005818128585815, 0.2845630347728729, 0.3488025963306427, 0.06612081080675125, 0.35654929280281067, 0.04988248273730278, 0.1293976902961731, 0.45266667008399963, 0.1413533091545105, 0.35607558488845825, 0.3275870978832245, 0.0903351828455925], dtype='float32').reshape([26]),
            paddle.to_tensor([0.4267351031303406, 0.21473924815654755, 0.4014376997947693, 0.257236123085022, 0.4741632044315338, 0.32984116673469543, 0.34452128410339355, 0.01699315942823887, 0.28742924332618713, 0.16062788665294647, 0.3685609698295593, 0.26740017533302307, 0.17078715562820435, 0.28727325797080994, 0.13167771697044373, 0.21939679980278015, 0.44342708587646484, 0.15507881343364716, 0.2920987904071808, 0.09330210089683533, 0.4759087860584259, 0.33576977252960205, 0.022386619821190834, 0.48154839873313904, 0.4719553291797638, 0.2560347616672516], dtype='float32').reshape([26]),
            paddle.to_tensor([0.385200172662735, 0.48427262902259827, 0.10361963510513306, 0.4903425872325897, 0.025020575150847435, 0.4088614284992218, 0.19924402236938477, 0.4146808087825775, 0.12579815089702606, 0.2559545338153839, 0.3351125419139862, 0.06134422868490219, 0.21278446912765503, 0.36882489919662476, 0.0232355035841465, 0.3056737184524536, 0.10767967998981476, 0.239004984498024, 0.008121483027935028, 0.13459455966949463, 0.3772476613521576, 0.04533796384930611, 0.06406395882368088, 0.0995941013097763, 0.20696648955345154, 0.2830551862716675], dtype='float32').reshape([26]),
            paddle.to_tensor([0.4827463626861572, 0.3697878420352936, 0.47253623604774475, 0.10342767089605331, 0.4929938018321991, 0.3428988754749298, 0.36478811502456665, 0.43513089418411255, 0.14468209445476532, 0.39597368240356445, 0.4834887385368347, 0.3593025803565979, 0.005689902696758509, 0.4396025538444519, 0.17922143638134003, 0.48752281069755554, 0.005445919465273619, 0.22784627974033356, 0.3225133419036865, 0.46217137575149536, 0.3693116307258606, 0.17717474699020386, 0.20483770966529846, 0.09976847469806671, 0.278450608253479, 0.05917947366833687], dtype='float32').reshape([26]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_931a9a62e548a147974c6c7df7b39da2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b05819ef0f13ea5374a4af2617dcddfa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 102, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([102], dtype='float32', min=0, max=0.5),
            paddle.uniform([102], dtype='float32', min=0, max=0.5),
            paddle.uniform([102], dtype='float32', min=0, max=0.5),
            paddle.uniform([102], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5829db0b3394b37a37fecc8d8e69be6f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 4, 3], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fdac85965ae5c5a88c16d689a593ac3a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.24492456018924713, 0.437023788690567, 0.41584718227386475, 0.23556089401245117, 0.2231484204530716, 0.07737638801336288, 0.19620658457279205, 0.46686214208602905, 0.19935914874076843, 0.204921692609787, 0.08090128749608994, 0.11220996081829071, 0.24207402765750885, 0.1517992913722992, 0.025212004780769348, 0.027264175936579704, 0.23482488095760345, 0.48895782232284546, 0.3865562379360199, 0.380399227142334, 0.04072338342666626, 0.35719555616378784, 0.34173107147216797, 0.38040173053741455, 0.2978140115737915, 0.05196928232908249, 0.4075467586517334, 0.26926955580711365, 0.1001051515340805, 0.30674222111701965], dtype='float32').reshape([30]),
            paddle.to_tensor([0.0582146942615509, 0.1482418179512024, 0.48172393441200256, 0.4780699610710144, 0.190209299325943, 0.1985977441072464, 0.21064576506614685, 0.43224194645881653, 0.1126941666007042, 0.18095701932907104, 0.38817745447158813, 0.39465174078941345, 0.3752039074897766, 0.04657117277383804, 0.30916550755500793, 0.4357938766479492, 0.15371979773044586, 0.09887757897377014, 0.09963072836399078, 0.40549713373184204, 0.10277106612920761, 0.13450083136558533, 0.05832957103848457, 0.37772229313850403, 0.020603489130735397, 0.25886303186416626, 0.13143962621688843, 0.13047181069850922, 0.39912018179893494, 0.35551148653030396], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4853742718696594, 0.0175083726644516, 0.1871105134487152, 0.2320319265127182, 0.1669328510761261, 0.06599237024784088, 0.39195337891578674, 0.28154653310775757, 0.05752316862344742, 0.11906493455171585, 0.34626996517181396, 0.4297071397304535, 0.1380140781402588, 0.1927156001329422, 0.10580360144376755, 0.22229570150375366, 0.1318928599357605, 0.06023962050676346, 0.23910939693450928, 0.21193276345729828, 0.44238370656967163, 0.4617689549922943, 0.3259750306606293, 0.040397390723228455, 0.07630154490470886, 0.4438491761684418, 0.07770861685276031, 0.19879017770290375, 0.35467544198036194, 0.24554701149463654], dtype='float32').reshape([30]),
            paddle.to_tensor([0.08009329438209534, 0.12874990701675415, 0.30043476819992065, 0.4607342481613159, 0.47658872604370117, 0.491068035364151, 0.213114932179451, 0.2902621924877167, 0.40085864067077637, 0.32688868045806885, 0.10031116008758545, 0.11503827571868896, 0.11923720687627792, 0.33575618267059326, 0.3918359875679016, 0.4327671527862549, 0.023064270615577698, 0.3078967332839966, 0.4803033471107483, 0.1359076350927353, 0.39687860012054443, 0.23164603114128113, 0.13270609080791473, 0.39148324728012085, 0.45427924394607544, 0.0018279470968991518, 0.48678460717201233, 0.30126941204071045, 0.4073616564273834, 0.475875586271286], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a60d3931089912ad341e5370bf14fd5b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 38, 38], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1b69e431a696c5c017256f9f7c404f64(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 2112, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([2112], dtype='float32', min=0, max=0.5),
            paddle.uniform([2112], dtype='float32', min=0, max=0.5),
            paddle.uniform([2112], dtype='float32', min=0, max=0.5),
            paddle.uniform([2112], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_efb196317faa5b3de5f9eb77096dc79b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dbc6c2c92d0e9d316a6383e46f051002(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 704, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([704], dtype='float32', min=0, max=0.5),
            paddle.uniform([704], dtype='float32', min=0, max=0.5),
            paddle.uniform([704], dtype='float32', min=0, max=0.5),
            paddle.uniform([704], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c5bf4c0c3081142aaa007d2caffb7af0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 1872, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1872], dtype='float32', min=0, max=0.5),
            paddle.uniform([1872], dtype='float32', min=0, max=0.5),
            paddle.uniform([1872], dtype='float32', min=0, max=0.5),
            paddle.uniform([1872], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5a16824f0dde32166108f8ca725b403e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 120, 120], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.385940819978714, 0.2197684943675995, 0.43205562233924866, 0.43479934334754944, 0.4952273964881897, 0.11227685213088989, 0.14602652192115784, 0.4227939546108246, 0.4971443712711334, 0.455171674489975, 0.2764974534511566, 0.2745450437068939, 0.07523717731237411, 0.19170741736888885, 0.2099989950656891, 0.3091142773628235, 0.010479512624442577, 0.15009412169456482, 0.04171832650899887, 0.20412087440490723, 0.33232825994491577, 0.0001039014314301312, 0.4046737253665924, 0.45149853825569153], dtype='float32').reshape([24]),
            paddle.to_tensor([0.17428112030029297, 0.34117260575294495, 0.09111352264881134, 0.39379066228866577, 0.14535851776599884, 0.3020824193954468, 0.37132513523101807, 0.34310752153396606, 0.38677123188972473, 0.4511317014694214, 0.00826855655759573, 0.2891339063644409, 0.08170641958713531, 0.4765937328338623, 0.4560593366622925, 0.12653130292892456, 0.32919904589653015, 0.18736380338668823, 0.1957118958234787, 0.4170483648777008, 0.3982004225254059, 0.4436439573764801, 0.08992653340101242, 0.4870105981826782], dtype='float32').reshape([24]),
            paddle.to_tensor([0.33384960889816284, 0.24773907661437988, 0.11895442008972168, 0.4857538938522339, 0.13607192039489746, 0.01861358992755413, 0.2510596811771393, 0.3492462933063507, 0.3459548056125641, 0.3658639192581177, 0.2189013957977295, 0.0766548216342926, 0.2252303957939148, 0.41779476404190063, 0.3793483078479767, 0.12572544813156128, 0.12908293306827545, 0.49643993377685547, 0.16805103421211243, 0.46444427967071533, 0.3618128001689911, 0.3745447099208832, 0.30045217275619507, 0.2615310847759247], dtype='float32').reshape([24]),
            paddle.to_tensor([0.46812328696250916, 0.41674086451530457, 0.44118237495422363, 0.1519007384777069, 0.38919487595558167, 0.17118091881275177, 0.43834027647972107, 0.48262980580329895, 0.25787267088890076, 0.18533259630203247, 0.039402034133672714, 0.48030638694763184, 0.27260613441467285, 0.26463866233825684, 0.3378414511680603, 0.08317455649375916, 0.1890694499015808, 0.08818383514881134, 0.2832752764225006, 0.006903572008013725, 0.14839446544647217, 0.4226289987564087, 0.3284074068069458, 0.15688784420490265], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_633db137d3002fc749b371ea7a7cb239(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bc49cca9da2b2e5faea680a7aaf788d7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 4, 3], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_31876c53c78274b8606e5e92525ad26c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 120, 120], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fc07cb01cd239f8d75676683df44b06d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 288, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([288], dtype='float32', min=0, max=0.5),
            paddle.uniform([288], dtype='float32', min=0, max=0.5),
            paddle.uniform([288], dtype='float32', min=0, max=0.5),
            paddle.uniform([288], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a7c8006e334dcf531512da7ef10b64b3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 1200, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fbf59433c7daa0f882b473f25ff37901(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2233419269323349, 0.3899807631969452, 0.09278964251279831, 0.045486174523830414, 0.1313668191432953, 0.21369358897209167, 0.05261613056063652, 0.33983471989631653, 0.18163157999515533, 0.38496920466423035, 0.09582896530628204, 0.47801366448402405, 0.4212133288383484, 0.02875342220067978, 0.22308604419231415, 0.16108150780200958, 0.4020889103412628, 0.24748283624649048, 0.21607881784439087, 0.08127342909574509, 0.29496973752975464, 0.43552908301353455, 0.45413193106651306, 0.09145502746105194], dtype='float32').reshape([24]),
            paddle.to_tensor([0.14387060701847076, 0.04198750853538513, 0.4105627238750458, 0.3734448254108429, 0.15150685608386993, 0.4515700936317444, 0.3998684585094452, 0.44681230187416077, 0.28689509630203247, 0.12791754305362701, 0.011679749935865402, 0.2901924252510071, 0.23490799963474274, 0.23609992861747742, 0.14299282431602478, 0.36132746934890747, 0.3786914050579071, 0.36876940727233887, 0.16891101002693176, 0.4563290476799011, 0.21474571526050568, 0.28412961959838867, 0.24959614872932434, 0.42601174116134644], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2757343351840973, 0.08286640793085098, 0.33922913670539856, 0.33375313878059387, 0.1542806625366211, 0.14317643642425537, 0.23650628328323364, 0.32981249690055847, 0.21058641374111176, 0.3845280408859253, 0.07233893871307373, 0.4743382930755615, 0.14933429658412933, 0.1629328727722168, 0.29776108264923096, 0.09752269089221954, 0.4780224561691284, 0.39363205432891846, 0.0696556493639946, 0.0328729972243309, 0.08886756747961044, 0.34908726811408997, 0.3845641314983368, 0.2299761176109314], dtype='float32').reshape([24]),
            paddle.to_tensor([0.16393226385116577, 0.1305268555879593, 0.19388635456562042, 0.4647626280784607, 0.13699936866760254, 0.17128367722034454, 0.13489162921905518, 0.12607096135616302, 0.07335714250802994, 0.42778950929641724, 0.21459278464317322, 0.07852786034345627, 0.42071762681007385, 0.08721788972616196, 0.2358451634645462, 0.23466132581233978, 0.1527710109949112, 0.10957807302474976, 0.1626620888710022, 0.010335223749279976, 0.19709216058254242, 0.37603917717933655, 0.16297034919261932, 0.23697015643119812], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dccb12b3fd812fc9c99e3018666b1f8f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.08616504073143005, 0.21239206194877625, 0.20102083683013916, 0.49481067061424255, 0.23867876827716827, 0.33785802125930786, 0.2038508951663971, 0.3968612253665924, 0.04102136194705963, 0.1906614899635315, 0.2143801599740982, 0.43945378065109253, 0.27556112408638, 0.14371420443058014, 0.49812787771224976, 0.13268563151359558, 0.08080396801233292, 0.4386495053768158, 0.47338590025901794, 0.34865882992744446, 0.1619325578212738, 0.3342285454273224, 0.4165421426296234, 0.34895381331443787, 0.058307748287916183, 0.17832733690738678, 0.21051988005638123, 0.3782675862312317], dtype='float32').reshape([28]),
            paddle.to_tensor([0.22457003593444824, 0.10668359696865082, 0.039358481764793396, 0.014331592246890068, 0.28721383213996887, 0.17788127064704895, 0.4804030954837799, 0.03946615755558014, 0.3390451669692993, 0.02407708764076233, 0.2796979248523712, 0.25274670124053955, 0.3342401683330536, 0.2454051673412323, 0.4543169140815735, 0.1221606582403183, 0.16768255829811096, 0.09952060878276825, 0.40856486558914185, 0.3341312110424042, 0.39247390627861023, 0.039767444133758545, 0.0924140214920044, 0.19796597957611084, 0.11090656369924545, 0.05109035596251488, 0.1830783188343048, 0.4031389355659485], dtype='float32').reshape([28]),
            paddle.to_tensor([0.4409940242767334, 0.14802414178848267, 0.3993210792541504, 0.34094858169555664, 0.21208937466144562, 0.10838307440280914, 0.1884031444787979, 0.34395846724510193, 0.3578169345855713, 0.07488525658845901, 0.07633419334888458, 0.23660197854042053, 0.4901951551437378, 0.019055914133787155, 0.14044947922229767, 0.06675947457551956, 0.1473292112350464, 0.4811117649078369, 0.057769775390625, 0.11254938691854477, 0.2771657109260559, 0.08788977563381195, 0.025838276371359825, 0.341246634721756, 0.28034499287605286, 0.13891896605491638, 0.35978084802627563, 0.21741797029972076], dtype='float32').reshape([28]),
            paddle.to_tensor([0.10403209924697876, 0.43186530470848083, 0.15412452816963196, 0.48676589131355286, 0.17094703018665314, 0.05579158663749695, 0.4973207414150238, 0.007938005030155182, 0.20611678063869476, 0.3633721172809601, 0.0722457617521286, 0.22405342757701874, 0.09443539381027222, 0.3060649335384369, 0.2332298904657364, 0.43274760246276855, 0.12705564498901367, 0.3242589235305786, 0.3045865595340729, 0.457951158285141, 0.20631779730319977, 0.19935348629951477, 0.23392805457115173, 0.0521482452750206, 0.3478827476501465, 0.3429156541824341, 0.05174562707543373, 0.15863341093063354], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f41a46677249c016e7d785f63058574b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 26, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.27263808250427246, 0.39072826504707336, 0.43103066086769104, 0.09646482765674591, 0.3614289164543152, 0.2506335973739624, 0.20669938623905182, 0.17518350481987, 0.44242095947265625, 0.12160883098840714, 0.13098247349262238, 0.2359718680381775, 0.280313640832901, 0.18081676959991455, 0.4574137330055237, 0.3191337287425995, 0.10739041864871979, 0.05003434419631958, 0.16004863381385803, 0.18636606633663177, 0.17708855867385864, 0.1527819186449051, 0.34157702326774597, 0.1630370020866394, 0.18956832587718964, 0.2474689483642578], dtype='float32').reshape([26]),
            paddle.to_tensor([0.42593860626220703, 0.0399530827999115, 0.37000536918640137, 0.12367767840623856, 0.15905477106571198, 0.2783099114894867, 0.025475451722741127, 0.3653164505958557, 0.16588981449604034, 0.37551626563072205, 0.4569648802280426, 0.4058127701282501, 0.14185097813606262, 0.3488267660140991, 0.04060050845146179, 0.22702154517173767, 0.1895018219947815, 0.049292340874671936, 0.42757004499435425, 0.44856974482536316, 0.04905911535024643, 0.276253342628479, 0.13457050919532776, 0.3856378197669983, 0.29453688859939575, 0.15114040672779083], dtype='float32').reshape([26]),
            paddle.to_tensor([0.284196674823761, 0.2382180392742157, 0.15500403940677643, 0.41480085253715515, 0.25814396142959595, 0.3411620259284973, 0.22015871107578278, 0.13718447089195251, 0.2125786542892456, 0.33270326256752014, 0.08526814728975296, 0.3706090748310089, 0.4923802614212036, 0.3916477859020233, 0.04719241335988045, 0.029751691967248917, 0.39800891280174255, 0.19041061401367188, 0.021761875599622726, 0.4073096811771393, 0.10842382907867432, 0.37257885932922363, 0.0033512988593429327, 0.04970552772283554, 0.31951746344566345, 0.1327485740184784], dtype='float32').reshape([26]),
            paddle.to_tensor([0.44951191544532776, 0.00679877819493413, 0.41713500022888184, 0.4776999056339264, 0.0158370528370142, 0.1983213573694229, 0.310647577047348, 0.24680958688259125, 0.268531858921051, 0.43246790766716003, 0.46012407541275024, 0.19855520129203796, 0.30444690585136414, 0.4559938609600067, 0.10108193010091782, 0.28962525725364685, 0.28857141733169556, 0.06525662541389465, 0.14784780144691467, 0.008810183964669704, 0.2050837278366089, 0.049224238842725754, 0.06763754040002823, 0.2964920699596405, 0.382913738489151, 0.4399905800819397], dtype='float32').reshape([26]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_38425915db3eb67adf6348bdb9fddc07(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8b7898463df8209e09c9834ca89125f3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.0809866264462471, 0.4075065553188324, 0.4285508692264557, 0.11312954872846603, 0.41010382771492004, 0.20229586958885193, 0.4978694021701813, 0.3402208685874939, 0.16496442258358002, 0.2707030475139618, 0.48165157437324524, 0.09806916862726212, 0.40381813049316406, 0.06652617454528809, 0.11017794907093048, 0.09662298113107681, 0.49905598163604736, 0.17708013951778412], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4802592098712921, 0.0927044227719307, 0.22409607470035553, 0.23767004907131195, 0.051596011966466904, 0.16013017296791077, 0.265546053647995, 0.2560306191444397, 0.18862558901309967, 0.12517167627811432, 0.21465274691581726, 0.36038121581077576, 0.2604924142360687, 0.015120760537683964, 0.2617741525173187, 0.14187146723270416, 0.47794678807258606, 0.15586057305335999], dtype='float32').reshape([18]),
            paddle.to_tensor([0.1410004198551178, 0.48773208260536194, 0.4026808440685272, 0.10342790186405182, 0.4824509620666504, 0.03264448419213295, 0.27749043703079224, 0.41950058937072754, 0.23158025741577148, 0.23262016475200653, 0.03420436382293701, 0.2260376513004303, 0.29284462332725525, 0.12981189787387848, 0.16815967857837677, 0.034157153218984604, 0.4140816628932953, 0.015367558225989342], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3008841276168823, 0.15764325857162476, 0.46503937244415283, 0.13664627075195312, 0.473117858171463, 0.08370909094810486, 0.3301112651824951, 0.1921476423740387, 0.46555638313293457, 0.3479683995246887, 0.11848126351833344, 0.43382391333580017, 0.31306084990501404, 0.09804695844650269, 0.15893231332302094, 0.4641159474849701, 0.006636658683419228, 0.2545160949230194], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fd49840e073a7b89e825278225d5f4f5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.13779857754707336, 0.4066685140132904, 0.36984387040138245, 0.304912269115448, 0.14925718307495117, 0.3746986985206604, 0.33506137132644653, 0.2933492958545685, 0.007188689429312944, 0.13432565331459045, 0.33688223361968994, 0.0622604638338089, 0.1275695562362671, 0.21918368339538574, 0.26373064517974854, 0.2404734194278717, 0.1706361025571823, 0.028648193925619125, 0.009536326862871647, 0.0759710967540741, 0.060299016535282135, 0.022924769669771194, 0.28480079770088196, 0.27220115065574646, 0.4885086715221405, 0.05737530067563057, 0.40843385457992554, 0.24555329978466034, 0.4411936402320862, 0.3988831043243408], dtype='float32').reshape([30]),
            paddle.to_tensor([0.1251993328332901, 0.27374282479286194, 0.46747249364852905, 0.4917803108692169, 0.22687149047851562, 0.177013099193573, 0.4203689694404602, 0.2569009065628052, 0.22684627771377563, 0.03284581005573273, 0.23965142667293549, 0.2736920416355133, 0.13768085837364197, 0.015320425853133202, 0.16593854129314423, 0.38672342896461487, 0.3209800124168396, 0.4193347990512848, 0.25951504707336426, 0.12030857801437378, 0.13935033977031708, 0.3018791675567627, 0.35044586658477783, 0.01154813077300787, 0.07773217558860779, 0.15721628069877625, 0.11552222073078156, 0.2775580883026123, 0.11500778794288635, 0.4763278365135193], dtype='float32').reshape([30]),
            paddle.to_tensor([0.1448729932308197, 0.19923852384090424, 0.11109929531812668, 0.1671263724565506, 0.2568339407444, 0.08712193369865417, 0.45163851976394653, 0.2504783272743225, 0.4864554703235626, 0.18409311771392822, 0.19583971798419952, 0.4350489675998688, 0.019235575571656227, 0.2586210072040558, 0.30368825793266296, 0.49473637342453003, 0.04320420324802399, 0.3432106673717499, 0.31735527515411377, 0.21299026906490326, 0.4055470824241638, 0.2719064950942993, 0.4358575940132141, 0.2738625705242157, 0.3598846197128296, 0.11051058024168015, 0.4708550274372101, 0.19666005671024323, 0.48797136545181274, 0.32376188039779663], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3763667345046997, 0.30909886956214905, 0.17039841413497925, 0.03545723482966423, 0.492816299200058, 0.13759072124958038, 0.43168383836746216, 0.45345792174339294, 0.4273371696472168, 0.11433912068605423, 0.44442906975746155, 0.37597453594207764, 0.3662737309932709, 0.35157090425491333, 0.21579419076442719, 0.14973284304141998, 0.24408864974975586, 0.06764215975999832, 0.006097371689975262, 0.0360199511051178, 0.2224162518978119, 0.49952417612075806, 0.2909536063671112, 0.2676407992839813, 0.33505526185035706, 0.16475243866443634, 0.39353668689727783, 0.08313540369272232, 0.09858300536870956, 0.3888959586620331], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_566e5ad7aab9c7a9659775886f48fc71(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 528, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([528], dtype='float32', min=0, max=0.5),
            paddle.uniform([528], dtype='float32', min=0, max=0.5),
            paddle.uniform([528], dtype='float32', min=0, max=0.5),
            paddle.uniform([528], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_50af66bcec555ba37164b987b3b50cef(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 8, 25], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_80ad23b8171b51feab9208d35ed8f28a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fa09aef14c0bc589e8bd66026b74667d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2580995559692383, 0.2655916213989258, 0.4984295964241028, 0.4820963442325592, 0.09519265592098236, 0.1573857069015503, 0.48489174246788025, 0.27873727679252625, 0.44413408637046814, 0.3713909387588501, 0.0732208713889122, 0.3790090084075928, 0.23271796107292175, 0.188054621219635, 0.422195702791214, 0.485923171043396, 0.34601351618766785, 0.4066877067089081], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2243979573249817, 0.2540023624897003, 0.24827222526073456, 0.4142000079154968, 0.14288362860679626, 0.16310220956802368, 0.34623169898986816, 0.005627672653645277, 0.054341789335012436, 0.44748222827911377, 0.2970227003097534, 0.11447446793317795, 0.05756741017103195, 0.26300302147865295, 0.4429970681667328, 0.24924369156360626, 0.36883556842803955, 0.4023253917694092], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4145503640174866, 0.20698508620262146, 0.3752933144569397, 0.43945667147636414, 0.08825819939374924, 0.22422391176223755, 0.26728227734565735, 0.4121426045894623, 0.03759179636836052, 0.32236501574516296, 0.33549144864082336, 0.0073458184488117695, 0.3350457549095154, 0.23753805458545685, 0.3419823944568634, 0.11691229045391083, 0.32417309284210205, 0.21794825792312622], dtype='float32').reshape([18]),
            paddle.to_tensor([0.15652941167354584, 0.1145993173122406, 0.2425708770751953, 0.4026177227497101, 0.4692733883857727, 0.22294339537620544, 0.2514851987361908, 0.27209794521331787, 0.33632543683052063, 0.127998486161232, 0.29800596833229065, 0.04726618900895119, 0.19230309128761292, 0.12677857279777527, 0.21566936373710632, 0.40091949701309204, 0.1574152261018753, 0.462033212184906], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_96d437f3a73ee5281ba42c5fc3ce882e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.35689854621887207, 0.4129399061203003, 0.13917051255702972, 0.1285857856273651, 0.3138962388038635, 0.3703988194465637, 0.24030251801013947, 0.3182324469089508, 0.40212830901145935, 0.11342892050743103, 0.0817747637629509, 0.17209647595882416, 0.06237610802054405, 0.3158491551876068, 0.26706990599632263, 0.1919136941432953], dtype='float32').reshape([16]),
            paddle.to_tensor([0.43074172735214233, 0.43064039945602417, 0.23865467309951782, 0.23739102482795715, 0.44518929719924927, 0.21083331108093262, 0.12639948725700378, 0.41139212250709534, 0.12354172021150589, 0.40911850333213806, 0.16244561970233917, 0.113762266933918, 0.015443689189851284, 0.3883121907711029, 0.3507366478443146, 0.4016294479370117], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4658375382423401, 0.2684691548347473, 0.3171631395816803, 0.13735271990299225, 0.23329277336597443, 0.3141646385192871, 0.32806095480918884, 0.3153589367866516, 0.4433744549751282, 0.4356722831726074, 0.28550732135772705, 0.2042483538389206, 0.39685705304145813, 0.46502789855003357, 0.054783985018730164, 0.005997234955430031], dtype='float32').reshape([16]),
            paddle.to_tensor([0.41334694623947144, 0.3340635597705841, 0.34730249643325806, 0.1558622270822525, 0.2388729751110077, 0.14084461331367493, 0.3493931293487549, 0.3212674558162689, 0.3348428010940552, 0.42021599411964417, 0.39047542214393616, 0.3259361684322357, 0.28181466460227966, 0.09000920504331589, 0.28872278332710266, 0.09021062403917313], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_07e13e17dfb82f76ef7bc250bc54d553(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6e216ebb9ec9b400df506ba1a685b877(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 2, 4], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4dc1d96f292ec4546f873c882de9c47b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 1, 1], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5d5e6a5eb0d7e8c80d683f2bb7c5aeb2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4518885314464569, 0.046402789652347565, 0.10312872380018234, 0.08441751450300217, 0.33456215262413025, 0.339867502450943, 0.1023484542965889, 0.07896941900253296, 0.035740334540605545, 0.290103018283844, 0.3112552762031555, 0.06649862229824066, 0.2567177414894104, 0.14448407292366028, 0.22188329696655273, 0.38152217864990234], dtype='float32').reshape([16]),
            paddle.to_tensor([0.020048169419169426, 0.0906161293387413, 0.24146181344985962, 0.2837277948856354, 0.4956282377243042, 0.4121628999710083, 0.13498711585998535, 0.3521844446659088, 0.2129736989736557, 0.15188397467136383, 0.052968382835388184, 0.2002338320016861, 0.19882476329803467, 0.47526949644088745, 0.14664265513420105, 0.19935324788093567], dtype='float32').reshape([16]),
            paddle.to_tensor([0.059002868831157684, 0.26080989837646484, 0.25108465552330017, 0.14813217520713806, 0.20901772379875183, 0.23379722237586975, 0.2917192578315735, 0.331900417804718, 0.25305256247520447, 0.07455778867006302, 0.4807872772216797, 0.04147396981716156, 0.2987484335899353, 0.11700697243213654, 0.012067693285644054, 0.2515403926372528], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4449212849140167, 0.10564915090799332, 0.06001371890306473, 0.28782474994659424, 0.3506321310997009, 0.17985278367996216, 0.1255178451538086, 0.04235289618372917, 0.08588181436061859, 0.13849575817584991, 0.4278721213340759, 0.13317859172821045, 0.3057837188243866, 0.4195081889629364, 0.022584890946745872, 0.43667125701904297], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6855c441b9468c3c38cf58200b014f94(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 512, 512], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.33110660314559937, 0.2729945778846741, 0.16502366960048676, 0.21866412460803986, 0.2003086507320404, 0.17349378764629364, 0.40480419993400574, 0.018399355933070183, 0.10533688217401505, 0.4509197771549225, 0.10327748954296112, 0.1701027750968933, 0.19656378030776978, 0.11530522257089615, 0.2933538854122162, 0.135148286819458], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4859219193458557, 0.44502073526382446, 0.006842652335762978, 0.18426638841629028, 0.2108858823776245, 0.44364020228385925, 0.32486385107040405, 0.18188346922397614, 0.08196959644556046, 0.4516720771789551, 0.15913976728916168, 0.15827666223049164, 0.3526807725429535, 0.0860091894865036, 0.006453161593526602, 0.09749267250299454], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3597443103790283, 0.1947346180677414, 0.10235504060983658, 0.3795605003833771, 0.30602264404296875, 0.04860084876418114, 0.4238246977329254, 0.3325447142124176, 0.15572911500930786, 0.47011926770210266, 0.30827122926712036, 0.4787434935569763, 0.30207350850105286, 0.3114553689956665, 0.0018687814008444548, 0.13998214900493622], dtype='float32').reshape([16]),
            paddle.to_tensor([0.14485564827919006, 0.18635240197181702, 0.10095147788524628, 0.21695968508720398, 0.31008994579315186, 0.27280527353286743, 0.008671344257891178, 0.37651899456977844, 0.15244652330875397, 0.47912752628326416, 0.18997159600257874, 0.2709721624851227, 0.2146686166524887, 0.3263709545135498, 0.06141214072704315, 0.4684334397315979], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2db8d87eacab93a0ea43adb97706b1d8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8f3237b6c613d1f300d146a45f4e195d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_8921d61813df679983676cdb4c46b3a7
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c3f48a295882b5ffeb687a94d7c7f3b8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_700733d2a36f7d97ace0dc442be87745(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_90baf495c7d922c2739f918743358b18(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3457498550415039, 0.3383738398551941, 0.4486600458621979, 0.2477879822254181, 0.4433296024799347, 0.4853805899620056, 0.2678544819355011, 0.23166175186634064, 0.34557101130485535, 0.26551946997642517, 0.018875550478696823, 0.4990295469760895, 0.41871288418769836, 0.2228257656097412, 0.31723567843437195, 0.029572155326604843, 0.28041350841522217, 0.21028003096580505], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3499705195426941, 0.4913765490055084, 0.15080681443214417, 0.22582677006721497, 0.42866283655166626, 0.47103413939476013, 0.3466514050960541, 0.09494490176439285, 0.2605070471763611, 0.08928845822811127, 0.008427433669567108, 0.03157012164592743, 0.31691423058509827, 0.24326586723327637, 0.4571186304092407, 0.0913253054022789, 0.19312816858291626, 0.2706681489944458], dtype='float32').reshape([18]),
            paddle.to_tensor([0.20090480148792267, 0.08948814123868942, 0.007440829183906317, 0.03658951446413994, 0.22289212048053741, 0.2717190980911255, 0.18217895925045013, 0.24638892710208893, 0.24843759834766388, 0.3934456408023834, 0.43211930990219116, 0.46209847927093506, 0.3206765949726105, 0.40534675121307373, 0.2646450996398926, 0.3463355004787445, 0.01321936771273613, 0.3458462655544281], dtype='float32').reshape([18]),
            paddle.to_tensor([0.41471439599990845, 0.39326146245002747, 0.3769964277744293, 0.2665340006351471, 0.2950681447982788, 0.33423975110054016, 0.0779104083776474, 0.03897619992494583, 0.0348249226808548, 0.05545414611697197, 0.43103429675102234, 0.28961461782455444, 0.42324942350387573, 0.2283099889755249, 0.17300230264663696, 0.3100125789642334, 0.4949072599411011, 0.09479106962680817], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a4b8984ea97ca2651294f643fdb66f44(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_186eae946956756b13f3654169c2ef9a
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 32, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6a2baf07d991fb347a6e837541114a17(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 184, 64, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([184], dtype='float32', min=0, max=0.5),
            paddle.uniform([184], dtype='float32', min=0, max=0.5),
            paddle.uniform([184], dtype='float32', min=0, max=0.5),
            paddle.uniform([184], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_61ddcc4fdbbd4159ea3d513439b94f07(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.21239235997200012, 0.32199370861053467, 0.18113787472248077, 0.40908050537109375, 0.2915138602256775, 0.36643412709236145, 0.021141907200217247, 0.051553625613451004, 0.4322589039802551, 0.4277651607990265, 0.17595042288303375, 0.049302197992801666, 0.3221301734447479, 0.4768602252006531, 0.1918158382177353, 0.4083111882209778, 0.13483361899852753, 0.34397953748703003, 0.25277233123779297, 0.01816040836274624, 0.02842409908771515, 0.15235237777233124, 0.24607574939727783, 0.17388905584812164], dtype='float32').reshape([24]),
            paddle.to_tensor([0.23508106172084808, 0.04412692412734032, 0.20053207874298096, 0.1326054036617279, 0.3314880430698395, 0.46255022287368774, 0.2798956632614136, 0.14078713953495026, 0.4414186477661133, 0.08529386669397354, 0.33614102005958557, 0.03186604008078575, 0.03421905264258385, 0.19063575565814972, 0.30041059851646423, 0.23245133459568024, 0.47020941972732544, 0.352214515209198, 0.15911217033863068, 0.07913827896118164, 0.2758857309818268, 0.3931213915348053, 0.023009922355413437, 0.06220148131251335], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3443496823310852, 0.2170463502407074, 0.1263856291770935, 0.20114067196846008, 0.40467023849487305, 0.2125988006591797, 0.23879405856132507, 0.23152203857898712, 0.07728972285985947, 0.1590125858783722, 0.0980241671204567, 0.26533961296081543, 0.4198727011680603, 0.4735155999660492, 0.2439226508140564, 0.08800984919071198, 0.34547650814056396, 0.43229031562805176, 0.04054354503750801, 0.47120511531829834, 0.29174020886421204, 0.34529656171798706, 0.26175302267074585, 0.4732053577899933], dtype='float32').reshape([24]),
            paddle.to_tensor([0.060846902430057526, 0.1677527129650116, 0.4141330420970917, 0.41343075037002563, 0.29635775089263916, 0.3854675889015198, 0.08708500862121582, 0.3577159643173218, 0.1367647796869278, 0.48845309019088745, 0.4918692111968994, 0.29605844616889954, 0.03830284625291824, 0.24624355137348175, 0.42270582914352417, 0.07830236852169037, 0.4470531642436981, 0.21702826023101807, 0.2232929915189743, 0.3520854413509369, 0.04820407181978226, 0.13796299695968628, 0.20787522196769714, 0.47259196639060974], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a30c2999d32367b78851e1b00533b536(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_abc15bd94e55776fdbcb840279865c01
    def get_inputs(self):
        return [
            paddle.uniform([49, 384], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e549edef480d6a27c3b5130986282c2c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.08547168225049973, 0.2572774291038513, 0.31009867787361145, 0.11188175529241562, 0.1967318058013916, 0.1728755682706833, 0.11298161745071411, 0.023370781913399696, 0.1790613830089569, 0.47644343972206116, 0.35790854692459106, 0.4260265529155731, 0.23932082951068878, 0.3819134533405304, 0.451264351606369, 0.28069743514060974, 0.18146447837352753, 0.09074681252241135, 0.4791644811630249, 0.2710596024990082, 0.32004427909851074, 0.11136254668235779, 0.4578844904899597, 0.48402246832847595, 0.35078898072242737, 0.34979134798049927, 0.3430575132369995, 0.29229190945625305], dtype='float32').reshape([28]),
            paddle.to_tensor([0.4916801154613495, 0.28866448998451233, 0.3784189224243164, 0.36027106642723083, 0.1737278550863266, 0.2762998640537262, 0.06109994277358055, 0.46514734625816345, 0.0168972946703434, 0.3456363379955292, 0.45887333154678345, 0.21224619448184967, 0.21586358547210693, 0.14571377635002136, 0.30883389711380005, 0.23139631748199463, 0.24457938969135284, 0.14926724135875702, 0.11265049129724503, 0.1695711761713028, 0.20765358209609985, 0.048584841191768646, 0.3275417387485504, 0.20341847836971283, 0.49229973554611206, 0.17370590567588806, 0.08675505965948105, 0.32386162877082825], dtype='float32').reshape([28]),
            paddle.to_tensor([0.11363277584314346, 0.2960645258426666, 0.4626273512840271, 0.0010548543650656939, 0.1732083559036255, 0.2034682035446167, 0.3777930736541748, 0.27759966254234314, 0.06118194758892059, 0.28208646178245544, 0.12680363655090332, 0.23829105496406555, 0.26964911818504333, 0.44475993514060974, 0.28119903802871704, 0.15811662375926971, 0.31156983971595764, 0.41728949546813965, 0.05920589715242386, 0.1302289068698883, 0.23314523696899414, 0.041755758225917816, 0.493198961019516, 0.49007001519203186, 0.3739961087703705, 0.49021923542022705, 0.4203105568885803, 0.1494341790676117], dtype='float32').reshape([28]),
            paddle.to_tensor([0.12322742491960526, 0.49176937341690063, 0.21143527328968048, 0.19391612708568573, 0.24277374148368835, 0.20109675824642181, 0.09411759674549103, 0.11784742772579193, 0.46431127190589905, 0.46844974160194397, 0.36598309874534607, 0.43102335929870605, 0.2890615165233612, 0.4210042655467987, 0.07559576630592346, 0.13112938404083252, 0.3740246891975403, 0.4099911153316498, 0.3304744064807892, 0.07513506710529327, 0.1968449503183365, 0.11691633611917496, 0.08764656633138657, 0.42771780490875244, 0.2549740970134735, 0.09445248544216156, 0.09000585228204727, 0.45227381587028503], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b2256e18a43f52c4c642e9647519ea21(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2f8bec7d8d930327432960bc625dad3e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.11431840062141418, 0.2677403390407562, 0.311677485704422, 0.24387219548225403, 0.3953794240951538, 0.35743358731269836, 0.23328155279159546, 0.11097776144742966, 0.3385692536830902, 0.43095093965530396, 0.3809731602668762, 0.36628392338752747, 0.15996399521827698, 0.3951093554496765, 0.10563203692436218, 0.2969590723514557, 0.31898051500320435, 0.005860962439328432], dtype='float32').reshape([18]),
            paddle.to_tensor([0.1372775435447693, 0.2804415225982666, 0.44393390417099, 0.2780267894268036, 0.364898145198822, 0.47771909832954407, 0.3259133994579315, 0.0005179363652132452, 0.1891656368970871, 0.17598550021648407, 0.4916669726371765, 0.3348884582519531, 0.04954376816749573, 0.28109633922576904, 0.4014080762863159, 0.16691137850284576, 0.106144018471241, 0.4440769851207733], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3632926344871521, 0.34051513671875, 0.0132602509111166, 0.3913445472717285, 0.47321319580078125, 0.33325105905532837, 0.13151322305202484, 0.2842383086681366, 0.11460299044847488, 0.3021980822086334, 0.4273012578487396, 0.2572910487651825, 0.04316994547843933, 0.2223699986934662, 0.24317245185375214, 0.4865466356277466, 0.019099146127700806, 0.418636292219162], dtype='float32').reshape([18]),
            paddle.to_tensor([0.47768187522888184, 0.10780085623264313, 0.3376212418079376, 0.39286336302757263, 0.281760573387146, 0.018810201436281204, 0.33030611276626587, 0.43289411067962646, 0.19169947504997253, 0.12136277556419373, 0.44387829303741455, 0.1693914532661438, 0.060570571571588516, 0.49416083097457886, 0.3861052095890045, 0.4355930984020233, 0.21680112183094025, 0.07684795558452606], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cf266945bd087e519fef5767efb8d899(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 400, 672], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_865974604f5c93fb53c44df82714df0f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.to_tensor([[[[262165.78125]], [[301366.1875]], [[285494.03125]], [[278164.875]], [[300847.28125]], [[299446.4375]], [[287754.71875]], [[290413.84375]], [[316849.53125]], [[269144.71875]], [[309874.75]], [[297350.09375]], [[274924.1875]], [[295684.5625]], [[303427.09375]], [[304227.28125]], [[324660.03125]], [[274505.09375]], [[326088.875]], [[296051.21875]], [[280004.8125]], [[338960.5625]], [[260596.453125]], [[259864.1875]], [[293210.40625]], [[296804.21875]], [[302362.34375]], [[307874.75]], [[287427.9375]], [[285420.71875]]]], dtype='float32').reshape([1, 30, 1, 1]),
            paddle.to_tensor([0.2196357548236847, 0.1722622662782669, 0.24714075028896332, 0.09603461623191833, 0.01741262525320053, 0.3563447594642639, 0.16162163019180298, 0.41378363966941833, 0.44816267490386963, 0.0606614351272583, 0.4411442279815674, 0.18065233528614044, 0.4013000726699829, 0.44372352957725525, 0.3820512890815735, 0.2125338613986969, 0.4398040473461151, 0.2665131092071533, 0.33021095395088196, 0.4286480247974396, 0.2667756974697113, 0.3434654176235199, 0.008802824653685093, 0.48242756724357605, 0.3468194603919983, 0.23067080974578857, 0.3775734603404999, 0.11801960319280624, 0.49276942014694214, 0.22862519323825836], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3228236436843872, 0.2986788749694824, 0.49873003363609314, 0.09356193989515305, 0.2078707367181778, 0.435421347618103, 0.01669752411544323, 0.43778523802757263, 0.0881272405385971, 0.07781863212585449, 0.13069984316825867, 0.07653684169054031, 0.2766371965408325, 0.2417926788330078, 0.4038500189781189, 0.009350462816655636, 0.18684561550617218, 0.18008174002170563, 0.07995486259460449, 0.3558190166950226, 0.01873510517179966, 0.3065436780452728, 0.029509158805012703, 0.13828623294830322, 0.15033243596553802, 0.4362836182117462, 0.2715112566947937, 0.3646222651004791, 0.42761069536209106, 0.15326553583145142], dtype='float32').reshape([30]),
            paddle.to_tensor([0.44060763716697693, 0.27992740273475647, 0.09863757342100143, 0.028782255947589874, 0.0066923475824296474, 0.17937469482421875, 0.0775059312582016, 0.1960301548242569, 0.35006067156791687, 0.02073785476386547, 0.16162700951099396, 0.4293176233768463, 0.2257109135389328, 0.06920590996742249, 0.16085948050022125, 0.4845716953277588, 0.2554422914981842, 0.186758890748024, 0.29714322090148926, 0.38952818512916565, 0.19873586297035217, 0.3275165557861328, 0.47655537724494934, 0.4752011001110077, 0.21810118854045868, 0.1338544338941574, 0.3936258554458618, 0.2793467044830322, 0.4708740711212158, 0.14979711174964905], dtype='float32').reshape([30]),
            paddle.to_tensor([0.35611245036125183, 0.08415929228067398, 0.46879711747169495, 0.43526026606559753, 0.4298991560935974, 0.4226277768611908, 0.19333086907863617, 0.21491141617298126, 0.07512743771076202, 0.42611148953437805, 0.020679965615272522, 0.14449840784072876, 0.3019634485244751, 0.24772761762142181, 0.017967594787478447, 0.4534437954425812, 0.027096958830952644, 0.011274001561105251, 0.43598756194114685, 0.13894985616207123, 0.24458497762680054, 0.38407009840011597, 0.4624986946582794, 0.043437980115413666, 0.0665753185749054, 0.2140047252178192, 0.167342871427536, 0.09366267174482346, 0.4348909258842468, 0.3927862346172333], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a0a8fcae377df16b52249fe80b8876f0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 38, 38], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2e50b227afb49f8b09173112e7f86234(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 960, 960], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3369600772857666, 0.022386549040675163, 0.1722637414932251, 0.24559153616428375, 0.28567203879356384, 0.030628589913249016, 0.10120659321546555, 0.45974424481391907, 0.07605833560228348, 0.1772577166557312, 0.2124110758304596, 0.1069398745894432], dtype='float32').reshape([12]),
            paddle.to_tensor([0.3547361195087433, 0.24572299420833588, 0.12544122338294983, 0.3543896973133087, 0.3981633186340332, 0.20454400777816772, 0.4419897794723511, 0.4928363561630249, 0.4205242395401001, 0.43499648571014404, 0.007228484842926264, 0.12384248524904251], dtype='float32').reshape([12]),
            paddle.to_tensor([0.1026315987110138, 0.06837194412946701, 0.2896094024181366, 0.08466225117444992, 0.2836708426475525, 0.49543628096580505, 0.23579047620296478, 0.4529530107975006, 0.19531165063381195, 0.11936372518539429, 0.003620004281401634, 0.4982287883758545], dtype='float32').reshape([12]),
            paddle.to_tensor([0.44307249784469604, 0.30958208441734314, 0.4099946618080139, 0.39023691415786743, 0.2653634548187256, 0.04209159314632416, 0.21062533557415009, 0.28720301389694214, 0.4164588451385498, 0.22167333960533142, 0.18075574934482574, 0.34907910227775574], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1a6ffa3a587395e0cee681346e61d15c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.44611620903015137, 0.22871921956539154, 0.08558564633131027, 0.10002721846103668, 0.17273519933223724, 0.43435534834861755, 0.20656578242778778, 0.29448840022087097, 0.3151504099369049, 0.21393297612667084, 0.0007771716336719692, 0.02730218693614006, 0.4572363495826721, 0.0012959996238350868, 0.22205400466918945, 0.3732522130012512, 0.3982493281364441, 0.34641408920288086, 0.24517510831356049, 0.30676257610321045, 0.4008972644805908, 0.03102351352572441, 0.0006503858021460474, 0.45044246315956116], dtype='float32').reshape([24]),
            paddle.to_tensor([0.008852292783558369, 0.45709094405174255, 0.44965511560440063, 0.2523304522037506, 0.47486498951911926, 0.2464587390422821, 0.3754480183124542, 0.16064217686653137, 0.29080620408058167, 0.015843844041228294, 0.08806508034467697, 0.3348114788532257, 0.48337438702583313, 0.22385814785957336, 0.43385425209999084, 0.48341506719589233, 0.3028405010700226, 0.2781982123851776, 0.2343873232603073, 0.46874672174453735, 0.22468243539333344, 0.023178037256002426, 0.472113698720932, 0.4847525954246521], dtype='float32').reshape([24]),
            paddle.to_tensor([0.19768677651882172, 0.3954637944698334, 0.2193802148103714, 0.4090190529823303, 0.08149288594722748, 0.3742677569389343, 0.2970820665359497, 0.1591944396495819, 0.2322264462709427, 0.4734557569026947, 0.03388796001672745, 0.4663986563682556, 0.04781777411699295, 0.3859807848930359, 0.21666386723518372, 0.41335242986679077, 0.06624466925859451, 0.3952715992927551, 0.25223734974861145, 0.20850801467895508, 0.17078818380832672, 0.44649678468704224, 0.40793341398239136, 0.3212492763996124], dtype='float32').reshape([24]),
            paddle.to_tensor([0.10708387196063995, 0.06762818247079849, 0.1655573844909668, 0.31958702206611633, 0.18903955817222595, 0.36588552594184875, 0.33759021759033203, 0.4403548538684845, 0.1887100487947464, 0.09555327147245407, 0.4027918577194214, 0.10407928377389908, 0.2094620019197464, 0.21806874871253967, 0.4845503866672516, 0.20972304046154022, 0.010409419424831867, 0.42757800221443176, 0.21801073849201202, 0.2168227583169937, 0.2010103464126587, 0.10854961723089218, 0.47708338499069214, 0.12362205237150192], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0495459c7a73d3b2db64df0a44d1035f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 1152, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_203d84dde06bfa27fc5f2725e4df1097(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.35647061467170715, 0.4171123802661896, 0.20987385511398315, 0.026731334626674652, 0.11276453733444214, 0.03393840417265892, 0.2459544986486435, 0.40146979689598083, 0.14864614605903625, 0.14285728335380554, 0.3086007237434387, 0.38611161708831787, 0.32903820276260376, 0.4630093574523926, 0.10463900119066238, 0.21921256184577942], dtype='float32').reshape([16]),
            paddle.to_tensor([0.05044718086719513, 0.36723577976226807, 0.15440095961093903, 0.3718316853046417, 0.27331671118736267, 0.19164885580539703, 0.19044044613838196, 0.32545772194862366, 0.40829068422317505, 0.36212337017059326, 0.3428773283958435, 0.46826139092445374, 0.33084261417388916, 0.2395133525133133, 0.26131758093833923, 0.20807719230651855], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4250239431858063, 0.42824941873550415, 0.49966129660606384, 0.007395823020488024, 0.1735871136188507, 0.3011173903942108, 0.1442929208278656, 0.45178160071372986, 0.2440318763256073, 0.41480836272239685, 0.16734974086284637, 0.34519925713539124, 0.1650436520576477, 0.1389121264219284, 0.031194167211651802, 0.25850948691368103], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2785152196884155, 0.3116534650325775, 0.15235409140586853, 0.270100474357605, 0.1875932812690735, 0.16516070067882538, 0.46729928255081177, 0.3618814945220947, 0.12976045906543732, 0.21781420707702637, 0.41459229588508606, 0.33097362518310547, 0.06605072319507599, 0.20177039504051208, 0.3220009505748749, 0.13323967158794403], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_e147f42ce1e944644aab09a0a921682e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-06'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_39d8bf3204c787faab0abc0cbb77edaa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e147f42ce1e944644aab09a0a921682e
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8b113a9231f208323bad8ee74f041612(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_120e61fe45694e82d59a3574fe6224af(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 512], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.24721233546733856, 0.23344914615154266, 0.11370117217302322, 0.10370507836341858, 0.2717936336994171, 0.0877007395029068, 0.38997626304626465, 0.3978397250175476, 0.30868127942085266, 0.056705597788095474, 0.20377308130264282, 0.4124017357826233, 0.44861748814582825, 0.3325532376766205, 0.016801919788122177, 0.38408270478248596], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3719504773616791, 0.18767032027244568, 0.2006048560142517, 8.580082794651389e-05, 0.26384246349334717, 0.197787344455719, 0.4749787449836731, 0.4248684346675873, 0.26264292001724243, 0.3983059823513031, 0.292255163192749, 0.02211974561214447, 0.4426426291465759, 0.0809875875711441, 0.4158145785331726, 0.03473062068223953], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3008679449558258, 0.22279143333435059, 0.019422976300120354, 0.44781017303466797, 0.12591999769210815, 0.2524556517601013, 0.21659308671951294, 0.037187229841947556, 0.3206874430179596, 0.48484477400779724, 0.45191043615341187, 0.12228886038064957, 0.30949512124061584, 0.21964222192764282, 0.3137161135673523, 0.05748729035258293], dtype='float32').reshape([16]),
            paddle.to_tensor([0.24759133160114288, 0.09151165932416916, 0.43388083577156067, 0.17392712831497192, 0.1907927542924881, 0.2748590409755707, 0.0016443359199911356, 0.45290425419807434, 0.20774145424365997, 0.3758194148540497, 0.12017546594142914, 0.13524691760540009, 0.47332486510276794, 0.3622967302799225, 0.3250270187854767, 0.1545129120349884], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6fe37dd32f9f9ddf4c65c717ad40ba87(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 480, 480], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f9a04576b17b03a05c867f8640fc18df(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3727201223373413, 0.4080023765563965, 0.4324847161769867, 0.10217861831188202, 0.14358511567115784, 0.19116675853729248, 0.47201529145240784, 0.4683881103992462, 0.09482798725366592, 0.11392958462238312, 0.48889562487602234, 0.49289438128471375, 0.47220730781555176, 0.0013651254121214151, 0.23283138871192932, 0.17076538503170013, 0.17823337018489838, 0.01075147558003664, 0.4459238052368164, 0.10673974454402924, 0.32454001903533936, 0.39811739325523376, 0.3905741274356842, 0.4018649160861969], dtype='float32').reshape([24]),
            paddle.to_tensor([0.1083713173866272, 0.36405664682388306, 0.21501781046390533, 0.3881540596485138, 0.4828353822231293, 0.18888218700885773, 0.37479278445243835, 0.02127368189394474, 0.029590509831905365, 0.4907401204109192, 0.32361918687820435, 0.22259895503520966, 0.3809778690338135, 0.3927680552005768, 0.024818547070026398, 0.3888349235057831, 0.0806569904088974, 0.3275274932384491, 0.4999140501022339, 0.44001081585884094, 0.0632767453789711, 0.10173658281564713, 0.1852051019668579, 0.3479541540145874], dtype='float32').reshape([24]),
            paddle.to_tensor([0.38621339201927185, 0.1544773131608963, 0.31616681814193726, 0.11631488054990768, 0.4844079911708832, 0.3028660714626312, 0.3588520586490631, 0.12114264816045761, 0.4412809908390045, 0.20974785089492798, 0.27592653036117554, 0.45746129751205444, 0.49668821692466736, 0.40690597891807556, 0.3441017270088196, 0.05161005258560181, 0.1583026498556137, 0.11475123465061188, 0.1430945098400116, 0.2874578833580017, 0.4169672131538391, 0.4189159572124481, 0.13948172330856323, 0.1907629370689392], dtype='float32').reshape([24]),
            paddle.to_tensor([0.15784533321857452, 0.25743529200553894, 0.37362802028656006, 0.35133785009384155, 0.18060192465782166, 0.15687842667102814, 0.2990628182888031, 0.1294436901807785, 0.19585742056369781, 0.04248394817113876, 0.36810755729675293, 0.34057310223579407, 0.3510139584541321, 0.17852407693862915, 0.1852637529373169, 0.2369564324617386, 0.24401366710662842, 0.0018427262548357248, 0.21125011146068573, 0.07810050994157791, 0.3945360779762268, 0.19568440318107605, 0.2789379060268402, 0.39945489168167114], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ec06ffd4d5cce5937cd47072903dfcb7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9d88846193019228f0c182068d476d06(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 702, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([702], dtype='float32', min=0, max=0.5),
            paddle.uniform([702], dtype='float32', min=0, max=0.5),
            paddle.uniform([702], dtype='float32', min=0, max=0.5),
            paddle.uniform([702], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b818da28565b833ab4573b30f716a088(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_295ae9b440b949c8adc491d9f3bc7170(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 12, 12], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4a2665a9b690b712ca0405add54e521b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.18452298641204834, 0.3289959728717804, 0.35778993368148804, 0.2646067440509796, 0.49352461099624634, 0.4252323508262634, 0.1811082661151886, 0.4060165584087372, 0.03150397911667824, 0.30484870076179504, 0.3306395411491394, 0.11368877440690994, 0.48925575613975525, 0.34483665227890015, 0.3929521143436432, 0.0664462223649025, 0.31279927492141724, 0.4022410809993744, 0.2840729355812073, 0.012669701129198074], dtype='float32').reshape([20]),
            paddle.to_tensor([0.49476397037506104, 0.29429060220718384, 0.4068068265914917, 0.3933607339859009, 0.04261093959212303, 0.350987046957016, 0.12168417870998383, 0.41109225153923035, 0.2919432818889618, 0.22794470191001892, 0.13214832544326782, 0.18465939164161682, 0.28525838255882263, 0.27627646923065186, 0.16046568751335144, 0.2636735141277313, 0.19966450333595276, 0.367479532957077, 0.45266449451446533, 0.33151134848594666], dtype='float32').reshape([20]),
            paddle.to_tensor([0.45611488819122314, 0.42491891980171204, 0.2671753168106079, 0.16789157688617706, 0.0075223702006042, 0.26290324330329895, 0.15571430325508118, 0.3860165476799011, 0.461481511592865, 0.47971343994140625, 0.24963724613189697, 0.27309978008270264, 0.3882467448711395, 0.060467422008514404, 0.3185918629169464, 0.010946125723421574, 0.15597139298915863, 0.4403665065765381, 0.05069345235824585, 0.024542152881622314], dtype='float32').reshape([20]),
            paddle.to_tensor([0.04474012926220894, 0.38547661900520325, 0.12269386649131775, 0.18229779601097107, 0.2012435495853424, 0.16795778274536133, 0.021531565114855766, 0.3598320484161377, 0.2497590333223343, 0.3014458417892456, 0.16652844846248627, 0.1755003035068512, 0.43717142939567566, 0.05800318345427513, 0.25664931535720825, 0.3734111189842224, 0.3181037902832031, 0.30627623200416565, 0.11923397332429886, 0.3441973626613617], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_146ef18b093e5a2c17e31a52cdf40a57(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e31be5949c011e1796f2a3643d6463c4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_699c8fdd5c2a35d6a2d66a5010db5454(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 144, 31, 31], dtype='float16', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5da8d85fe24cee7db55647323b582ddc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 2112, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([2112], dtype='float32', min=0, max=0.5),
            paddle.uniform([2112], dtype='float32', min=0, max=0.5),
            paddle.uniform([2112], dtype='float32', min=0, max=0.5),
            paddle.uniform([2112], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e9bb8fd77dbe02ba5da13dae57c2920e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 366, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([366], dtype='float32', min=0, max=0.5),
            paddle.uniform([366], dtype='float32', min=0, max=0.5),
            paddle.uniform([366], dtype='float32', min=0, max=0.5),
            paddle.uniform([366], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f35eeaeb5c94d349aec781697c2aea04(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 320, 320], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d7d2e861aafb1ce742ed079f6d3da647(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_321756ad5ffa94e9fadcc820acc52ca5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.23240569233894348, 0.4039681851863861, 0.36036112904548645, 0.2934834957122803, 0.49834132194519043, 0.1986037641763687, 0.2707991898059845, 0.283255934715271, 0.006549939513206482, 0.3679533898830414, 0.10480914264917374, 0.08805350959300995, 0.025001196190714836, 0.39311763644218445, 0.3444688320159912, 0.3853999376296997], dtype='float32').reshape([16]),
            paddle.to_tensor([0.24457576870918274, 0.023745983839035034, 0.36290261149406433, 0.07860155403614044, 0.11974501609802246, 0.14607101678848267, 0.07227755337953568, 0.28874194622039795, 0.030603256076574326, 0.23478980362415314, 0.44613561034202576, 0.2915273606777191, 0.09751886129379272, 0.05286078900098801, 0.4829390048980713, 0.05728340148925781], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2371767908334732, 0.1791459321975708, 0.19074393808841705, 0.08633571863174438, 0.11206964403390884, 0.2509329319000244, 0.1474442332983017, 0.3647375702857971, 0.04847536236047745, 0.039398886263370514, 0.06201688200235367, 0.44339534640312195, 0.03962333872914314, 0.02262279763817787, 0.46073272824287415, 0.2007117122411728], dtype='float32').reshape([16]),
            paddle.to_tensor([0.311354398727417, 0.18905985355377197, 0.013142936863005161, 0.1744125485420227, 0.2339058816432953, 0.2783186733722687, 0.2985161244869232, 0.4620778560638428, 0.3448452353477478, 0.12051492184400558, 0.46354416012763977, 0.2070186585187912, 0.19917364418506622, 0.01997048780322075, 0.0913451761007309, 0.2493324726819992], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_047c89688bfd6e6c0b38c80d3f379ad6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4951370060443878, 0.14544197916984558, 0.4240577816963196, 0.31011471152305603, 0.3696189820766449, 0.33110755681991577, 0.2819228768348694, 0.37061843276023865, 0.4507155120372772, 0.44778549671173096, 0.2699485123157501, 0.45442530512809753, 0.33266758918762207, 0.18268659710884094, 0.25467216968536377, 0.17335866391658783], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4244139492511749, 0.3904414176940918, 0.3402145206928253, 0.2867731750011444, 0.17043550312519073, 0.25096380710601807, 0.04807181656360626, 0.21502327919006348, 0.4486314058303833, 0.16923299431800842, 0.4133416712284088, 0.2909104824066162, 0.04128635674715042, 0.18623541295528412, 0.06433293223381042, 0.30354124307632446], dtype='float32').reshape([16]),
            paddle.to_tensor([0.34384340047836304, 0.18158316612243652, 0.4015551805496216, 0.42459601163864136, 0.30241018533706665, 0.10821923613548279, 0.3756267726421356, 0.18529127538204193, 0.3000108003616333, 0.4639424979686737, 0.21200120449066162, 0.37987956404685974, 0.2971721589565277, 0.3571662902832031, 0.38166186213493347, 0.3823928236961365], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4643201529979706, 0.017985627055168152, 0.20646005868911743, 0.15173830091953278, 0.20213820040225983, 0.06918279826641083, 0.4330299198627472, 0.4423370063304901, 0.22156693041324615, 0.3258947730064392, 0.32904350757598877, 0.39176368713378906, 0.06716515868902206, 0.18906915187835693, 0.3992937505245209, 0.12191025167703629], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f18da74d98ce63375a9e6add3be0b03e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.05717325210571289, 0.13319139182567596, 0.06993014365434647, 0.31463876366615295, 0.21255122125148773, 0.2602342367172241, 0.31375399231910706, 0.4292970299720764, 0.22060145437717438, 0.06005256995558739, 0.43936479091644287, 0.058588720858097076, 0.4116406738758087, 0.11906934529542923, 0.17178115248680115, 0.17660731077194214], dtype='float32').reshape([16]),
            paddle.to_tensor([0.33453020453453064, 0.33348068594932556, 0.4100126028060913, 0.11018501222133636, 0.15867745876312256, 0.4242570996284485, 0.45202013850212097, 0.0008992005023173988, 0.10164973884820938, 0.10894617438316345, 0.49089184403419495, 0.4801679253578186, 0.11957819759845734, 0.08424359560012817, 0.16052912175655365, 0.10303957015275955], dtype='float32').reshape([16]),
            paddle.to_tensor([0.05586537718772888, 0.2438967376947403, 0.05707644298672676, 0.46151408553123474, 0.45843204855918884, 0.07628282904624939, 0.34958747029304504, 0.21772952377796173, 0.4015364944934845, 0.2566602826118469, 0.15741963684558868, 0.49472054839134216, 0.1472548097372055, 0.13444726169109344, 0.10195912420749664, 0.11462219804525375], dtype='float32').reshape([16]),
            paddle.to_tensor([0.37894973158836365, 0.11210813373327255, 0.2113020271062851, 0.3092266619205475, 0.3630170226097107, 0.27303194999694824, 0.29973167181015015, 0.06542801856994629, 0.3966043293476105, 0.027126025408506393, 0.4120808243751526, 0.005613011308014393, 0.19510707259178162, 0.19533509016036987, 0.1290498822927475, 0.25719931721687317], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cff2212ea134166a45466ccf5a0d9a8f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.11572396010160446, 0.01783200167119503, 0.36842989921569824, 0.44908425211906433, 0.4197228252887726, 0.2825964093208313, 0.1385059803724289, 0.3778762221336365, 0.14665034413337708, 0.38044750690460205, 0.338095486164093, 0.4871945381164551, 0.19614747166633606, 0.24379314482212067, 0.37259188294410706, 0.46448424458503723, 0.07665403187274933, 0.4868628978729248, 0.1943899691104889, 0.2492128163576126, 0.08230777084827423, 0.22641627490520477, 0.482931524515152, 0.3557959794998169], dtype='float32').reshape([24]),
            paddle.to_tensor([0.09063555300235748, 0.07048924267292023, 0.2018461525440216, 0.4259723424911499, 0.14854782819747925, 0.25919365882873535, 0.3387049734592438, 0.23531386256217957, 0.029412658885121346, 0.05238361284136772, 0.2786380648612976, 0.48135361075401306, 0.3343011438846588, 0.162289559841156, 0.35390031337738037, 0.15379418432712555, 0.4379773736000061, 0.02852126769721508, 0.21275849640369415, 0.16576866805553436, 0.4087180495262146, 0.3632993996143341, 0.4264473617076874, 0.31276798248291016], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3604597747325897, 0.23168127238750458, 0.37034836411476135, 0.006783931516110897, 0.3945225179195404, 0.16344496607780457, 0.13582216203212738, 0.2670499384403229, 0.19783246517181396, 0.1713150143623352, 0.49954190850257874, 0.0490315742790699, 0.23249875009059906, 0.014494315721094608, 0.04169255122542381, 0.18871620297431946, 0.2984579801559448, 0.41637012362480164, 0.2712044417858124, 0.2210828959941864, 0.17313511669635773, 0.05238509923219681, 0.29754403233528137, 0.35910576581954956], dtype='float32').reshape([24]),
            paddle.to_tensor([0.09134787321090698, 0.2113497406244278, 0.29697972536087036, 0.37343963980674744, 0.13217297196388245, 0.1871018409729004, 0.29156044125556946, 0.2627846598625183, 0.3352363705635071, 0.141323059797287, 0.40082862973213196, 0.0321907103061676, 0.18606922030448914, 0.3511466085910797, 0.34373345971107483, 0.21781404316425323, 0.2840118706226349, 0.10968642681837082, 0.4967445731163025, 0.16702648997306824, 0.2131885290145874, 0.29036271572113037, 0.23735161125659943, 0.16463235020637512], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8bb53f834cbcd470945fa819cfca1b77(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 480, 480], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3265897035598755, 0.4069804251194, 0.35577091574668884, 0.3533031642436981, 0.13295023143291473, 0.47354868054389954, 0.47603729367256165, 0.17615735530853271], dtype='float32').reshape([8]),
            paddle.to_tensor([0.0801922082901001, 0.4585306644439697, 0.4206865727901459, 0.3414522409439087, 0.11717330664396286, 0.19539175927639008, 0.15689799189567566, 0.08564522117376328], dtype='float32').reshape([8]),
            paddle.to_tensor([0.11025417596101761, 0.3973366618156433, 0.2377457618713379, 0.0904635339975357, 0.09050504863262177, 0.1253300905227661, 0.4532129168510437, 0.25862905383110046], dtype='float32').reshape([8]),
            paddle.to_tensor([0.38011395931243896, 0.49590784311294556, 0.37156111001968384, 0.30525076389312744, 0.4092392921447754, 0.20931988954544067, 0.49749743938446045, 0.49499738216400146], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1744314d6b1310e3db2f222b7c932bdc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 1152, 6, 6], dtype='float16', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fcec5dfa6051495ef1bfb9ee80dfd953(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_52ca03339ac7323e3d233ecf17f9be13(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 320, 320], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0db9f8c47508345bf1645315fad27a0c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2126719206571579, 0.19583459198474884, 0.06476398557424545, 0.26503893733024597, 0.30064794421195984, 0.08869337290525436, 0.3473629653453827, 0.11321216076612473, 0.12804901599884033, 0.12204106152057648, 0.3387289047241211, 0.4011867940425873, 0.3952479958534241, 0.13828884065151215, 0.45311063528060913, 0.46903204917907715, 0.11600922048091888, 0.29251858592033386, 0.2794850170612335, 0.253030002117157], dtype='float32').reshape([20]),
            paddle.to_tensor([0.050782158970832825, 0.3178183138370514, 0.30782318115234375, 0.07977919280529022, 0.020947515964508057, 0.10707193613052368, 0.08772831410169601, 0.49283552169799805, 0.17207074165344238, 0.43104997277259827, 0.30205038189888, 0.35036566853523254, 0.14940354228019714, 0.3852488100528717, 0.3106640875339508, 0.07293470203876495, 0.14738239347934723, 0.4130994379520416, 0.32879936695098877, 0.068628691136837], dtype='float32').reshape([20]),
            paddle.to_tensor([0.386148601770401, 0.07010137289762497, 0.4426521062850952, 0.08781219273805618, 0.4600958228111267, 0.26894769072532654, 0.4297001361846924, 0.17192968726158142, 0.2528396546840668, 0.3108048737049103, 0.323967844247818, 0.3509473502635956, 0.0588485449552536, 0.010042881593108177, 0.016719309613108635, 0.34731215238571167, 0.0016203061677515507, 0.08804350346326828, 0.41627228260040283, 0.21040397882461548], dtype='float32').reshape([20]),
            paddle.to_tensor([0.06499350070953369, 0.13076151907444, 0.3263663947582245, 0.07332487404346466, 0.14320334792137146, 0.2322169989347458, 0.37648651003837585, 0.314850777387619, 0.40727439522743225, 0.4623100757598877, 0.46425318717956543, 0.3561066687107086, 0.025111403316259384, 0.33954089879989624, 0.022877812385559082, 0.3860912621021271, 0.46678677201271057, 0.3066346347332001, 0.08988188952207565, 0.12308624386787415], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d2b87358a52d86c45d28d9d7df70953f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7848e1f052e1db40c7ffa39f10bc8cb3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 512, 1024], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7cf513ac35d47f6be9755ea40454fb3c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 228, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([228], dtype='float32', min=0, max=0.5),
            paddle.uniform([228], dtype='float32', min=0, max=0.5),
            paddle.uniform([228], dtype='float32', min=0, max=0.5),
            paddle.uniform([228], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_85cb105f3b1cbe33698d0c0c83af23f0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a38edb63fd30d70981bf24e551da21f6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_a2fd76ef6d10c288900c0fe38e7a817a
    def get_inputs(self):
        return [
            paddle.uniform([49, 512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2b430e7b879c2e3e62d5ffa0ab73963a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 5, 5], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_95f07fd5858718f0363f7065d6900e38(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3023957312107086, 0.4777205288410187, 0.17385943233966827, 0.14922262728214264, 0.428158700466156, 0.2608737647533417, 0.06711156666278839, 0.03072606585919857, 0.43008631467819214, 0.2576166093349457, 0.18706870079040527, 0.25370845198631287, 0.18362848460674286, 0.42008012533187866, 0.07057424634695053, 0.13323596119880676], dtype='float32').reshape([16]),
            paddle.to_tensor([0.03694642707705498, 0.05816188082098961, 0.3547808527946472, 0.03133751079440117, 0.09795482456684113, 0.30831170082092285, 0.08460652083158493, 0.03528047353029251, 0.34466448426246643, 0.16565600037574768, 0.08263106644153595, 0.16787195205688477, 0.353746235370636, 0.3634662628173828, 0.48238497972488403, 0.30434972047805786], dtype='float32').reshape([16]),
            paddle.to_tensor([0.12801693379878998, 0.11748912930488586, 0.4003865718841553, 0.3671870529651642, 0.2897184491157532, 0.48251935839653015, 0.17315402626991272, 0.4691517949104309, 0.3996962606906891, 0.42914247512817383, 0.21226753294467926, 0.12194547802209854, 0.21273060142993927, 0.36528944969177246, 0.1762869507074356, 0.17892302572727203], dtype='float32').reshape([16]),
            paddle.to_tensor([0.005351056344807148, 0.062024399638175964, 0.16096550226211548, 0.3268592655658722, 0.3537759780883789, 0.3984638452529907, 0.2518756091594696, 0.49741536378860474, 0.22870837152004242, 0.1986103057861328, 0.36098355054855347, 0.16917546093463898, 0.07335725426673889, 0.3493822515010834, 0.19786331057548523, 0.3084055185317993], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2a063378c955857c256570be1a1a26b4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9f1e71efbe64880b261123f71f113eb0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.to_tensor([[[[36238.18359375]], [[43825.06640625]], [[42203.59375]], [[39782.23046875]], [[41444.72265625]], [[41690.75390625]], [[38549.1015625]], [[45009.34375]], [[44431.90625]], [[39391.8828125]], [[44090.39453125]], [[42454.7265625]], [[43449.640625]], [[40438.46875]], [[45929.60546875]], [[46706.46484375]], [[42172.41796875]], [[39302.36328125]], [[42220.8984375]]]], dtype='float32').reshape([1, 19, 1, 1]),
            paddle.to_tensor([0.10252021998167038, 0.4786222279071808, 0.31985294818878174, 0.04100524261593819, 0.02350415475666523, 0.4234643578529358, 0.25441113114356995, 0.20676633715629578, 0.2431822568178177, 0.06917975097894669, 0.350414901971817, 0.21707242727279663, 0.1581239402294159, 0.26964056491851807, 0.2578038275241852, 0.2547881007194519, 0.19480274617671967, 0.052994318306446075, 0.104143425822258], dtype='float32').reshape([19]),
            paddle.to_tensor([0.20734745264053345, 0.013994228094816208, 0.48105183243751526, 0.06352926045656204, 0.3129720389842987, 0.2404029667377472, 0.4100032150745392, 0.17918173968791962, 0.41604456305503845, 0.045764174312353134, 0.1993921548128128, 0.18024225533008575, 0.47289150953292847, 0.47515013813972473, 0.13050463795661926, 0.22027435898780823, 0.17437206208705902, 0.26280906796455383, 0.03319060429930687], dtype='float32').reshape([19]),
            paddle.to_tensor([0.24675622582435608, 0.15897946059703827, 0.03289247304201126, 0.45707613229751587, 0.048177171498537064, 0.04711160808801651, 0.13530290126800537, 0.16338856518268585, 0.11256365478038788, 0.394036203622818, 0.09216394275426865, 0.011262230575084686, 0.16650286316871643, 0.21183238923549652, 0.34193482995033264, 0.3552004098892212, 0.21154749393463135, 0.14968155324459076, 0.20084363222122192], dtype='float32').reshape([19]),
            paddle.to_tensor([0.4750960171222687, 0.10709168016910553, 0.48336878418922424, 0.3347184658050537, 0.2695125341415405, 0.44209402799606323, 0.35227978229522705, 0.3459603190422058, 0.2981957793235779, 0.3881434500217438, 0.29545408487319946, 0.04205555096268654, 0.4603670835494995, 0.4086361825466156, 0.24051788449287415, 0.14465485513210297, 0.44814208149909973, 0.3741939961910248, 0.07148704677820206], dtype='float32').reshape([19]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ae1769c6f6fc04f6d6f84fee8b4cc030(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_0336412036bb12c89542873a9d0cc312
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3c50558c0878ba4c664071da46201d53(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 100, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([100], dtype='float32', min=0, max=0.5),
            paddle.uniform([100], dtype='float32', min=0, max=0.5),
            paddle.uniform([100], dtype='float32', min=0, max=0.5),
            paddle.uniform([100], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0843269a2733678c78949dca28122fd3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 71, 71], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_91f2800e95a93e36b48a9f855313f5f4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.48713019490242004, 0.34821999073028564, 0.16917937994003296, 0.38073307275772095, 0.11909398436546326, 0.11038713902235031, 0.037570659071207047, 0.38534408807754517, 0.46975085139274597, 0.4483060836791992, 0.1914401352405548, 0.29197490215301514, 0.021800508722662926, 0.020367341116070747, 0.08768510073423386, 0.04974105581641197, 0.44153478741645813, 0.45663413405418396, 0.23891890048980713, 0.48731234669685364, 0.054344553500413895, 0.10469212383031845, 0.06077637895941734, 0.10164470970630646, 0.46092256903648376, 0.05339294299483299, 0.44060981273651123, 0.49075788259506226, 0.04821045696735382, 0.4755065143108368], dtype='float32').reshape([30]),
            paddle.to_tensor([0.07481857389211655, 0.12918537855148315, 0.02893480844795704, 0.041126567870378494, 0.1665024757385254, 0.21073517203330994, 0.2508930265903473, 0.40072447061538696, 0.1928962767124176, 0.11829948425292969, 0.2557642161846161, 0.28078192472457886, 0.06364432722330093, 0.04268983379006386, 0.11905473470687866, 0.10276353359222412, 0.0915696769952774, 0.3500012457370758, 0.23294728994369507, 0.03702529892325401, 0.17934010922908783, 0.4301984906196594, 0.07601670920848846, 0.31542983651161194, 0.46037137508392334, 0.14934012293815613, 0.29867780208587646, 0.33314403891563416, 0.40808647871017456, 0.02045614831149578], dtype='float32').reshape([30]),
            paddle.to_tensor([0.1448144167661667, 0.46479740738868713, 0.47707855701446533, 0.016351202502846718, 0.2690710425376892, 0.16811932623386383, 0.20693445205688477, 0.34238460659980774, 0.08136892318725586, 0.0064823217689991, 0.31028130650520325, 0.4904315769672394, 0.03266213461756706, 0.18088141083717346, 0.25476816296577454, 0.4772998094558716, 0.12929973006248474, 0.02338365837931633, 0.07836129516363144, 0.3296700417995453, 0.03974401205778122, 0.3249656856060028, 0.33736371994018555, 0.10116958618164062, 0.34560519456863403, 0.2645998001098633, 0.05223853513598442, 0.1407724767923355, 0.18948638439178467, 0.13907913863658905], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4189318120479584, 0.38324642181396484, 0.3898526132106781, 0.008849459700286388, 0.47515013813972473, 0.43449118733406067, 0.2216401994228363, 0.10083730518817902, 0.415144681930542, 0.22840411961078644, 0.43702423572540283, 0.27453547716140747, 0.0008201148011721671, 0.13804084062576294, 0.22758711874485016, 0.3666727542877197, 0.07463668286800385, 0.4067649841308594, 0.14969617128372192, 0.3085364103317261, 0.03671992942690849, 0.11581388860940933, 0.28434857726097107, 0.21921306848526, 0.2880363464355469, 0.3370411992073059, 0.35005536675453186, 0.2433149367570877, 0.4529173672199249, 0.09756502509117126], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_93882bda0db4f1efe2e8f90b2ff44231(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6e6f9ab43e9747c0717b8ff3ab72ed16(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 4, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9774919f79040f503e56cfd26b9e11f2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 82, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c16e86201faeded7ad5f6f927df85ccd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_190f5489288e3db92fdca6b9490f8160
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 32, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4163292348384857, 0.2014176845550537, 0.43756958842277527, 0.3483900725841522, 0.15163592994213104, 0.14797720313072205, 0.2520701587200165, 0.4086662232875824, 0.42584821581840515, 0.2517516314983368, 0.41264796257019043, 0.37048402428627014, 0.4021521508693695, 0.4956122934818268, 0.004317514598369598, 0.39193886518478394], dtype='float32').reshape([16]),
            paddle.to_tensor([0.21033386886119843, 0.22442863881587982, 0.4476010203361511, 0.09603522717952728, 0.12464374303817749, 0.4548804759979248, 0.2177024930715561, 0.2030923217535019, 0.41125720739364624, 0.32829487323760986, 0.10962234437465668, 0.4782717227935791, 0.15911583602428436, 0.07940927892923355, 0.13474519550800323, 0.4866145849227905], dtype='float32').reshape([16]),
            paddle.to_tensor([0.018692051991820335, 0.254595547914505, 0.4293234348297119, 0.4878758490085602, 0.2965351641178131, 0.36150476336479187, 0.1766732931137085, 0.3850911259651184, 0.03245783597230911, 0.47617393732070923, 0.0769343450665474, 0.42305952310562134, 0.4766278862953186, 0.4484362006187439, 0.38360998034477234, 0.4407566785812378], dtype='float32').reshape([16]),
            paddle.to_tensor([0.44676509499549866, 0.4848065674304962, 0.2850697338581085, 0.31865137815475464, 0.22913460433483124, 0.18194763362407684, 0.34359171986579895, 0.3950370252132416, 0.30924955010414124, 0.10691529512405396, 0.1013934463262558, 0.17151403427124023, 0.4274539649486542, 0.23192480206489563, 0.41397354006767273, 0.19775183498859406], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3bd582e656b5635ba56bdc260acb746e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.25643476843833923, 0.1550280600786209, 0.335690975189209, 0.23401273787021637, 0.1251901090145111, 0.20575278997421265, 0.07469829171895981, 0.0901918113231659, 0.2265409678220749, 0.11972051858901978, 0.008996800519526005, 0.4454549551010132, 0.2960285246372223, 0.3889369070529938, 0.3516213297843933, 0.21898068487644196, 0.2766415774822235, 0.3212399482727051, 0.4156580865383148, 0.007241958752274513, 0.2627626955509186, 0.3040832579135895, 0.3104763329029083, 0.2167564481496811, 0.4808092713356018, 0.38322070240974426, 0.41868528723716736, 0.20916980504989624, 0.407019704580307, 0.14501139521598816], dtype='float32').reshape([30]),
            paddle.to_tensor([0.27884969115257263, 0.32549983263015747, 0.09180717915296555, 0.06359956413507462, 0.0752716138958931, 0.4146058261394501, 0.2026098221540451, 0.39035719633102417, 0.047089140862226486, 0.4927539825439453, 0.43964049220085144, 0.19227245450019836, 0.05324828252196312, 0.17673036456108093, 0.09217680990695953, 0.0378219410777092, 0.4576546549797058, 0.39983808994293213, 0.23919758200645447, 0.444393515586853, 0.15052753686904907, 0.0936393141746521, 0.1366172730922699, 0.2507403492927551, 0.30104807019233704, 0.4020487666130066, 0.450735479593277, 0.14157897233963013, 0.09314224869012833, 0.282038152217865], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2390875518321991, 0.05703646317124367, 0.045753274112939835, 0.2682349681854248, 0.10807821154594421, 0.4531961679458618, 0.4218319058418274, 0.27774274349212646, 0.4416887164115906, 0.3753499984741211, 0.07038021087646484, 0.2130052149295807, 0.4456669092178345, 0.09582637250423431, 0.03868403285741806, 0.47095081210136414, 0.25747212767601013, 0.0913296565413475, 0.4362722933292389, 0.2858050763607025, 0.07113157212734222, 0.49345171451568604, 0.31913602352142334, 0.2168101966381073, 0.43804606795310974, 0.12851427495479584, 0.33846747875213623, 0.3484944999217987, 0.18299952149391174, 0.4056527614593506], dtype='float32').reshape([30]),
            paddle.to_tensor([0.28967079520225525, 0.15317286550998688, 0.4912624657154083, 0.05284833163022995, 0.28469401597976685, 0.18788690865039825, 0.3472355008125305, 0.21895025670528412, 0.0823771059513092, 0.49269425868988037, 0.44413599371910095, 0.18536952137947083, 0.029093749821186066, 0.17694154381752014, 0.048256926238536835, 0.2203478366136551, 0.17394183576107025, 0.197540283203125, 0.3867035508155823, 0.12459874898195267, 0.0785854160785675, 0.3114511966705322, 0.15569382905960083, 0.2734454870223999, 0.06098685413599014, 0.0711183100938797, 0.08865325152873993, 0.425449013710022, 0.46300774812698364, 0.17456598579883575], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_47182699e8bb2ae312f438af6d4fbb0f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d9055d1b498f2cf4a9e5d750d1d6a2a0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.08325017243623734, 0.13571281731128693, 0.38848981261253357, 0.43929263949394226, 0.3157976269721985, 0.48432767391204834, 0.04445337504148483, 0.39170923829078674, 0.4310605525970459, 0.2130504995584488, 0.16851451992988586, 0.47349467873573303, 0.026389051228761673, 0.3668370842933655, 0.45297837257385254, 0.11074461787939072], dtype='float32').reshape([16]),
            paddle.to_tensor([0.05214647203683853, 0.39553317427635193, 0.019910778850317, 0.22448496520519257, 0.34880295395851135, 0.29247039556503296, 0.1410074383020401, 0.3925188183784485, 0.38543739914894104, 0.2679220139980316, 0.04601586237549782, 0.471561461687088, 0.05280352756381035, 0.06350220739841461, 0.23022417724132538, 0.16395163536071777], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3409043848514557, 0.11811140924692154, 0.02611803077161312, 0.1841050237417221, 0.3554224669933319, 0.035638999193906784, 0.36601224541664124, 0.16241773962974548, 0.09813696891069412, 0.3268324136734009, 0.13260343670845032, 0.2902713716030121, 0.2717934250831604, 0.22592668235301971, 0.1771867275238037, 0.14827489852905273], dtype='float32').reshape([16]),
            paddle.to_tensor([0.0057951053604483604, 0.08109794557094574, 0.4189961850643158, 0.25804218649864197, 0.010490893386304379, 0.2707458734512329, 0.24614626169204712, 0.30810531973838806, 0.373837947845459, 0.37740999460220337, 0.1892412006855011, 0.006108250468969345, 0.2003825157880783, 0.3067989945411682, 0.15145006775856018, 0.03770999610424042], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f6ceaa14d09685b82b44e36d47b58dcf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_827c6cae0607cd4dafcd8db91491702c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_13375d5ffa3c775217fb61602ae0a303(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6b138d40bd5a55fc83b66ab1c72e853b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 702, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([702], dtype='float32', min=0, max=0.5),
            paddle.uniform([702], dtype='float32', min=0, max=0.5),
            paddle.uniform([702], dtype='float32', min=0, max=0.5),
            paddle.uniform([702], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_59c612d34a93f3b346fb702deeab48bd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 512], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4556802213191986, 0.15811525285243988, 0.10991481691598892, 0.06094135344028473, 0.3345448672771454, 0.34672465920448303, 0.09672702103853226, 0.18838734924793243, 0.4216996729373932, 0.2503177523612976, 0.31909772753715515, 0.16760289669036865, 0.34078383445739746, 0.04756497964262962, 0.052178144454956055, 0.1971580684185028], dtype='float32').reshape([16]),
            paddle.to_tensor([0.08240585029125214, 0.2606712281703949, 0.38567984104156494, 0.2301231175661087, 0.3732958734035492, 0.33902251720428467, 0.4513823091983795, 0.4458756446838379, 0.34171250462532043, 0.10808973014354706, 0.30157899856567383, 0.4944247007369995, 0.44229447841644287, 0.4071337878704071, 0.4580428898334503, 0.2575664520263672], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3849663734436035, 0.41291794180870056, 0.36298197507858276, 0.1399349421262741, 0.06652078777551651, 0.45678141713142395, 0.43818458914756775, 0.4024379551410675, 0.11073407530784607, 0.0866103321313858, 0.08520592749118805, 0.014609850943088531, 0.07876333594322205, 0.4504677951335907, 0.1071450337767601, 0.44042810797691345], dtype='float32').reshape([16]),
            paddle.to_tensor([0.1316787451505661, 0.1827155202627182, 0.21333234012126923, 0.34372615814208984, 0.37673860788345337, 0.08282612264156342, 0.011564049869775772, 0.2848856747150421, 0.08043335378170013, 0.1358252316713333, 0.1211763471364975, 0.1416143774986267, 0.20730774104595184, 0.47652384638786316, 0.3785230219364166, 0.29196733236312866], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_81c5413ab310cc328ffc949ae3903d4c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 1280, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_77eedc2ce0a6a2b3c51ca30b377c6b17(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.21750593185424805, 0.4037325382232666, 0.18376797437667847, 0.4953782856464386, 0.37531617283821106, 0.1602388471364975, 0.3357410132884979, 0.17403683066368103, 0.4861847162246704, 0.08876661211252213, 0.09665237367153168, 0.21919064223766327, 0.05080966278910637, 0.4300001263618469, 0.20992650091648102, 0.03975328058004379], dtype='float32').reshape([16]),
            paddle.to_tensor([0.28515395522117615, 0.4809120297431946, 0.465503454208374, 0.45616090297698975, 0.1135249212384224, 0.46959424018859863, 0.24688825011253357, 0.12141550332307816, 0.266364723443985, 0.4159550070762634, 0.16098006069660187, 0.24900202453136444, 0.48865431547164917, 0.23827233910560608, 0.10313710570335388, 0.14498700201511383], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4792666435241699, 0.2985221743583679, 0.13029198348522186, 0.10971886664628983, 0.34512022137641907, 0.41655072569847107, 0.39385318756103516, 0.34749943017959595, 0.15812216699123383, 0.17289724946022034, 0.3166691064834595, 0.31614723801612854, 0.27998319268226624, 0.4738523066043854, 0.3079715967178345, 0.46037909388542175], dtype='float32').reshape([16]),
            paddle.to_tensor([0.010129549540579319, 0.32405343651771545, 0.15574081242084503, 0.03350208327174187, 0.009879881516098976, 0.021555883809924126, 0.12916718423366547, 0.11141657829284668, 0.04466188699007034, 0.4338795840740204, 0.27237996459007263, 0.34617671370506287, 0.4660573899745941, 0.3763246238231659, 0.12641729414463043, 0.2739376723766327], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d46da2ee660f1dce9c0694873468d6f9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 16, 50], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.08312945067882538, 0.33972346782684326, 0.20367281138896942, 0.2785378098487854, 0.4386095702648163, 0.025657234713435173, 0.49495458602905273, 0.07309824228286743], dtype='float32').reshape([8]),
            paddle.to_tensor([0.4234171211719513, 0.3014677166938782, 0.39732035994529724, 0.0631236732006073, 0.18891310691833496, 0.009977707639336586, 0.35711610317230225, 0.10053335875272751], dtype='float32').reshape([8]),
            paddle.to_tensor([0.3216117322444916, 0.21567848324775696, 0.03361278772354126, 0.20516638457775116, 0.4016854763031006, 0.33926254510879517, 0.46116361021995544, 0.38786038756370544], dtype='float32').reshape([8]),
            paddle.to_tensor([0.09597788006067276, 0.2795749008655548, 0.2138143628835678, 0.4733499586582184, 0.08504846692085266, 0.1257549375295639, 0.41368958353996277, 0.3606123626232147], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_541dab5958d1eff3dcd5f2b237b47b6b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 60, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0c6ba412ea1c6fb431a65eb08c1b0520(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 14, 25], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7b4d7f44dab955b13de1156d871e7aa1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_161f00f11c95f43425b39801522b4043
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e77b7a2976a15be684546d43784fc581(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.07918675243854523, 0.26822832226753235, 0.1821460723876953, 0.20497092604637146, 0.3862767517566681, 0.01838398166000843, 0.47853460907936096, 0.017079127952456474, 0.06579862534999847, 0.25064292550086975, 0.3910471200942993, 0.012858308851718903, 0.3170005977153778, 0.3986634612083435, 0.001878694398328662, 0.3076784908771515, 0.4030120074748993, 0.24743753671646118, 0.2606681287288666, 0.07054062932729721, 0.020081035792827606, 0.3324480652809143, 0.49542540311813354, 0.0638716071844101, 0.37010398507118225, 0.03971528261899948, 0.3311935067176819, 0.44036704301834106, 0.49868446588516235, 0.39327213168144226], dtype='float32').reshape([30]),
            paddle.to_tensor([0.11797516793012619, 0.19534558057785034, 0.2637092173099518, 0.1948598027229309, 0.313019722700119, 0.421663373708725, 0.32534798979759216, 0.1858769953250885, 0.1363065540790558, 0.3771340847015381, 0.12374808639287949, 0.37995821237564087, 0.3602536916732788, 0.3115241229534149, 0.02638327330350876, 0.13123439252376556, 0.19900627434253693, 0.43956682085990906, 0.2898918688297272, 0.13191679120063782, 0.43929174542427063, 0.1231955885887146, 0.26213154196739197, 0.058900024741888046, 0.2978214621543884, 0.4109954833984375, 0.40245845913887024, 0.33063244819641113, 0.3358319401741028, 0.19855839014053345], dtype='float32').reshape([30]),
            paddle.to_tensor([0.1459229290485382, 0.39574599266052246, 0.022799931466579437, 0.3071049451828003, 0.3084653317928314, 0.2684774100780487, 0.16177330911159515, 0.06390038877725601, 0.175841823220253, 0.4439903795719147, 0.2419375777244568, 0.48898962140083313, 0.34740349650382996, 0.4397154450416565, 0.20273272693157196, 0.17408952116966248, 0.17674998939037323, 0.27250534296035767, 0.3302621841430664, 0.0010146822314709425, 0.08057273179292679, 0.15898096561431885, 0.16862021386623383, 0.366995632648468, 0.4223939776420593, 0.29342910647392273, 0.217984139919281, 0.4482690989971161, 0.3597964942455292, 0.44610631465911865], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4961030185222626, 0.46724992990493774, 0.07792960852384567, 0.019012168049812317, 0.20444490015506744, 0.4989708662033081, 0.34537971019744873, 0.4557821452617645, 0.061595041304826736, 0.23084771633148193, 0.016674388200044632, 0.1005309671163559, 0.13913963735103607, 0.19326791167259216, 0.3861686885356903, 0.47632595896720886, 0.21173922717571259, 0.43712282180786133, 0.40770596265792847, 0.22999617457389832, 0.22233790159225464, 0.017999552190303802, 0.01705045998096466, 0.15516382455825806, 0.44339272379875183, 0.4438440203666687, 0.34118807315826416, 0.4051145613193512, 0.2325010448694229, 0.31498900055885315], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ee622a322118127986709e89afa25398(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 1, 1], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_da7a87293ab41d63ea6067868329e7f7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d265e39acc47d9e3f0153aa141c00c8f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 120, 120], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3755c8dd0f36d32b9f9870aeac11dd02(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 1392, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1392], dtype='float32', min=0, max=0.5),
            paddle.uniform([1392], dtype='float32', min=0, max=0.5),
            paddle.uniform([1392], dtype='float32', min=0, max=0.5),
            paddle.uniform([1392], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e67f6216f8fbb3edf3a79fc36040e40d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 28, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fc3a88d68e255208e293affd0f183b38(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 2, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_39756c446d985af06e30010b8c616ea2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 61, 61], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fb693d1287e556656bc0c6b97631e16a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b17de551da0d217da11411810bbf7687(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.25900593400001526, 0.3016943037509918, 0.10246036946773529, 0.18283218145370483, 0.30795159935951233, 0.06326505541801453, 0.10630135983228683, 0.08390747755765915, 0.3528512716293335, 0.40784159302711487, 0.22995789349079132, 0.13429231941699982, 0.4461113512516022, 0.24251949787139893, 0.11966745555400848, 0.06926146149635315, 0.008744833059608936, 0.0830690935254097, 0.30395668745040894, 0.4949319362640381, 0.41631025075912476, 0.1660909205675125, 0.16021203994750977, 0.31609490513801575, 0.4517571032047272, 0.13177911937236786, 0.36322420835494995, 0.23821736872196198, 0.15312229096889496, 0.05576659366488457], dtype='float32').reshape([30]),
            paddle.to_tensor([0.47774600982666016, 0.3463347554206848, 0.041129227727651596, 0.41248688101768494, 0.4991436004638672, 0.15027107298374176, 0.4043543338775635, 0.13109718263149261, 0.46848323941230774, 0.23910477757453918, 0.4012528359889984, 0.12532669305801392, 0.12649230659008026, 0.13600069284439087, 0.3212319314479828, 0.20252327620983124, 0.07650468498468399, 0.3262889087200165, 0.23893827199935913, 0.25402116775512695, 0.41185232996940613, 0.2012706845998764, 0.3011179566383362, 0.23673588037490845, 0.3459121286869049, 0.27025657892227173, 0.05457771569490433, 0.3840228021144867, 0.3306182026863098, 0.2507909834384918], dtype='float32').reshape([30]),
            paddle.to_tensor([0.46433427929878235, 0.3231654763221741, 0.33371108770370483, 0.4361797869205475, 0.2633761465549469, 0.14079663157463074, 0.011365391314029694, 0.38565176725387573, 0.47259894013404846, 0.3086032569408417, 0.3855474591255188, 0.2240372747182846, 0.4948878884315491, 0.08940063416957855, 0.44936785101890564, 0.05532252788543701, 0.4038351774215698, 0.10038519650697708, 0.40291503071784973, 0.21071873605251312, 0.33962947130203247, 0.2799956500530243, 0.4140721261501312, 0.30586686730384827, 0.4665623903274536, 0.25316673517227173, 0.3950357735157013, 0.46923577785491943, 0.3474266231060028, 0.055546730756759644], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4901510179042816, 0.4946971535682678, 0.2741009593009949, 0.3451135754585266, 0.10690884292125702, 0.10652755945920944, 0.3578670024871826, 0.24889540672302246, 0.049988824874162674, 0.32886946201324463, 0.4070175290107727, 0.4036727249622345, 0.23598206043243408, 0.0643220841884613, 0.09207824617624283, 0.19591215252876282, 0.3043794631958008, 0.2578171193599701, 0.028432784602046013, 0.24904827773571014, 0.3877798616886139, 0.0029567598830908537, 0.30496302247047424, 0.02342008240520954, 0.092081218957901, 0.49736183881759644, 0.4554305374622345, 0.17665214836597443, 0.028361838310956955, 0.036342158913612366], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9a20f5e4f43cefd0e66964f6d617e7d8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.09394732862710953, 0.4665793478488922, 0.21457713842391968, 0.26322922110557556, 0.015095827169716358, 0.33136582374572754, 0.12120848894119263, 0.09394030272960663, 0.301147997379303, 0.37692391872406006, 0.0647304579615593, 0.09898554533720016, 0.4554978609085083, 0.35275977849960327, 0.17130303382873535, 0.3972175419330597, 0.4620005488395691, 0.06397197395563126, 0.37505006790161133, 0.3134455978870392], dtype='float32').reshape([20]),
            paddle.to_tensor([0.22228583693504333, 0.02335645817220211, 0.27998989820480347, 0.4488039016723633, 0.4455971121788025, 0.14665699005126953, 0.49158746004104614, 0.03404729440808296, 0.36169490218162537, 0.17340847849845886, 0.02879597246646881, 0.33720290660858154, 0.44849225878715515, 0.3583280146121979, 0.3712087869644165, 0.26491114497184753, 0.21065275371074677, 0.2325333058834076, 0.0011308793909847736, 0.45575767755508423], dtype='float32').reshape([20]),
            paddle.to_tensor([0.17589695751667023, 0.4783703684806824, 0.3751729428768158, 0.20646773278713226, 0.40001755952835083, 0.14636006951332092, 0.3891795873641968, 0.0005309433327056468, 0.4866829216480255, 0.36760425567626953, 0.3708769977092743, 0.47086331248283386, 0.13475115597248077, 0.042895615100860596, 0.4136628210544586, 0.051946964114904404, 0.11161163449287415, 0.10473016649484634, 0.1808280199766159, 0.007164333947002888], dtype='float32').reshape([20]),
            paddle.to_tensor([0.09545152634382248, 0.3910638689994812, 0.344430148601532, 0.10425035655498505, 0.13049812614917755, 0.19799140095710754, 0.3537917137145996, 0.208924800157547, 0.1173187866806984, 0.20927301049232483, 0.3376099467277527, 0.4510734975337982, 0.07174517214298248, 0.1953955888748169, 0.12113863974809647, 0.17032983899116516, 0.13190223276615143, 0.0283507127314806, 0.44797495007514954, 0.18146125972270966], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6fd8a851f3ad7e49ac03b151799912ea(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_13025b164297d9226f9641b7f0715c76
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a503cf0656f17dd6980ab3e6fc603e3e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 8, 6], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dc559efff3fa84c82ceaca2bee647865(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_161f00f11c95f43425b39801522b4043
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f62857f3e4852ac5af5ef996992976bf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 528, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([528], dtype='float32', min=0, max=0.5),
            paddle.uniform([528], dtype='float32', min=0, max=0.5),
            paddle.uniform([528], dtype='float32', min=0, max=0.5),
            paddle.uniform([528], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_676a4297752213b54709518e1bb14607(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_551b7b803f5be7f435279fa2aa263ba7
    def get_inputs(self):
        return [
            paddle.uniform([1, 640, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_53a7cf716b4f08a09d087096416b18ca(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_190f5489288e3db92fdca6b9490f8160
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 32, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4a9cffcb342a791c932195316fbf35a6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 31, 31], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0d369b4cf94d2a10f0a2f26d9c9aed7e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_30adf1ae81ee898ff64b7fd4f3da9702(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 8, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_92ed7344984869f48820e6097ae5c602(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 8, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_227f54f7f9a872561ebaf9d0fdb2ac8b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 480, 480], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.01626468077301979, 0.1413673311471939, 0.16434559226036072, 0.41665220260620117, 0.4673815667629242, 0.1030464768409729, 0.1806848645210266, 0.30661267042160034], dtype='float32').reshape([8]),
            paddle.to_tensor([0.0866289734840393, 0.03746914491057396, 0.06794602423906326, 0.4826781749725342, 0.43145713210105896, 0.07880423963069916, 0.3379207253456116, 0.3739354610443115], dtype='float32').reshape([8]),
            paddle.to_tensor([0.08984030783176422, 0.32518094778060913, 0.264690101146698, 0.3742768168449402, 0.04824957624077797, 0.36206042766571045, 0.4158797264099121, 0.1794283241033554], dtype='float32').reshape([8]),
            paddle.to_tensor([0.24100954830646515, 0.14611414074897766, 0.4359045624732971, 0.23402133584022522, 0.0654241144657135, 0.44681984186172485, 0.22308683395385742, 0.25427067279815674], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7a0b765830ec5859daea08e0f44bbc98(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2619378864765167, 0.1471647024154663, 0.023679561913013458, 0.16835972666740417, 0.16714496910572052, 0.4334937632083893, 0.05550726503133774, 0.030023636296391487, 0.15235385298728943, 0.13426482677459717, 0.45809367299079895, 0.34877389669418335, 0.25595030188560486, 0.10683498531579971, 0.21987846493721008, 0.2603115737438202], dtype='float32').reshape([16]),
            paddle.to_tensor([0.199910968542099, 0.23504014313220978, 0.324985533952713, 0.19293567538261414, 0.41289663314819336, 0.43253058195114136, 0.3866577446460724, 0.39558619260787964, 0.2759841978549957, 0.009733356535434723, 0.05903110280632973, 0.2672715485095978, 0.146864652633667, 0.39904165267944336, 0.09493780136108398, 0.08157949894666672], dtype='float32').reshape([16]),
            paddle.to_tensor([0.33956998586654663, 0.32365575432777405, 0.16718779504299164, 0.29771801829338074, 0.19543398916721344, 0.08395123481750488, 0.3963339626789093, 0.23924462497234344, 0.2086191624403, 0.33280783891677856, 0.28552234172821045, 0.43536922335624695, 0.06038569658994675, 0.36773329973220825, 0.0025365431793034077, 0.392034113407135], dtype='float32').reshape([16]),
            paddle.to_tensor([0.33480021357536316, 0.03499554842710495, 0.4223916828632355, 0.3477746844291687, 0.2833713889122009, 0.029668789356946945, 0.29431840777397156, 0.18523761630058289, 0.22974896430969238, 0.23438721895217896, 0.28907570242881775, 0.3112882971763611, 0.2984703481197357, 0.17884382605552673, 0.290938138961792, 0.45189350843429565], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f56cc458f32b57e305550c0c8af486dc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a0f21d8e9498d16c7d3e9c547efc30f0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.28910258412361145, 0.25740134716033936, 0.16429993510246277, 0.10342897474765778, 0.009601610712707043, 0.2670423090457916, 0.23187050223350525, 0.35784950852394104], dtype='float32').reshape([8]),
            paddle.to_tensor([0.2860925793647766, 0.3252042233943939, 0.0319829061627388, 0.17601299285888672, 0.2443285435438156, 0.44335681200027466, 0.007723261136561632, 0.15585646033287048], dtype='float32').reshape([8]),
            paddle.to_tensor([0.11246201395988464, 0.4190787076950073, 0.09012904018163681, 0.04785613343119621, 0.27738597989082336, 0.3306020200252533, 0.1252262443304062, 0.012651109136641026], dtype='float32').reshape([8]),
            paddle.to_tensor([0.2735901176929474, 0.46757498383522034, 0.4745998680591583, 0.06467106193304062, 0.10824362188577652, 0.06010302156209946, 0.21123401820659637, 0.06754591315984726], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_96164e739f1bc835c536058a3ff15b42(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 224, 224], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.19517457485198975, 0.3500088155269623, 0.46418967843055725, 0.24883165955543518, 0.08723337203264236, 0.2051541805267334, 0.2380986362695694, 0.1467835009098053, 0.3624769151210785, 0.3626359701156616, 0.24162177741527557, 0.48991620540618896, 0.059002071619033813, 0.1184527650475502, 0.05479247123003006, 0.23672685027122498], dtype='float32').reshape([16]),
            paddle.to_tensor([0.0965627059340477, 0.015059360302984715, 0.17222006618976593, 0.11960010230541229, 0.16912002861499786, 0.41259050369262695, 0.0024571367539465427, 0.2617804706096649, 0.2443811595439911, 0.44684839248657227, 0.4117703139781952, 0.2771565914154053, 0.47876688838005066, 0.08505606651306152, 0.10617577284574509, 0.2522379457950592], dtype='float32').reshape([16]),
            paddle.to_tensor([0.13600219786167145, 0.44191011786460876, 0.1749274879693985, 0.13264068961143494, 0.33473992347717285, 0.05298791453242302, 0.19555681943893433, 0.29233646392822266, 0.019133321940898895, 0.4139125347137451, 0.2098948359489441, 0.3019152283668518, 0.3349595367908478, 0.40054887533187866, 0.3137645721435547, 0.08609435707330704], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3593828082084656, 0.24030794203281403, 0.1607658863067627, 0.38493072986602783, 0.11682944744825363, 0.29760971665382385, 0.0706244483590126, 0.27066656947135925, 0.18972648680210114, 0.08680589497089386, 0.4964824914932251, 0.3774012327194214, 0.3915899991989136, 0.374388724565506, 0.38306301832199097, 0.38774558901786804], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1c01ef864d1ee1cd971630759873e8af(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.22742792963981628, 0.003622547723352909, 0.4442311227321625, 0.13528509438037872, 0.1729438453912735, 0.30701327323913574, 0.3491496741771698, 0.3049052655696869, 0.25548720359802246, 0.16304875910282135, 0.30817514657974243, 0.03912696987390518, 0.46799808740615845, 0.021926257759332657, 0.0075540426187217236, 0.40814676880836487, 0.46701860427856445, 0.24891869723796844, 0.26173919439315796, 0.4148802161216736, 0.13931280374526978, 0.43556687235832214, 0.475165456533432, 0.2547464668750763], dtype='float32').reshape([24]),
            paddle.to_tensor([0.014297197572886944, 0.010750184766948223, 0.3559022843837738, 0.39410892128944397, 0.3613215684890747, 0.05809088051319122, 0.06709280610084534, 0.10371871292591095, 0.22376875579357147, 0.054640963673591614, 0.33909761905670166, 0.43752843141555786, 0.09917797893285751, 0.2800910174846649, 0.06644579023122787, 0.23893898725509644, 0.10933571308851242, 0.009824550710618496, 0.24264709651470184, 0.030584024265408516, 0.09261504560709, 0.30909910798072815, 0.21970641613006592, 0.305763840675354], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4882301092147827, 0.3966989517211914, 0.08216102421283722, 0.12427258491516113, 0.25387826561927795, 0.21914957463741302, 0.24591165781021118, 0.054135166108608246, 0.06400047242641449, 0.14427483081817627, 0.14928336441516876, 0.3141727149486542, 0.4840693175792694, 0.20081225037574768, 0.26585811376571655, 0.025181438773870468, 0.2902103066444397, 0.02805042639374733, 0.23458831012248993, 0.15798601508140564, 0.38489675521850586, 0.41182658076286316, 0.2403412014245987, 0.2545246183872223], dtype='float32').reshape([24]),
            paddle.to_tensor([0.051755230873823166, 0.24116995930671692, 0.2711049020290375, 0.3906853497028351, 0.22983010113239288, 0.37794405221939087, 0.4782891273498535, 0.026282289996743202, 0.333360880613327, 0.40586134791374207, 0.43107694387435913, 0.13309718668460846, 0.47565197944641113, 0.45193931460380554, 0.4205389618873596, 0.10599935799837112, 0.10742058604955673, 0.3171803057193756, 0.4508088529109955, 0.49387043714523315, 0.46420180797576904, 0.2270546704530716, 0.10991884768009186, 0.2790968716144562], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ddd09742f36aef6c825c9d429169b266(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 144, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a4e9ab6bb85db327e90ae61840881d6c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4768325090408325, 0.37055525183677673, 0.038175057619810104, 0.2640500068664551, 0.30093488097190857, 0.22798536717891693, 0.26327916979789734, 0.3076167404651642, 0.3827345371246338, 0.4175407290458679, 0.03621648997068405, 0.010896412655711174, 0.02315184660255909, 0.44659608602523804, 0.3460846245288849, 0.06440868973731995, 0.445469468832016, 0.2871064245700836, 0.14807800948619843, 0.35728809237480164, 0.25836485624313354, 0.3450824022293091, 0.08590054512023926, 0.4094885289669037, 0.456020325422287, 0.43668028712272644, 0.06689339876174927, 0.3977929353713989], dtype='float32').reshape([28]),
            paddle.to_tensor([0.2503736913204193, 0.17973929643630981, 0.4775756895542145, 0.2570408284664154, 0.08975191414356232, 0.14957399666309357, 0.05186548829078674, 0.13735432922840118, 0.12533435225486755, 0.3526204526424408, 0.11628592014312744, 0.2869771420955658, 0.1859275847673416, 0.3091171383857727, 0.23604771494865417, 0.27144095301628113, 0.16516688466072083, 0.09447533637285233, 0.4171603322029114, 0.08468306064605713, 0.13168363273143768, 0.4685361087322235, 0.2039119005203247, 0.36335328221321106, 0.3937399685382843, 0.2477668821811676, 0.10905305296182632, 0.024873021990060806], dtype='float32').reshape([28]),
            paddle.to_tensor([0.4934234023094177, 0.39789924025535583, 0.059784431010484695, 0.3903769850730896, 0.17679482698440552, 0.1966589242219925, 0.29167211055755615, 0.3715648949146271, 0.1651517152786255, 0.04595214128494263, 0.36044394969940186, 0.1623825579881668, 0.1765686422586441, 0.4882775843143463, 0.18177612125873566, 0.23249827325344086, 0.3755839765071869, 0.20310769975185394, 0.3368982672691345, 0.361636221408844, 0.29701870679855347, 0.11561954766511917, 0.2904485762119293, 0.29667553305625916, 0.21613655984401703, 0.24211426079273224, 0.4842260479927063, 0.3941210210323334], dtype='float32').reshape([28]),
            paddle.to_tensor([0.24958562850952148, 0.21459133923053741, 0.3911770284175873, 0.05060761794447899, 0.027640873566269875, 0.4918992817401886, 0.4034786820411682, 0.4528641700744629, 0.13133256137371063, 0.14656370878219604, 0.12511713802814484, 0.16237570345401764, 0.28481167554855347, 0.10077548027038574, 0.27081459760665894, 0.09633024781942368, 0.4630846381187439, 0.18690428137779236, 0.0956876277923584, 0.20676982402801514, 0.09216219931840897, 0.26970377564430237, 0.005756981670856476, 0.36343830823898315, 0.4191298484802246, 0.4467913806438446, 0.007890086621046066, 0.17524923384189606], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a9fd9f7636988f9242391c473717915a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.29487672448158264, 0.31037047505378723, 0.047812145203351974, 0.005356001667678356, 0.46005770564079285, 0.47240403294563293, 0.15036681294441223, 0.21008731424808502, 0.3941618800163269, 0.02715287357568741, 0.3264729380607605, 0.4336228370666504, 0.05158329755067825, 0.04086536169052124, 0.3626789152622223, 0.2613224983215332, 0.039493996649980545, 0.25931796431541443, 0.12509021162986755, 0.16028614342212677], dtype='float32').reshape([20]),
            paddle.to_tensor([0.17031584680080414, 0.1705082803964615, 0.40450045466423035, 0.18633463978767395, 0.3992248475551605, 0.12196683883666992, 0.03694020211696625, 0.3584408462047577, 0.22000345587730408, 0.338961124420166, 0.33690154552459717, 0.4521956741809845, 0.3885057866573334, 0.2949892580509186, 0.10510018467903137, 0.23498982191085815, 0.4169871509075165, 0.012089742347598076, 0.1753474771976471, 0.19730935990810394], dtype='float32').reshape([20]),
            paddle.to_tensor([0.22376765310764313, 0.1345779150724411, 0.24134834110736847, 0.005894141271710396, 0.43002647161483765, 0.3762635588645935, 0.06562212109565735, 0.09382372349500656, 0.34260258078575134, 0.10472963750362396, 0.30669695138931274, 0.08361228555440903, 0.25923028588294983, 0.4014757573604584, 0.08536623418331146, 0.11418116092681885, 0.27800852060317993, 0.4667262136936188, 0.15695281326770782, 0.06445737183094025], dtype='float32').reshape([20]),
            paddle.to_tensor([0.2564709484577179, 0.36581555008888245, 0.12253091484308243, 0.118594229221344, 0.36718839406967163, 0.4371260404586792, 0.006531696766614914, 0.09143028408288956, 0.47117775678634644, 0.04304753988981247, 0.3775070309638977, 0.41963404417037964, 0.22470572590827942, 0.40351539850234985, 0.33900120854377747, 0.3144055902957916, 0.09005291014909744, 0.08945005387067795, 0.16913555562496185, 0.21205541491508484], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_80e4a794b51d54bc65eb83a6a1b6249b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c2e433dd3c6257587ea3066ab9b0539a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_71adf679806cab2a22b147a9b47d0cbc
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3c795337975f7f0d9f31e28e7cef3dc3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 320, 320], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4361139237880707, 0.40485164523124695, 0.21605412662029266, 0.06636353582143784, 0.314058393239975, 0.27154186367988586, 0.0956709086894989, 0.32544517517089844, 0.4046751856803894, 0.328480988740921, 0.4912833571434021, 0.4775387644767761, 0.09695988148450851, 0.3952134847640991, 0.0754711702466011, 0.03757114335894585], dtype='float32').reshape([16]),
            paddle.to_tensor([0.11926839500665665, 0.37414613366127014, 0.0027437726967036724, 0.08017315715551376, 0.018337823450565338, 0.10047416388988495, 0.2991826832294464, 0.12087471783161163, 0.17222493886947632, 0.05055268481373787, 0.4277680218219757, 0.46876367926597595, 0.14622527360916138, 0.4212961792945862, 0.22868585586547852, 0.129954993724823], dtype='float32').reshape([16]),
            paddle.to_tensor([0.29275888204574585, 0.3796958327293396, 0.49299928545951843, 0.14823417365550995, 0.07886269688606262, 0.4415159821510315, 0.039335206151008606, 0.20798900723457336, 0.4624486267566681, 0.21436159312725067, 0.47464439272880554, 0.058298323303461075, 0.36946383118629456, 0.25906461477279663, 0.24432408809661865, 0.46792012453079224], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2624132037162781, 0.06771066039800644, 0.4674084782600403, 0.008673831820487976, 0.02507828362286091, 0.43224582076072693, 0.27156779170036316, 0.4972272515296936, 0.24896419048309326, 0.10623430460691452, 0.0684865415096283, 0.19674798846244812, 0.05457602068781853, 0.0487050786614418, 0.15327033400535583, 0.46036064624786377], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6550711361f41c4259b5478442e3e65d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 6, 6], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_386c701e3c779dbbb2ba13e32a395506(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_440fdfc034546c2c17e65da553a88b58(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.03982791304588318, 0.05867328867316246, 0.2956106662750244, 0.4036146104335785, 0.039199844002723694, 0.14174489676952362, 0.27537161111831665, 0.3968981206417084, 0.43409448862075806, 0.005635166075080633, 0.1114116758108139, 0.15002772212028503, 0.15214909613132477, 0.2821120619773865, 0.11841058731079102, 0.3192465901374817, 0.32534119486808777, 0.35441553592681885], dtype='float32').reshape([18]),
            paddle.to_tensor([0.15075142681598663, 0.42053747177124023, 0.1616511195898056, 0.2604857087135315, 0.3932032883167267, 0.3871365487575531, 0.3426858186721802, 0.47477853298187256, 0.32818835973739624, 0.4186248779296875, 0.46161746978759766, 0.4242309331893921, 0.20190376043319702, 0.44644874334335327, 0.42020970582962036, 0.23777320981025696, 0.009861784987151623, 0.15797680616378784], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4329032599925995, 0.379017174243927, 0.4762166440486908, 0.47246015071868896, 0.4105611741542816, 0.35139864683151245, 0.06794172525405884, 0.1626628041267395, 0.33275890350341797, 0.13196004927158356, 0.2439970076084137, 0.019654540345072746, 0.498565673828125, 0.2948332726955414, 0.16533516347408295, 0.009379923343658447, 0.3244253098964691, 0.1693751960992813], dtype='float32').reshape([18]),
            paddle.to_tensor([0.193328857421875, 0.3699406683444977, 0.4574235677719116, 0.35049355030059814, 0.2775197923183441, 0.1910908818244934, 0.46466800570487976, 0.31619754433631897, 0.48316922783851624, 0.22837504744529724, 0.18372845649719238, 0.07411191612482071, 0.2476743757724762, 0.051927655935287476, 0.305589884519577, 0.08795023709535599, 0.4095195531845093, 0.4206371307373047], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_adb4d13d3466da02d06a0b69d3e75061(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 256, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8490c5b4517c5cdf6396ee4a5fcaeae4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 768, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aacde9b9cd4bf85ee370e7dbb6976921(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 62, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([62], dtype='float32', min=0, max=0.5),
            paddle.uniform([62], dtype='float32', min=0, max=0.5),
            paddle.uniform([62], dtype='float32', min=0, max=0.5),
            paddle.uniform([62], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4e2974060bc2b3523093a641f746a698(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 144, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b6a2473157008a2dbf2f355138bbd361(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_652e26b1fd1837f9fa67eddaa68a4346(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 64, 128], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3799101710319519, 0.0678723007440567, 0.16489137709140778, 0.2680802643299103, 0.4094977378845215, 0.060219746083021164, 0.33149927854537964, 0.19673433899879456, 0.051886748522520065, 0.28670111298561096, 0.30406704545021057, 0.2199527621269226, 0.27903375029563904, 0.23962165415287018, 0.03237216919660568, 0.44387462735176086, 0.06478887796401978, 0.24949896335601807], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2794859707355499, 0.1535852700471878, 0.04954055696725845, 0.16990190744400024, 0.16424952447414398, 0.49629145860671997, 0.3123099207878113, 0.17606830596923828, 0.26661571860313416, 0.2682880759239197, 0.03792354092001915, 0.021553650498390198, 0.23462924361228943, 0.34824100136756897, 0.45510533452033997, 0.19052575528621674, 0.19239208102226257, 0.46351149678230286], dtype='float32').reshape([18]),
            paddle.to_tensor([0.07386632263660431, 0.2822993993759155, 0.41706544160842896, 0.2898222506046295, 0.18153832852840424, 0.4435865879058838, 0.23558218777179718, 0.4372590482234955, 0.25068673491477966, 0.13808156549930573, 0.045424994081258774, 0.34802114963531494, 0.4440460503101349, 0.060948919504880905, 0.4394126534461975, 0.4960276484489441, 0.490683913230896, 0.154680997133255], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3752051293849945, 0.1481875628232956, 0.39947089552879333, 0.4479997158050537, 0.22762326896190643, 0.4156741201877594, 0.4592435956001282, 0.10186871886253357, 0.09103930741548538, 0.06253323704004288, 0.37887042760849, 0.19057029485702515, 0.012450059875845909, 0.4378795623779297, 0.1576809585094452, 0.08256760239601135, 0.43118900060653687, 0.48825275897979736], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b56b9e67f17a4b2539230cc3a65b46f3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2761649787425995, 0.06453987210988998, 0.05710891634225845, 0.2600129246711731, 0.39424949884414673, 0.10910872370004654, 0.17291201651096344, 0.044613469392061234, 0.4048316180706024, 0.13920120894908905, 0.4689478874206543, 0.1758279651403427, 0.23424772918224335, 0.4944416284561157, 0.1626269519329071, 0.1846885234117508, 0.4453665316104889, 0.39135175943374634, 0.3898947238922119, 0.4183669686317444, 0.009072898887097836, 0.37703660130500793, 0.0811418741941452, 0.1384756863117218, 0.043479032814502716, 0.3100231885910034, 0.238081693649292, 0.46018052101135254, 0.44717738032341003, 0.16009798645973206], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2993089258670807, 0.08552095293998718, 0.47239434719085693, 0.2833605408668518, 0.2554444670677185, 0.24394099414348602, 0.08390152454376221, 0.36774182319641113, 0.31748461723327637, 0.15706011652946472, 0.17758294939994812, 0.21143269538879395, 0.03921553120017052, 0.3467044234275818, 0.4634041488170624, 0.43336939811706543, 0.11737360060214996, 0.33704960346221924, 0.17110376060009003, 0.21965262293815613, 0.11800320446491241, 0.04175037518143654, 0.02365121617913246, 0.14312119781970978, 0.4313027858734131, 0.41478368639945984, 0.1730533242225647, 0.1994200199842453, 0.38367557525634766, 0.3320018947124481], dtype='float32').reshape([30]),
            paddle.to_tensor([0.20667988061904907, 0.47413814067840576, 0.40525534749031067, 0.4152054488658905, 0.35359305143356323, 0.12955859303474426, 0.10074619203805923, 0.355752170085907, 0.3669416308403015, 0.15595267713069916, 0.22221986949443817, 0.061584629118442535, 0.11366531997919083, 0.39262646436691284, 0.4461474120616913, 0.29822736978530884, 0.0008303766953758895, 0.4643472135066986, 0.017710965126752853, 0.05701500549912453, 0.37382546067237854, 0.05065729841589928, 0.4868534207344055, 0.07055648416280746, 0.37710896134376526, 0.3307819366455078, 0.29217100143432617, 0.21404418349266052, 0.24577632546424866, 0.05105753242969513], dtype='float32').reshape([30]),
            paddle.to_tensor([0.28311991691589355, 0.41586971282958984, 0.44890591502189636, 0.17698486149311066, 0.31300777196884155, 0.33902883529663086, 0.38547348976135254, 0.2755289673805237, 0.22989192605018616, 0.10285606235265732, 0.04323408380150795, 0.11816611886024475, 0.4867260158061981, 0.07998337596654892, 0.40932950377464294, 0.3728933334350586, 0.3023664653301239, 0.1642976850271225, 0.15755361318588257, 0.0028848382644355297, 0.007173486985266209, 0.2698723375797272, 0.04737696796655655, 0.3928386867046356, 0.1936667561531067, 0.3506600856781006, 0.22256968915462494, 0.012298195622861385, 0.011018194258213043, 0.46864351630210876], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9992597dd7e5905afac9cf1df0d7ad3d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 1888, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1888], dtype='float32', min=0, max=0.5),
            paddle.uniform([1888], dtype='float32', min=0, max=0.5),
            paddle.uniform([1888], dtype='float32', min=0, max=0.5),
            paddle.uniform([1888], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6ba304b6da1c32bd01fe28f94ec3f9e2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 64, 128], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.435128390789032, 0.4540840983390808, 0.030406439676880836, 0.048854246735572815, 0.3871344327926636, 0.4536330997943878, 0.3312188386917114, 0.25108689069747925, 0.096920445561409, 0.2574867010116577, 0.021573582664132118, 0.45828482508659363, 0.03493616357445717, 0.42416274547576904, 0.2151491343975067, 0.2888018786907196, 0.2457217276096344, 0.17604269087314606], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2870374917984009, 0.2615804076194763, 0.13788267970085144, 0.334370493888855, 0.2336120456457138, 0.38230031728744507, 0.38308870792388916, 0.2762501537799835, 0.2681020200252533, 0.3440627455711365, 0.185459703207016, 0.33330631256103516, 0.30097177624702454, 0.17892910540103912, 0.2939603328704834, 0.07288344949483871, 0.3746172785758972, 0.27110859751701355], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2904798686504364, 0.31211531162261963, 0.19462190568447113, 0.04090822860598564, 0.05179934203624725, 0.4683256149291992, 0.19321386516094208, 0.44050437211990356, 0.18179704248905182, 0.18289317190647125, 0.48698899149894714, 0.35649609565734863, 0.11041396111249924, 0.41629427671432495, 0.4925854206085205, 0.46013525128364563, 0.02426588535308838, 0.14231745898723602], dtype='float32').reshape([18]),
            paddle.to_tensor([0.08189523965120316, 0.24659723043441772, 0.016488563269376755, 0.44186869263648987, 0.4821322560310364, 0.17460960149765015, 0.48320838809013367, 0.3768680989742279, 0.015447242185473442, 0.19131693243980408, 0.24067233502864838, 0.43446674942970276, 0.04609197378158569, 0.03135751932859421, 0.016076022759079933, 0.43967676162719727, 0.06364382058382034, 0.06094741448760033], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dbd608c1752bdd9e0408918eeeb43c13(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 270, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([270], dtype='float32', min=0, max=0.5),
            paddle.uniform([270], dtype='float32', min=0, max=0.5),
            paddle.uniform([270], dtype='float32', min=0, max=0.5),
            paddle.uniform([270], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4ba57aa035994e9654eb4693ddc7be42(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([8, 512, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e854c723cb72dd394aea74adacf3bba3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_186eae946956756b13f3654169c2ef9a
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 4, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d3d2b54b577eb04a0406db2ee7f5eaec(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 164, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([164], dtype='float32', min=0, max=0.5),
            paddle.uniform([164], dtype='float32', min=0, max=0.5),
            paddle.uniform([164], dtype='float32', min=0, max=0.5),
            paddle.uniform([164], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_931ba6fcc67572ca7fe8ea12e6138d29(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 14, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9c9fc4bedff03e93f49eabb01c03fe5a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.13271036744117737, 0.3538474142551422, 0.40810704231262207, 0.1901330202817917, 0.14710858464241028, 0.35629507899284363, 0.0977836400270462, 0.20664364099502563], dtype='float32').reshape([8]),
            paddle.to_tensor([0.34491971135139465, 0.06489346921443939, 0.09573733061552048, 0.10813833773136139, 0.35112708806991577, 0.32799088954925537, 0.09466199576854706, 0.043205760419368744], dtype='float32').reshape([8]),
            paddle.to_tensor([0.4067850410938263, 0.46911486983299255, 0.48359790444374084, 0.20585428178310394, 0.3202250599861145, 0.14138370752334595, 0.33449938893318176, 0.39670059084892273], dtype='float32').reshape([8]),
            paddle.to_tensor([0.19157874584197998, 0.370841383934021, 0.34342116117477417, 0.21817909181118011, 0.3230808973312378, 0.07990069687366486, 0.3608561158180237, 0.35376161336898804], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_670e47fd0153b710973d66617baa5621(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 16, 32], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4640059471130371, 0.22591568529605865, 0.41882267594337463, 0.10104542225599289, 0.31917789578437805, 0.29693615436553955, 0.13111084699630737, 0.20944805443286896, 0.22089000046253204, 0.15435214340686798, 0.1577744334936142, 0.42492106556892395, 0.19150660932064056, 0.17839494347572327, 0.3353731632232666, 0.16221699118614197, 0.35209617018699646, 0.32838281989097595], dtype='float32').reshape([18]),
            paddle.to_tensor([0.08168409019708633, 0.4182048439979553, 0.3448251187801361, 0.24092085659503937, 0.1366872787475586, 0.10490816831588745, 0.015992436558008194, 0.0009047143976204097, 0.2864993214607239, 0.32409393787384033, 0.3090129494667053, 0.3174169659614563, 0.11572839319705963, 0.4839567542076111, 0.36657124757766724, 0.3276100158691406, 0.18163436651229858, 0.24844233691692352], dtype='float32').reshape([18]),
            paddle.to_tensor([0.0030819305684417486, 0.2092568278312683, 0.12090504169464111, 0.2910896837711334, 0.013497328385710716, 0.056066349148750305, 0.4316134750843048, 0.4156913757324219, 0.07014522701501846, 0.47866055369377136, 0.14744174480438232, 0.47360801696777344, 0.09787741303443909, 0.47753164172172546, 0.04399087280035019, 0.10409653931856155, 0.07295048236846924, 0.14717136323451996], dtype='float32').reshape([18]),
            paddle.to_tensor([0.16894754767417908, 0.026148037984967232, 0.15802012383937836, 0.4410521686077118, 0.2912531793117523, 0.21285544335842133, 0.08045773953199387, 0.35078102350234985, 0.22185075283050537, 0.04426635801792145, 0.33757156133651733, 0.4688931107521057, 0.09719979017972946, 0.34601062536239624, 0.4513553977012634, 0.21610301733016968, 0.2835630178451538, 0.16094860434532166], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5db19c14b892db67de656da0df51c6c7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.08254275470972061, 0.49139711260795593, 0.2331169992685318, 0.023418409749865532, 0.3306407034397125, 0.13551712036132812, 0.28090062737464905, 0.1804364025592804, 0.44898825883865356, 0.09898856282234192, 0.494906485080719, 0.08279460668563843, 0.4686565399169922, 0.3141557574272156, 0.49470195174217224, 0.21031935513019562, 0.09845640510320663, 0.19450438022613525, 0.21342261135578156, 0.36280959844589233, 0.4918099641799927, 0.013474561274051666, 0.4354483187198639, 0.45632168650627136, 0.3786742687225342, 0.40594378113746643, 0.390210896730423, 0.18420156836509705, 0.20424804091453552, 0.08946259319782257], dtype='float32').reshape([30]),
            paddle.to_tensor([0.160052090883255, 0.05403684824705124, 0.3661932647228241, 0.015372480265796185, 0.4622872769832611, 0.39364734292030334, 0.273037314414978, 0.28301578760147095, 0.19662033021450043, 0.06485012918710709, 0.3566167950630188, 0.21123260259628296, 0.3864724934101105, 0.32695573568344116, 0.49292126297950745, 0.2544303238391876, 0.40399014949798584, 0.46472981572151184, 0.4654015302658081, 0.23716077208518982, 0.2533395290374756, 0.26153385639190674, 0.3288000226020813, 0.08569202572107315, 0.03737679123878479, 0.2582475543022156, 0.13942508399486542, 0.2889498174190521, 0.3913726210594177, 0.11985740065574646], dtype='float32').reshape([30]),
            paddle.to_tensor([0.41691988706588745, 0.02248236909508705, 0.46979185938835144, 0.2246718853712082, 0.256488174200058, 0.4274750053882599, 0.2653026282787323, 0.12011350691318512, 0.09294399619102478, 0.09336061775684357, 0.42314577102661133, 0.0058619375340640545, 0.46626439690589905, 0.36832141876220703, 0.13498854637145996, 0.4694344401359558, 0.1917131543159485, 0.056042399257421494, 0.1115918904542923, 0.17517192661762238, 0.14138078689575195, 0.3378610908985138, 0.1376802772283554, 0.09534503519535065, 0.2960609793663025, 0.46960321068763733, 0.01893697865307331, 0.4801577627658844, 0.20179380476474762, 0.08269628137350082], dtype='float32').reshape([30]),
            paddle.to_tensor([0.49341005086898804, 0.4732664227485657, 0.3348506987094879, 0.29845696687698364, 0.38756030797958374, 0.028742913156747818, 0.3152404725551605, 0.1728993058204651, 0.19101199507713318, 0.400981605052948, 0.1361132711172104, 0.4288650453090668, 0.4233120083808899, 0.05691011622548103, 0.24417372047901154, 0.160164013504982, 0.340585321187973, 0.48972228169441223, 0.0385177917778492, 0.15488332509994507, 0.2600475549697876, 0.4303971529006958, 0.20084795355796814, 0.3554169833660126, 0.06882592290639877, 0.04341956973075867, 0.29904496669769287, 0.2870573401451111, 0.371064156293869, 0.3148040473461151], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7e59718d4bc141e1118107d7725cb762(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.370017945766449, 0.3515392243862152, 0.10501354187726974, 0.4703279435634613, 0.1466074138879776, 0.35729703307151794, 0.48049694299697876, 0.18964934349060059, 0.3999618589878082, 0.08936327695846558, 0.27758416533470154, 0.4499010443687439, 0.1686873435974121, 0.10291910916566849, 0.12369296699762344, 0.05339512974023819, 0.1797252893447876, 0.05240679904818535, 0.38491514325141907, 0.07206897437572479, 0.15768174827098846, 0.4010673463344574, 0.4269670844078064, 0.38092947006225586], dtype='float32').reshape([24]),
            paddle.to_tensor([0.09263693541288376, 0.09670310467481613, 0.354386568069458, 0.09839422255754471, 0.2290104776620865, 0.3903789520263672, 0.2337942123413086, 0.43508490920066833, 0.26363542675971985, 0.3195001780986786, 0.16049844026565552, 0.16814041137695312, 0.21955221891403198, 0.24071986973285675, 0.40486425161361694, 0.19059769809246063, 0.15658031404018402, 0.4801381230354309, 0.38585934042930603, 0.23987680673599243, 0.42821192741394043, 0.37566664814949036, 0.30720242857933044, 0.08553310483694077], dtype='float32').reshape([24]),
            paddle.to_tensor([0.12862449884414673, 0.15161874890327454, 0.19711828231811523, 0.17109692096710205, 0.22508658468723297, 0.47765693068504333, 0.10415107011795044, 0.1430245041847229, 0.28769025206565857, 0.07464080303907394, 0.28259578347206116, 0.0667867511510849, 0.20640681684017181, 0.2804536819458008, 0.2877569794654846, 0.11138026416301727, 0.0908595398068428, 0.3717741370201111, 0.08971194177865982, 0.2700096368789673, 0.4871317148208618, 0.049827877432107925, 0.053140923380851746, 0.32696035504341125], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2092071920633316, 0.19860555231571198, 0.07658062130212784, 0.27994439005851746, 0.2087436318397522, 0.05877649411559105, 0.3807428181171417, 0.17638112604618073, 0.22986575961112976, 0.05372798070311546, 0.4495547115802765, 0.11967463046312332, 0.40127041935920715, 0.40827903151512146, 0.4119202792644501, 0.1700730174779892, 0.34114745259284973, 0.36083200573921204, 0.1619160771369934, 0.21851401031017303, 0.40062376856803894, 0.45815780758857727, 0.2413964867591858, 0.331148236989975], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8745f4aee6ced5040527e374e543918d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.27617937326431274, 0.33143675327301025, 0.005370850209146738, 0.30128079652786255, 0.20929963886737823, 0.4363754093647003, 0.3523562252521515, 0.44577574729919434, 0.004690611269325018, 0.19658984243869781, 0.3977589011192322, 0.29147517681121826, 0.22023338079452515, 0.29969272017478943, 0.06460549682378769, 0.06813565641641617, 0.08481722325086594, 0.15003663301467896], dtype='float32').reshape([18]),
            paddle.to_tensor([0.22475893795490265, 0.205235555768013, 0.3149069845676422, 0.268838107585907, 0.03561994060873985, 0.2343737632036209, 0.22207897901535034, 0.25980353355407715, 0.43811067938804626, 0.3810494840145111, 0.04620771110057831, 0.3972708582878113, 0.007334838155657053, 0.21385370194911957, 0.0433635376393795, 0.44000378251075745, 0.12699627876281738, 0.13525693118572235], dtype='float32').reshape([18]),
            paddle.to_tensor([0.20574884116649628, 0.34785231947898865, 0.049886807799339294, 0.2645573318004608, 0.19074448943138123, 0.06716408580541611, 0.02832474559545517, 0.4328599274158478, 0.1736193299293518, 0.20223407447338104, 0.14633555710315704, 0.08144912123680115, 0.40105512738227844, 0.13704217970371246, 0.26822662353515625, 0.08547381311655045, 0.28443026542663574, 0.1779901385307312], dtype='float32').reshape([18]),
            paddle.to_tensor([0.1013454869389534, 0.3135380744934082, 0.015952564775943756, 0.2859717905521393, 0.27459990978240967, 0.007391512393951416, 0.07516838610172272, 0.22756783664226532, 0.2998214364051819, 0.2500460743904114, 0.25530537962913513, 0.43482545018196106, 0.3284297287464142, 0.33380764722824097, 0.3893304169178009, 0.15334844589233398, 0.46571651101112366, 0.1738927960395813], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cf6a66ee145cf54bed4ddd309b353229(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ece40c89444b728eb73f1f8870fe3f72(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 31, 31], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a9ba7f3a60e57d9b2bf08702f093eece(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.05718138441443443, 0.39122629165649414, 0.08013688027858734, 0.43273869156837463, 0.20807428658008575, 0.03414080664515495, 0.11931239813566208, 0.2093530148267746, 0.014209595508873463, 0.4377251863479614, 0.2029404491186142, 0.21465842425823212, 0.018366234377026558, 0.16647638380527496, 0.1530904471874237, 0.3000648617744446, 0.40619614720344543, 0.04335281625390053, 0.33406251668930054, 0.2082383632659912, 0.33232906460762024, 0.31334373354911804, 0.4024452269077301, 0.3025133013725281], dtype='float32').reshape([24]),
            paddle.to_tensor([0.018956152722239494, 0.346476674079895, 0.36694976687431335, 0.3427880108356476, 0.3895837068557739, 0.437728613615036, 0.04310787096619606, 0.04476151242852211, 0.31748372316360474, 0.24472548067569733, 0.2361549288034439, 0.2530132234096527, 0.1387278139591217, 0.3955644965171814, 0.4308384656906128, 0.08095734566450119, 0.05018581449985504, 0.06985943019390106, 0.1617896407842636, 0.27981430292129517, 0.04369422793388367, 0.17362062633037567, 0.16565588116645813, 0.2053123265504837], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3306882977485657, 0.3507307171821594, 0.4144345223903656, 0.04978819936513901, 0.2484506219625473, 0.07973793894052505, 0.031753476709127426, 0.43349531292915344, 0.26026618480682373, 0.3837102949619293, 0.07870787382125854, 0.26306137442588806, 0.209665909409523, 0.3512316048145294, 0.39244577288627625, 0.21716512739658356, 0.18629051744937897, 0.4686649441719055, 0.2757360339164734, 0.1921900510787964, 0.47142255306243896, 0.3157888352870941, 0.46969014406204224, 0.38293108344078064], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4078306257724762, 0.02235247939825058, 0.0854964554309845, 0.327958345413208, 0.4319191873073578, 0.4606173634529114, 0.19631259143352509, 0.4891527593135834, 0.22053837776184082, 0.261773020029068, 0.09072409570217133, 0.4538058340549469, 0.1345313936471939, 0.3248348832130432, 0.03211451694369316, 0.040537670254707336, 0.34816253185272217, 0.4421176016330719, 0.4051562249660492, 0.18903817236423492, 0.3572579324245453, 0.2406974732875824, 0.40037819743156433, 0.11555727571249008], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4835f215365066180598d95934314873(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.15361613035202026, 0.04626777768135071, 0.21802781522274017, 0.21375218033790588, 0.2810066044330597, 0.292397677898407, 0.14909464120864868, 0.4826369881629944, 0.02920559048652649, 0.384348064661026, 0.3068808317184448, 0.4365445375442505, 0.42171281576156616, 0.4155443608760834, 0.21188445389270782, 0.246217280626297, 0.30503904819488525, 0.04666509851813316, 0.49316683411598206, 0.34977447986602783], dtype='float32').reshape([20]),
            paddle.to_tensor([0.33294612169265747, 0.2930332124233246, 0.014383404515683651, 0.43315666913986206, 0.05804898962378502, 0.4617963433265686, 0.17204752564430237, 0.13543042540550232, 0.13723519444465637, 0.3069029152393341, 0.37190404534339905, 0.06616602838039398, 0.30955150723457336, 0.3015839755535126, 0.2349964827299118, 0.12864230573177338, 0.017646020278334618, 0.03175894543528557, 0.4397567808628082, 0.37377670407295227], dtype='float32').reshape([20]),
            paddle.to_tensor([0.08131515979766846, 0.07579726725816727, 0.27062293887138367, 0.01287353690713644, 0.36548948287963867, 0.10123098641633987, 0.1396770030260086, 0.3408966660499573, 0.3850771188735962, 0.12452337145805359, 0.4177669286727905, 0.2105550616979599, 0.06822940707206726, 0.43666526675224304, 0.3188123404979706, 0.193433940410614, 0.42245545983314514, 0.4934382140636444, 0.31301149725914, 0.1426059901714325], dtype='float32').reshape([20]),
            paddle.to_tensor([0.1962813436985016, 0.0784488096833229, 0.3566800653934479, 0.45967891812324524, 0.14472365379333496, 0.09800637513399124, 0.136883944272995, 0.30220669507980347, 0.18986475467681885, 0.4037151634693146, 0.22601576149463654, 0.4614656865596771, 0.06399325281381607, 0.13456213474273682, 0.14625656604766846, 0.25710999965667725, 0.4597911834716797, 0.06429515033960342, 0.24887493252754211, 0.1124459058046341], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_43bf1d232ebdb1add472e5e804163d15(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_f563867af680a9ee1502aac2665534b4
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 2, 2], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_84d56c03033d820a1ad51f9e66b52bad(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2176172137260437, 0.363209992647171, 0.13060735166072845, 0.18562375009059906, 0.48801788687705994, 0.0031611602753400803, 0.11674261838197708, 0.34925082325935364, 0.06831350922584534, 0.2903958559036255, 0.06977187097072601, 0.019517522305250168, 0.2794342637062073, 0.20782217383384705, 0.3453490734100342, 0.3330771327018738, 0.4187190532684326, 0.4251404106616974], dtype='float32').reshape([18]),
            paddle.to_tensor([0.36029642820358276, 0.06374750286340714, 0.24377304315567017, 0.11805664002895355, 0.21505966782569885, 0.061166465282440186, 0.25069230794906616, 0.4136498272418976, 0.3440582752227783, 0.2915797531604767, 0.1352241039276123, 0.11673322319984436, 0.4717003405094147, 0.31942787766456604, 0.49386337399482727, 0.3455265164375305, 0.11084984987974167, 0.36669403314590454], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3208845853805542, 0.13277997076511383, 0.4313904345035553, 0.4418594241142273, 0.28856179118156433, 0.39165186882019043, 0.47607603669166565, 0.023114746436476707, 0.345430850982666, 0.36471790075302124, 0.3926560878753662, 0.01654365099966526, 0.06262286007404327, 0.13651511073112488, 0.04815387353301048, 0.2416485995054245, 0.34345200657844543, 0.46579185128211975], dtype='float32').reshape([18]),
            paddle.to_tensor([0.054289642721414566, 0.400870680809021, 0.07304088771343231, 0.018451698124408722, 0.29965323209762573, 0.2412867546081543, 0.3100171685218811, 0.29044878482818604, 0.14436045289039612, 0.3543185591697693, 0.4826163649559021, 0.2740587592124939, 0.44003748893737793, 0.06321389228105545, 0.4384590685367584, 0.37406519055366516, 0.47917118668556213, 0.32125717401504517], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_90ed975cdbbbaba69273997458e143db(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 1044, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1044], dtype='float32', min=0, max=0.5),
            paddle.uniform([1044], dtype='float32', min=0, max=0.5),
            paddle.uniform([1044], dtype='float32', min=0, max=0.5),
            paddle.uniform([1044], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dab6ef2f9b46a8556d56f05b9fe24bc4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_97e61c27ab9fc13ad7be75bef8fd4d31(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 640, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_083c01809d5cbf9e1ba653058167a6a8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_14824b40d756435e4c1db4ec1a5e43a3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.15263642370700836, 0.42952457070350647, 0.4848281443119049, 0.057161107659339905, 0.13874030113220215, 0.3711596429347992, 0.4168872833251953, 0.28574228286743164, 0.39607831835746765, 0.3173339366912842, 0.383038192987442, 0.19989493489265442, 0.3782835304737091, 0.23295384645462036, 0.3720239996910095, 0.35497918725013733, 0.3093591034412384, 0.009406417608261108, 0.41031503677368164, 0.22459298372268677, 0.25956839323043823, 0.08750059455633163, 0.21543511748313904, 0.39485204219818115], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2513081729412079, 0.1801169216632843, 0.17670731246471405, 0.44431692361831665, 0.30016782879829407, 0.4172929525375366, 0.21455764770507812, 0.4615234136581421, 0.09558585286140442, 0.07896331697702408, 0.007191285956650972, 0.4425552785396576, 0.4272306263446808, 0.2540055215358734, 0.3167640268802643, 0.4755077362060547, 0.05555304139852524, 0.29575785994529724, 0.24152407050132751, 0.49579447507858276, 0.25423792004585266, 0.3326232135295868, 0.1409877985715866, 0.37793147563934326], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4272802174091339, 0.4346573054790497, 0.06656006723642349, 0.14307788014411926, 0.4454118311405182, 0.2574120759963989, 0.2546030879020691, 0.31790006160736084, 0.44417521357536316, 0.2700127363204956, 0.04173439368605614, 0.11823300272226334, 0.4544254541397095, 0.14349250495433807, 0.4234054982662201, 0.018872201442718506, 0.4235785901546478, 0.420307457447052, 0.414766788482666, 0.04273714870214462, 0.008507915772497654, 0.09265531599521637, 0.3085230588912964, 0.14207783341407776], dtype='float32').reshape([24]),
            paddle.to_tensor([0.48899996280670166, 0.19614970684051514, 0.369899183511734, 0.22308191657066345, 0.27828505635261536, 0.2867506742477417, 0.030319377779960632, 0.35189589858055115, 0.278929740190506, 0.2268601506948471, 0.15809930860996246, 0.019533807411789894, 0.44590526819229126, 0.10713964700698853, 0.024017514660954475, 0.3774050176143646, 0.18120703101158142, 0.44417986273765564, 0.4147886633872986, 0.2624897360801697, 0.10804421454668045, 0.36634600162506104, 0.41030383110046387, 0.0885002389550209], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ef60b6570928728598baabedc482ae9a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.08589935302734375, 0.3722672462463379, 0.3425038754940033, 0.3029724955558777, 0.4510924816131592, 0.11664686352014542, 0.27777937054634094, 0.056052275002002716, 0.04358561336994171, 0.09856507182121277, 0.1953926384449005, 0.17467240989208221, 0.2174665778875351, 0.31312355399131775, 0.26388609409332275, 0.36136600375175476, 0.18478740751743317, 0.154459610581398], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2853325307369232, 0.34580036997795105, 0.27729639410972595, 0.09751240909099579, 0.48363181948661804, 0.06490335613489151, 0.3621772825717926, 0.3388826251029968, 0.3233813941478729, 0.42953112721443176, 0.24556311964988708, 0.14039765298366547, 0.18132075667381287, 0.25992244482040405, 0.10792665928602219, 0.2370431274175644, 0.15208160877227783, 0.27216750383377075], dtype='float32').reshape([18]),
            paddle.to_tensor([0.008281152695417404, 0.37589943408966064, 0.11815567314624786, 0.08357857167720795, 0.0356268510222435, 0.2528540790081024, 0.4388974606990814, 0.3537788987159729, 0.3930468261241913, 0.020467082038521767, 0.10100726038217545, 0.49081090092658997, 0.35932859778404236, 0.4092620611190796, 0.3818504214286804, 0.06616616249084473, 0.3232974112033844, 0.16401498019695282], dtype='float32').reshape([18]),
            paddle.to_tensor([0.0195851419121027, 0.4531116783618927, 0.17333410680294037, 0.06506562978029251, 0.2872266471385956, 0.1479274183511734, 0.19070185720920563, 0.09709187597036362, 0.427514910697937, 0.4161641597747803, 0.25565552711486816, 0.47682127356529236, 0.42808547616004944, 0.27636009454727173, 0.2153112143278122, 0.11375030875205994, 0.38185593485832214, 0.3976857662200928], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ac5047b20e910b889207dc0eb7236f99(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 120, 120], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4985247850418091, 0.1185322254896164, 0.4177471101284027, 0.43834611773490906, 0.46432486176490784, 0.04619839787483215, 0.3514931797981262, 0.09926982969045639, 0.37846407294273376, 0.39260804653167725, 0.011225792579352856, 0.3509501516819, 0.03367668390274048, 0.4542522430419922, 0.15336847305297852, 0.06863170862197876, 0.3038928210735321, 0.15946675837039948, 0.33603402972221375, 0.30933788418769836, 0.2661513090133667, 0.3159225583076477, 0.3059334456920624, 0.26452386379241943], dtype='float32').reshape([24]),
            paddle.to_tensor([0.12983159720897675, 0.07290066033601761, 0.09995181858539581, 0.2267037034034729, 0.07982015609741211, 0.4538978040218353, 0.04851071909070015, 0.34087052941322327, 0.07162949442863464, 0.08655156940221786, 0.2863326370716095, 0.3290863335132599, 0.23778873682022095, 0.487350732088089, 0.2221771776676178, 0.2089247703552246, 0.003208818379789591, 0.4861908555030823, 0.07276204228401184, 0.4132940173149109, 0.33806490898132324, 0.16270990669727325, 0.42181193828582764, 0.24551892280578613], dtype='float32').reshape([24]),
            paddle.to_tensor([0.35548996925354004, 0.22917209565639496, 0.4214314818382263, 0.061359986662864685, 0.2712291181087494, 0.05786227062344551, 0.004628999158740044, 0.4702821373939514, 0.3624049425125122, 0.07772145420312881, 0.34351834654808044, 0.13950957357883453, 0.24940736591815948, 0.40386471152305603, 0.45538559556007385, 0.3333359658718109, 0.42505502700805664, 0.4797585904598236, 0.2127990424633026, 0.19277557730674744, 0.3651147782802582, 0.06605132669210434, 0.05045609176158905, 0.3595210313796997], dtype='float32').reshape([24]),
            paddle.to_tensor([0.43755263090133667, 0.46890607476234436, 0.4964199960231781, 0.009878006763756275, 0.3638480603694916, 0.4375229477882385, 0.4939367473125458, 0.20524701476097107, 0.4722214639186859, 0.05262500420212746, 0.34569302201271057, 0.487074613571167, 0.35439926385879517, 0.07958396524190903, 0.14252127707004547, 0.04397926479578018, 0.14687474071979523, 0.38859376311302185, 0.2575731873512268, 0.03476594388484955, 0.4652351438999176, 0.11458606272935867, 0.010906857438385487, 0.23756590485572815], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_10a29028d65c7561ed33584155358ef0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 24, 24], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_49d748a857a7cff83e1bbff72257084e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 960, 32, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4dc2a88a64478ee50ce1959aae5b4a08(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.1351631134748459, 0.24706724286079407, 0.15245139598846436, 0.26901331543922424, 0.015805978327989578, 0.2713090777397156, 0.36605048179626465, 0.2887127697467804], dtype='float32').reshape([8]),
            paddle.to_tensor([0.2873467206954956, 0.45044878125190735, 0.4959665834903717, 0.48272958397865295, 0.4766245484352112, 0.2734883427619934, 0.30054888129234314, 0.08597763627767563], dtype='float32').reshape([8]),
            paddle.to_tensor([0.05697038769721985, 0.3775826692581177, 0.13877664506435394, 0.22523987293243408, 0.2994493544101715, 0.04214458540081978, 0.46678784489631653, 0.4078289866447449], dtype='float32').reshape([8]),
            paddle.to_tensor([0.10735016316175461, 0.275422602891922, 0.3193312883377075, 0.23950016498565674, 0.3186553120613098, 0.48698702454566956, 0.08590719848871231, 0.05088891088962555], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_42088c8d66de60ccd9847fbb04a8af27(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.38421785831451416, 0.49233531951904297, 0.4699110686779022, 0.3747326731681824, 0.2617549002170563, 0.23820553719997406, 0.07261112332344055, 0.4887304902076721, 0.06595171988010406, 0.23573090136051178, 0.06442786753177643, 0.26564091444015503], dtype='float32').reshape([12]),
            paddle.to_tensor([0.47807008028030396, 0.2776629328727722, 0.16754083335399628, 0.02813914604485035, 0.46087172627449036, 0.4068272113800049, 0.2665857672691345, 0.29737240076065063, 0.4665673077106476, 0.45638489723205566, 0.17780672013759613, 0.09237784892320633], dtype='float32').reshape([12]),
            paddle.to_tensor([0.05892206355929375, 0.09157782047986984, 0.16665364801883698, 0.1972789466381073, 0.04883917421102524, 0.11043696105480194, 0.4463222026824951, 0.08168816566467285, 0.213820219039917, 0.48806166648864746, 0.229207843542099, 0.2880784273147583], dtype='float32').reshape([12]),
            paddle.to_tensor([0.41529110074043274, 0.08918095380067825, 0.118984155356884, 0.07645975798368454, 0.2858685851097107, 0.34746289253234863, 0.4595067501068115, 0.11426994204521179, 0.014340235851705074, 0.27817869186401367, 0.2559531629085541, 0.1339949518442154], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_61a95b6991993279397e2f52834b7dba(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 4, 256, 512], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.292324960231781, 0.27809378504753113, 0.16747350990772247, 0.37379327416419983], dtype='float32').reshape([4]),
            paddle.to_tensor([0.24977649748325348, 0.08845029771327972, 0.12591645121574402, 0.034950368106365204], dtype='float32').reshape([4]),
            paddle.to_tensor([0.23067085444927216, 0.046508532017469406, 0.125669464468956, 0.4288064241409302], dtype='float32').reshape([4]),
            paddle.to_tensor([0.36575621366500854, 0.037453558295965195, 0.03913281112909317, 0.31405967473983765], dtype='float32').reshape([4]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_28ffba4045b11b5fbaf8d6ce4ef971e7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.42104411125183105, 0.4952838122844696, 0.28063225746154785, 0.07470646500587463, 0.32615038752555847, 0.040464021265506744, 0.07846037298440933, 0.34389638900756836, 0.37970170378685, 0.1425851285457611, 0.38738518953323364, 0.4973982274532318, 0.34917399287223816, 0.2759864926338196, 0.34695279598236084, 0.3537884056568146, 0.402783066034317, 0.17039597034454346], dtype='float32').reshape([18]),
            paddle.to_tensor([0.303667813539505, 0.2669636011123657, 0.031813111156225204, 0.02952342852950096, 0.05278587341308594, 0.44472676515579224, 0.41204941272735596, 0.3057861626148224, 0.29577526450157166, 0.47838982939720154, 0.39619699120521545, 0.08593747764825821, 0.09162028133869171, 0.11702263355255127, 0.032720908522605896, 0.07847119122743607, 0.47664353251457214, 0.4961246848106384], dtype='float32').reshape([18]),
            paddle.to_tensor([0.19777502119541168, 0.2389424592256546, 0.4765344262123108, 0.06943992525339127, 0.4227600395679474, 0.14288383722305298, 0.4153275787830353, 0.23848958313465118, 0.27650997042655945, 0.4939560890197754, 0.48212555050849915, 0.3591154217720032, 0.2812785804271698, 0.4552029073238373, 0.43141263723373413, 0.4602172374725342, 0.10682427138090134, 0.16670475900173187], dtype='float32').reshape([18]),
            paddle.to_tensor([0.34879258275032043, 0.14840249717235565, 0.07797698676586151, 0.3330880403518677, 0.16394786536693573, 0.4513413906097412, 0.3143025040626526, 0.06054028496146202, 0.35890960693359375, 0.4090479910373688, 0.06411580741405487, 0.266603022813797, 0.0007769876974634826, 0.34625333547592163, 0.12741906940937042, 0.3636993169784546, 0.17403848469257355, 0.46372348070144653], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d9a9996ab6bd73db2dbeb5d57bc27437(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3247254192829132, 0.30188795924186707, 0.4988363981246948, 0.03727499395608902, 0.11740756034851074, 0.371415376663208, 0.27315253019332886, 0.31556132435798645, 0.4056639075279236, 0.2830328941345215, 0.47580474615097046, 0.1287907361984253, 0.19019095599651337, 0.17393611371517181, 0.3274492621421814, 0.3976007103919983, 0.2936316430568695, 0.24969562888145447, 0.47394701838493347, 0.38735511898994446, 0.1259085088968277, 0.08045624196529388, 0.1020800992846489, 0.40627720952033997, 0.13518904149532318, 0.3666890561580658, 0.2119814157485962, 0.14946642518043518, 0.04899347200989723, 0.21584349870681763], dtype='float32').reshape([30]),
            paddle.to_tensor([0.0181763656437397, 0.11845342069864273, 0.4639076292514801, 0.2207445353269577, 0.13873638212680817, 0.21583642065525055, 0.17500054836273193, 0.39962154626846313, 0.23408760130405426, 0.027591833844780922, 0.0828780084848404, 0.14652109146118164, 0.39326366782188416, 0.3147282004356384, 0.4616442322731018, 0.28886452317237854, 0.0003794555668719113, 0.23173025250434875, 0.018561456352472305, 0.3763425052165985, 0.05656234174966812, 0.11849343776702881, 0.38450226187705994, 0.15457554161548615, 0.19432853162288666, 0.2913629710674286, 0.22849203646183014, 0.03684913367033005, 0.08432599157094955, 0.22667379677295685], dtype='float32').reshape([30]),
            paddle.to_tensor([0.16711680591106415, 0.09641019999980927, 0.0890788808465004, 0.28550252318382263, 0.27474138140678406, 0.05131318047642708, 0.4693432152271271, 0.1664205640554428, 0.010821731761097908, 0.30284619331359863, 0.29703813791275024, 0.3896382451057434, 0.30339863896369934, 0.4224218726158142, 0.4994892179965973, 0.08716219663619995, 0.3711755871772766, 0.428718239068985, 0.38304629921913147, 0.332973450422287, 0.40395981073379517, 0.034404415637254715, 0.25685545802116394, 0.40204116702079773, 0.12324642390012741, 0.35539761185646057, 0.006545349024236202, 0.2638784945011139, 0.36433181166648865, 0.27358928322792053], dtype='float32').reshape([30]),
            paddle.to_tensor([0.24597902595996857, 0.4725562632083893, 0.1449650228023529, 0.4989321827888489, 0.06073959916830063, 0.4970291256904602, 0.20961782336235046, 0.0406181663274765, 0.08260422945022583, 0.4696732759475708, 0.11021271347999573, 0.48297640681266785, 0.4613170921802521, 0.07010713219642639, 0.27924588322639465, 0.46894052624702454, 0.00443302234634757, 0.13793706893920898, 0.2410225123167038, 0.37688180804252625, 0.3024172782897949, 0.11537888646125793, 0.2997547388076782, 0.28840163350105286, 0.46058571338653564, 0.12997184693813324, 0.034383755177259445, 0.49542003870010376, 0.1988268494606018, 0.46832016110420227], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3bd415014d8edd2aad2470cb34c8717b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 208, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_68400ec55334d2f957e3de2305873818(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.043257735669612885, 0.16976286470890045, 0.462146520614624, 0.04404108598828316, 0.08072153478860855, 0.2775175869464874, 0.05112399905920029, 0.14553236961364746, 0.28622421622276306, 0.362289160490036, 0.2548457086086273, 0.23166337609291077, 0.06977226585149765, 0.3847857415676117, 0.38890278339385986, 0.42720890045166016, 0.19462203979492188, 0.07775553315877914, 0.4808318316936493, 0.08798105269670486, 0.152518630027771, 0.2672223150730133, 0.04860023036599159, 0.2572932541370392, 0.1781669557094574, 0.30906158685684204, 0.23516324162483215, 0.3756459951400757, 0.47416266798973083, 0.24927009642124176], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4433979392051697, 0.30507731437683105, 0.11392027139663696, 0.2348099648952484, 0.283876895904541, 0.39031049609184265, 0.209075927734375, 0.06763026118278503, 0.49989163875579834, 0.0378885418176651, 0.19109401106834412, 0.05969110503792763, 0.02590988762676716, 0.1010553240776062, 0.27286097407341003, 0.3497874140739441, 0.14444048702716827, 0.09371858090162277, 0.4958146810531616, 0.39771342277526855, 0.13751518726348877, 0.22088241577148438, 0.02542903460562229, 0.4287279546260834, 0.1415145844221115, 0.4811260998249054, 0.13439464569091797, 0.16886211931705475, 0.05969061702489853, 0.43030571937561035], dtype='float32').reshape([30]),
            paddle.to_tensor([0.05527319759130478, 0.17700828611850739, 0.11066260188817978, 0.4570172131061554, 0.27734890580177307, 0.1443411409854889, 0.22823385894298553, 0.026728777214884758, 0.030345335602760315, 0.06669163703918457, 0.38459905982017517, 0.4231535792350769, 0.282678484916687, 0.0965161919593811, 0.20420971512794495, 0.032661762088537216, 0.3648344874382019, 0.4001738131046295, 0.21585191786289215, 0.09231392294168472, 0.22694101929664612, 0.3839384913444519, 0.3962491452693939, 0.33574599027633667, 0.13793829083442688, 0.44442811608314514, 0.28939002752304077, 0.27946943044662476, 0.17093397676944733, 0.3820123076438904], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2833215296268463, 0.12230470031499863, 0.004682149272412062, 0.40175098180770874, 0.12830939888954163, 0.18243800103664398, 0.42114657163619995, 0.0501244030892849, 0.03962255269289017, 0.27050209045410156, 0.0639030933380127, 0.19837374985218048, 0.1680150181055069, 0.2983115613460541, 0.16045014560222626, 0.1130414828658104, 0.47964105010032654, 0.06258411705493927, 0.4945563077926636, 0.14586478471755981, 0.006153411231935024, 0.40440717339515686, 0.40138161182403564, 0.061130277812480927, 0.16464467346668243, 0.008586596697568893, 0.40954887866973877, 0.40734580159187317, 0.27061352133750916, 0.4906522035598755], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_062e08c815e31ca90caeb791206d17f4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 24, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_297931ecb0936dc62b458595bd6efe54(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([1, 102, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([102], dtype='float32', min=0, max=0.5),
            paddle.uniform([102], dtype='float32', min=0, max=0.5),
            paddle.uniform([102], dtype='float32', min=0, max=0.5),
            paddle.uniform([102], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_07466e7035356ecc4f56ec248b7fac67(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1d993d813fe53799d6a745c044502b8b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 120, 120], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_120217dc3356768026e55cd8c1a1255b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3320964574813843, 0.22810085117816925, 0.02201037108898163, 0.3983711898326874, 0.4617225229740143, 0.29461073875427246, 0.1240522488951683, 0.23890407383441925, 0.33328089118003845, 0.21557891368865967, 0.09728049486875534, 0.004911532625555992, 0.2526858448982239, 0.02763277292251587, 0.2928253412246704, 0.4930940270423889, 0.44076740741729736, 0.2707681655883789, 0.20847776532173157, 0.2984605133533478], dtype='float32').reshape([20]),
            paddle.to_tensor([0.1390441656112671, 0.1765548586845398, 0.46661853790283203, 0.3064592778682709, 0.24687975645065308, 0.007006834261119366, 0.4129047691822052, 0.0777476504445076, 0.3628873825073242, 0.07890837639570236, 0.10956505686044693, 0.15109214186668396, 0.09460974484682083, 0.382159948348999, 0.13675688207149506, 0.4442286491394043, 0.41951486468315125, 0.40989238023757935, 0.17044566571712494, 0.19254593551158905], dtype='float32').reshape([20]),
            paddle.to_tensor([0.17384548485279083, 0.3017316162586212, 0.19874964654445648, 0.3824807107448578, 0.16236044466495514, 0.25985437631607056, 0.34759286046028137, 0.19065335392951965, 0.41404202580451965, 0.15573279559612274, 0.17175647616386414, 0.2003815919160843, 0.30800095200538635, 0.038280002772808075, 0.32248347997665405, 0.20326775312423706, 0.09268881380558014, 0.29477548599243164, 0.10507392883300781, 0.41675469279289246], dtype='float32').reshape([20]),
            paddle.to_tensor([0.2680684030056, 0.3776101768016815, 0.2629605233669281, 0.15975242853164673, 0.4532463848590851, 0.2163974940776825, 0.01972535252571106, 0.13189657032489777, 0.3934813141822815, 0.4348861277103424, 0.3715344965457916, 0.42130613327026367, 0.10009981691837311, 0.0823485478758812, 0.26716309785842896, 0.15917125344276428, 0.341778427362442, 0.23565655946731567, 0.42899289727211, 0.42366525530815125], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6ed8f3968b5858866125199e7d5a6721(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_e299f7de2c7b0059d9ef9d5dbea67818
    def get_inputs(self):
        return [
            paddle.uniform([8, 64, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_44d384aef6769e329a9937e6dbaa5203(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a8c26641e88acbb5b9772c6dcd4eac92(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_ffe6db27f65675d4da5719adfa845451
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 480, 480], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4587131142616272, 0.06913690268993378, 0.28051698207855225, 0.03159644082188606, 0.09798169136047363, 0.21112698316574097, 0.1529124528169632, 0.30281785130500793, 0.1464276760816574, 0.1289004385471344, 0.18832381069660187, 0.2859799563884735, 0.421872079372406, 0.4250953495502472, 0.3457820415496826, 0.17965997755527496], dtype='float32').reshape([16]),
            paddle.to_tensor([0.09270536154508591, 0.010809263214468956, 0.29873883724212646, 0.28666359186172485, 0.28624123334884644, 0.04391380026936531, 0.07624668627977371, 0.2040114551782608, 0.026804009452462196, 0.3283340632915497, 0.44990384578704834, 0.19199132919311523, 0.30561208724975586, 0.2851046919822693, 0.47255104780197144, 0.38854044675827026], dtype='float32').reshape([16]),
            paddle.to_tensor([0.31881389021873474, 0.09299047291278839, 0.04910462349653244, 0.13205409049987793, 0.32719963788986206, 0.31612905859947205, 0.366342693567276, 0.2571426331996918, 0.41879621148109436, 0.2524104416370392, 0.044244565069675446, 0.32961609959602356, 0.28023916482925415, 0.3115490972995758, 0.3127271831035614, 0.39775633811950684], dtype='float32').reshape([16]),
            paddle.to_tensor([0.392035573720932, 0.1698167771100998, 0.36876481771469116, 0.4582176208496094, 0.49132710695266724, 0.3802676796913147, 0.23344311118125916, 0.2016393542289734, 0.3067430853843689, 0.029044706374406815, 0.4782528579235077, 0.2554386854171753, 0.24520142376422882, 0.18605351448059082, 0.4004283547401428, 0.18603681027889252], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_45ce6c365e16f1c16693617511aa700d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_963f9b1b6797756d1f0ce17787b3924f
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_89a66e26fd8dc2c230f6641fb8ab21da(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 1632, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1632], dtype='float32', min=0, max=0.5),
            paddle.uniform([1632], dtype='float32', min=0, max=0.5),
            paddle.uniform([1632], dtype='float32', min=0, max=0.5),
            paddle.uniform([1632], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_58007b1b2055ac0ed9451afa43a0c39d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cb953705f3de8a2fae64a9b903f9f0ba
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.30197733640670776, 0.3017837703227997, 0.13374149799346924, 0.004965309984982014, 0.42568445205688477, 0.036463260650634766, 0.4492867588996887, 0.2793543040752411, 0.020033152773976326, 0.30960384011268616, 0.49148231744766235, 0.16786986589431763, 0.08852798491716385, 0.34350088238716125, 0.47203126549720764, 0.14556971192359924, 0.10242896527051926, 0.3298778831958771, 0.47566187381744385, 0.48830947279930115, 0.09967558085918427, 0.37533846497535706, 0.14687852561473846, 0.20701169967651367, 0.06238021329045296, 0.23961730301380157, 0.3580165505409241, 0.04826691001653671, 0.29166555404663086, 0.15164372324943542], dtype='float32').reshape([30]),
            paddle.to_tensor([0.38306352496147156, 0.4048800468444824, 0.15580050647258759, 0.21469125151634216, 0.2738591730594635, 0.21754615008831024, 0.2803274691104889, 0.2541767358779907, 0.3750326335430145, 0.23142771422863007, 0.023002641275525093, 0.2706369161605835, 0.25450676679611206, 0.3514459431171417, 0.24980990588665009, 0.0849657878279686, 0.22476980090141296, 0.18422165513038635, 0.18344783782958984, 0.09244702011346817, 0.48457518219947815, 0.17026075720787048, 0.13398073613643646, 0.31915533542633057, 0.006234819069504738, 0.3301388621330261, 0.10953669250011444, 0.3694423735141754, 0.026694785803556442, 0.14521333575248718], dtype='float32').reshape([30]),
            paddle.to_tensor([0.011358612217009068, 0.10820265859365463, 0.22271513938903809, 0.34521228075027466, 0.08938224613666534, 0.3587585389614105, 0.27783316373825073, 0.38698309659957886, 0.2556461989879608, 0.30727601051330566, 0.36640554666519165, 0.4340045154094696, 0.47901469469070435, 0.028464606031775475, 0.19527453184127808, 0.2867242395877838, 0.4046407639980316, 0.21984004974365234, 0.3510229289531708, 0.08111130446195602, 0.256378173828125, 0.3026692271232605, 0.2989003658294678, 0.4022776186466217, 0.45594683289527893, 0.49001559615135193, 0.01082339696586132, 0.26948943734169006, 0.039324529469013214, 0.299130380153656], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4379531741142273, 0.3338795304298401, 0.40518486499786377, 0.15753060579299927, 0.482328861951828, 0.2017510086297989, 0.31937921047210693, 0.10305673629045486, 0.14228318631649017, 0.07317134737968445, 0.49272963404655457, 0.18620459735393524, 0.15285025537014008, 0.09219857305288315, 0.008607625029981136, 0.45940178632736206, 0.4984169602394104, 0.009620344266295433, 0.029259774833917618, 0.010020670481026173, 0.2123871147632599, 0.2970297038555145, 0.17833702266216278, 0.4364016354084015, 0.2957473397254944, 0.48839646577835083, 0.39682191610336304, 0.3848196864128113, 0.222023606300354, 0.0036678898613899946], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_41db80710f2ae2a2a8c51c3a8d5acbd2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_190f5489288e3db92fdca6b9490f8160
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 4, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_9dbc09a9a0e76db524f3f84b5ac5f886(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_64886fe83824e5b052b03ba6d79380c3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-06'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bbc81edc245de845d262a89353925f09(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 14, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3439593813df58ab84d5d11a8f3bc70f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 35, 35], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b70e18568d05cd1e2af869b3ae7c808e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_941e431545d3e7c4ca3dc8d90500440e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_165e4da143e7c7668a53c2561845486a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_894042905cecc8d21d6fa93d19df4609(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 18, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e479eb000ecb377412535ceefe642a1e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_475d86b4e9facfaa4be31cf8d1950010(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9c0284886d49f7a74821cbdee8f6d4eb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_db13cf9aff90f090a981a700350297ad(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0dd6b3913d8e822694927a8b582de3c3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_805497baea5436ab89ee46c52432f80b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2fc2a94b0efcc3dc8050f64d6378a46e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7fdcc0eeb232afc4f0e6c41bda4a9b79(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4dc01af07d76b20274a31f62299f9de9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3c089193f1caf8aea4a16bec01521c86(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 8, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_74f2cb575bf2c6b47ca0d0067863db14(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b280631c8f4c7dd6b97e271a7a4cdfde(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_516c4be51f91d98dc42ee88f9faef4eb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f685baa66a0476741483143a130047d0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 28, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_704ba2e36bd257628fb76c552b1f60ef(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f8181cc286ce1ef7b9a696dde7bc6532(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7bc2d2b02a8c7bae9d36efb832276eff(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b48573eff67644076411e1b5b972d657(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 16, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4e01f576ff9b5b61b3fbaa2b12058c61(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ed9212f3f866df415fb40f6ae119eb2a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 8, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_60dcc2cd5a649bb465693bfee0b6d4e9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2e653191cee4c256f950bf625519cf27(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 16, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d51df6962bf2057ba020b60aa289dadc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3715b532e180e58e70fb3ad29ae1b85d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d5e242dd858f4df618e388e1cc167916(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_75800fb555e7e3785b8fba1323dcb246(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_17ff8bb074e7c4adb32c98c744547862(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_41ed66cbff549f51be5e23c37055c367(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_368029c6fbe24b61e29e167a65f9e2f8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dc3d007e4f20690fbc1800f5d84104d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 4, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_be89627fa78ee23e9fcd8b750e5ec82f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 26, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_12d4ac16b5bf1b926abdb9eb8e1e12e2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_69e7d8342669d2942f4393fc928aa100(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 20, 32, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_34236f335f51085e45a8f00b45a61715(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_04c3aae3441adb0d1910cab756d8e702(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_88c7f2d8ae60a395af4995fe8c3de96b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c48a099314688d45ce9484a50c6e2ab6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 28, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d46b1ecb024f379094de5dc11a137e75(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 128, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_609a57e99345b93f82775f43b676be5b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9aee53c7225546d5592242ed5fa6ee7c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ed1a0724b4c7788b95d275f6b30aa644(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6678937eb23e340026d38bce8fd72aef(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3422685b6dfb9daf41cca6ba7fe62ef4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fd39d743a896186d779275b2d391a936(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a7cb51864cad5fcc886421f8ec347f00(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 75, 75], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b073a7d84a61c843e8beed9d56f287d2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_db078ff44325e958a6de99fa2d650459(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3610d3bade6fe2925189977657bc5a79(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_59b3e74281e90097c473b3e9f61aa3f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3d6a43dd2c5cac41a33d42d476dc06be(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b666bab61e5bc53358ac0087e76451ef(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 32, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d0e8cb2e531d29af36554b1539a2ed62(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_42f7879036284601ac75cba2ace72268(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8f3028d468082801b8b256577bed9820(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3b171158e20cdcbba3ec1fa824375203(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ed827675d4efb82598986591d39ae789(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2983a4410ed04cd1e288c430e9f4b68e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d719c184f8a1bc91fa3530818ccc600e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d7efd4e8cc86dbcf10dc0059ef5b18a4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_17ca7ac10fa8f0f1fd626ce9d8a3479c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1584, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_125c4429c41dc950e15b089a228bf7b9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bf5d0bc99f60faaeb40a411b181eb8bf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_60c6ea4d1779a2cfc29b4976a4a8ef4c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6a0354b9de8c201019e1a6ac4deb3a5f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_50a08de42e3d952ca8cff006956516ca(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, None], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c1c1d5d55d94341fb4ebd8944c80a149(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5b9b32321f35be2818c960ee8511969b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 81, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8935287c4921056af5c99c31d8f54142(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9794aa6c4a1dc20924b9a412a25bd08e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0b047824ec8230c2622b1a03a182074d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2b056b3f7ce51f080635d7ddacbaf689(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9253a3dd2e1412c21d5e7e4be6b68f67(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7443c63d49e7aa290c0439b7a9ae83d0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_610a674c984ed56e7e57d3c31168d3e5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a7ff3720d5dc00fb01b82b470858fdf8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 26], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_04faa3b7b0db6b5f1ca9595f18d4c8f7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8abe7085c58f46e71e336ef6f9189a52(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3419e384acb8198ee16a9ea99a6df966(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_83a579f57939e545864e31f3fc19c15c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1eb915ee608828a12483d15f416cb408(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_79440f138e0689c09304f44d07147118(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_af6b918908f742ed3ec81e1ff5ba2a1a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eeece03e7ce1e23fdfd701204a5204b0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 20, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c269520eb3e7cfca50033d82e031311d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b230709489f2305d34369454284cf2d6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 35, 35], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8c80415970f1251392c079279a9edbd4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b102249cc3f9c1a682da123ac4c769e5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c926f7eb62614b8dcae5dd19dde1a643(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_79d0f5f1242418b50f8a378c99dcf1fb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_21e7edf70dff19aada43a985fd361793(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_56eaf9bbeafc531fb339e434f6e1a1ac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5728380766de4cc46eb84b3663d9c643(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b6d17e16e19fa446bc9f3fd4686b4ea4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_78cedaafd0bb13a1df551d99cbc20c4c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_97a71f6fdd2096888f132c1703f4febe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bafef94c4d4645b8202dd00740ff54c6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_94330ca2e992a65c769c24cddf46a941(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9549d2d40ae7c005a31a0e3ba5f852de(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1120, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d32c97e6beec8ee3a9f7559f577ff551(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8f43ba088ffb921774380b6a0fd9927a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 728, 19, 19], dtype='float16'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f7fff14d71fde1aaac01aed547d4c930(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_369e57addbbf0e0c59501717d1fa9f59(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 38, 38], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_746c40c13fc148ecb767483e4fc368f1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_46ef318388ad29bfe327f15c95b17630(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_362b6d039046fd4e8f71c100136dad6d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d6aca269f977fedba58cc99c0216449a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7c8e3eaf9d5f89529f7f34dc6e573b30(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1ea4dceebc7a5a2db42bbd8122cbd1ad(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_627088a1e3637e072029739fa1f54c23(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 175, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b78029c0ad6f623ef011ba0065e07e43(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 38, 38], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2a328c811ba286a8f8ac65f8c6872931(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_37e992038224c6c946600e054fc8d79c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7af1b62048bfec15b58c218ca937dded(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e38d69da962b46457d00cc006f16a4d3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_85162d3ee71a92e1e411b57237971c8a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_98fa15be0d871ea97caee144696ccb7d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d9ac8ef6d698417a23275ceb4f72f548(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_52a1fb463eab11dde283c57008fed323(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6d30f68e1f4959530180286a04c221f8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b72e1819420aa31f4d5dee2278c58d72(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 2, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b0c5ba9af71d4d4a262966d41e9033ea(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 26], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8eb70020e00543575c7c326bcb7248ae(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9cdceca5a1e372138f6411b5b0101415(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a2806a4c95452de7e21f8ad5c4f82069(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 972, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f283fe5dcf638f2b67be7ae7925f4c3f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b94d27bcc765abee48922649c87f4c9f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 416, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8c5b39d2a3484fbbc48b36205ab359b3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ed12134a0cc46712d65a740bd5d19b42(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 800, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_33b6efcd1f0859ded9a4c4f54014f354(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b00b71c1e09083b3b1aa95ce5909fc04(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_21caa81ad3e2f781d23421f35ed42d69(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 2, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9ef02be32df299c3bf67c478b1e1059f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d8b1a9ad5b983a344b13cbc65c009b94(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9deb801ad72d1d56b2c94d1920b5e28d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 432, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6e7d30ce04790beaa2998429bfd4be9f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 75, 75], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7472ed2992de551c9526288cf5008255(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_12115e39d999f041142c7b3263086028(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d3ee19dd42d201f91fe4d8302403c308(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fd9379adcb05d6289d55952652c39b82(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1632, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_92b0326326a3c29ec8517a166a727456(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0bf8f8d6d4e43fb540d56c11e67beadc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e64d59a28811ae09ab0b5c1bbc1462f9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6dd04bcce93676be8737abfe499f1ad8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 88, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_00fe9dd0579e7152da78b5df9b6bcade(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7481dcb60b8794aa208503857b0de6ec(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9445b627104ff7404d95e235986e2de8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 4, 512, 1024], dtype='float16'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_794582ed12dd1db0b8fa2836072b29d5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 32, 128, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bcd6d54de24ff041cc1c8ea3dad5907d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_566908dc3f95fff9d2cbb66d8da31333(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 32, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_15f4f81c084ae28f93bca162c0bd6aee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c8de50bba8c0f57e30ffce7fde5eae38(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_df91eca2a501ca3c58bb8807a236233c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 17, 17], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_850f3e091e8d651b0716d02d816a1b01(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a4ff88022df63fcf1c546c0b6f25c478(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8e57a18a8612132973d593e4b7bec5b5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e735fbd6a7e6676e35e964d7317a923e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_907f6db52eafea2ece16e0d8b960cbfd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 320, 320], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b554ff02cd36fb94a644d1e4a07dabdf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c6b5eeb6973d778857ccbcf004d0771a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, None], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_704e6371d677aded522efbd07360165f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_38bb888c47ce5f11b440455f67221fe9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2f4cf6cd0284436c6428eca3d463ba24(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c83c78b8041bad1c68c3005c6a63315a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5ded9d217089eb5553792671070f9146(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a7ee99be552d1512755b74f8e18aab95(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9177e7d018602997d2c75e6810ca860b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 350, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_db1646161932f584a52d5cda4cf04e6a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ce79d83843acd3e8dc6414c791f2484b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dc1e0a4e6a545fc4b3a0625f3e1feaba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 24, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b4c93d25e954270892d0ab6ab07a59b0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f01d06036c8619197d34a0b85b451f72(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 18, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5244b795e4f432146d7c07352e7cb3ea(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_39ba8b4f9402442b05959c5f37423c85(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0206c3a2d0d5c7c9fedfa9999adcf4d7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bef52405d01b0752c8e841c491af8b8b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 350, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cef811bee1dfd3b07a77ea21ae621bdf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b46ca35fb93c80444647e857dd758e5f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_641ece90fd4ce2f94ab2b6410a4b6e5a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2148ecdb467f5ec39cedac09a7d62f88(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_df4df54404f76f4c1b3dfe9c9ab71726(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_25426fa1edf42420b5aac6da9373a844(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f10eb36bf288a1ecfd12b23f5e019b5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 38, 38], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d049e35e0fa4aac5e6bda635ddb18cb4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6047af449351e0ae6c4e4445bedfbfa9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 20, 32, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c8e8b98d5451a209db460ddb051b0754(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 76, 76], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f337be7e3f21a3200cb2535daba3d57c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0dccad230652d24b357703d6ef9ea62c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 88, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5e5f72ba66cd492fdd0eb79e0604870e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c0d48ba769e212958d8858afce1939b9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a54f1400c5f3a96c1a42e69396d04eb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 32, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_af6ce7be6d26a8bb68f0e13ac8b7e453(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_18649b218000e0d5e5430f96f26a7d86(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_50a0e7a344b3d28bf5771d8801bbea00(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c4db268fb1b4f1d58ba1b43e125a09c4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 244, 244], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e3a7233d67ff55254fa5c1e32dc63816(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ec3a54a59cd6bcd6848240698b412122(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f23f1278b4dbb848f82f9579d459a0e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b7244ad91132091cd221663678de5af7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d47237c38df099bd4681249f36a9f301(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2df97d4727ae19a9bdf83d9a3d7b2d9a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6efd7aa298f14c401d2ee4c775faf554(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 4, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f117c9da0ff074b6dca94ee2f3b46f45(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_06fd872f0eaac81839c1dee09935bd24(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 16, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_307f19644334105609c2cf21d1ed0142(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_718356bd05aadbe1fe0e3139a0199774(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f604b005f0dec0f4e39fa07e1aeaf52e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7f5f298e73f32b3c00cf47e381c5a898(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_16d3da922b6f9618e0aa7f5735b9fd74(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, None], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_43f5e4298e7d33b0a95a30f7ede6fcf2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_54e396f15b6c589ad806265f4fabcfe3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_83c2076762e3f0b5ad42edbca8949f80(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_594b0b12f2c9f686a474136a557a4680(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 200, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4ed2b5e02fc13dc44862013b33045ed9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_11829a4dcb5cfb24083cc762727b5cf5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cff457e7b4053aa56cec25aacd7c33e6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1d2fd368b9b0079aae92756be83e5eb4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 28, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3d88f5bafa273d7460bab69854b74e2f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 24, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_57e6e3475aa737575490d67060615281(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a2d6f6db5649e5d88e1cc5770afbb394(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_46c658bf9c053cde6cd50e8249bac2ad(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a74f4330502375b59044745d68af843(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_519a7c0f6adee07e36765c2ca8751abc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0a0c188fc21843e024a6befb52ba35d2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4d2accb888c5c659ef6a778f01c19e08(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dd6fb2c3c882600cf58e6b9c4ec11f7c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bbf202161f0ed6c705c3cdc53e88be04(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d9375c15407930981f3def3f3fddb861(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 12, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_89e6c4d9018e25a7e949002bc2d7026d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_60a5c30071bf2e80afd00b6d6aef40cf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b74ac190e24ca365b8c08b3513782c99(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1344, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_32cf2aa13af31ab4d172e5bad066d6d8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c450803181dcd86ea181e938712361cd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 35, 35], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f68ded36e04e78e177333a55d654635(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_63430943cebcf70b02ad5b607f103e8c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ecd64bd9619144a0d696c02836052b72(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_24cccdee768cf177758af6ba468f189d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ac8a42a19a5c3ff8edc0aadc46b704c1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1b832100a02850859398bd3c18bc2101(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3c0737d2e021ac53703a2f1f996370d7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 8, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8f0345c72eb0e0bbb120617602f06d2e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d680f3b6826ffa596dc56d69065da542(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_98c1afa28aa437381db6fd802511b575(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_817adb7e82a225f412a55174f124b2c9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 4, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a0fe3be5004ad53cc4ea850e0f95103c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_400eee4cb24449243d15b4b1d2a04360(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e5ee2620f343634b4712e64cf0a66f4c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_90c82aa784ac2eaba6eec7b13dd9991b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_36bffb1ba72eccb6101ccf70fd7138f3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9269df08bb8bbea719c68b317f48e547(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 61, 61], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b7c391d1ec2236df29311d9953185f6b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 6, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6e4ab1522d4928243ec8c008a9957e2a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_436d3a81b619cf620942d36f5cc36ba3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 52, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2f9eace2903929ae6a5b50b371ac3e22(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_559c583a96561ba9c7e95d32ed84a8b8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_69fd28825e8e607047415f4f425e2e12(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7c338692b03a63406e8508a74552305b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f085f4c0ed68210003ecc20b5f639cc9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4078175824aeb9d03e18e5c04fd3b605(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_74e354f4888477f1eb877016725ac31c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b18b7787d311911d97db2acedfbe03e8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_52a487448a0b9009381c36f4d553c04c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_41942bb49f4b1d089a5f2c5ff30fbe46(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a2e655c70a95912cfd86e3586af8031d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_042d32e20cb43f99d0b00939366a4dd1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a1c51534af5cb5fce739e0e9f33db1b6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0cf59c2b3c49411bc7d82912f2dea3d3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 350, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_85d7cb62e95a9581b1568fb2b7436636(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1760, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fafad7bdcd228b4562dc983107f2bedd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_85e749643da312b0dfb52c184e3d39d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_36af61195d011d531e41a2607f4f60be(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5301e4db40513909d77efce18417537e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_69c75b1e68ea73720ba1cb0d5693f78a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7e627cdeec25ec3251d60dfbe4366ea1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0eb60c1ac439a9315e0bc2cdda871bee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cd5f12a5d56ac902b05f2bda0f770a02(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 416, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_615fdec8d82234b5b92f46b908fed8ee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0868d2a5d46ba0e176874702c9c6cfce(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_05aff47964f2d3a72c797a2b32ea3ec6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 320, 320], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a73f461dc8fcfab386bbce242c9295ac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d74fae23da91c3392c86e1ea6a18479f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f3717dbc11a90ec673357357a4f5a121(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 2, 27], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a8a5f392aec056c041469416fa938e9d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6bb01592f973b51aa4f68cf8bd15214d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_422c626522c979000da5f6f30593a1e1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_33236631ef191e9001919d67a69c57e1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aabd60632b3ee72c70f3c6a8c2980fd3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_28e323e511882874f6053cf8a3df0711(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ba28959135f192dacde7d6572111d606(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2b0f82177d61cc5a0ce89595c06de839(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0f3d7900ab336c7c161e79d33e376fc4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 4, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_71ca65cb60c3e24b4c037c1c1e3bf96f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 100], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cb7c1434c46138273b95634a1b4edc52(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aca5a80f33b530c6b54cb76ac1c82ef3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 32, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_389a1a65df80b04387838a9cfd7d9dd2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cfe16df94cf539e9b2e8220560085147(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 816, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_691aa259d0178214f839505533812dd8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5e5570bf67f19bfb60e749fbf424e4b6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ddd56609c6eba5f451b97b624f37f979(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b0a9fc267b5c55fcd3b3a38689ae3d8a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f2f68d7d937a60459a57fe4bb82d8ebb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f6ab1250bf687ea866ea192ef4ddd794(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 17, 17], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2a95d9b9c3dcdc277120a4974574ae63(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_afda51b391ead699b11f731b7d823ac7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6f11de78dde49b2063f1e3d92c065011(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 2, 4], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8974de2df4d625594331a928d032d70b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e5af726ebf85c4775f202962ef89c26e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e29d8236468fd2404f3fb3e44c314cb7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 728, 19, 19], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_397016b219f06b29b17e9e14e8d3bb5c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1680, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_06230cf04fd7959b980f9e8ad7aabd6c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6d4c958d163d0b816fd3891a8b298bd1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_20124d8318651ca20cf7585e2b26a4af(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_22881deca07b2fa142d8e836412fe0ce(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a9b7ea3a4307c2d923905e5ac2df7c97(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5499d3f528e022dbecd096a826291507(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_79e603a7d6d1fc79b40cf4f10d289aa7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_77dc967eac5748cb9f75f0d0340ae439(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_304f7f30c7937fd58d07ecfa15780f9e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_190b673fba0024ccf25f54dc158770a7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 4, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_52023e9d2585cee5cef43ecaf28c6114(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5ec1d2aed445bc1f81c054a7408ae6d7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e8335b8e632aa0895f7179f540daa3e2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 8, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_df69275beee22c2a560efeeec93b6ec5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_21df08f8f15b4cb3df8c8853dda53e48(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 10, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a866980fe15605704579faa64e07d847(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280], dtype='float16'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f455e5ce3ebc466579c35ff722de9709(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 8, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_43f6e43e43b6da5de93c2fe31fe2028d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, None], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bbbc7ac5d6a40407fbbf5d43beb33829(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c485cab548958d76b5a792c44ab5b853(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1e1ee1e34b037301aad02e1a831e7810(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 17, 17], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7b0b1ce13f2c97ac68fa1ff97944614b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 300, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0a8442d530cc811185a7cb14f2c9bef8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 26], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d08befad9b02dba3acd33a41a3668531(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_58db29fc1de01cc3db228c642c1b2abf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3e4cba43cfb1977ef51da8fc12922dc4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_843452f1917334f2e8e121f7ff1d4ae6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 528, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_87e9337e54cfc38e9b073761b60cc8cf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 175, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f543b975a624a8be1479d85b28c6a0b5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 49], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_91e1d28d0c04beb42103cb8edb9b04f4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_187e4eec2a85f0988381fdb1e1156618(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bbb60f8efe58e4ef4a4ba3e8eb69d896(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ffac8b5fc5739555059d9155bdcd4d96(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_937a3aa2c9fa6d3edee86ab28a821fd1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 52, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_098896cf317c3b38fe783fc3f3f93187(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_93aff87aa296e52b8eb4e36afabf0e81(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dcab3f55130887d497db16607044fd7f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 8, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_baa9b6d674577233244a41b5ac34055f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_820c6cfade1def0daba51d1a93975c0f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e8cf2cd7520051f6ef11dd1194b6b35f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 2, 4], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f8ba3b5177b82caa033f75154da7a9f9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d9eb8ddc05336218534fbd6a5c951768(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6c51d991664266d35455e1d51f8e955c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e714f1906b3bf0d2b68008c46e6fa700(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c174acd1efcd32526b5b9f421f3d781f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 26], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d9b8dd5debdf0975c85fc073a80365fa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_64ab8a61c090c400033014ce0ed895de(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 88, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dec9683d9b9851cc0c332a4bb4452866(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_12ad77ed14ed99ed8a5c31ce4bf8276a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e64a2438b06feb1e6ac9ea5f9c61bec8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 88, 61, 61], dtype='float16'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0b21422369f76e7b5fc8249742bd0f79(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 152, 152], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_63c67b21957d014ea8d2a64a18dec015(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 76, 76], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8a406b6ab4a14b1a52c6e2bf064b0077(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 636, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a9b5f605cb20e27c3a16dde09b025197(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 100], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eb43ac70f74f19864d76416ef6633c44(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, None], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_786b73acb691b8276c62d2e43279625e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_78eb2b2916b6f03dbdc73e30f9b265d7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 28, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5e1a28fd9b30e9c72ce606aa66568a51(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bd679389e788a4c3ece56cc5ece1230f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 32, 128, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_995129d7b3ba4b09d9341f57d05268ee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 17, 17], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6bb7b63542cfc2e2978027131ec90b46(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6d5825e9eec38c24a01b52702ffe3bbc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2277be8ec734b411774b9baf8dc9aaa1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_275fb08fd51e0a2920f4979f72947700(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2536235eba7c885119b330de998fc5fc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_28e41f26274dd6d753a11fc927efad47(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fe42f695e8660d6e1aca77044686994a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cc0968d4101286a718b9b91b26f43874(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f32d6868eed8191d37e3b46f7fa779c9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c59cc180742a3b6a74a58b1964fd2c03(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_871ea6902b40663c335d0c3250b7b575(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1728, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_387c908236342074d32ec048fc7e55fd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_877454d546e3a1c203f817f1b80e1309(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_972f052fc40523a2a3f2971e3ea24a4a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d7a4eaa083e2e3f2789c3f0b325c5879(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0d43fd01cdee876e4b599beb9a7d3b60(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 16, 256, 512], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b04b56bfe7b9f96719fedd9aa17ca632(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c89f63dcccffe181d31430137126075a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_846a623ad65dc52041e38641c860b638(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e43ef70a6136804fa349a544a37ad4d2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f21f05dd5ec099678b345cf8cb55767c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f9ebe71b10b43facb80c66816f81801c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6140fafc09afd097d4f07f24f839acad(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 20, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eb3d0a61d160894ed0fe61aecac7e0cf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f7aa314b6aa5356993074542ebcf82ff(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_627cab3627565e8808f9a8c2f063ea79(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_42bc89cb8c1213c96fd350715e46c46d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4915acf731f68bb3e933cfa85a74b2ab(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_efc0d27af3c013b82bec7e72d5ea64b9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_060b7801f18a1f7426d2afd99b9a47cf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_90a401d651277ed20e19bcb09e1861af(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 118, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9bd3e1568123bed6ebe4dbd97f6b1929(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_32d723717b2697bc47877295400f7aae(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0c4e75fe0317562b41eecedf8a8b19bd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 12, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e0802b55b7f3e0d098722c02b5809fb2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e77ab0dcdaf85fc24f393995b6812126(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b33300c55881fa9d8fcfd255ca632783(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0ea4e924ada0f8a0fd64a7241aab7774(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e9ec5115357a29c1c24c1caa459aa346(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 75, 75], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8daa2f23d0aaeb4e79f533f03a2d094d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_525f8c594c9f34322079a6a1cb5fd4db(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3cf734b3b52e1c1411f2a05de7012c2d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0323de09fc88b4953e9c9885b825dec6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1544af6f2e285889a14762f0b0be20bb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 82, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7f0247af5a4d409771d4000f7af64251(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 17, 17], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6bd9f0e1684d8a91cd710fdc4013b3cc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7962e73565e84c018fe37199f82147e7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1248, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_70e6634496b22e86355203199d5b69d8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b7cff1468e1d5ec0b2e14786a5cb3863(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3ac5fe46ec8cd62a47842c1b55268c98(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_36586ad2dd23ed1a5375496d452960b9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_766ad9ab2a628ecfa9f32750c340366f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 1, 26], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_018a2a1888e4ef10afe40af394ac81f9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 8, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b4a7cb46d16d1c8d2d70b1027c4a4663(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 132, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_828480dfe97a62d30b8f578c20fd84e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 88, 61, 61], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5559d21259382446a7a661098df85f68(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 27, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dc367a583bf6fa0a54c012c0fd5d2b4c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d779636b62442026d5edf54f948fddcb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a72bab3b98c26101c8158b566cd53320(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 16, 512, 1024], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_59404292c9a6fc84ade6bf940d75e165(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_08d2f55c0c4886afd008251b401da075(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_77c59847593e7a98e5c6757b5b43ee62(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cf8ed4fd3e38faa99b07c612fbcbec91(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 12, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dbecb135c5ad303ca96827bcb8028905(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_61a1a2fcd7c16c379d9dfdd82d10d0ce(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_909111fa134e3f38e9e3f1c15bba8761(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 304, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e4ecf2189cb2adea5fccd792535b5e31(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_157c92cb303b1bb764a60cb4c8a7c485(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9d26bb58b7e1f84d85ddcd64612237f8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f817cb3637380ea7504362a6d8829e96(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 320, 320], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0a482a0b96bf4a75771d0022db3592a5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d1b33a6b835bd216956b399a2c716682(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 224, 224], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e196b456c57be05f1ceb7eaf11017755(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 208, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_834fb55bce153716104901de9054c984(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 200, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_53ed5e558674c2aebf4457e18909e923(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_26f34644de3fe5ccfb0fd9160cdc2f04(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 16, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5553bcb965630948844ccb3d72ecaba4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d25715f8061a85fbf129ed595ad74e37(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6cf567132eb6d3cbee9c61eeed33681f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 960, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ac68e046074d5349ada0e9392f3e5726(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e51864b5c63ddaf8d0a2a58e7597a6eb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 48, 48], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2ced17de23210472148d16815fa0be0e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2ad42d8f2df479d7a427377f2b4fc0cf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_243d31f3c4aef73c09c26606c64755b6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cacd03d978b45a1e11757d79db1d7317(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_69d96d87db044d8834dfc0de7827a279(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_18b9682e207a706c8f8a2ef0e3aced1e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4f21d752b54327857aa0147c04700c62(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3da406d39be742586141cd3fc8d6fe45(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 150, 150], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8caf326eb90f30ef6f7c546936d9c350(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_08d0ff3dd28439a6011e0961e3aece28(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 832, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_300c298853615b3c0170db8d49bdca2b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7ee0f5db480d94d96344809885517c56(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_828cbef0321735ab025d781d9ffa73e2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2208, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8339c3981e0a6b560775299ef0069159(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_daac59f2cba55871d22d85778d9d0eaf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_94a9e4bca189e4375b0a83ec770c0779(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 16, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bcbe03e9fd7f0a6fdce8db1c8e706eb2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 4, 512, 1024], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d8b0efc1a6c45657b65a4633ed444db0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 224, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4ad84879fb67aef2b5982b5e9376e901(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad29f851596ff68f1c71c08da1289e6f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1200, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6255c45d7575ac222699b008398bab2e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b16a81b9c4a18e944b89ad5615e64999(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1104, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_548fe739b2a15f899594566f38252c21(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 208, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ab625304d3ec82cc0f291b5b52d6d5af(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3735cbaaba66a5ac0277dc88ba3d8bf7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dc0ee143dae516488ad22513f1d47ad2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1872, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_85d9bb757032a5daa8c15f5d3c0ca69f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5002a283fb3b35afed691055bde2e360(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eeb7437e608c01f134fdfde857b5bff1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 152, 152], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d015b25ea3c3817f4e8504c33762351c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dc55e03a3bea90ac577ac722b89516a8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_63f7224e658d6f5eb960f62ee914d64f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0c7045a3020f18435773de2207058596(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8d085d9c756eb58f5cbd200616143cd7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1584, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d054c409aa632ed0e583413ea057192d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f48ddb85b5a53765ae374fb9726ec4a6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 350, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_252b66e62a5acc03bc075fafba5459ad(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2560, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e62e2ba30b3494e0ce7f8c07d13d1349(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2bbf50a6af0c0b357fcc5fffb73f3cf8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3c20c2e9732c862fd6346511e5d36ef7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1296, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2a30e451964607c3c365b5a85b7ac199(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f8d1802e38e60b1e9891a50d1fe2d033(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 96, 96], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ce4f5a7bfa3a655cf8046aff5e81e65c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ec390cb9798ee6bb43d07a6372fb3259(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4b997a0ed486a86093f0e2bdf6948221(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_373455cd01909cfa5d24f59aa15d38ac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_11ad791ca58de1b617b441bc59df19e4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e42d1abadc180feb3e1761d3450bc251(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 624, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5c1e5f1bdb9e120dabe92b71f1446c41(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 208, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2ed8c5f7440700ddd195b18a3ce4f1b5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 61, 61], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f92b5fd9df20014a9d46d7808ccc3a6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_465fc32e66f7fd33a467acab8152d17b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_90017019151eb6d35503fbd10b331034(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_12943aa5721042c75b18e441c84a9cac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1bdea83581d64ba2f99dad8e64df89df(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_77d207275c14f564a42204bb7e7de1fc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fe0b45d92e5a4ad3b02036b1874d9878(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 228, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_89b6ca35f2d1e62e2b8fe63166442a5f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 8, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f1910966fff428c8a3138c4ac7cea020(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_46321734af77111945d76476ea955ee8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_32fa315878407d9d46b0fa44aa0ab6c0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 64, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ae451aa490001ae9676af8b5e9b14a1a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_01901e3e147f93141ff85d65cb68a64e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 2, None], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_837642840756344308a57320e08d7cb2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_66e30de244d57c8c1abdbcc3a59c938b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_791c4c3f2d4243df9eef5eeb353a79a8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8ea4331776a52ae07e57ec38724ac144(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4998bf77c1751ed938eb5899f4099f17(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6374283b95c834183789717f24fbc1b5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0864b86dab9fcbb53b9d4cad1736879b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9dfabbaefaf11c6b5b9d8eaa311db70a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_efa61a738219d6b30fe7c5ea55bbf8c8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b493a7dc321cbbff26270648a14e89fe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ba7ca504a05648299a09f381a6d00a82(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 100], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9aa2afffbdbcc5fd4c12c0827ef073fd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_acd21e6ba9b85d81f4c436b54d616163(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5d13d42b9125e719dce84a393c7e7415(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 96, 96], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fdd975de3a07c0b8c0199f67fcbdc20a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8f1e6efd271410375760f08a0f3826f1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f8894c9f5f674bdd98545a1c891b7475(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_54b1c98d9795f029edb6880e085a2801(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 70, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4826f9e08de009acbc31b79866413c76(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5035233d11cca3f5196c48b2be1d8002(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c868130dff35d2719eaf830794a4e835(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_707f8608eec2a5e8d71c0de449e7ccfd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c06e5b5c812c198a7a435bd5f5ea19b5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, None], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_96d8fb03a2666c166a6b162b2a9b7c66(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a2be8a240de48b3be18822701f6678dd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, None], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0642e1f3140e1fe3c756913dcda72187(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_17f894f6b2e440326a4091f71fce3d5c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 20, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b90e5747bd4a9bb186d61daa694e03b7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_527e7c05c5be0990b060b34b86737bcc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 17, 17], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0a4196f3a8cd2a7076b666b54920276c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6640493857713736d3bc50c6a469126d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 50, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9d85ef0c8f5278abc51637273da7f079(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 2, 2], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dedc7bd4a62301898505e50a456c383f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_37931cf65dab743e5b36182441f73359(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 4, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b9d956cd031c7ebec3777eb78579a943(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8899a0e476750ed83645f4a090346369(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 224, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3fdab227a067b957d5b4122f0f7906a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b2f4cb0b723c02f8cf0002af69a3511f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e3096592d63a9fd8b51a7a790bea3754(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4e84214c5fc0e0938faea579c9be6f88(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_539bc6aa1665a2a7f256432b65e4f6a1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 4, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1fb18efcc490a9944189eaf65c2adb73(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_427afc6abbd3e50c11b94d44af2edf8e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 122, 122], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_be01266d59ff21a851b7d1620d340c1d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_18e11b20ef7825457cb3f45b01647b1c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_50921e6ce5dea0875cf3a44f3a214431(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_449494112c0c5212da530c6e0f30c90e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 2, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d46d0710e4b784f993a67a029979404b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 162, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_53b838fbc001b9be218b0a25f7cd237a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bf0eefc006af9a96a10ceaac712b09bb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0d86a09c5288edcc64fc41a00de07552(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 12, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_81c9af7ed80ebb5169215159987866c4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1200, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c977fb075b6e32a6cb60f7b4437bd67c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 26], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4bfe5cfb42c80abacc8e7d7a419c68a6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b7b3b585afe2bed5b77e5f796ce8ef89(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 19, 19], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_00fdf9607a0841d765a892ae64f69fbb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3bdbca4df8e2c83e0b2b6079fdb29159(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dbbc9f78978f44efdcfa262e45b0192e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2688, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7f7fd9189725e708dca625cc2e465e53(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 352, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_db27887d293799a65701ade4be7ec7dd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_81396c959690e53b91b6ddacc309a403(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_877b09b1545510583958277bafabef7c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 138, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ceb0cce1fda59ebe3c1f57b30c6d63fb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 1, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ec7e93f27ac639ee50c0f9d3d29e2757(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_26cd3bc7f96d1426df8446d89063a96c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_58058b3e1ad1f6362ff1bb7d7f58312f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e540e4dd6ca0b6cbb351f7466d0e6a00(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1488, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8d29441e67eb55c86aaf9ac0c105f65d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_26af145236ebb2d14226ce3094c1e6eb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e1695a967058940db4520912ebc3b994(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1728, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_264b087a49879acc409c7be34c9e0735(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1216, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9828305acc701420dd974d16fe6a0165(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 4, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_11fd21e11ebbaf702cff89b02a6b880a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7cefd34cb706861e913941c512cc608b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 162, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e7e525193c7665ff6055d07319e94732(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b277c48785d0ffe0e8fd9ad64761d968(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b8241b38ba6250a9f96b62ebe0e6f436(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_55d7454d99913ce0f6dea6531a9db63a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7b1d0e8cd8694a257c1e8f7ec2b4e5b5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_486b250e3464949be6e757d190b00a93(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 2, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1c0413eb29322e14a45576f2f97e76af(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bd5ad272cfafd7982791a8048ec274be(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1824, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_23dc6aefdfb0ac2cfa665d2c6948efd3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3f7c662e88a9c2d406cf081d79e6a198(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_655df57c472c4d0c6f16129956b7bac0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7cb4030d4fd59fdc667e95f9121cb14a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 128, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3623b341c582ecd6f296e33c0a697863(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5dae025fba5a2bfc2390785b5f27b398(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_156e71d964257e50a4062519e8d7be1e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dcc69134c8547dd0c3bf1cf4929a0409(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fadf8f2769307038f4a8808ec5529513(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 432, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_690e49586aea11a1eb7c15b673cb0ee4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 19, 19], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cf52302a3b62a987d1ee64227da0860c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1344, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9966cc016893f4a127b083ab5ac5ec11(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b145d8fed30684d70b811e52357a8647(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1a7736659c4f1c4bb653cbdf896a310c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_00d33c83fce4f795c8edf889d85fe88c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1e4cd2c3f221cd605d35640d37ae5b11(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 96, 96], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0d18988fbc149c61066c3fe7b209e854(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1472, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7912194493de55571014b034d2daeda8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a73ccdc1c7cc92d7572c73cae79fbc51(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7f67d90a101ba3af05b15526be4b4c1d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 8, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9215f4007d09e24bda79784874e48006(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 320, 320], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fbe5e98c3c2ae9ea22c78b9619ad70c0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_983326e04182a3eb0a8a18de61718b73(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 132, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_db30e8c64705528b981e4b69ca81a347(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_38a9857a5fa9b0f704274bdc957f06a2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2ddda23b94e0a01959dc2d8b51456792(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d7769f646fe9294398eda4846445c79a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, None], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6cf84960c0981c9221bb1a6fb4ed538c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 32, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f20e7a4da83383ddee762d219238f897(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 102, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_476c319404c61056f5047549c005b424(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 4, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f3d7317a2ae435f2e29b26fd0a769151(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 38, 38], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_10fc2c42c327b1dc746800437c1045aa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2112, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f2b2f6026b616e322ac98897a2044459(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9a509496bbef70adb0671a90e1202962(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 704, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_57f43866ed51d0f98c2499462714c155(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1872, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0cdce5c6427a22820951992371e873ef(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 4, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6c725904d6b4f1e090c09a901deef328(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3f01ecf003383466c84f75003e00e54d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1200, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1dd7677a22a2a5f11aa93ace94d0a08a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 26, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_10e57738b0fc2cea0a3598db0078e874(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6c43170c4fda4b7c882bab4de3f6116f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 528, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_145322d5d618af1f386d614891679642(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 8, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_86d8e6fa6765ca0064d421a142c1a7ea(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_71941f5a5f61979e31983b1ba35acfd8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eb11a5941d56b198439e52b59f16e9f8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 2, 4], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0ad5aacc30287d9a96abe68bb3e40977(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ed0a068b85ce249f3c58e109b85a0ffc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7325ce5abcceffcdad1d75fc05ac886f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_79c6bb65d9499e69b9b1f9b8f8ea4dcd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6d584f74e24cd77c47643be865d67231(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e2e34454371ff61fd0dc59d19a5b6d0c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 184, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_90582af620eaf7361b8a05cd367bd17c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_821e1d4b60fb13bb36bf40dc4edf5ac1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_47e28fce3c93836bcd05e6e0b9291f91(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 38, 38], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5c387aa6cd67a0f9d92f1865f2cd42fa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 256, 512], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ef5d743a04e82e74e0d50f94eeef1579(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0c7dff3f5f67277377a747beb5718466(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c85d0809e393254cdf73ab76b016fce0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9f080f9c47a610c3c11129f9544d079e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-06'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_11b68be0a8994e55693b0e7eccf7c8cd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7ad21d3cd4def74c095eafda6d4f661f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 16, 256, 512], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_95484624bcbee64e084a57570a7270e1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 702, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1b91e432c0d4c6ee80e3300faf1ed169(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5746a55d52558e27cd8637bb5c2ac3df(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 12, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f47fd081989874c9be0b2a07b71d405c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9485ada2f58dc62240c79ed561d455b6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_743dde7e8c4becd09dcb0bfc4eaa78f8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2112, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_09e93f2ed05fc31e0c67bbb322c87ac0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 366, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_51a57265c5f8536c6c608fb264eb4c1b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_38dcd83e47eb90c01088a765a504f3b0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_80c678f7fb5377ad38898c7906799f04(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 320, 320], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9b3cb778bf13f66561aef74bd59bd157(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d9a735862a0365b5c91492af228d0710(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 228, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_050b5cffbd4c88fc07350c44cdf41f37(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_64bcaca796b74e0dd05e8d15867d06b3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 5, 5], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_72f2db0673b6445e43b5244c013c3345(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eae3a445418638d490ef940fef10aa86(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 19, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5d6cf2cdcb8128bf0107ef682c877b19(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e59b32e97c7c115098f78c269495195e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 100, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[100], dtype='float32'),
            paddle.static.InputSpec(shape=[100], dtype='float32'),
            paddle.static.InputSpec(shape=[100], dtype='float32'),
            paddle.static.InputSpec(shape=[100], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d2a810cd64f3f8f821e4818f3a813bbe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 71, 71], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2e48d9442b2f56f39d26fd629a82e609(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6d13b1e55f1eaf091f61800a27fc1e24(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_14a2bbb563ca84e1285d8245814bdfc7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 82, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ec08b4cc96d3d69edad4843e85e756e2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c8d39d007b90a58dc37cc7576a510ac3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_63aee18d887275d41feb40676b693a5f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d206761d00a236c497a87aad020c8362(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 702, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0596c8a78d7aae6806e7a586a9d7ebe6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_37c54eff98f8d5ceec5d7cd8bad89210(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9bc71ba61636b44c8e8eb246e6d67834(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_230cf392b912d0875e0031ac9a00d800(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6f0c03162ee675a30dabcd783955ad04(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bb85b77d2d09614e9ce2f42a2acf3161(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1392, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_88409826f0a5b4315a2c70974d21c913(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 2, None], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b0196f876f85f8b3e269b27d211c7ff4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_636cce976a0e8a0b3675ef04b099ee37(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f495bf5bad77847a64c21c00d3ad1c93(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 8, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b64d37a8c0b9550cdeb7ed0941847465(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_47820881e00ec3c702c3a5226ad5af1b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 528, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a0be91d88ac265caa2667213ed5a55dc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 640, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_41e967a1ff401d829fd710825f39d947(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6c6a8505f1b172112a05b80d1e67bed2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2233eb4121455b05a23c5abf321d66bf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e59e5b6407d45e68d382dc1189fb5318(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2f90254f3cbea63057615693abfd13bc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_772aa7d0f299d572704b76e5236d8837(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_09df02b895f91c82cb13cec243aaa9ad(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 224, 224], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_79946443b4786235e63f52c4e44c5338(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ed12f3e30a660ed888eb75bf888b9b7e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c09c09ab48f2345e5cdcf78821319919(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dfc46bfb796aa1dc94e44c8854b9fed2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d1aa06409bb127d99118fb9ea6db1dd4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a64ff0d227fd1f5c498dd0b1b49831c3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 320, 320], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ec45da25144c7a280ed3682130bb3339(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8c850d9689c62b23f27a1520dd33287b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_be166592397a29da58f28b70159ed0ce(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_96f8b04d959b79a12d7a801ba2fa61e7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 62, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_13f3ad17d3120ad19bc0f80910e70c21(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_46a23f4956d7884cfed43c7130b173e4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_536c1efd10ada28f7f08ee907cb76a10(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1888, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4e89cc41cf0c6d362ceb30511f3baf3a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 270, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_98e263991ee9093fb45a92f044143221(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c7b743e54c1c8d4700e6b9f6f2e12e9c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 164, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cb6e03f16fd85a3561e4ee903e2f87b7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_23437b71da3aef8af3b87d974248a31d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8e0d740833413a737bc504259ea94d0b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1044, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a933c7131aa0bb80468e9ca7031e37bd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1cc79bc4ea15b9c11d63d99479feca2f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_56ed83c1d6f5f9215f9f4e836c4dbea2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 24, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6ef276ff9c9676cf35aa0efad3b28620(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 960, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e65e9ef8389a7b4522c50cdd505cf444(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f505dd254ab089f3db2b49b03d8cd87d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 4, 256, 512], dtype='float16'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_02604289111fa85a417ebd1fad7ab312(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 208, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4c67165e53a71a639de0e0ffca70600c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 24, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_31b786ae7b432143eab18cfb7285860b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 102, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3bb83d4ae75bce450fa9d58e1211391e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1fb8662ecde4fa375c9f7a58c245ffbf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1632, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b9c29bf35f34aba098016c23a88fb9e2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 4, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b1ef8c5ef250e5110c43217d9b6d0a78(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 150, 150], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_11c7715785b6852393c6b581d01e60ea(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1776, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f552e4baed6f4728abb382b28165052d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_600271b6236cea38e9470f9e04f7dbe4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fed5bda247ce29e32889523eb08ed75b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dda4845a26a87a559e1eb186149a4c59(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 88, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3da96324f86f7046a3ed30605c919216(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_72a49177365a624e265037248702368f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cc654c4a53c4297d042d4fa4b495b417(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a25c276124e953c3764c6ab1538ab436(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d4a37eeb6f84ee3a63ca2cc9f5c6275d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fa647b484c90201f1ac028819889c936(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1296, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4f47bcb496edd882d011067b484d2a2c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fcddf299b239e1d66d41cf348dfb48ed(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 32, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bc5c57c792178adf0d35470dd88be057(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 8, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7f2f9939c62c013c5e3955ea200a6f8f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 38, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_37c404f7b081854dfaafa5f2830e7e36(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4b7c7cf358a1e3fd1b91c6d6f3a7797b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 76, 76], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bd245e42ff2a87985c51c5b327a3ccc7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 26], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0e7b6d436003561b3cca38a9dd9c2e81(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e10ee92bb1ab4901c9894b6350c6f1ca(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2dd7e31bb9e63f2cafa7f574b5a54ee2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 12, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bbe6026a5b7ee376e1781f7131e7e8ad(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a4a252846e5abae860c130569ee30146(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 140, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_393d0c94e70b5132962844cbd9cb0245(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5c7585fe54e0f4f148dc21220ccb77ef(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 100], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9b5189f9542bd58efca5bb83aeb6fde7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4c89172aa225f3e2c507eefa7ca5cb12(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7e8b6fe7ab9bc5728ac7dddbc96ab1a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1920, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ee21f16640d97e6bf4de48c97ccfeeaf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 64, 48], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_69423d82f520890cd218acc79c4fdd09(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 2, 27], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0d0466bf8166b81264e9962906afdb04(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_27560a54630138a7624abb15f3810a78(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 304, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_91794f999164f50e2db60181376d944e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fe2a39ba30253890f25ecc1905dc2690(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 61, 61], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0dbe917cb5d2897d353f12ba73a0bf98(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bf552f0f531b8b02a14e45cfbf16ac5b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_34b5eedbbcb664ecf55b0a165536ed98(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_91998974a9a9c757128952a83944a47e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 5, 5], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d021e11322ce02fc7d50928b430574be(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2f34b103bb99d7ac849648950e0cf8ef(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4ce49076a6358304687efe00e78042e4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 608, 608], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4e8d1d4db12e407fe96619e6f6e4bb1a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_af32604f262708480bee23bd73d1d0a4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d06063cb975d75249276e02586d364fb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 608, 608], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1bf1d7934f23ec1972a7e6e862d21b17(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a4c960ebe1ef173a35880ac3519053b3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1392, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7fa820280b172660d093c886851f9b87(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e8c78b04962ef00245ccac46bff272f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 147, 147], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bbf304aed67388f1370c1b26b07c7b06(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_733e4dffd5bf18d0f4e67fc718aecad0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1056, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b636fede3bbde7f00b9a695b1841f771(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 50, 350], dtype='float16'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5a8de4243c8f5bb75ad6451c3a3b50f7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1920, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4175dce9b1cc1cd32394bd5bd6e8da04(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3395c166532a9326c5b362b596cfb6f8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7555f7c9019a36f815e0fb3fdc85823b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 16, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f793756dc79518113fc3485de56f7d17(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6225dcd782fe9b56d585b3dfe867866f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 200, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c81bb9c9cf20688c74af4191b2445936(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a887fcc06d4bab4838eae83298f2aee6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bd867a93055a5139e62d133e16bb23d5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 200, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d65b8227e08fe7036bc73fc628cc19bd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 50, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0f272340492cd2cc01b42d4e39db348f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_48f893076af0bae0b16aa3c7bbf9f059(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 6, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6b55d4cff36c6c7e62bf7b519232b0d9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c0041c4f778e6fc184717d448f0b285d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1000], dtype='float16'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f6cdf3990652127698b5b79e104e67c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 64, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f49f96affc05c07110bcf5cc127e9cea(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 175, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2e3350ae584ef8d51f03965de3485711(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1056, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dd1600ea9d942849af1fe7b61c8065c1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_36b2e8186cd38340a27ba96fab1495f2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f66e49712fea5485e25c7695e4f87bba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 228, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c1373b1523969d3cd294bf5cfd777cf8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0d3ef54db2c2b094f87810bc2d758946(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a3182318b3e38f89a89cc28056bb26d6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 50, 350], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_25ced98314fbdb4fd3e92b3e66cae7f3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 4, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f9cac33e9f96ccf562df2f9bc1401199(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 76, 76], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_53cbb59044691df29c4b90a99aa0e819(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 2, 2], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a400d6a58b6faa780e9da030d9891bb1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0d08dbe847fb1ffffa4abaa361c335bf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d915026c18c294eab714704904222726(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dda6c8614e3027d2bc00cb0029bac381(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ebb9a8cd7b482194b00535ef670658f2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 48, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_16c63f185dcbf83f7b909755c2f2b56f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 88, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c87ad56daf6f7269e7309ba593b8d57b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_721afb56f118bfe3484b4d9acec16bb7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 10, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d7ba2fee227c9407d58edf2fa177c916(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f415effa6dc695d90d5c2b6fe007d530(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 58, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c9ec872267d8aed45f9ee56c3e22342a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_07d82a607ead5c6409a8049b931b760e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_814cdfdf045f84f91cee7529c0f0f4cc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 350, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ee562c85ddad986b0b399ccd730b09a8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 75, 75], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dec025cac5039c507b56ec6c5e4436df(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_42d0bdf971f1f73c48157f5f2ac690e6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, None], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_60e344d6a2a89de52c29439c970b91cb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cc901f86c443db036c6c53602f6178f3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_35cfcf4f840d7c8208c9cbe5587c8f5f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1056, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f6ff6fe36dfa9add9ecc6032fc1bcd8d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 52, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cce7ca0a0eca462ae625a1582c543952(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3dee0860bf7000a2818035097bca2806(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_352bf7f98b6811c942fc8ed6bf5c7a8d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 175, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d5f00cf840ba23ca4c93eaefe0f0336e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1632, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_785616c62386859880cad264554648ce(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4bfe175eea6e59670dc41200dec40e2a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 228, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0e2c52d0eda54c60340513542c77be74(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 152, 152], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2ca801a1dcb24175501aa0bffd7c6fab(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b47ea48798265d1060f7b0dc4804aa1b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 504, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b367b1d45946d63df336300935f21f3a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 728, 38, 38], dtype='float16'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6d579ff9d5696f612561e3909546ded2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 48, 48], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6a6a8b71e333e0c9d75d3024fc5b6cab(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 906, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9ccaa3ac597a90e06fcbcea64e5ac153(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_60d6ad811148171567a678d06049f66e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 1, 26], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4d8d39911a7a3d668d15b8a8223fd130(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0944e8f37060246d908f3008bb9bd030(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2d41abc4229e29f433c49102e719bee5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 832, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bfbd15aad82f89ad4ef1b0f113a4c9a4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2560, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f59db64dc38edc77487ad7e612dfa24(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bb7662117a751d0668174c5125747db4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 304, 304], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6b4c58fe9388e2b6d10b3d5570066759(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1824, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b09618b70aae6f8dbff23c5151d07530(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 73, 73], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3d0e3d34d4df386bee8aeae1dcb6e0a0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_223b90ccee84c2f79bd6132e3d1bfcf0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 28, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fa22299464f154535f860856b607a3cb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c148acfd857b832857787bb2fb3b4423(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 208, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_449ebfe9d587ba5042e1d52f3e043f09(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 48, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_90bcec94729d3769f92ec682a7559a34(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0161cbf27fac7f71f0eaa2c7f4f66d08(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1504, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ae16dd3bdb1260a43d8228fae0430b56(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 972, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_796667f042e6ff92be62ef7e9b71db5a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 2, 2], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f9a1ab1432b6d32846bdf5a93d6c04a0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1664, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7d84c44501d6443dfcd3a799fb2e3bc1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 8, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_094a8788b813bace498a212d13996e6c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e06afb9a8b7ea61b3545e819824e338b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 840, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b26d97d0adc29c642b4935c21c878c57(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2688, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5a1616486a1fbb48e0d1ceb64c13d6a7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 32, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d45c252f5ecfd0cee699749e0ccbabe7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_555542af95fe76dfecd11cad64c35582(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_35b16f32ab0f3735848da150202ef23a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4a91c55a410e77128a62a388647e01df(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 84, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_893a6d2db7700355464ac3dd80eb9c38(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4883c7bfed3e7a895e8bc369d0efd19d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_be9a5147f73f924b30b923813c99383a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 366, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bb31f1b1695b2d6fbc6b2c43d49e8f18(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_168666ca75f03e7921b77b7df4bf4b1f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 175, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5616ad50de17afb20d2b5b303daa8c25(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8cc81f92bb78283811e60b6ee2e5f88e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 122, 122], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_37c7f78cee04397bcc1b5c3164755e3b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 164, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6cf95c10300b7e88b77335b25ebbebc0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a6351ae47cf8247687545c31c74783ec(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_499e3c1f8c9ad1131ba9154aa6f01f62(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 304, 304], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5c38f168008d012148229b1087b704d8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_118d61f5d6bb64f4fa6e37efae793ea4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7582c8aee57dfec01a8891d96fbe361c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4dfa6049f3118b1c271b62e4ca695477(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 82, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6435ae2e69bf0cccb9fd8aa0c48d5236(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 8, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_edfaef3dbe59c3eb6eecb475e4cb8953(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_88431baf8c1e285658f197a95b6cf9d3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2ab1fb697db06bafa2f1aaa950cf83e7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1920, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cb4f73152930d2248e4d94dd24489255(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_909af922590524547f0d2bd2e2e98236(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 864, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_21958488aaadfd5346c2d5d8646bf700(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 304, 304], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f42df1d085c4a56ad5abe3dea1bda55f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_598bddf84d9f54f4e225a92a616dff27(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0f870e86e0066fb2e5011af114ef1986(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d218240ee2009d812ec6ba2e73158aa6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 52, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4703c1e3a580abcc9cc5bc2af6cda950(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 300, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fb2b6cfae40a9295ad541d7e2f3188c9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_620ac5ffaeda914bc9a9312132b75b0e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_78f09f2751fc03b353ca8aa162f887cc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 56, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9f43e2205bb2c6fb567a5563884152f6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 8, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_760645d0a13c52c098b8d1fb12e86f56(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 147, 147], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9533ab2615c5090cd4e6ec59c341bb7e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d7738f84f02b744108730aea0576e552(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2432, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eb0fec85200b2fc15fd63e6fb43b696d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-06'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_da58d51d5fb31cd56aebf75eac624f62(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 38, 38], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b87c76715bbb4325a9fd9c6d24af7149(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f1b2077b6dfde7e5c1c0ebbc019c52fb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 151, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_137a12e490e2679f8888426c4eb18054(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 64, 48], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b670033bf57d5fbe5185c169e53064fa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 704, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3229c684f72e0032ca0b75ae91a0afb6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1376, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c12a5ad17d2e4760655009c420f5477e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 95, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6eed3333539628f5c9767d24ed2a552d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bae239aa9888fb6c2d9bb120a0d14030(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 19, 19], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e25f3e9df6316800801352668e11e4c8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_db2e8e8d78a9ee31e8e1aaf90b715ec2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 140, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_046ff470a799ae740d946b1fc83e03d5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f4160ab6e5a12db2734e6d00cb501281(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eb4604bfa39673526139594cd8f5b4f3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 35, 35], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2c830974dc5913da4ff303918ff9844e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5641479b20af308832c803fa384815a0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_720bc9e9d166821bc5e90ef04fb6bd66(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 174, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_626ba74ec2b02122f6b8e7c8a50b91d2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b2cae63fabb6c09d5182cdd653ae5c95(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_12df638726b600c4afe8b53771214e2e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 8, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_06b4f48e132c632d63a09370dd3963be(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3e9650ef714bde1ef12b46b80ee252bd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1c57374d7153beafb4393ddc4641c7c9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 256, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_35cc7abb5d8dab33d1dd3574296bf7c9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_33f0295369ba8743eee3b842ae56c435(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1e31298fa5ca70ebde79bdaf2f2b80c8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9e876efa6d7d281d838b824b12a54f1d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 25, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_beae53612f857c1a8dfffa3b2b1d5805(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0d6c5109b1f8a3470881e5a4b8be9859(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0901dc366e208356614127aeb3599819(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2fd1fa8275b84016dde2fcbac4d4de63(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1392, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d52ea689742ae707229fdd3b734c24a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 27, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b99da8d1c4b637c4a84188dc027be5c0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 19, 19], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2ba6b40b81f696fad914997d27d84ec7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_017726de269e4c9b5d3372c65648927a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2064, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_19b0ab79a6575c9fd9b748ab88a07aae(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 162, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6795d1bf550ef5ff0d398cdea8daa4e0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1968, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2f64aa4adb2e881249566a07e0eeec61(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 61, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0a7e787d280906eed0db79e1f2ad96af(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_08ab656721b45dee740700a53f2c6717(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2fa4c26454bf7bb6fd011cd0febb5ee6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 300, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3bdafba87aa84e00b6d3206878ece961(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 352, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_91c8a9afe9a9d0b24ff271857fe6cf8d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9ca78185ba8488d03428ab8491daad9d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a2ca7df3ce440bef0a8041bae0badeea(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1824, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e02713ea85616dfa334228280e4137a4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 92, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[92], dtype='float32'),
            paddle.static.InputSpec(shape=[92], dtype='float32'),
            paddle.static.InputSpec(shape=[92], dtype='float32'),
            paddle.static.InputSpec(shape=[92], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fcc15fa6aec5dd79e94f3dce416fa64d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a646a8039ffa6453ec6961f3a12ecefc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_55d17a468b9571337b7365d1bcf9d55a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f337d3c4efc7db5b4122dfa19c7adc77(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 728, 38, 38], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_da57b1ffcd64deec106a41c3eb4720cd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1044, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8c75603889fa4ee465d05081ed5b6f02(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 117, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0e9d2942c3a6651c78adefe1b478c1ee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 352, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c494f62f4be60aeea8fbeac6ea1fff44(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bf86ca174606123ec955896b4d7eced3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e6cb7c04de058f6439a6e07c5f2cb0b3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1680, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fc1ce8beb88c62c97592d733b5460317(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 106, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_279bdad06bdd26a5040f12d9ed401902(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_46b97e1719c35850fe5a320995b5d483(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 4, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c462cbacc6445c4679ef0edf8778c245(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 150, 150], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_de3a2a7943e5154f0967be179ef5fad7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 256, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3e5921ca97262cb5e9d157ba6a20129d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c9a3afe74654214f49f146a0671407b4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_14bafb76e907e139470a757b510cae0b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 174, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_961cd887d6d3a2637d86089812edfbf2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9da9828487716387d21868142d1f3c3b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e70f89c5bdffd8c91930b2fc861f90c4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 544, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad45815c78c130854e676ff56ac8742c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 42, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_225277f42713276a869599d100ca6154(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ee420b1dfd4fe6debfdd5d6ba3b8d2d3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5bfe2ef0c282bdc1b3446651c1514db9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 138, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_13b2be437eb98d0a99d2548c8144c7a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e07bc58abbcf80cf5b33bae7d30b4fd7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8a4301dea9d884d58e07aed6b8dd5dca(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 47, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_75c1e30f4888059c2cd845031e514e8e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5cd9b2e244f510d633d22218c33dd68d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2016, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1ca6179eaa6fac551cde0ba27ece8af9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 19, 19], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d32b403fe907691260e2fbbf4bd9fb60(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b42da946448b13bac5fba8e225e1d2af(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b0c69fdab8cde1c334c81a87887cea74(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8f4f6c63d487d3234fe69e6a13c93779(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d026e93db6afd833e224db298c7e5f1e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 176, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3769245acd81d06e048de993c488dfba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1888, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e522e62ca7c5111b142d36721055cc20(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 262, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_698b2f9b07722fd95f67003bb40117c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a136ba385b5c72c6161d6fce7be93d39(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 704, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aece4db9bad89f98c657fc0579f8367e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 24, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_af334a9354bcf527ad7bba3108abbbb8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eae56c42964d9c988459a93321364b15(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_53db53fbd451dea90acd4f9c4bf9dd17(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 70, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f0fb43b880ff9b48906f171aadb7e7c8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 448, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_012ab9a83c8bcaf37ce36bf76aaf9780(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1984, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dea331aa74c6a88d0650e2c66ad0194a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1872, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_87711fa9f72760e4f745a653a5faa86e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 49], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e3fc602620abb2f6df9f0d1dbe87c2d9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1600, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7161eaca5175093dbb03ec339d97383a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_727edec3fec15f4f5ae156a3d8f327fc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 16, 128, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_79a0f7fde3e2e5622f3ee9a9963d4fb4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_64f3a1df2c0c2be8e0a9b966246bbcad(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eca2841a2f6c7409e3286fac1394a802(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bbddbec309df2061064e2547cd24d393(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_17570123f97b3848c17614fcc87e2b5a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f6a29ac3abb5a505db2d31b8c5ebc4f2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_87ce1175562b6ef67e5051677b6b3490(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 48, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7d18a1db0c6d4db7bb762a76cbc81445(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_79e72109dc4ce984fae1a50b24472eda(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 10, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_91eecf58ddfd53ba4db13e0114125275(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 12, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4ce248a51a137dcf45d78d0ed62bd5f8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0b4584f8fce129637d7da6bef7d45e8b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 81, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_34aa86cc720fd6cc30310d59a121d3d0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 16, 128, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dcdf56cde3d2f2c5363c0b84df8ea942(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 150, 150], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_387422b52eb4b1ceeb92350d6482d650(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2f6fe5b7d77529a90ba2538cba92ed9b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8a3a3346a82f43b6a16dc01f89939f7e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_70a845a21263d3d009fe4c110113b996(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_edce63feb5218ddb19efa83788139789(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0b4c9c914c9d8fcae17c0d7c7b121f49(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_57c7a2c2c35932c5ae601a1a8c884af8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_41ad01ab8bac145a873c03cc536ce5a3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_49b37e7d44b23b25bd2d941e16ac350b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_739b6448f49d372022d378643ba266a3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 832, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_79596129f588714edfe7497d85a3fac3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_670cda1d268f562ca69f4ab05711b6f2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a06580b37d5f0f15d89c4f66213e81df(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_74bb5acb57b94daf3bfa30d245f0d804(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 32, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_34d01bb9eefd3a8df51232e40be75e08(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f467746a2d2cb25c28f2b5264094c023(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 19, 19], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_87d98586fd270d77978adb5f140238e0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 4, 4], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9512bcfb4b7767c687b487cc43c655fb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 12, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f1e47730f1f4e40808054695e5b41510(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1184, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a540ca37c14ba7d384cfaa237302db0e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 175, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8ca3ceeb9926ab7679fd9b252dfcd9b8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b8f83b00ef8405870abe87170a972232(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_51f17ec25806e842a58e7413001c47b1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 200, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_240c754c596855b1d45021935e7150c1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_712979444f4eeb9d0a875752669f3685(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 2, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3285831782df125fbd3b912e8ac8cd01(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e7c96d1a789f994a767504f1f5f2c46f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1584, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7f6784d25b84770b64798c86f3879ef2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 38, 38], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6da4f604b871f0ee20a171f254463e00(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 304, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f8e66d2ed001c19c6238b498fd5a7e0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 100], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9ff365f1edda074f3bb4d6af9ec18a80(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 62, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_114ba9d5c752ab7d4b00339eb1cf8fbf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 232, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3742cd59e8d04bb1262384d6ad6ae602(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 100], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_450be72768eca7c5740a2be72a343374(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 138, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4b6e908a520da098a6f5429b29e2e156(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_516b7508bf90cf5f2bd323aeac16aed5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1504, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6d49de96646a28431f8cbd1d15efbf4a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 35, 35], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6c876094d62ae29c843a9f20ed03d7a8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 704, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d905b7876799571d2c4c3cff792bf61f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_450b544d15f5728151463b96684e267e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 100], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ff3b0cea112b3b157eb8e4297d4885cd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8510c61da75ed753e95d9be26fc3a92d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 636, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_67215941bbf3f92b88f3c5e290c056e8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8326d9fd122cb6b06a5b97e7cc0e60bc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1824, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_140bfa0706c092eb995aaadd28125053(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1968, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0a5ed68b0b06771b1bd082dd98394622(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_06a2ec81e913a71c7715cb8fb91a2df4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_810d6979806aa4fc8b5d7aaf1b636856(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e3c89e164cceeff4c4d09870a1cfffa5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 38, 38], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7d6652d8faa1b4d7864a93d614c52436(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3698770d0c3cddd56b66b5f8aa4db49f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dc71b17cdec7c905816752a81d8ce04c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a1c264731ada4ee4d7a36ecc897e7967(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 350, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b5f4fb2581e2510571ebce121766185b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fade3ecf45a293abc63e2ff674f0eedc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1312, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_656eaf89c2567d307523269782d82447(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cd6c0a427e0c9e1d2485a197686b003e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a4cae7338244353db09e7f344d8c7734(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 128, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_16fcb690255e7ac9fb634f05d0383a5e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_53644b75daf0315e2539116cc42f5e9f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 35, 35], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6a0f97d2cc017d85c7531c972d4534b6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b927d028402bbc07467fafcc8cadf208(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 256, 512], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1ff4426d26d067b977e22c90c0d35011(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d3a67b99deadf1e1deb85f90dbd6fc7b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 10, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d9db31d8c6e9e13ad341e4e03e54181b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 432, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_30c385042ffa0e27769b653a3c49434e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1008, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_83d9a66296c532d220a4eb7ef4ff8825(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 2, 27], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_481c85a0c7e7b265bddf2007dea9e7cc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 4, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5dd47a5fdb6a32c1aca06b0304106960(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1440, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b4e38c51a25b46967ac2fb3892ef4a4e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 16, 512, 1024], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_acc9b550ee47d65245c83454e01c7685(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_747bdd2a3cca1601a31575f7aa4c7c7e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1248, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_93178a24beba881bd80af4a4d6d2c9f6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 232, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5c087ccc0737b755b754d51bbc1fdc7a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1248, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e611a983f580de47d6cad7b996e6bed6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 960, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5dd734daf6353894ac0574101db75bbc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1200, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c2e98f0f469d729fb97a81ae7c63198c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_076e23c80561a13facc3a4b4372dbf16(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 360, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dd64de9d454d8478f5d191c8005d596b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 360, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6eddfd0f3b44fc3a3df610de3299e93c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 168, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3dccbc486a059975984ebeee6d78aae1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 912, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c9e7e552f29406560613cdcf6edf1f60(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e0b8a468afa42acb554f6fd847b69fd4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_607419ae25c982be082aa693fe0c0499(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 832, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a6961af97a7325e9f8cee0830f14c4e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1dab39dc48e3976058964edea8bf1c8b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 432, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0170d858893c04ef1f764b8f454b1c07(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7ffb1baca89f7e93c352fe6779a1043a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 56, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_07c74da51ae1fbf655d3c32de2c9a846(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2558a4057535842ddd524a0509d9dfe8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 151, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7d75081894512a239de9baa6286d60ec(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 4], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_36867cfe0d427d0bf65674dc33f5242d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_302b8ac4be4641781bdad7e5bd462770(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 4, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b28e20ca96f6299c34ee0b9ed59dfdae(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 320, 320], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_33a1dc8f35fb9d3998e8fce4d1639b6a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 32, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_06008dff2bb6ed2d74a79abd98d5d209(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 2, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7eb3f173e0f486da352fdf72b7ae6550(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1440, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b2f867bf02b018cad316e750a052f504(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 366, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ecbe3f5e8232cc53a768d6c182b73d43(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_11dd5cf67625b11036d758eecb1b42ad(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3221e33a21ff351838887752183998ad(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8d234da127f620b1a3e18deabc540999(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e1ac63ef9adcef19b2334f96fd46e78e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_646cc790ede1a27d43b86cd7f6132614(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 234, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fae605f8f8fcbe1eed967f3eb64410bf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 256, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9cd9fdf0f0ba3414a86e6d2cebce7413(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 400, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b303f48a2aefc1cd17b59ae87b7f6f40(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 12, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_497860c8f1c0e72e0f5e0e2455750baf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 544, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9d2630e27ead1a0e7c01e247588cb65f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1776, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4ab8019795726746d62eda95b4846a9e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 432, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bd1960e236b094a99e5c348578b3351d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_58782030231505a96f33cf46249d3ee5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 232, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_719a37335ef78b87f0d429d84dbc0a78(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 44, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8f1817a54b7cf3b9732ac6ea88cdd75e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 624, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7fd813cc17caee7ff042a856fe1f81ea(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 432, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0b2944e68527ac88420019b5b51b2c94(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e345465b8a6475f676d9bd9f0fb2fbf9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 152, 152], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e10e11f30707bb41e59ba054d49b4446(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 88, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c5022c0ae4ec0ca6037f6f90cde59944(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1344, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_36833711feb0f4e2646dfae5104bc61f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 88, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f044c2d32a67fa5d11955f94bca4241e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_27db30941192b4e9aa125d2a52091117(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 147, 147], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6d7e44f23db7ebe89adfe1546d89c361(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 8, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4cfef3a590dfa367c7cde4f2dba2a634(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_48c128e2ea846d7424d929b08dc987fb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2bf9762d2fbb52a1b18ce17080cc019c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7544801020f3f2b950d5abb95dba417a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d95a87d34e04b8927bc9ed8fa4f8a8de(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d5b4c302d669e818313376291ba97907(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1344, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1052b13741792bb8e26c5143fe6cbede(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 8, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f0dc0d5586000229fe0ccef6d31e820(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 4], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f83d5d16586d1184c646d856d84929a2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 1, 2], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3947d05c01e0c5c057ec7dc4437c266f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 832, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_28f52c107a0b944c9f7543cf49695ddd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6830ac769c0dec5c76b21a9a309767f6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1872, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bc0354c6856429f4f6f3b20a8dd960eb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 410, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0c20c860fe4191afb55ee347faf404ba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6f0a497df196f87be438e689c9069a45(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 70, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_876c44264e906f50698c87a4feb9bc76(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2016, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0bdda4ae8026b844bb28643630e48459(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_392b6e6d2395056fede932e5192400cf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 82, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8eaddab02a6cf56bf0bbb0ba1dc3d8a1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1968, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bd114e207227f47e52655676b5f29b86(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 26, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9c09f120fb8fd128941d3314cb4b2b15(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a24178228a1bd7e23678f934e57d8a10(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 4, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e2d73cea34ec42bac77d26c99e0a36be(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 14, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_57fa71eb73514281463edc0ebfb2d144(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6372d0adc3a6ad62c44cf699982030e0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 906, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_75666d559218d785b9736c5a7760330b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d71dd061ec69d366f3a1994b75e209f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 48, 48], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_769ecb922ac47f424d2c94a07f872846(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c085ebaacab2ced2ab4a7af4d328ce9c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a6b1cd24e629b9388b14cc5103a65256(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 3, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aca9e60e78c60407955ff79052684d13(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 24, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_89c3b7a82aac670fc0dd97e9d86ec2ee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ae264b60110edb9df9d089d1a533158b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f04c2590566cf261939f7b00b8eefc3a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b72f9bcd9549c78c2c38be50dce2b64e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_268cab802ef8b3db725ce5584fde94ac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 2, 2], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ba22964962e08a1a38c286c94069ba3e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 410, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1ae30a30b9508b91487b7a9cdf1976df(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_99330eb6fd79a713347e8965b0c2d0af(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eb06ed33380ff5128794d2cad91788e2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 48, 48], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_200831cd13e7c6ffd486b3f50b280712(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 53, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2e024a00c36ba90e8bf743d7ffa4f4f1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 8, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2ec87daf2ba62a7d3db054019cd0776b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0db6c2df3a82416665f1661bb9e6069b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 2, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4faaad98174275e85e351b8321cc0737(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 1, 2], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aa754e0dec1fdaab2d2ef2a6953e806c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 150, 150], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a757ff0a49312760e11ad98a9f187288(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 4, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_42eb78d6c0104d701f4266c3162d47ce(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 448, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e070fc8706e89dcad544e80fbff99f9e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1408, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f0d017553de8350ad319c50ccb7af9a3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 200, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_df1a0d68f0c8a8c5985e43941202b327(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 61, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d0b7855174f95152462812e764c69fec(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1776, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4c9baf5b1e89eb55c64960ee18a1526a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 4, 256, 512], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_53a6bedce736a23c67bfed29cbeb1428(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 122, 122], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6a6baefe127f2aed92be67483842d1b8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 17, 17], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_59cc761d101593bc68e8d84a85c7e3a0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_649689231efe89e4acce06ae6d97ca99(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 19, 19], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0d6c4b50213dea12cae8c7dcada9bd4a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 100], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6b1be04aed189758f89c4dfae398afaf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a58fd30b2da1c3d59d00405af06bcb29(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1488, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_629af9234bc2af091ccbcb6f6c8a25d5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ac0790aa4b975c00bafa20cb1a162e04(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 270, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ca47415dc94e177da5399f9d2d05098f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_440699e0655218e36e49ff2de8131e1c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 366, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_42f3324d84977ee2a65e5cbdb0ee49ee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 184, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c634b875a716a3263e57797818352afa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1006bd1d35a2104efdbbe5a463b41996(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dcf9ee436befc941b5eb1c6978df2eb7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1392, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a46685e3a65d023536694d5acb19a474(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 47, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b5756c96804fd04f514d15ed5d26053d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 82, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4ca462692455fe22b08516c47aac4ca2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1696, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_504295f1a62817ed127e3c5047a771b9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_73e1b6d35ddbd22377bf5b3e20d2c063(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 17, 17], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7bc64274aefdefe010b3d1a2ca6f25ae(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 35, 35], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_73d1e3c136fe27e39601af79f4d3b780(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 32, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4c20e621973e9b169cb07a9388237f69(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 42, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2eea14b360e5218cadc042f4c37b4b25(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_885e016ac1c8f783960bdffc216cdb92(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1584, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4da0a829033f0b2d78be4e2cd1d17a84(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_27c01116e8373c9e400e6b2d1447a36d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4966945761b6a46abb2bab0d2ea303b9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3c1bceed4d9793e784b5af9311322939(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1376, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8005451b74156974b9f8c3bdecd1869a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 2, 2], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f4c953ace6219937dc5f3d29d56acc6f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 32, 256, 512], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6e64dca1f46e6db2c88c53c2f7071b1a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5970ebbf9e5ca37c909537d73c8c23b2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1104, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ba76283ac7864b3a44b6c2d1390eb005(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1664, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d027fc005a6588b6a5d0af75f8d25de4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 608, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a3ea7363cbaa6e7e586808adff53029c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2112, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4c57babe6181bcb45d08d01bca69c845(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0d715453fa1dd0517050f2b2e4b60cc5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c75e04496b6a6d54939e7c5fa2603b4b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 185, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aecaf4fe547a4b15bb29a70ce4c14be3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8362c0c40148fb78949ae8f25676d4b5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2208, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_954d349de3204a04035d96522134fd1c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b2605c47322bac3fc7b9b464224428dd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 570, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_802976aa2851b4a44a5e2a55280b6d73(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_549ec1bde5e06bcf051dda988d144794(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_969bdb606fc7df02e690edf32b4b6791(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_05a9cc3991b548380e322b3c623f0681(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_72385eed3d39b003d1292173e174fe58(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8e0eefb7b44abdb991061b8dc85599dc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 26, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_57d23a2ee53078649f2da6f94b896d0e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 87, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dcb4163818040f35adbaa54b38e97efb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_694df246a76a76566c1eca2426f24f48(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0c81981c01ad0a9edffcad553f43ce21(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 70, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c97c5341716ce308dec686efb2f12014(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8594b38456674c855f96c73388d87329(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1488, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_35ac1833b9e85383f2c459c8eddb04e9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b3b8b8ada2d30c4d759d1aeaf57c73ec(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b60ce7d7383b951c6c09035cd7c6ae72(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e11c95cc7427f9cc0bd8024558550e34(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1856, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7d7569b76d5c52ab8dfd466bf1e5f871(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a605aac016e97c6fcd434528314ec375(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_71643efcc00b40708c627945ece2b572(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 570, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1c04f25345cfdc3e01d9d5e29e3f1aea(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 150, 150], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0385aeb7f2592d049824dd6e0013a6bc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 82, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f35256c6987ae01fc59282858f016f74(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 1, 2], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bf252dd6f02b5e37cf045c7d6e4e77e0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 100], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c3f1b0d30f74cb3c1ec346d58cd0fb61(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2160, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fbdbb081f3e66a964396b5dadaf497e8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_67fb8612c9edfe5fac732401c499467e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 162, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0fa416a092fd26aa12327bb198b1ea53(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a3b48e69f617103c490a25a1463e6b6e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 4, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_82b8c229db6bac4047a2f6e051bbb939(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 816, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9fe8e318f9e3f1b99bf5afa3e614a390(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_98c9f8784c086c4b6003104c118cb41e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e091360a9dc8551bc9dfac31faa9bb86(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 61, 61], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_87609f7363ccfd0c9ad41d192d83c7bf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 106, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_229c4f5bdf2276e8fec5181915ef194f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3870136c55a61dbe4255f290f9f5229d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 35, 35], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6334b612830eea6ac377aed5d9c78d7c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad318dd75aa284f637b0e7f5eb998e77(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 2, 27], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1a0e4eb2d1458f317a54727f113a0f3a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_904d7c955ca2f29d87f71da96ab252a3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 117, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_34de9e07c9cd2c8ccb8a8ac8f6a118f2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 24, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_be456d79894f01470baf5d2adf47947e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bcd788e5034f3b64e37b82333b0c68e9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 4, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ced5bbc6d21d54d8516234798260a49a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c5926ecd252b574e5ffe2d623c46579d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f4f099bbafb2f8621462d2874a854505(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 640, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_818897f12c3e58af7a971e83894468ea(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 138, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f682dbbd686d5b25920b96638a789226(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2064, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5decc7cf36479641bfdceff2ef0b67ab(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d5b6b18fd83739929d4bf29ad21aa5d2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_479d1265d3940f6837168d2409bfab0c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6b67da2dc6cd498426b872463d124e80(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 61, 61], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_13de14c868a426721c0cb96db785900f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1920, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e628e759f551bcf3b1119f798e6d3c5e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f0b7dbf2343349a472f7c31db15249eb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f90543b094f5964d1cc75f84a42293a6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_300f5347e627b91e634e43032854e474(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_659f3f6c85f838a8953c76269a34f5e6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 416, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_72f1faf0d5dd60b5daae5c7c6aeb5525(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c7e4bd0785a3af95610aea77a329c761(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 185, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5658d425e3a629b11a169393ce491196(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 624, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1a7689754b10b757f495da621c155d04(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_26c2da0ec89bb1c16ecba137e96be629(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1472, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_df9a0e7206fb5523e7afb7eeb7240d52(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 149, 149], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eca46c0c0dbd9c42017d23d983248ff6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 304, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5621921342d2c7be7a6eaa0d99ec22c0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, None], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_92861e8f6358fdd7f8a3a9bfded4cae9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 7, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bcc380c0de9c566ab786073789905932(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 175, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ff78f7148dd14e23868929aaa57eb367(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 256, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8f17dbd5c6331345e4569fb87675ecd0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bda9edf9b54ea15877d93bbca90ef910(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 122, 122], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_22df8eff6f23a5c7fee75de99652a049(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1296, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a7ec28d79f1897ba4de7a783f74e4f27(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2, 112, 224], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d686072b3e435f59dc66e05339d2094b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 118, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cd0fa32e19221f8adbc9f2f420d5e103(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c748f6276d6dd7f7b45095268c74e34e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 168, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c2552c959feb2b4f9ce417f7b10dcd4a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_98ef736b83bc840321dd0706a3ef451c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2064, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9e24f6db7528bbe29cb6241bea4517c8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2, 112, 224], dtype='float16'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_96686e671db99fb56744fdcea90d94d9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_143171d1f65966790b65d55152abdec6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_601b6094c5cff05a20545a9eb961936a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 320, 320], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b627036bc3fb73b304bb72d74010417c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 800, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_93cb320ac254b431ba344460d306545a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 7, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5c2d86855092c40008889a654e6eccac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1b5d5dce7b635cdc705d6feebbe455e4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7739b4a5d8d215f0f7c41cb3ad8963db(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 224, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7a528d72cf832a7c5e4025abacbaef16(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e3ac002200ddd63946ef172938496af2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1952, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a6f6b0fc53f06f7eb488c25b2340b132(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 96, 96], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2fc5d4cfc6216dc71537bdb886ca60d7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1216, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fdae437e89e3e33bb9d0339de6445330(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a8597f8944c41cc89c931d785dc5b11a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 740, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f95bb9ac7d2f7f9f93c65d3836dc4ac4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aded98e76d05a0fdc962c4b59ed802ae(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c0c46a61d1a5ec54df3f807140cfa14e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_938d3c94f90e95a9ae0ffc4b20d3fecc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7c548a1ac1f39154ef8d1fffb5c22961(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a9e62ad354549d5663a624eaf90a41d8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1776, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_24a0d504bcb01e59cc6b9d339762c22b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b63f9a719dc776087cc96332a4bac651(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 12, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e39a6547cc7706001dc864480c8528c7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-06'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5aae600a71f4d4d54e4d91295a00c001(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 25, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_874f08923df5e3ae309589bf5cb392de(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 736, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5611ace1ca2dc4710ff1936ef434e5fb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 262, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a7b1943c476d38849ad556d9e3cd3ae1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_880ba3b5b6d4eca5e7aaff3c0d891e16(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 704, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_89baf72a5d487c9f56b563d86cfaf6e0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1104, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_50105d752212c7034c7f1a8c820b384b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1696, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fd488a54e0d353a7c640c782aabb1556(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_86ad272ab53e05b64ce164ae11e192a2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1440, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b8cd59a4f84b403dfd7e1a05ee3f9b9b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bb134e5cc31613bdb80dde3321d2af92(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 70, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cd492c51515037a69d636a3cdd23c96b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_39d7d868284bf05023b9e05055a83459(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 352, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_66bb2afd815be38ef9ebfdc703c3d00a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1792, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6555a3800dc6569fe24a47606da814d0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 49], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0c35c6a1bfb68a3f49970db044e7a722(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3384cf0f1a84400dd4c1538ab8437cf7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fca80ae17e75f89f2442030b9b0ee953(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 608, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f2153fbadfd81c26739b3fd1b42cdd8c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 12, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1b43f04c395b2995cd2a261567bbb188(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 28, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_254afa34bc0deb2a09aa46755aeeacc0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 96, 96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ec4e60b4f6fa4d2ae636cf4a470dbc22(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c886e9e0b61e94057a195f2a5d8a62a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 244, 244], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3855be21f7f6c1e06a43226451d2fa9d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1680, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_403a60594d7f08a9f716db95879af34a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1008, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_43a674d58b70eca9ec9e017a6508bc9a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1968, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ac3066ac3a8407dd837bc5573682e0c3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1984, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b6d2d1bc1dba803f2819f526653f61d9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_07ffef74791a7f8c9605461b7fc2aa2c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f86c083c6447c147936e4451eac4a968(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1248, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e7dc883c55c362000cc66d708c52273b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 2, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b95d76b02c8748435f870e3f9a91e4da(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d34f2c21ac7a968e129e44a18e3a6083(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1312, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_71f92d3de05a13e0d440c523adf31f01(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 1, 2], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_510a51441ae3be71d1b9c12a0c615041(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 61, 61], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dd08cd4a2f410aa2cc417bf700e494a4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 436, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7180571d272402dcbd5271361f36fa2b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 58, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3035e61238ed1c0d4be9b740d7683fbb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_983eef777acf9a7aefdc96b9c6601bea(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f15ab9e32e17bc4c1e69d0a5ac419bd4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c822c21d116c98477992b55e675000bd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1e817b2e16824af6b0cbae1e7682cefd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8d532e006727e94686350be773d24991(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 608, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b75b89f64116cecb2321fdfeab13443c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1c85abdb446c4922a45efa6015b2e757(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 350, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_430f52ffa401694663e67b9e1ac47655(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 49], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e8e11688194d527731b690562dd01468(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b95a3b045f9469f763becf58da32d7c5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 1, 26], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_89f4410d039f182043fd36394bb3b16c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 832, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a4f9cd12f805d6a455cdf7f9194ae953(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 87, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b7b8fb171e8678861c638e56f2e7a318(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_57d674166ec1b9ad58428e5f8a45fdde(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 3, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_306c3cd8a840d91482481b6f03929bac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1200, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9d33ae34714c925e8a0ef95d5ccbe595(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 448, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d9e83ff33ce1c55d4a2b8e015cb948bf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 704, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e21c5ca9305b5b198e3f18abd34adf4f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 448, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0953a04ddae5c94777e8f40924fda912(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 840, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c5a58fc070072d4d0ac2eaba6bde1680(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 48, 48], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_074b9c4af1a5c37b5704be002efa50c6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_20fdd61c40ea40ac93e57a14448bd357(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 3, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_65dd1f31a6480ff4489eb0f997ff109d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 75, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6b121c8d719f189b982b8e5d9e4b948c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2e6712e20e6d8f284f636ba686fce6d1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0a9e8e778d9a53f24e88808760026198(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1792, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b17f1e87686840224b9d4e2624e4ab44(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 32, 256, 512], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_395b324074745aede20335d007bb6862(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e283e7fe79b11599faf419ada3264ed8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2d0ad91c15283b4d3555f2c26302d57c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1120, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_60f5a8220a50cd84fde0d9b04d5bd647(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2016, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0b3c8901368465fbb082106df284000c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 608, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b346fa274c1d0f7d28e110e75045d4c8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_44b3ac852c8f8302f330f4e80a3982a2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 4, 4], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_02036483bcb4ffc4dc0d7e3dd8d8557d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8fa04200ea5df44c3aea918524f6a11a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4c6e24ac0e4424149f77e83ad02db4b6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1632, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_75acb11511d57eb6ef3f21fe1b4c3983(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 44, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_78d351d4d4701205a62ba2d7c5a8238d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 70, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4c9418f1eaab0d40d05d6cdf4a3309d7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c01e9b7b03892ac427e2fa50ba985b26(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c27dbc1b15c80b4ee14bf7e6fd8c4fe5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 88, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ab649cc550a5ee183ed24541c3047195(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 504, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f0e683cfd73303d343d5d73efeb4fbf8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a01ea4e4cdc8c80215f53d33cfbc1d38(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8017524d12acfd44972f8667aa999324(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-06'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_68b578d8fbb3450824b5069e5729f05b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2160, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_385a5f4efa667322e6aa029c742806d2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_59de49cd29b9f0ee1ec221c024727934(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 300, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_678242881fe9e097a74108666aaba129(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1408, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e6cc101059f1248f6e2fe1aef47236c3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 232, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7c568c11d8c3163482b95c3d3a657f92(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 28, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_db627954081ea8a56368241520f7b59c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1680, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2298a68696cb52b5fdfc89a2339435e6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 436, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9ab75b2ab28546bf1f47e69787df1fa3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 236, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_26c05a2ef984366fbd30f57ca9ea1899(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3cf81c893ce79f7263032b4be8273a5b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 75, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_466c22f028bc8a33c2201746c72e5f5d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_797784368912c867192c03ae1e1468e4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 19, 19], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fe66ea4b4c8d35570fffeb3019a6f0fc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 100], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b9de34a4771006842f586b2d9cf2adaa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 8, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f296f9f1bf6bcdfa7399ed96a952830f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8ab03d09e6f76c6f4d3788057e3fef08(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2bd84bbf1a121dfe73236493fa149bf9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_08fe3b59f6228c60210022a15b53991f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c72b7f0b7aa403554fbdc29953918512(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_59356da0c8c029c3f09733b93d87e678(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1200, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7de7640443fa7ca33b12acfa61c9b9eb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 740, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0f0465c67aab045c3b8ee96263830081(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 208, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_504180e823759d7333ab93001c5bc171(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ed9cc4fe183548a0abbe4c901140f54f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 96, 96], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_06a2084c35d8fa6a945757f45fb82cc5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1728, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_208ac7c87f49fc0d3ac6a3fc2be80d35(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 19, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6e28d4998653af5d2efd9a7fa917d975(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 1, 26], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_268854f799869ad24742f0691bc947a1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a99c739294a6ab928cfcabbd26c5064d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 2, 4], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fb440414beeddf080737e1e3c0e3676f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 864, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_58b7ad7d3d4e1e838bf3cf1624f2b8eb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 17, 17], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_38401f5de89704897e13930d6f4e5041(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 528, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_22f25e9ca31486b06bc34db77d3abec8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1760, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6f7b9f25d66a0f3a1786ce953f6d675a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 912, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e7d9ae236cd7955cf8c420a74962da83(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d70bf74bad32b47447c8c800f3725cb9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1488, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_336efc7ccaedba2accd041ac755ae2d8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 149, 149], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_92889d170764bf722abfae6a3fec558e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 53, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d190ae1e7d4022884d32df66410bae66(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 44, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2f8b94283e43cb7d5aba69a95cc44b4b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_835032c0c9609219b9f668566bdbaf05(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_916fdcf47c8919b47f4843dfe7b7d4ba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7e96bec2afc11012236ef0dcb9ad3c94(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 147, 147], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_482c9dac8592a0b6031b4249440413fb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1440, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_48bb4a5f6251cc42551e90f595a3f3a1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 624, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_487d6d99c23fe7a1f914d061ea68fe57(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 960, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_53a3c3f0fa225a8d48629fdc9ef3cbfb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 234, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5b6956cfb0ed4f18b4735f6111a110af(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 128, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_af5ce8a9fe36f2c2dc49bdf82904e453(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 175, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1de7808d953ceb7c0601af9e94b2ccbe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1728, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_246cc761ab607e2b7b05d4f9124510b1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 118, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6dc1ebd92c1e2a000e0c40fd1d98cde1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3c8999f3e943997c8ad6248cd94eabd3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 95, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3e9be5809c8e7303ee5cbb9d4e65e373(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ab2f907e333eeccd91e733552ea243a0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 48, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6ae8d8d2108984e85d6927fd134c865e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 48, 48], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0e9da9b0c8527c12b8c2ae5cfbc7b5b2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 236, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fae8faf6c3097eb6fac4da990e34e1d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5837fe86d17c24898cab46aff984c571(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 118, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_37b9b1530dea0eecb6ed6b91a3dee491(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_358d9f4e3597f9d26bbd66a986fdee59(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0422459bb693833ee36909f7e473c767(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 44, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_677ca42810d7150fd4f03f5ace523ad0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a8559fe053cc9b81082a70295bbab6d2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8341d1141c635cbf4891cfb8a173f173(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 350, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bf444c5e5ebe5b539e02bc7aa3355a6c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_13c6487f2e9d1445e8a76a008e335d17(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9e27dba01bbc3b0ca3b95500fa28f676(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1568, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a1d9db8e4f2a866b260ea866f1d969e0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6aa858d005a3a6e367ab6194177e0f0e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e0e009e815552d0f71bed77a5b27eda5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f8666f7deb247bb1039cc5b499dc1af7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a66155bf6b0a298685b46ea71d80334e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 960, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_59e727a10381e499385a14e5fbd06426(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4a5c58fd1328c566f93558ae4ad6900e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 1, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bdda24c01791ca492b294af5596be01c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_521f6c392b69f184cdb9fff1b7369ab2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3eef649b1afca82aab794ca9afb58e65(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1104, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c0f960c5d993ef13a21af08b96baff94(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 17, 17], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_562f9d0851ec4870c3c9e0225a3b68ee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 400, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5be11773eb8ceeff075f52a13134ba13(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1600, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a2620e7ea5f8ae39aee4fcb1bac8b5d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 38, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_179fc988084476500e9e5faea6618ec3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5702c67e6d3ae76585ecc12dd09038b2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2112, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3bc79aafa31b1499569141b0504c805c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e227e1f1f4042a7e59003ffaa9bcfbd7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 416, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7f4c1238afa6b935a360b35f6872525a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 2, 2], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6b8734c8025880d94878a1d66bdbb112(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 84, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_212cc0eba121100150c3b71ac89d0345(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 736, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2a8e02206f72431eda668eaa1469ad03(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1296, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_706af40768bacf0a8e2e4c57dc3541a6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 28, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a1c9bd4c10b0d793e6f0732046e76545(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_779f76660be854e9a304f1fecb94102e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 224, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eb7995854a25eac55a2f84d06805ec62(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f0a87dec6fa26c0e5a385af114cfc7e0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e38a3a3b08732be2343eb8f45a513da3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 176, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a4567af831f78611d177a75d74a3d3d8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 73, 73], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0b64bec70d620a42a7ee08f2964135ac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ddb406a8cdfb48d33057426c80f42358(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_71cf6398bde2b789fbd9208603d84167(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d4c0386f721d42b05b909863dd5202ba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1856, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_62cdcece8b80703162417621dc7a9429(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2016, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d5c105049ca799360d749135f8cde2b3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 71, 71], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_44695e83a47c04833da21c83cce404e9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 4, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fe8126fc9e2ddc15dc56516bfb9f54c5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2064, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fdd2f8048c32861813b20f0025db51a6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1184, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0528bc1276d6578de90b64e3b84a9c9c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2432, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b4d1a2ab163b6bc4659dccdba1f5a4a5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9a0db244280f9394b8ffb739c92e64c9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1056, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8eadd6408644b6b0026d5e22421f41d8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 24, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_436379556c4304423b2b219f2931ddaa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 3, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c7f2aa2ff0169ceb9a96978ab6908cf9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1568, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f20122a24edbf3f8de1e93f9d667586(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 304, 304], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a73987f836ac9bd9919c3063b2b119dc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c06aefd9192633ff58e04c1d03b1bcfc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_70cd8be3abe10607d29b24db78c23b3e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_afdbe08c4cca6d6ffe3574109c40a613(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 12, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7e2434225e53abd8789e8c3a1d3ff9e2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_649a2dd567654df86e1c7132bb70fc07(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d3afc6fc2fb812a45115be7d1f811789(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1952, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bc412d75852964586ad628ccdb876c16(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm_(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None




if __name__ == '__main__':
    unittest.main()