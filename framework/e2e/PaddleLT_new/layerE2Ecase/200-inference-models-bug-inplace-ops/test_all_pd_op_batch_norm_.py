import os
os.environ['FLAGS_cinn_new_group_scheduler'] = '1'
os.environ['FLAGS_group_schedule_tiling_first'] = '1'
os.environ['FLAGS_enable_pir_api'] = '1'
os.environ['FLAGS_cinn_bucket_compile'] = '1'
import sys
import unittest
import numpy as np
from dataclasses import dataclass
import typing as t
import itertools

@dataclass
class Stage:
    name: str
    env_vars: t.Dict[str, str]

cinn_stages = [
    Stage(
        name="dynamic_to_static",
        env_vars=dict(
            PADDLE_DEBUG_ENABLE_CINN=False,
            FLAGS_prim_all=False,
            FLAGS_prim_enable_dynamic=False,
        ),
    ),
    Stage(
        name="prim",
        env_vars=dict(
            PADDLE_DEBUG_ENABLE_CINN=False,
            FLAGS_prim_all=True,
            FLAGS_prim_enable_dynamic=True,
        ),
    ),
    Stage(
        name="infer_symbolic",
        env_vars=dict(
            PADDLE_DEBUG_ENABLE_CINN=False,
            FLAGS_prim_all=True,
            FLAGS_prim_enable_dynamic=True,
            FLAGS_use_cinn=False,
            FLAGS_check_infer_symbolic=True,
        ),
    ),
	Stage(
        name="frontend",
        env_vars=dict(
            PADDLE_DEBUG_ENABLE_CINN=True,
            FLAGS_prim_all=True,
            FLAGS_prim_enable_dynamic=True,
            FLAGS_use_cinn=True,
            FLAGS_check_infer_symbolic=False,
            FLAGS_enable_fusion_fallback=True,
        ), 
    ),
    Stage(
        name="backend",
        env_vars=dict(
            PADDLE_DEBUG_ENABLE_CINN=True,
            FLAGS_prim_all=True,
            FLAGS_prim_enable_dynamic=True,
            FLAGS_use_cinn=True,
            FLAGS_check_infer_symbolic=False,
            FLAGS_enable_fusion_fallback=False,
        ), 
    ),
]

def GetCinnStageByName(name):
    for stage in cinn_stages:
        if stage.name == name:
            return stage
    return None

def GetCurrentCinnStage():
    name = os.getenv('PADDLE_DEBUG_CINN_STAGE_NAME')
    if name is None:
        return None
    stage_names = [stage.name for stage in cinn_stages]
    assert name in stage_names, (
        f"PADDLE_DEBUG_CINN_STAGE_NAME should be in {stage_names}"
    )
    return GetCinnStageByName(name)

def GetPrevCinnStage(stage):
    for i in range(1, len(cinn_stages)):
        if stage is cinn_stages[i]:
            return cinn_stages[i - 1]
    return None

def IsCinnStageEnableDiff():
    value = os.getenv('PADDLE_DEBUG_CINN_STAGE_ENABLE_DIFF')
    enabled = value in {
        '1',
        'true',
        'True',
    }
    if enabled:
        assert GetCurrentCinnStage() is not None
    return enabled

def GetExitCodeAndStdErr(cmd, env):
    env = {
        k:v
        for k, v in env.items()
        if v is not None
    }
    import subprocess
    result = subprocess.run(
        cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
        env=env,
    )
    return result.returncode, result.stderr

def GetStageExitCodeAndStdErr(stage):
    return GetExitCodeAndStdErr(
        [sys.executable, __file__],
        env=dict(
            PADDLE_DEBUG_CINN_STAGE_NAME=stage.name,
            PADDLE_DEBUG_CINN_STAGE_ENABLE_DIFF='0',
            PYTHONPATH=os.getenv('PYTHONPATH'),
            ATHENA_ENABLE_TRY_RUN="False",
        ),
    )

def AthenaTryRunEnabled():
    return os.getenv('ATHENA_ENABLE_TRY_RUN') not in {
        "0",
        "False",
        "false",
        "OFF"
    }

def GetNeedSkipAndSkipMessage():
    current_stage = GetCurrentCinnStage()
    assert current_stage is not None
    if not IsCinnStageEnableDiff():
        return False, ""
    last_stage = GetPrevCinnStage(current_stage)
    if last_stage is None:
        return False, ""
    exitcode, stderr = GetStageExitCodeAndStdErr(last_stage)
    if exitcode != 0:
        return True, "last stage failed."
    return False, ""

def GetCurrentStageTryRunExitCodeAndStdErr():
    if not AthenaTryRunEnabled():
        return False, ""
    current_stage = GetCurrentCinnStage()
    assert current_stage is not None
    return GetStageExitCodeAndStdErr(current_stage)

def SetDefaultEnv(**env_var2value):
    for env_var, value in env_var2value.items():
        if os.getenv(env_var) is None:
            os.environ[env_var] = str(value)

SetDefaultEnv(
    PADDLE_DEBUG_CINN_STAGE_NAME="backend",
    PADDLE_DEBUG_CINN_STAGE_ENABLE_DIFF=False,
    PADDLE_DEBUG_ENABLE_CINN=True,
    FLAGS_enable_pir_api=True,
    FLAGS_prim_all=True,
    FLAGS_prim_enable_dynamic=True,
    FLAGS_use_cinn=False,
    FLAGS_check_infer_symbolic=False,
    FLAGS_enable_fusion_fallback=False,
)

import paddle

def SetEnvVar(env_var2value):
    for env_var, value in env_var2value.items():
        os.environ[env_var] = str(value)
    paddle.set_flags({
        env_var:value
        for env_var, value in env_var2value.items()
        if env_var.startswith('FLAGS_')
    })

if GetCurrentCinnStage() is not None:
    SetEnvVar(GetCurrentCinnStage().env_vars)

def GetEnvVarEnableJit():
    enable_jit = os.getenv('PADDLE_DEBUG_ENABLE_JIT')
    return enable_jit not in {
        "0",
        "False",
        "false",
        "OFF",
    }

def GetEnvVarEnableCinn():
    enable_cinn = os.getenv('PADDLE_DEBUG_ENABLE_CINN')
    if enable_cinn is None:
        return True
    return enable_cinn not in {
        "0",
        "False",
        "false",
        "OFF",
    }


def GetTolerance(dtype):
    if dtype == np.float16:
        return GetFloat16Tolerance()
    if dtype == np.float32:
        return GetFloat32Tolerance()
    return 1e-6

def GetFloat16Tolerance():
    try:
        return float(os.getenv('PADDLE_DEBUG_FLOAT16_TOL'))
    except:
        return 1e-3

def GetFloat32Tolerance():
    try:
        return float(os.getenv('PADDLE_DEBUG_FLOAT32_TOL'))
    except:
        return 1e-6

def IsInteger(dtype):
    return np.dtype(dtype).char in np.typecodes['AllInteger']

def ApplyToStatic(net, use_cinn):
    build_strategy = paddle.static.BuildStrategy()
    build_strategy.build_cinn_pass = use_cinn
    return paddle.jit.to_static(
        net,
        input_spec=net.get_input_spec(),
        build_strategy=build_strategy,
        full_graph=True,
    )

class InstanceTrait:

    @classmethod
    def instance(cls):
        if cls.instance_ is None:
            cls.instance_ = cls()
        return cls.instance_

    @classmethod
    def static_instance_with_cinn(cls):
        if cls.static_instance_with_cinn_ is None:
            cls.static_instance_with_cinn_ = ApplyToStatic(
                cls.instance(),
                use_cinn=True
            )
        return cls.static_instance_with_cinn_

    @classmethod
    def static_instance_without_cinn(cls):
        if cls.static_instance_without_cinn_ is None:
            cls.static_instance_without_cinn_ = ApplyToStatic(
                cls.instance(),
                use_cinn=False
            )
        return cls.static_instance_without_cinn_


class CinnTestBase:

    def setUp(self):
        paddle.seed(2024)
        self.prepare_data()

    def _test_entry(self):
        dy_outs = self.train(use_cinn=False)
        cinn_outs = self.train(use_cinn=GetEnvVarEnableCinn())

        for cinn_out, dy_out in zip(cinn_outs, dy_outs):
          if type(cinn_out) is list and type(dy_out) is list:
            for x, y in zip(cinn_out, dy_out):
              self.assert_all_close(x, y)
          else:
            self.assert_all_close(cinn_out, dy_out)

    def train(self, use_cinn):
        if GetEnvVarEnableJit():
            net = self.prepare_static_net(use_cinn)
        else:
            net = self.prepare_net()
        paddle.seed(2024)
        out = net(*self.inputs)
        return out
    
    def prepare_data(self):
        self.inputs = self.get_inputs()
        for input in self.inputs:
            input.stop_gradient = True

    def prepare_net(self):
        return self.get_test_class().instance()

    def prepare_static_net(self, use_cinn):
        if use_cinn:
            return self.get_test_class().static_instance_with_cinn()
        else:
            return self.get_test_class().static_instance_without_cinn()

    def assert_all_close(self, x, y):
        if (hasattr(x, "numpy") and hasattr(y, "numpy")):
            x_numpy = x.numpy()
            y_numpy = y.numpy()
            assert x_numpy.dtype == y_numpy.dtype
            if IsInteger(x_numpy.dtype):
                np.testing.assert_equal(x_numpy, y_numpy)
            else:
                tol = GetTolerance(x_numpy.dtype)
                np.testing.assert_allclose(x_numpy, y_numpy, atol=tol, rtol=tol)
        else:
            assert x == y





need_skip, skip_message = GetNeedSkipAndSkipMessage()
try_run_exit_code, try_run_stderr = GetCurrentStageTryRunExitCodeAndStdErr()
class TestTryRun(unittest.TestCase):
    def test_panic(self):
        if not AthenaTryRunEnabled():
            return
        if try_run_exit_code == 0:
            # All unittest cases passed.
            return
        if try_run_exit_code > 0:
            # program failed but not panic.
            return
        # program panicked.
        kOutputLimit = 65536
        message = try_run_stderr[-kOutputLimit:]
        raise RuntimeError(f"panicked. last {kOutputLimit} characters of stderr: \n{message}")
class PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4f998d3d9ac3feb4d8b3e7092686ec07(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_4497fcdfb9639144ea558daa18214543(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2ed4c31fcd8c6db6a7350e137957e302(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 640, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8bf76ce93bf64534ffab3fde21bdad3a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.0929960235953331, 0.3299316465854645, 0.32210904359817505, 0.10116861015558243, 0.43714964389801025, 0.07809676975011826, 0.09739432483911514, 0.1271452009677887, 0.36612775921821594, 0.22209827601909637, 0.09272671490907669, 0.43896085023880005, 0.15252798795700073, 0.2475278526544571, 0.3550485670566559, 0.07734047621488571, 0.048469312489032745, 0.14390376210212708, 0.3266080915927887, 0.35103318095207214], dtype='float32').reshape([20]),
            paddle.to_tensor([0.0382099449634552, 0.06149868667125702, 0.4355745315551758, 0.13235881924629211, 0.48605552315711975, 0.4794461727142334, 0.2580254375934601, 0.23622417449951172, 0.47897812724113464, 0.4117967486381531, 0.03594609722495079, 0.18518002331256866, 0.2055714726448059, 0.1650618463754654, 0.28536802530288696, 0.43322983384132385, 0.06785562634468079, 0.23277021944522858, 0.4377361834049225, 0.12351282685995102], dtype='float32').reshape([20]),
            paddle.to_tensor([0.1781703680753708, 0.010039914399385452, 0.2136092483997345, 0.4563579261302948, 0.39703986048698425, 0.09708216041326523, 0.037634868174791336, 0.43432652950286865, 0.17420221865177155, 0.15866105258464813, 0.1313372254371643, 0.4603956937789917, 0.2930104732513428, 0.24716943502426147, 0.03328990936279297, 0.3346344828605652, 0.07909317314624786, 0.14388561248779297, 0.24053314328193665, 0.08807466179132462], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4325007200241089, 0.31734418869018555, 0.362140029668808, 0.1303953379392624, 0.05371370539069176, 0.4817274510860443, 0.3985459506511688, 0.36128655076026917, 0.4654996395111084, 0.4578417241573334, 0.24464978277683258, 0.485271155834198, 0.24362289905548096, 0.149263396859169, 0.20875486731529236, 0.01681029237806797, 0.3997032642364502, 0.4595152735710144, 0.09891675412654877, 0.34893226623535156], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_1a2b114b88ba56b95370f75ce469d256(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_752a1e247095788c510518931c242ac8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 1, 1], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_548e5b257d3e6edd65480a65972b2be6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_92401916800b421a48c54bfd781a24bc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 76, 76], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2aed4fdf51323f531ec7773bdc527819(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 360, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([360], dtype='float32', min=0, max=0.5),
            paddle.uniform([360], dtype='float32', min=0, max=0.5),
            paddle.uniform([360], dtype='float32', min=0, max=0.5),
            paddle.uniform([360], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_908f29e2942977ed1a09d74ec5005e39(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 62, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([62], dtype='float32', min=0, max=0.5),
            paddle.uniform([62], dtype='float32', min=0, max=0.5),
            paddle.uniform([62], dtype='float32', min=0, max=0.5),
            paddle.uniform([62], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_0edd578f48a07aa5d1d79f9f678b472e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1eed38cd5670f1650acc51cbfb8d06bf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_0edd578f48a07aa5d1d79f9f678b472e
    def get_inputs(self):
        return [
            paddle.uniform([196, 640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1d1109c16c316b9a965a4a94622f0c0c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9ebd08b35877c937f8f7edfbd60efc98(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 2, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e59e5e3b4e3af3a1769b356c33d1f663(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2afe8847fe333d0d2c78ba47be862446(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 35, 35], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1ccb5464a2502be094df9404274e1449(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cdf543476e4156bf1d278fb16b955099(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([8, 512, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_db635fe6fc7b1030375ebe702087e43f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.18617188930511475, 0.41273292899131775, 0.050670623779296875, 0.414217472076416, 0.12903977930545807, 0.4555644094944, 0.45220711827278137, 0.2498123049736023, 0.3578297793865204, 0.15566779673099518, 0.42064908146858215, 0.4068164527416229, 0.16336190700531006, 0.20112964510917664, 0.44904330372810364, 0.3609147369861603, 0.48778900504112244, 0.17508704960346222, 0.23714041709899902, 0.22308598458766937], dtype='float32').reshape([20]),
            paddle.to_tensor([0.24583891034126282, 0.035848524421453476, 0.10394634306430817, 0.11367375403642654, 0.07565726339817047, 0.22519145905971527, 0.08864652365446091, 0.010285874828696251, 0.08008681982755661, 0.3985644578933716, 0.0320260114967823, 0.17137138545513153, 0.33609914779663086, 0.43460166454315186, 0.48845458030700684, 0.3374215066432953, 0.02584076300263405, 0.29477372765541077, 0.14197205007076263, 0.2691424787044525], dtype='float32').reshape([20]),
            paddle.to_tensor([0.18751801550388336, 0.03014674223959446, 0.4992310702800751, 0.03735034912824631, 0.2970842719078064, 0.454681396484375, 0.012447167187929153, 0.1682669073343277, 0.4136621356010437, 0.32204076647758484, 0.38922935724258423, 0.3355993628501892, 0.2839016020298004, 0.08882974088191986, 0.42225658893585205, 0.38037630915641785, 0.3133946359157562, 0.2582109272480011, 0.47355833649635315, 0.21847373247146606], dtype='float32').reshape([20]),
            paddle.to_tensor([0.14051496982574463, 0.36598631739616394, 0.1032949835062027, 0.2939830720424652, 0.4844250977039337, 0.30086955428123474, 0.3774814009666443, 0.2511349022388458, 0.4994999170303345, 0.24560685455799103, 0.24561640620231628, 0.008182420395314693, 0.04292561113834381, 0.2090734988451004, 0.45154839754104614, 0.18497346341609955, 0.11129896342754364, 0.14391888678073883, 0.06939919292926788, 0.17205381393432617], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_9d5dde09230a5209a57565a3b7304f49(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_95e811825649fd76b0bef5efe3ef21de(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_9d5dde09230a5209a57565a3b7304f49
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 49], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_78d05869059c0de0a3565dd91bb748aa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([8, 256, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4dbe1df24e24eacf815abd30731be171(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_da4d85bc213f70819a72e68f21214cdb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cb581f74c4127889a3da3402b1dd7167(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 4, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f03a2c54d78746e32418082eb7ba541e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 17, 17], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f489b6ee7d6d925c1b9fcc59b3cd7892(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3345983624458313, 0.10529537498950958, 0.3726085424423218, 0.4054034352302551, 0.3715493381023407, 0.36441683769226074, 0.2361483871936798, 0.38307252526283264, 0.2566564679145813, 0.1955818235874176, 0.42530420422554016, 0.3540489375591278, 0.44766417145729065, 0.11149060726165771, 0.12198227643966675, 0.4300611615180969, 0.4458991587162018, 0.21189606189727783, 0.4043492376804352, 0.4832376539707184, 0.08697855472564697, 0.31989073753356934, 0.3751211166381836, 0.34742629528045654], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4093407094478607, 0.19344313442707062, 0.19331668317317963, 0.2650316059589386, 0.08007247000932693, 0.18899822235107422, 0.25640732049942017, 0.1178121417760849, 0.2897116541862488, 0.13862182199954987, 0.3567503094673157, 0.2680903673171997, 0.18460234999656677, 0.4421272873878479, 0.3428021967411041, 0.20557573437690735, 0.3328528106212616, 0.47537195682525635, 0.07582822442054749, 0.16090208292007446, 0.22294896841049194, 0.34060385823249817, 0.13094212114810944, 0.19461673498153687], dtype='float32').reshape([24]),
            paddle.to_tensor([0.35687732696533203, 0.4041483402252197, 0.08819630742073059, 0.4885686933994293, 0.2366734892129898, 0.10634986311197281, 0.42049071192741394, 0.45696839690208435, 0.08770962804555893, 0.13975979387760162, 0.07407425343990326, 0.3014792203903198, 0.23528477549552917, 0.06370242685079575, 0.08239643275737762, 0.09709655493497849, 0.3304489254951477, 0.4386419951915741, 0.1659458726644516, 0.43240687251091003, 0.3755645155906677, 0.1253439486026764, 0.4600987434387207, 0.12582357227802277], dtype='float32').reshape([24]),
            paddle.to_tensor([0.169979989528656, 0.3707655370235443, 0.07771031558513641, 0.08857599645853043, 0.45834973454475403, 0.044428750872612, 0.1175752580165863, 0.14650949835777283, 0.14913801848888397, 0.39531031250953674, 0.3089844882488251, 0.2316598892211914, 0.35684946179389954, 0.4667351245880127, 0.01309904083609581, 0.10560253262519836, 0.3530164062976837, 0.3077116310596466, 0.23164628446102142, 0.24504032731056213, 0.012348643504083157, 0.16973495483398438, 0.08411580324172974, 0.22583888471126556], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_fc6b6917e04b7b7cf9fdb39dde97ec40(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e3998578c3662eed154471ae3513e67f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_fc6b6917e04b7b7cf9fdb39dde97ec40
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_41c4069d4ffdc77ecbca165e7912fd61(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_183fecd1ba0631fd4f29b967c2b6012d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 350, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b31703b7bb1999e0429b34c03ceda206(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 4, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cdcd10a9a13f61fccab1d926f6ceacdc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 240, 240], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_849cdf651ac47a2e10e3c9852ebd6703(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 112, 12, 12], dtype='float16', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_765095a358e3cb8a79ab260d3782d182(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 32, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_53b04dff33e61d0d4776e6a2bad8a478(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1888, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1888], dtype='float32', min=0, max=0.5),
            paddle.uniform([1888], dtype='float32', min=0, max=0.5),
            paddle.uniform([1888], dtype='float32', min=0, max=0.5),
            paddle.uniform([1888], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6145d805b9d2025bec8ae85e89b783ed(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 16, 12], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c2ed835ccb7cea31f6d8f17e874f319e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 2, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1b1c7e4c8d991282aa0d0aa711fd3448(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 175, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4796b3ac372d92b0919f1deee45e56e5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 4, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bd6e0669c42e2722bb6a19dc9b64ecb4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([8, 128, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f69a2e73735b68ca9a3ae9e3efe0d08a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a806f0d0d33b40f69c4d4f3030b694ef(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 8, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bd78e080520db67791c9f82f48afc04c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9fa679bc1dd184b4cc747400a78fb632(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 32, 8, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b11522da1514d39c31ec9e60418e62d8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 60, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6122b1a0441921508d39c58afd6eb10e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e3ea92bb071d55144e266a6c8f999e16(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e2e55b2214eeff4b7df8046b485348f7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ce5072c310c224cb156b6b98d333d3ae(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1488, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1488], dtype='float32', min=0, max=0.5),
            paddle.uniform([1488], dtype='float32', min=0, max=0.5),
            paddle.uniform([1488], dtype='float32', min=0, max=0.5),
            paddle.uniform([1488], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_eb2a519f04601244cfdb59c497a966d4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_650cb3d5dac21c3a05ed2dba9346ea24(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 8, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_eaa680a4ae5cdb85f42a36941b55ea3b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_43b7fef28f60b921a302b4d5bd0c9f06(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b1f8c57a96ac87455883a1c03cde7874(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c40ea426a5f666f725c8c86c84ac4540(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1280, 6, 6], dtype='float16', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a33430326591022362acec9607a09fce(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 8, 6], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b9f2630dedb605cb6cc4d1e88a6fabd0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fe1797688acc54ad18730fa2adcedda9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bb7958a3e57751b8d279354558eab8fd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4940708577632904, 0.3126141428947449, 0.2945505976676941, 0.2934877872467041, 0.13540011644363403, 0.18751005828380585, 0.20768435299396515, 0.38041266798973083, 0.3974311947822571, 0.04516473412513733, 0.36021268367767334, 0.04215725511312485, 0.2826055884361267, 0.029989928007125854, 0.17779958248138428, 0.12371079623699188, 0.2088773399591446, 0.1613120287656784, 0.02617960423231125, 0.17587776482105255, 0.06178323179483414, 0.27068212628364563, 0.4742787182331085, 0.008340580388903618, 0.26609355211257935, 0.33264413475990295, 0.1261967569589615, 0.4834485948085785, 0.17592217028141022, 0.3396080732345581], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4315936863422394, 0.43031057715415955, 0.20507924258708954, 0.17745479941368103, 0.4615541696548462, 0.3057343363761902, 0.12337325513362885, 0.36712440848350525, 0.36028963327407837, 0.20780718326568604, 0.18961188197135925, 0.4962827265262604, 0.2742822468280792, 0.2114616483449936, 0.49365347623825073, 0.06842691451311111, 0.40834951400756836, 0.30206161737442017, 0.32420557737350464, 0.2696823179721832, 0.4549886882305145, 0.036523137241601944, 0.3269031047821045, 0.2969520390033722, 0.17941054701805115, 0.3433779776096344, 0.31414926052093506, 0.21595604717731476, 0.06300316751003265, 0.45669978857040405], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4811457693576813, 0.0465245395898819, 0.037401046603918076, 0.264100044965744, 0.20264288783073425, 0.43355193734169006, 0.28472158312797546, 0.05245402082800865, 0.4424992799758911, 0.3729780912399292, 0.3962555229663849, 0.46373483538627625, 0.29386094212532043, 0.14264130592346191, 0.09795472025871277, 0.31807029247283936, 0.1573382169008255, 0.22565671801567078, 0.4904884397983551, 0.3870457112789154, 0.16708317399024963, 0.02437737211585045, 0.4808324873447418, 0.19839096069335938, 0.43374520540237427, 0.007968166843056679, 0.22882795333862305, 0.18878817558288574, 0.39246034622192383, 0.44981649518013], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4272633194923401, 0.2462128847837448, 0.0653102844953537, 0.19120238721370697, 0.18390822410583496, 0.4962614178657532, 0.057647302746772766, 0.20805519819259644, 0.411813348531723, 0.30936166644096375, 0.4455847144126892, 0.3841199278831482, 0.1423860341310501, 0.3353794813156128, 0.1929810494184494, 0.03816034272313118, 0.10275478661060333, 0.09548665583133698, 0.019651882350444794, 0.07930415123701096, 0.1952114701271057, 0.3612549901008606, 0.2505890429019928, 0.2975568175315857, 0.02822338417172432, 0.25285640358924866, 0.46629101037979126, 0.16496969759464264, 0.41881677508354187, 0.46857014298439026], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_013ddcec6017c89b64f9987936c1a751(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 256, 512], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bf87ae2f597cb157cf84607637714ea8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1bd65f8b66a9a27a5a6e440cb030323f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 16, 12], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_85b3ea24693c6539d08bc70f949fb8d1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_23c2b10d7a8abf9a67611a173566f3d2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f30f31a791a16d3d5cfc356b7c96af82(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3728828728199005, 0.245957151055336, 0.2461325228214264, 0.2450975626707077, 0.4912070035934448, 0.3820791244506836, 0.32198187708854675, 0.13735108077526093, 0.0037221868988126516, 0.4969993829727173, 0.299316942691803, 0.032163046300411224], dtype='float32').reshape([12]),
            paddle.to_tensor([0.46754997968673706, 0.21409131586551666, 0.06433898210525513, 0.49279868602752686, 0.03601136431097984, 0.11282198876142502, 0.3159431219100952, 0.24394042789936066, 0.16925333440303802, 0.37640053033828735, 0.14619217813014984, 0.08104430884122849], dtype='float32').reshape([12]),
            paddle.to_tensor([0.3374769389629364, 0.21004988253116608, 0.4855033755302429, 0.008970551192760468, 0.3803161084651947, 0.37643948197364807, 0.052396103739738464, 0.22740492224693298, 0.48365914821624756, 0.44759222865104675, 0.05645539611577988, 0.07419141381978989], dtype='float32').reshape([12]),
            paddle.to_tensor([0.31045979261398315, 0.3576076328754425, 0.3421265482902527, 0.1611228883266449, 0.33879971504211426, 0.10361538827419281, 0.19658969342708588, 0.33772918581962585, 0.014893615618348122, 0.32995283603668213, 0.1191001832485199, 0.14358378946781158], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1fa25d8d2ba881f7c57fa471e85221a3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_04e184098d1665a7bb3a3d3b51d7d29a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_268ed7fe830c2a9b0807c0851cd9624d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c54541fac09d4234cd8280aeba8d76f0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c42a23b980578ffac4603921e0282918(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 184, 64, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([184], dtype='float32', min=0, max=0.5),
            paddle.uniform([184], dtype='float32', min=0, max=0.5),
            paddle.uniform([184], dtype='float32', min=0, max=0.5),
            paddle.uniform([184], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0dc18fb58b442e885e791aef4f565826(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dbc805aaf9a3720fa287c2e5d6468b97(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_12e013c41b36e1e01b8b71fdfc12e411(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4302850663661957, 0.13489335775375366, 0.3634539544582367, 0.1471184641122818, 0.18079186975955963, 0.1170794814825058, 0.16709338128566742, 0.49413514137268066, 0.340761661529541, 0.34969255328178406, 0.10427352041006088, 0.15821218490600586, 0.36374521255493164, 0.15457534790039062, 0.1473153978586197, 0.3766234517097473, 0.08194763958454132, 0.3208276033401489, 0.16475889086723328, 0.2741300165653229, 0.18259766697883606, 0.3801257312297821, 0.07409727573394775, 0.09679041057825089], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3244454860687256, 0.38225287199020386, 0.2013530284166336, 0.3622274398803711, 0.4700028896331787, 0.07543723285198212, 0.09469465166330338, 0.005603897385299206, 0.4840991497039795, 0.16329601407051086, 0.3507671654224396, 0.03105047531425953, 0.0759570375084877, 0.1806868314743042, 0.32444503903388977, 0.3023732602596283, 0.305645227432251, 0.2881982922554016, 0.34267914295196533, 0.4099988639354706, 0.2841653525829315, 0.4697672724723816, 0.37029892206192017, 0.4662044644355774], dtype='float32').reshape([24]),
            paddle.to_tensor([0.10478995740413666, 0.40864935517311096, 0.016138838604092598, 0.429513156414032, 0.48441365361213684, 0.2091531604528427, 0.4995878338813782, 0.006512475199997425, 0.08939997106790543, 0.06489542871713638, 0.4323040843009949, 0.23402763903141022, 0.37983712553977966, 0.01937948912382126, 0.23846247792243958, 0.49028971791267395, 0.2013600766658783, 0.42286187410354614, 0.18096910417079926, 0.02194424346089363, 0.13227114081382751, 0.4203052222728729, 0.19079042971134186, 0.13192398846149445], dtype='float32').reshape([24]),
            paddle.to_tensor([0.11823331564664841, 0.31966981291770935, 0.3226107954978943, 0.26514971256256104, 0.18346218764781952, 0.05976405739784241, 0.3334977626800537, 0.2694248557090759, 0.24176280200481415, 0.07807822525501251, 0.46272528171539307, 0.377981573343277, 0.05895180627703667, 0.3030843138694763, 0.43495193123817444, 0.33230870962142944, 0.1382901519536972, 0.024191904813051224, 0.038734350353479385, 0.47426486015319824, 0.06631603091955185, 0.360516756772995, 0.2106352150440216, 0.2674599289894104], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bde1c16c29a25806cede94c737d59361(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_09203dcc29a15f88b9ae20bea9b1cfa6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cd9aae4fc180c83520e004d951968db9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9896baa85c577d8626e158f27caf5d23(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d8d6f16cdbf1f1096df1810058621e0f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_523959420b2303693f728d445dac6d32(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4a81b38ecb5cf95d75bb8e36ec109d70(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 64, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cd7eb1b0c25429421d48c29b50497aa9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f8a8dd35be62d5174a829c2d9a123d4a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4f683497f258f869d3291d480f4d70ac(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 256, 512], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_08bf57d1b77b8dba80b71ddecc2eaf1f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 50, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([50], dtype='float32', min=0, max=0.5),
            paddle.uniform([50], dtype='float32', min=0, max=0.5),
            paddle.uniform([50], dtype='float32', min=0, max=0.5),
            paddle.uniform([50], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_57d9fb8e66fb4073fa138e5760a3af6a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 31, 31], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d3e2359ca4e3523a60d293f7adbf1408(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0366cd303febed6bbb667de44d70348e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c9e24031fa87c6968e4c41876914940e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 8, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7f87325796c8b773ed8597e60ca11d5f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a7182c7f8fc75ff74caed0a40e67032f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fe687a83c7395703e953e11f1301849f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e7124c660170bbc9bf39d526aa359339(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 8, 25], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_baa2acc5ac4f28070257c4799a8e0fbc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_67e392c0725d6c60c2d63690e582977c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_fc6b6917e04b7b7cf9fdb39dde97ec40
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b2e37db6b8a1221d2be4c191c5093745(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 244, 244], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.22934052348136902, 0.4126609265804291, 0.22316130995750427, 0.08636989444494247, 0.34242546558380127, 0.08152291178703308, 0.44280698895454407, 0.25462764501571655, 0.2881367802619934, 0.4396360218524933, 0.41978931427001953, 0.2057546079158783, 0.2904003858566284, 0.34852707386016846, 0.24106957018375397, 0.097950778901577], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3520200252532959, 0.39978599548339844, 0.46319580078125, 0.28812581300735474, 0.46854737401008606, 0.060958582907915115, 0.112653948366642, 0.022584127262234688, 0.2737313210964203, 0.0023701442405581474, 0.30485549569129944, 0.41536763310432434, 0.4148125946521759, 0.255208283662796, 0.4413672089576721, 0.1632508635520935], dtype='float32').reshape([16]),
            paddle.to_tensor([0.07498341053724289, 0.17566081881523132, 0.17753815650939941, 0.1778489053249359, 0.3261728286743164, 0.4984162449836731, 0.21024520695209503, 0.255894273519516, 0.1292119175195694, 0.043276097625494, 0.36905139684677124, 0.3235417604446411, 0.12441897392272949, 0.43773484230041504, 0.05663332715630531, 0.4091644883155823], dtype='float32').reshape([16]),
            paddle.to_tensor([0.25764480233192444, 0.3449071943759918, 0.21418875455856323, 0.1526186615228653, 0.2541500926017761, 0.36331337690353394, 0.4732820987701416, 0.27027419209480286, 0.03312653675675392, 0.2592480778694153, 0.025265520438551903, 0.2249787449836731, 0.05431161820888519, 0.47522222995758057, 0.437723308801651, 0.1824684590101242], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_caac2e5705decda254b8afc74b0178cd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2295154482126236, 0.1877695471048355, 0.35220885276794434, 0.42590928077697754, 0.4096359610557556, 0.07089602202177048, 0.02534540370106697, 0.08814965933561325, 0.26005053520202637, 0.31770971417427063, 0.07713356614112854, 0.11731886118650436, 0.05078565701842308, 0.48008713126182556, 0.286984920501709, 0.26892247796058655, 0.24825184047222137, 0.3834441900253296], dtype='float32').reshape([18]),
            paddle.to_tensor([0.01603945530951023, 0.02917901985347271, 0.48131802678108215, 0.10680422186851501, 0.17996548116207123, 0.18533587455749512, 0.12088099867105484, 0.41055428981781006, 0.42423346638679504, 0.4885938763618469, 0.2671149671077728, 0.08178795129060745, 0.4173489212989807, 0.03994375467300415, 0.29222193360328674, 0.024091003462672234, 0.4883362054824829, 0.24268364906311035], dtype='float32').reshape([18]),
            paddle.to_tensor([0.41895049810409546, 0.30543696880340576, 0.17616020143032074, 0.28266775608062744, 0.48023438453674316, 0.11039085686206818, 0.010261671617627144, 0.21325860917568207, 0.12293114513158798, 0.48331451416015625, 0.05793411657214165, 0.10189058631658554, 0.10968099534511566, 0.19201262295246124, 0.1518307477235794, 0.40314775705337524, 0.4503065049648285, 0.4236295521259308], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3261798620223999, 0.23484037816524506, 0.26225653290748596, 0.06489346921443939, 0.4654269516468048, 0.18284675478935242, 0.3325689136981964, 0.20363090932369232, 0.49157479405403137, 0.3695725202560425, 0.0032663224264979362, 0.15590029954910278, 0.032183341681957245, 0.3255021572113037, 0.4158121347427368, 0.04757511615753174, 0.30275824666023254, 0.04792343080043793], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_75ac840bb5a2c5077b68bddcab1f1008(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 576, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cdce9e7521fae024dd6ad189261766a2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4256410598754883, 0.08596032857894897, 0.13153904676437378, 0.3074158728122711, 0.4439489543437958, 0.1631956249475479, 0.13370564579963684, 0.3551734685897827, 0.39689916372299194, 0.044526610523462296, 0.024722609668970108, 0.38078176975250244, 0.18476659059524536, 0.44051745533943176, 0.0357009582221508, 0.4015274941921234, 0.2722163200378418, 0.4421426057815552, 0.19496256113052368, 0.2853604257106781, 0.03621283173561096, 0.4390636086463928, 0.4973220229148865, 0.3988356590270996], dtype='float32').reshape([24]),
            paddle.to_tensor([0.23152132332324982, 0.2132306545972824, 0.10426706820726395, 0.3521568477153778, 0.34114158153533936, 0.2839389741420746, 0.15997827053070068, 0.004550943151116371, 0.3223985731601715, 0.2329445630311966, 0.37815725803375244, 0.4473070204257965, 0.42994874715805054, 0.13671454787254333, 0.18951450288295746, 0.24239717423915863, 0.21605288982391357, 0.21575301885604858, 0.48372623324394226, 0.010198154486715794, 0.08856033533811569, 0.12310407310724258, 0.26204943656921387, 0.37174585461616516], dtype='float32').reshape([24]),
            paddle.to_tensor([0.0003287257277406752, 0.448462575674057, 0.07323849946260452, 0.1712462455034256, 0.16537652909755707, 0.4704532027244568, 0.25941896438598633, 0.003054636064916849, 0.10443750768899918, 0.40483447909355164, 0.17039398849010468, 0.16346606612205505, 0.48222529888153076, 0.3584286570549011, 0.47650253772735596, 0.4581372141838074, 0.2681620419025421, 0.14054860174655914, 0.4246332347393036, 0.11070496588945389, 0.3377334177494049, 0.1712481677532196, 0.026681816205382347, 0.29852503538131714], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3333283066749573, 0.4554673433303833, 0.4422856867313385, 0.4168201684951782, 0.4551025331020355, 0.4461086690425873, 0.4394800066947937, 0.29612523317337036, 0.2127273827791214, 0.40486201643943787, 0.44964540004730225, 0.323113352060318, 0.2246733456850052, 0.29466286301612854, 0.49447959661483765, 0.16808399558067322, 0.17066989839076996, 0.017540084198117256, 0.33645814657211304, 0.17645615339279175, 0.16425776481628418, 0.34073567390441895, 0.44412025809288025, 0.3308018147945404], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2164574ff74f35635274332f27de5667(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 480, 480], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.29960012435913086, 0.44959044456481934, 0.157735675573349, 0.3822827637195587, 0.21356745064258575, 0.21056555211544037, 0.12715449929237366, 0.4022742509841919], dtype='float32').reshape([8]),
            paddle.to_tensor([0.35077109932899475, 0.021083608269691467, 0.3888012170791626, 0.380607008934021, 0.4744172692298889, 0.4666883051395416, 0.26183757185935974, 0.17251592874526978], dtype='float32').reshape([8]),
            paddle.to_tensor([0.05770876258611679, 0.24275754392147064, 0.07344423234462738, 0.13751067221164703, 0.3141070008277893, 0.3512933850288391, 0.00421029981225729, 0.3246776759624481], dtype='float32').reshape([8]),
            paddle.to_tensor([0.36069536209106445, 0.3857799470424652, 0.15913936495780945, 0.35281386971473694, 0.4596364498138428, 0.2462533414363861, 0.37283363938331604, 0.4321642816066742], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d191a054145823f5d69526872265eb77(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_1a8301b810bccba10e4dfdee0df73334(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1012792595c8e203b63c6e4e0f975c52(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 728, 19, 19], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_cdc3b0a148b551f86897e4668e0bdb6b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_430c17b2ee9c2844639b1464b5ef8b16(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cdc3b0a148b551f86897e4668e0bdb6b
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 2, 2], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_145ad20e7a3b3c8aa31f6b40d4fe3d45(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2cab8c585b11a3f571db1e74acd5c81b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_145ad20e7a3b3c8aa31f6b40d4fe3d45
    def get_inputs(self):
        return [
            paddle.uniform([196, 640], dtype='float16', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_58d1065aba52e05d23a9eefdd612bf64(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 2, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6f1bc158519854c96ce4c18119f95ff1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_759135d1a53afcf66f97008c893151f3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 576, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_abfa64291101931771724a3de634b52d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.13917848467826843, 0.3606226444244385, 0.21857628226280212, 0.34711816906929016, 0.37614086270332336, 0.21288107335567474, 0.34988224506378174, 0.30875858664512634, 0.4608027935028076, 0.4202629327774048, 0.08629898726940155, 0.20326879620552063, 0.42403918504714966, 0.2159840166568756, 0.25025254487991333, 0.3571765720844269, 0.23270483314990997, 0.31768253445625305, 0.3585566580295563, 0.20319697260856628], dtype='float32').reshape([20]),
            paddle.to_tensor([0.49463680386543274, 0.3895648717880249, 0.0019133610185235739, 0.2527717649936676, 0.12523670494556427, 0.17441512644290924, 0.47243547439575195, 0.11193689703941345, 0.21408292651176453, 0.028032220900058746, 0.4292023479938507, 0.49547436833381653, 0.2797597646713257, 0.33262401819229126, 0.09344401955604553, 0.49240514636039734, 0.22755184769630432, 0.26185503602027893, 0.14374800026416779, 0.27439191937446594], dtype='float32').reshape([20]),
            paddle.to_tensor([0.31604287028312683, 0.1621476411819458, 0.31608545780181885, 0.28679731488227844, 0.13422410190105438, 0.26390865445137024, 0.14171147346496582, 0.07967007905244827, 0.31437948346138, 0.058177098631858826, 0.12842713296413422, 0.3689775764942169, 0.011913804337382317, 0.0016859746538102627, 0.41603100299835205, 0.3796810805797577, 0.198333740234375, 0.08217191696166992, 0.09023341536521912, 0.15269793570041656], dtype='float32').reshape([20]),
            paddle.to_tensor([0.15764743089675903, 0.463741660118103, 0.043360281735658646, 0.27383700013160706, 0.45112499594688416, 0.12230321019887924, 0.23114407062530518, 0.3272472321987152, 0.14913150668144226, 0.16209425032138824, 0.21986111998558044, 0.445037305355072, 0.4961748421192169, 0.4423202574253082, 0.22473396360874176, 0.26076602935791016, 0.014667777344584465, 0.28813305497169495, 0.06698217242956161, 0.05779644474387169], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d3eeb39e5f446c318b345b5858d89ea0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 8, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e6e39807847c6ee65447dc26eeebb3f2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.058232974261045456, 0.4779120683670044, 0.07515112310647964, 0.26822471618652344, 0.36396175622940063, 0.008461957797408104, 0.3231441080570221, 0.40106120705604553, 0.3249211013317108, 0.48543551564216614, 0.08291634172201157, 0.40847527980804443, 0.3074730634689331, 0.2605549693107605, 0.09585760533809662, 0.28904464840888977, 0.40210360288619995, 0.05298145115375519], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4556999206542969, 0.34621843695640564, 0.016378631815314293, 0.24461598694324493, 0.3548525273799896, 0.3812183439731598, 0.4519044756889343, 0.24285836517810822, 0.40893182158470154, 0.240820974111557, 0.2638605535030365, 0.04780193045735359, 0.12658870220184326, 0.13614854216575623, 0.05858604982495308, 0.44490736722946167, 0.37837010622024536, 0.28553080558776855], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2956016957759857, 0.40024903416633606, 0.3821949362754822, 0.0767131969332695, 0.12693943083286285, 0.39804917573928833, 0.2136426568031311, 0.04727429151535034, 0.3272775113582611, 0.47595223784446716, 0.17634564638137817, 0.3514128029346466, 0.0004366906941868365, 0.4858632981777191, 0.22771747410297394, 0.0499541275203228, 0.07318422943353653, 0.17949917912483215], dtype='float32').reshape([18]),
            paddle.to_tensor([0.09409989416599274, 0.1455603539943695, 0.01999255083501339, 0.48730534315109253, 0.15965577960014343, 0.4888770878314972, 0.4069903492927551, 0.4777085483074188, 0.4403578042984009, 0.4752667546272278, 0.01753881946206093, 0.2377179116010666, 0.07667527347803116, 0.10629034042358398, 0.41085997223854065, 0.3443153202533722, 0.20567859709262848, 0.1639949083328247], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fc73aac58a9b6821e3763ad976b07acb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e974c50b77aea63b55b5d8f8715227b0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 30, 30], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f7cde1e3dc2d36571261679838498427(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c5a19d89a7c1d2d8fea770a79ce81a0f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.09769285470247269, 0.08474906533956528, 0.11385495960712433, 0.31379109621047974, 0.003288924926891923, 0.34077534079551697, 0.12470238655805588, 0.2793956696987152, 0.38697606325149536, 0.34561747312545776, 0.221944198012352, 0.263346791267395, 0.2674078941345215, 0.03383522853255272, 0.28573644161224365, 0.11331799626350403, 0.21511310338974, 0.43632805347442627, 0.49361181259155273, 0.32887762784957886], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4981323778629303, 0.41708073019981384, 0.44500255584716797, 0.27664363384246826, 0.31985771656036377, 0.09836208820343018, 0.07368607819080353, 0.012374378740787506, 0.060824666172266006, 0.4284464418888092, 0.4887802004814148, 0.07962394505739212, 0.30741867423057556, 0.03559104725718498, 0.13255849480628967, 0.232900470495224, 0.2238374650478363, 0.4212493598461151, 0.0693124458193779, 0.43092402815818787], dtype='float32').reshape([20]),
            paddle.to_tensor([0.11309084296226501, 0.43530482053756714, 0.28293970227241516, 0.40985530614852905, 0.2708750367164612, 0.4490046203136444, 0.3916938006877899, 0.33867618441581726, 0.19005553424358368, 0.11279451102018356, 0.4247075617313385, 0.20429439842700958, 0.03413102775812149, 0.3033546805381775, 0.21145598590373993, 0.14732974767684937, 0.4009283185005188, 0.3546355366706848, 0.15758016705513, 0.3567136526107788], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3432331681251526, 0.4459032714366913, 0.11538808792829514, 0.189644917845726, 0.14531002938747406, 0.11513417214155197, 0.1777128428220749, 0.09160160273313522, 0.25928381085395813, 0.47433653473854065, 0.423393189907074, 0.37602078914642334, 0.004431691486388445, 0.2605704665184021, 0.34946927428245544, 0.413819819688797, 0.4200857877731323, 0.3250505328178406, 0.17157231271266937, 0.1009947657585144], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a56f45f32136d8931f4bd1978a289cf6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f14a1f7fccd4a55476e2f3701b907f2a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_75b484fb67bae77154cceb7c4256d256(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_65887cca0d5578f0c3fe5fb8291914c5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 12, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8392266e22057d57d07b65bb0b93c6f8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9225e615465c1c888698e73a2fe2d888(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2e9f9fcca10bfe21e7624c6ee257662b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 512, 1024], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.400596559047699, 0.2517097592353821, 0.41648703813552856, 0.004789044614881277, 0.3714235723018646, 0.434613972902298, 0.06728190928697586, 0.2339375764131546, 0.019698811694979668, 0.2572837471961975, 0.21706622838974, 0.06001481041312218, 0.1454361081123352, 0.1649511456489563, 0.3831094205379486, 0.18644805252552032], dtype='float32').reshape([16]),
            paddle.to_tensor([0.43036985397338867, 0.2322125881910324, 0.47804954648017883, 0.3560066521167755, 0.4574275612831116, 0.39718466997146606, 0.35491085052490234, 0.38722825050354004, 0.4294089674949646, 0.401964396238327, 0.21926380693912506, 0.1856372058391571, 0.2036818414926529, 0.32622823119163513, 0.4993648827075958, 0.27293822169303894], dtype='float32').reshape([16]),
            paddle.to_tensor([0.20842570066452026, 0.31184065341949463, 0.23800025880336761, 0.2827455997467041, 0.2614925801753998, 0.4887232482433319, 0.2562789022922516, 0.17977294325828552, 0.4105094075202942, 0.2804975211620331, 0.10255145281553268, 0.26179754734039307, 0.29195475578308105, 0.36351898312568665, 0.38093632459640503, 0.15490055084228516], dtype='float32').reshape([16]),
            paddle.to_tensor([0.04213879629969597, 0.2461296021938324, 0.17414245009422302, 0.007162023335695267, 0.3329910337924957, 0.14711259305477142, 0.09470799565315247, 0.33313941955566406, 0.3836694359779358, 0.06651677191257477, 0.3760070502758026, 0.3458557426929474, 0.2298523634672165, 0.12064173072576523, 0.40460404753685, 0.41154778003692627], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_db302fc68674ced0050fd6a551693744(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f27b73d4f01bbe03786b1a2c1cd9ca5b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_975b258f7fdc5512a8c22ad1eebf2c8a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a4f28d1c9faba36fdf23c515ebcba825(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 2, 2], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7569faf3a0500c16ffe7002d00afb43b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.07599343359470367, 0.4467722773551941, 0.35521841049194336, 0.4288024604320526, 0.16963541507720947, 0.2385096400976181, 0.35738304257392883, 0.0021063718013465405, 0.05020337179303169, 0.3712537884712219, 0.43467211723327637, 0.44841882586479187, 0.4266217052936554, 0.10744255781173706, 0.1780976504087448, 0.07762159407138824, 0.02759735658764839, 0.3927483558654785, 0.35911229252815247, 0.25897908210754395, 0.006515820976346731, 0.4637700319290161, 0.2042512744665146, 0.07516227662563324, 0.050826288759708405, 0.33391109108924866, 0.40343567728996277, 0.3687380254268646], dtype='float32').reshape([28]),
            paddle.to_tensor([0.38811013102531433, 0.1919737011194229, 0.18601477146148682, 0.36636078357696533, 0.4223811626434326, 0.04141354188323021, 0.22421634197235107, 0.05672285333275795, 0.11106430739164352, 0.002564014634117484, 0.20564912259578705, 0.02814868651330471, 0.42825791239738464, 0.18767303228378296, 0.3054153621196747, 0.014012443833053112, 0.06899836659431458, 0.47318246960639954, 0.3826514482498169, 0.09420763701200485, 0.09970618039369583, 0.4864612817764282, 0.11272069066762924, 0.2896740138530731, 0.27088600397109985, 0.11022341251373291, 0.3112134337425232, 0.08326209336519241], dtype='float32').reshape([28]),
            paddle.to_tensor([0.3533664047718048, 0.461162269115448, 0.3349079191684723, 0.2380921095609665, 0.25860166549682617, 0.21359507739543915, 0.4919842779636383, 0.13094067573547363, 0.022923966869711876, 0.2687991261482239, 0.31095966696739197, 0.45589137077331543, 0.27144479751586914, 0.11822029203176498, 0.12344605475664139, 0.03251737356185913, 0.28948596119880676, 0.17087116837501526, 0.11105991154909134, 0.4053689241409302, 0.3479534983634949, 0.13352110981941223, 0.27104824781417847, 0.267336368560791, 0.1100190058350563, 0.42015165090560913, 0.2943207621574402, 0.2947775721549988], dtype='float32').reshape([28]),
            paddle.to_tensor([0.01176836621016264, 0.38377630710601807, 0.39007332921028137, 0.19652816653251648, 0.2621138095855713, 0.04228368401527405, 0.24075667560100555, 0.31414011120796204, 0.4619760811328888, 0.2493874728679657, 0.42608463764190674, 0.017618032172322273, 0.46418851613998413, 0.12099067121744156, 0.20502205193042755, 0.38726794719696045, 0.4125724136829376, 0.09907101094722748, 0.21923622488975525, 0.12947978079319, 0.26473766565322876, 0.10972682386636734, 0.11026900261640549, 0.2952747941017151, 0.15327882766723633, 0.4987851679325104, 0.42971271276474, 0.371963232755661], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_58c3adacf0d2f4b7df3d8db3096515c4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_585f319f0a336dc373218775aa1e9029(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 14, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_994885f947bf35890cd54100a71ca8e4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 4, 8, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_11dc0f839239edc26db3ec13c87c3cb1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4233af795a92b4c97303357fd3bb10ea(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 240, 240], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e05b80f7b3a899460748f64ebe53c56d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6213deb209e311651f3c95d19b0c3642(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c33c791ecfbbbd6da897e74885ee1f7d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 736, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([736], dtype='float32', min=0, max=0.5),
            paddle.uniform([736], dtype='float32', min=0, max=0.5),
            paddle.uniform([736], dtype='float32', min=0, max=0.5),
            paddle.uniform([736], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9f45d081c0b6f9e483d080e1b3c5115c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_694bf375306736e2aeab3f152a5b906c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.021178843453526497, 0.40105244517326355, 0.08577173203229904, 0.15964242815971375, 0.34415578842163086, 0.07298293709754944, 0.2708853483200073, 0.4230838716030121, 0.47043082118034363, 0.28515249490737915, 0.3122330605983734, 0.3219279944896698], dtype='float32').reshape([12]),
            paddle.to_tensor([0.49149006605148315, 0.07015515118837357, 0.10691476613283157, 0.22765205800533295, 0.17893318831920624, 0.08630309253931046, 0.29707688093185425, 0.19245938956737518, 0.048676032572984695, 0.37023311853408813, 0.4386802315711975, 0.013050246983766556], dtype='float32').reshape([12]),
            paddle.to_tensor([0.23213306069374084, 0.23235087096691132, 0.478219598531723, 0.3159979581832886, 0.41010618209838867, 0.2686922550201416, 0.08784854412078857, 0.2008952498435974, 0.22737441956996918, 0.479053795337677, 0.1478906273841858, 0.13498353958129883], dtype='float32').reshape([12]),
            paddle.to_tensor([0.09574209898710251, 0.34575411677360535, 0.041608911007642746, 0.4932270050048828, 0.44602128863334656, 0.2250276505947113, 0.45141416788101196, 0.039288006722927094, 0.44424474239349365, 0.37978872656822205, 0.34970736503601074, 0.1349869817495346], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9b3627338b3797f2e7b2926f3183f549(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.21239235997200012, 0.32199370861053467, 0.18113787472248077, 0.40908050537109375, 0.2915138602256775, 0.36643412709236145, 0.021141907200217247, 0.051553625613451004, 0.4322589039802551, 0.4277651607990265, 0.17595042288303375, 0.049302197992801666, 0.3221301734447479, 0.4768602252006531, 0.1918158382177353, 0.4083111882209778, 0.13483361899852753, 0.34397953748703003, 0.25277233123779297, 0.01816040836274624, 0.02842409908771515, 0.15235237777233124, 0.24607574939727783, 0.17388905584812164], dtype='float32').reshape([24]),
            paddle.to_tensor([0.23508106172084808, 0.04412692412734032, 0.20053207874298096, 0.1326054036617279, 0.3314880430698395, 0.46255022287368774, 0.2798956632614136, 0.14078713953495026, 0.4414186477661133, 0.08529386669397354, 0.33614102005958557, 0.03186604008078575, 0.03421905264258385, 0.19063575565814972, 0.30041059851646423, 0.23245133459568024, 0.47020941972732544, 0.352214515209198, 0.15911217033863068, 0.07913827896118164, 0.2758857309818268, 0.3931213915348053, 0.023009922355413437, 0.06220148131251335], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3443496823310852, 0.2170463502407074, 0.1263856291770935, 0.20114067196846008, 0.40467023849487305, 0.2125988006591797, 0.23879405856132507, 0.23152203857898712, 0.07728972285985947, 0.1590125858783722, 0.0980241671204567, 0.26533961296081543, 0.4198727011680603, 0.4735155999660492, 0.2439226508140564, 0.08800984919071198, 0.34547650814056396, 0.43229031562805176, 0.04054354503750801, 0.47120511531829834, 0.29174020886421204, 0.34529656171798706, 0.26175302267074585, 0.4732053577899933], dtype='float32').reshape([24]),
            paddle.to_tensor([0.060846902430057526, 0.1677527129650116, 0.4141330420970917, 0.41343075037002563, 0.29635775089263916, 0.3854675889015198, 0.08708500862121582, 0.3577159643173218, 0.1367647796869278, 0.48845309019088745, 0.4918692111968994, 0.29605844616889954, 0.03830284625291824, 0.24624355137348175, 0.42270582914352417, 0.07830236852169037, 0.4470531642436981, 0.21702826023101807, 0.2232929915189743, 0.3520854413509369, 0.04820407181978226, 0.13796299695968628, 0.20787522196769714, 0.47259196639060974], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1c58dbbd5d6c8f1f8cdf9dc2753dff50(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 16, 50], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.49291813373565674, 0.3518295884132385, 0.373944491147995, 0.4144655764102936, 0.10248830169439316, 0.3377249836921692, 0.30567505955696106, 0.24692052602767944], dtype='float32').reshape([8]),
            paddle.to_tensor([0.09928520023822784, 0.2282657027244568, 0.11835620552301407, 0.05726465955376625, 0.48918861150741577, 0.2860201299190521, 0.2825014293193817, 0.03767079859972], dtype='float32').reshape([8]),
            paddle.to_tensor([0.12263370305299759, 0.3564383387565613, 0.3110109865665436, 0.1287813037633896, 0.46347516775131226, 0.318218857049942, 0.13695752620697021, 0.355124831199646], dtype='float32').reshape([8]),
            paddle.to_tensor([0.4968148469924927, 0.08124049007892609, 0.4002050459384918, 0.4279220998287201, 0.07235563546419144, 0.46644800901412964, 0.38585996627807617, 0.17504280805587769], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_021dbfe50b452fa8f526ee574764307f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.49545538425445557, 0.1879841685295105, 0.3301108479499817, 0.43485045433044434, 0.36258581280708313, 0.43075239658355713, 0.46671897172927856, 0.4536816477775574, 0.39384856820106506, 0.21889878809452057, 0.48394274711608887, 0.07040031254291534, 0.29117098450660706, 0.21238981187343597, 0.09560593962669373, 0.4676995575428009], dtype='float32').reshape([16]),
            paddle.to_tensor([0.22677867114543915, 0.41542643308639526, 0.2238338589668274, 0.15342622995376587, 0.03962181881070137, 0.21874892711639404, 0.10806606709957123, 0.0811639353632927, 0.44485098123550415, 0.12192057818174362, 0.4634605050086975, 0.4405481517314911, 0.2167171835899353, 0.06654486805200577, 0.26720908284187317, 0.021461108699440956], dtype='float32').reshape([16]),
            paddle.to_tensor([0.24462249875068665, 0.477321982383728, 0.06572746485471725, 0.06733056902885437, 0.2196885049343109, 0.14542056620121002, 0.02034091018140316, 0.27383357286453247, 0.18002651631832123, 0.3357079327106476, 0.2563454210758209, 0.19066955149173737, 0.3903244137763977, 0.19424790143966675, 0.2318670153617859, 0.4547523856163025], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4477929472923279, 0.47383370995521545, 0.09458969533443451, 0.11116643249988556, 0.030205126851797104, 0.324928879737854, 0.17260241508483887, 0.3176364004611969, 0.24915166199207306, 0.18782739341259003, 0.47602906823158264, 0.045448724180459976, 0.15149052441120148, 0.2604852616786957, 0.37185510993003845, 0.37106621265411377], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_18c85722b7f424e734508dd0a966147e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c241f252de1ab2ef5efabc745ef603bf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 224, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9e60825b03f828fb83877bb718213267(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9a5c8fd16e5e495fe7f909f999eb01a9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 4, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f5a80c48136e1995557c728933ce4483(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 256, 512], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1aca5b2f7d5193b8c956405eaa029246(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 31, 31], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_84d939733796fc236cb10d79b8053a56(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_20a4151a7d8d2f84eda8efbfc3dd3382(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4582fccff5640d081456d2851c800c49(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.17092907428741455, 0.4213002324104309, 0.01329751405864954, 0.14939908683300018, 0.19864988327026367, 0.3008122444152832, 0.016121860593557358, 0.05876121297478676, 0.28930914402008057, 0.0843275934457779, 0.3437248766422272, 0.46187636256217957, 0.2098454385995865, 0.3201295733451843, 0.40524953603744507, 0.052638940513134, 0.4819216728210449, 0.40456056594848633, 0.4896799623966217, 0.30383285880088806, 0.4356555938720703, 0.4573444724082947, 0.3360327184200287, 0.2019134908914566], dtype='float32').reshape([24]),
            paddle.to_tensor([0.26037952303886414, 0.4420481324195862, 0.23191280663013458, 0.39592576026916504, 0.18266212940216064, 0.06831963360309601, 0.010967166163027287, 0.3364937901496887, 0.3152658939361572, 0.3583914637565613, 0.12026852369308472, 0.13741587102413177, 0.13624882698059082, 0.04713556915521622, 0.07191180437803268, 0.3649233281612396, 0.4510887563228607, 0.24981589615345, 0.43849068880081177, 0.236541286110878, 0.326026976108551, 0.04058626666665077, 0.290848970413208, 0.3088628947734833], dtype='float32').reshape([24]),
            paddle.to_tensor([0.14379718899726868, 0.24642132222652435, 0.2908400595188141, 0.08336609601974487, 0.13887456059455872, 0.12321893125772476, 0.3139238953590393, 0.38652676343917847, 0.11172255128622055, 0.332876592874527, 0.4889141321182251, 0.47647640109062195, 0.4778138995170593, 0.01965024136006832, 0.15481220185756683, 0.19974592328071594, 0.22657270729541779, 0.0035864466335624456, 0.48268240690231323, 0.48762330412864685, 0.02015502378344536, 0.27945029735565186, 0.08938030153512955, 0.4821912944316864], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4650062918663025, 0.03831016644835472, 0.03124404326081276, 0.2756634056568146, 0.4693965017795563, 0.4971008598804474, 0.24056404829025269, 0.1639561951160431, 0.4911552667617798, 0.07349097728729248, 0.4295581877231598, 0.4049869179725647, 0.38912466168403625, 0.07548838108778, 0.45897215604782104, 0.45118656754493713, 0.28855034708976746, 0.06229918450117111, 0.17647796869277954, 0.24084196984767914, 0.03271142765879631, 0.2944563627243042, 0.34060609340667725, 0.15148890018463135], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6a9b6334f75cb517fd3b5aff823dace1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_08ecfbca71b5233f6357b289e88ae3f0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_70a18695eedfefde1f6a8246cc95ec55(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 528, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([528], dtype='float32', min=0, max=0.5),
            paddle.uniform([528], dtype='float32', min=0, max=0.5),
            paddle.uniform([528], dtype='float32', min=0, max=0.5),
            paddle.uniform([528], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_76a6b0b7ce29c2d0b5a7c2c27507c271(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 972, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([972], dtype='float32', min=0, max=0.5),
            paddle.uniform([972], dtype='float32', min=0, max=0.5),
            paddle.uniform([972], dtype='float32', min=0, max=0.5),
            paddle.uniform([972], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d9169045ab642af435891b0879aadaa9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aacda73b6901e5e7c068cbe2e0c28e13(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d76dc775e3dff6308121c6b6063db1f8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9f48d70023c69288d00c6f0215424282(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 768, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7cf0e49397bc65bd9b722f9b03a00623(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 64, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b2df2ce156ba2a71e0dc28492905ca9d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aa92acc5daa3be34542850d1371b9f27(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7a51618b2bb8f8ad94ac0a39323d25d5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 8, 6], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c1f33cf53455cb0ef3e1a8824fe888d0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e54cf321e9a40e7cf7f363082c996f64(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 768, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_652ce28fc0831eb1763337e78f4ae79b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.23334133625030518, 0.3907596170902252, 0.0788259357213974, 0.4910481572151184, 0.4509000778198242, 0.16622081398963928, 0.09080775827169418, 0.4576227366924286, 0.4779561161994934, 0.23685434460639954, 0.4393400549888611, 0.16770829260349274, 0.2698846161365509, 0.049625445157289505, 0.1367047280073166, 0.460152804851532, 0.31220361590385437, 0.392628937959671, 0.4658283293247223, 0.3049454689025879, 0.10449658334255219, 0.10403524339199066, 0.022083405405282974, 0.33564144372940063, 0.16325892508029938, 0.16618245840072632, 0.4869925379753113, 0.33471816778182983, 0.4954524636268616, 0.40056294202804565], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4944629669189453, 0.25198158621788025, 0.39639830589294434, 0.16850495338439941, 0.010471801273524761, 0.17947256565093994, 0.23359443247318268, 0.4636206030845642, 0.018524104729294777, 0.06378135830163956, 0.22004231810569763, 0.2081693708896637, 0.3216434419155121, 0.21072497963905334, 0.1634129285812378, 0.26392802596092224, 0.2408500462770462, 0.18840396404266357, 0.3016379177570343, 0.29370594024658203, 0.26882949471473694, 0.44857707619667053, 0.4929727613925934, 0.14124952256679535, 0.01040601171553135, 0.08222069591283798, 0.03412210941314697, 0.27295365929603577, 0.33371037244796753, 0.4325226843357086], dtype='float32').reshape([30]),
            paddle.to_tensor([0.39446771144866943, 0.035156141966581345, 0.21385914087295532, 0.4541499614715576, 0.16162444651126862, 0.0530681237578392, 0.30605578422546387, 0.1888696849346161, 0.1899416446685791, 0.40229329466819763, 0.3275219798088074, 0.2848433256149292, 0.019612591713666916, 0.3779054582118988, 0.42056623101234436, 0.07302428036928177, 0.4559575617313385, 0.1705545336008072, 0.15512186288833618, 0.017278973013162613, 0.21125265955924988, 0.4918425679206848, 0.024378083646297455, 0.3451847732067108, 0.24248002469539642, 0.04326450452208519, 0.3481388986110687, 0.453596830368042, 0.4643040895462036, 0.23848211765289307], dtype='float32').reshape([30]),
            paddle.to_tensor([0.43124815821647644, 0.018462542444467545, 0.09967245161533356, 0.20498117804527283, 0.4967961311340332, 0.3594551682472229, 0.3652020990848541, 0.21225237846374512, 0.32896509766578674, 0.47932472825050354, 0.34614652395248413, 0.4041846990585327, 0.20102746784687042, 0.051967766135931015, 0.4287923574447632, 0.23217613995075226, 0.3714008629322052, 0.3291308879852295, 0.27932098507881165, 0.07403171807527542, 0.12814782559871674, 0.2974807024002075, 0.06201113760471344, 0.11158443987369537, 0.05248243361711502, 0.44997256994247437, 0.0397239550948143, 0.20788082480430603, 0.07532946020364761, 0.2137436717748642], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c99fde2121ca9becd4ba587e9aaf98c4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b46042fd4384181bc348ef81c010d449(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 8, 6], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e54d5841d053102fe6c9b571c065550d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 26], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f1c904092ea8394ce9f25737cf8479ed(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e93468c7803886f2b0aca38e561760b7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d60342d62641ba53e36849c52cc5a9fe(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bb182dec18071514978b6fbe32bd98fa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5608b3b7b344235002e92ff2b7205172(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([8, 2048, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2f5fc2b98d6821f038f60e03fd3443d4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_766724a8d011b266035a1b1ed5c2d503(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_145ad20e7a3b3c8aa31f6b40d4fe3d45
    def get_inputs(self):
        return [
            paddle.uniform([49, 512], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e019fb13c2774a1ba9c3252289915b90(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 256, 512], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e62051d30473a8835c28f0200d96df4f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 10, 10], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_77c96a32af44d736ea42aaa65d0ef19d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fc659a2ea6091af53e425e7806f98003(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_14cd2430129ad120f4d011c17b1b697b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fbdb8ac77e97c0db51c073d4b42eb789(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4f81eb475f9ceb8fd6358ef73a947efb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_148e5e94a11833f351678c562ddc2b60(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 60, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e955537f4e8f61f85f209244cfb2b955(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 8, 6], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bdf39b87d742a27dc992d097a1f96540(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 768, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2942f520dd9e84879bd7c88157982673(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f67aaf10828d2d53d7e48405f98d5357(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_def578a427cb3c6a5d6ba0b59092d5c9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 32, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_622d009296f54dd559eea7ea0997cb02(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 61, 61], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4416224956512451, 0.24920964241027832, 0.2972535789012909, 0.4638751447200775, 0.2642826437950134, 0.111289843916893, 0.2614758610725403, 0.36923953890800476, 0.33365771174430847, 0.025705907493829727, 0.125798761844635, 0.457283079624176, 0.371685266494751, 0.4481516182422638, 0.293795108795166, 0.40077507495880127, 0.45512935519218445, 0.03302113711833954, 0.44509097933769226, 0.1231444776058197, 0.15843245387077332, 0.4367409646511078, 0.10580598562955856, 0.09286121279001236], dtype='float32').reshape([24]),
            paddle.to_tensor([0.21227067708969116, 0.04869798943400383, 0.2092919945716858, 0.2636047303676605, 0.4102656841278076, 0.1564319133758545, 0.4992210268974304, 0.36293500661849976, 0.3555189371109009, 0.3248082101345062, 0.022072970867156982, 0.0410025380551815, 0.2870810329914093, 0.24538542330265045, 0.3646320700645447, 0.35033538937568665, 0.42765745520591736, 0.015743685886263847, 0.41890764236450195, 0.1293918341398239, 0.20709587633609772, 0.04103146120905876, 0.439505010843277, 0.09459011256694794], dtype='float32').reshape([24]),
            paddle.to_tensor([0.32929471135139465, 0.43117377161979675, 0.3795616626739502, 0.1820286363363266, 0.2259184569120407, 0.0492953285574913, 0.14352735877037048, 0.038030050694942474, 0.32519593834877014, 0.20311765372753143, 0.2090936303138733, 0.25760897994041443, 0.11312398314476013, 0.27227795124053955, 0.14390528202056885, 0.3661414086818695, 0.38132330775260925, 0.10665101557970047, 0.3788362145423889, 0.0168942678719759, 0.156399667263031, 0.1580875664949417, 0.4234282374382019, 0.21545861661434174], dtype='float32').reshape([24]),
            paddle.to_tensor([0.08811981976032257, 0.26930102705955505, 0.3380807340145111, 0.40888309478759766, 0.035612113773822784, 0.07008909434080124, 0.49441537261009216, 0.2542111873626709, 0.09929822385311127, 0.1770831048488617, 0.3170780837535858, 0.470419704914093, 0.28888532519340515, 0.3196685314178467, 0.4215903878211975, 0.2656005620956421, 0.49414244294166565, 0.4695664942264557, 0.04321594908833504, 0.04292939230799675, 0.10811364650726318, 0.14033068716526031, 0.31823429465293884, 0.4897870123386383], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2b470e817e801a61466d52708374cb2f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 8, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c3c5faf99de1ca42aa0607f79f4c1ba3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 38, 38], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7334d82914828d2e07ceb816ab037200(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b59f47c4d53477462af5d042f93103b4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 76, 76], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0d50fc08897cef4f389e717f72a0a98e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 4, 256, 512], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.08663096278905869, 0.04222305119037628, 0.2605833113193512, 0.018985005095601082], dtype='float32').reshape([4]),
            paddle.to_tensor([0.4786698520183563, 0.1420125663280487, 0.4117678105831146, 0.2734152376651764], dtype='float32').reshape([4]),
            paddle.to_tensor([0.39367759227752686, 0.12265711277723312, 0.08828174322843552, 0.15837982296943665], dtype='float32').reshape([4]),
            paddle.to_tensor([0.15595173835754395, 0.013269967399537563, 0.09825888276100159, 0.22608250379562378], dtype='float32').reshape([4]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_385d768ff95cf75f94de6693ba2a2e51(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 4, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c44a22b74ee31fb7f1ce33abbdad5a27(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c8d28c65eb3b7522679776e65c02b3b3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8121ff122a866ad7f46e7f3d482e3e91(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bb66abaf33964acf8d2499cd219c5e82(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 19, 19], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a142c909e153d8fb714a0d19c8065431(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_73bc17bd75d5f77e970b0b6758de3277(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 350, 25], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4ed36c561db9348c57239e2504b42148(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 32, 100], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d32d5087fd068eafe1179401dcef04f0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c3f352fca7c4a9234b3a30630a15e146(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5f414939bf280b87e1fa443c0c619365(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8907ebe5b54236f34c2c366f97ba615f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bba9b303b0422ac975f682136cfb4ea4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1408, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1408], dtype='float32', min=0, max=0.5),
            paddle.uniform([1408], dtype='float32', min=0, max=0.5),
            paddle.uniform([1408], dtype='float32', min=0, max=0.5),
            paddle.uniform([1408], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1607281dae9b06cea99104cef7fb27c9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_41b6de854fd474036a513ea9a448d340(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 61, 61], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_32d97dc4625dc16e30d6c81590385748(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 8, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_00b0eea3c9dd41d33d77fade4f171d4f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 25, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2253648191690445, 0.26054441928863525, 0.15435004234313965, 0.24118159711360931, 0.23475724458694458, 0.4318467378616333, 0.024315999820828438, 0.179804727435112, 0.4316023886203766, 0.18038299679756165, 0.30340778827667236, 0.4202222228050232, 0.4173734188079834, 0.42408064007759094, 0.4182860553264618, 0.326717734336853, 0.3111678659915924, 0.3716884255409241, 0.17868666350841522, 0.07781557738780975, 0.3171732723712921, 0.39755669236183167, 0.4698485732078552, 0.35374686121940613, 0.27982789278030396], dtype='float32').reshape([25]),
            paddle.to_tensor([0.06500446796417236, 0.4609496295452118, 0.02059381641447544, 0.36165136098861694, 0.046055857092142105, 0.3280317187309265, 0.08395928889513016, 0.30318892002105713, 0.37480440735816956, 0.45755523443222046, 0.1809343546628952, 0.28869009017944336, 0.3718055486679077, 0.26555153727531433, 0.35818567872047424, 0.35632550716400146, 0.30934733152389526, 0.47847697138786316, 0.3479849398136139, 0.4161262810230255, 0.3588229715824127, 0.4333958625793457, 0.3042227029800415, 0.1508304923772812, 0.14933206140995026], dtype='float32').reshape([25]),
            paddle.to_tensor([0.4779307544231415, 0.0668032318353653, 0.11454756557941437, 0.257935494184494, 0.4256996512413025, 0.07117816060781479, 0.43763527274131775, 0.005105442367494106, 0.22478468716144562, 0.28699398040771484, 0.32828834652900696, 0.2198735624551773, 0.16249129176139832, 0.3178780674934387, 0.4608938992023468, 0.12155067175626755, 0.29330578446388245, 0.4079459011554718, 0.2544366419315338, 0.23160111904144287, 0.08364453166723251, 0.4497530162334442, 0.2920922636985779, 0.10601788759231567, 0.3686981499195099], dtype='float32').reshape([25]),
            paddle.to_tensor([0.3397134840488434, 0.31325510144233704, 0.30040881037712097, 0.4052117168903351, 0.2212599366903305, 0.07012638449668884, 0.4185038208961487, 0.20809847116470337, 0.45300376415252686, 0.33608049154281616, 0.36559078097343445, 0.4504281282424927, 0.35341230034828186, 0.20644983649253845, 0.2904188930988312, 0.4908723533153534, 0.06453346461057663, 0.26988932490348816, 0.19179293513298035, 0.3870117664337158, 0.29282814264297485, 0.1851629912853241, 0.3977486491203308, 0.4184046983718872, 0.44359782338142395], dtype='float32').reshape([25]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e3763bd8da1c1c0e689c35286aa131e0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3021823763847351, 0.03386687487363815, 0.357286274433136, 0.30138954520225525, 0.16166052222251892, 0.4916943609714508, 0.20417886972427368, 0.3830450773239136, 0.1907367706298828, 0.061362702399492264, 0.054146431386470795, 0.017375705763697624, 0.07465563714504242, 0.3088071346282959, 0.22685085237026215, 0.3688933253288269, 0.2570941150188446, 0.053327761590480804, 0.15567117929458618, 0.2743202745914459, 0.10419095307588577, 0.38275185227394104, 0.4037300944328308, 0.11895124614238739], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4169319272041321, 0.4402500092983246, 0.26778528094291687, 0.11097373813390732, 0.14317700266838074, 0.4849144518375397, 0.47388187050819397, 0.08715640008449554, 0.4192657768726349, 0.2658165991306305, 0.07675578445196152, 0.4470506012439728, 0.22250863909721375, 0.3440728485584259, 0.4362157881259918, 0.14226365089416504, 0.4036441445350647, 0.3850240409374237, 0.001563934376463294, 0.4488089680671692, 0.3822866678237915, 0.49667274951934814, 0.16623955965042114, 0.15129369497299194], dtype='float32').reshape([24]),
            paddle.to_tensor([0.14827071130275726, 0.08087601512670517, 0.01846451498568058, 0.10560838133096695, 0.16491568088531494, 0.12357046455144882, 0.03408773988485336, 0.23434048891067505, 0.4170524477958679, 0.3977212905883789, 0.1284727156162262, 0.2856045961380005, 0.2584035098552704, 0.1565197855234146, 0.112917460501194, 0.2802169919013977, 0.3532522916793823, 0.09649670869112015, 0.32575345039367676, 0.19178497791290283, 0.27310970425605774, 0.3064919114112854, 0.45310407876968384, 0.4423828125], dtype='float32').reshape([24]),
            paddle.to_tensor([0.385082483291626, 0.09969649463891983, 0.46587103605270386, 0.4481007754802704, 0.3433004319667816, 0.33787617087364197, 0.05657866969704628, 0.4109802842140198, 0.36908072233200073, 0.33569201827049255, 0.24397864937782288, 0.3373814821243286, 0.1299925446510315, 0.34493744373321533, 0.09597811847925186, 0.3105235695838928, 0.22883912920951843, 0.4569389522075653, 0.07674623280763626, 0.1209821030497551, 0.06835327297449112, 0.26408156752586365, 0.17604543268680573, 0.005792063660919666], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2f7708197cbf2a878490f5319b673eaa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 144, 16, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8b9b4a3cc08e30f84f507863d51d9db3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 60, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c93e195426e7db47c1e96bb7ca009b65(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 16, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_71298adb35dbb06a6d3ad8a560f5cedd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3165677c2a6b2034b9b695b44e44f7cf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 48, 48], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.23528601229190826, 0.27962401509284973, 0.07544071227312088, 0.4784315824508667, 0.19883517920970917, 0.20411869883537292, 0.4390125274658203, 0.4916110634803772, 0.4785987138748169, 0.021579941734671593, 0.26187217235565186, 0.24474194645881653, 0.03003578446805477, 0.0460980162024498, 0.17010273039340973, 0.10999061912298203, 0.41335904598236084, 0.10702096670866013, 0.2550758719444275, 0.1775282472372055, 0.1864432394504547, 0.24201029539108276, 0.4076138734817505, 0.07609423249959946], dtype='float32').reshape([24]),
            paddle.to_tensor([0.16628038883209229, 0.05406036600470543, 0.14236116409301758, 0.43068552017211914, 0.3734295070171356, 0.2504466474056244, 0.28175976872444153, 0.24807271361351013, 0.11473218351602554, 0.18797250092029572, 0.49335944652557373, 0.12138016521930695, 0.43478164076805115, 0.06763157993555069, 0.07352671027183533, 0.33964017033576965, 0.327796995639801, 0.16426295042037964, 0.17229343950748444, 0.04902995005249977, 0.2917639911174774, 0.01707496866583824, 0.12246642261743546, 0.044162869453430176], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4329681098461151, 0.3519529104232788, 0.10174218565225601, 0.25558966398239136, 0.14086420834064484, 0.002704623155295849, 0.1751701682806015, 0.10668773204088211, 0.2076646238565445, 0.39986908435821533, 0.4932407736778259, 0.3730515241622925, 0.45289626717567444, 0.07757285982370377, 0.19149115681648254, 0.2866968512535095, 0.2794446349143982, 0.1932276487350464, 0.42558687925338745, 0.30179888010025024, 0.20384834706783295, 0.34639495611190796, 0.13964074850082397, 0.05137939751148224], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4466427266597748, 0.2181367576122284, 0.25489541888237, 0.2598198354244232, 0.14920152723789215, 0.11983185261487961, 0.108746238052845, 0.4570101499557495, 0.2665177881717682, 0.26921120285987854, 0.21392008662223816, 0.4999412000179291, 0.19596651196479797, 0.14686106145381927, 0.3508358895778656, 0.033138714730739594, 0.338857501745224, 0.2142268568277359, 0.15864767134189606, 0.07552947103977203, 0.3511648178100586, 0.35734495520591736, 0.2893056571483612, 0.35540029406547546], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4de5d81e8af6b44d3ec66d3cb146fdf1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 76, 76], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_021252efb52ede89c8b44ec6099311e5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1f6a740214568bfffc91c14bcd016a71(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 6, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_420c34a900b204bc2025a0703f392c58(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f2d2e41711f3bf9a8fcc623b5d127d07(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.24937504529953003, 0.4014616310596466, 0.38759368658065796, 0.4824288487434387, 0.4077066481113434, 0.04884202033281326, 0.31708478927612305, 0.24707697331905365, 0.07380077242851257, 0.22354480624198914, 0.19187794625759125, 0.1543058305978775, 0.21026328206062317, 0.49676239490509033, 0.29652467370033264, 0.15112356841564178, 0.1973855346441269, 0.22371335327625275], dtype='float32').reshape([18]),
            paddle.to_tensor([0.02075483463704586, 0.3770371973514557, 0.3510066866874695, 0.24694077670574188, 0.026818785816431046, 0.16298778355121613, 0.07080868631601334, 0.27724215388298035, 0.460767924785614, 0.45195087790489197, 0.1798756867647171, 0.4317183494567871, 0.03861412778496742, 0.49782928824424744, 0.46913325786590576, 0.19959399104118347, 0.35775119066238403, 0.3280910551548004], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3054800033569336, 0.3389516770839691, 0.026330601423978806, 0.12240523099899292, 0.029329825192689896, 0.1725928783416748, 0.43799448013305664, 0.46589356660842896, 0.0610555075109005, 0.05644688010215759, 0.3607160747051239, 0.21502217650413513, 0.21115081012248993, 0.2271115779876709, 0.21829190850257874, 0.07490643113851547, 0.40398308634757996, 0.1738128811120987], dtype='float32').reshape([18]),
            paddle.to_tensor([0.31649351119995117, 0.12819960713386536, 0.4778773784637451, 0.03462212160229683, 0.03804953396320343, 0.3276126980781555, 0.26588281989097595, 0.26068761944770813, 0.2527407109737396, 0.4133330285549164, 0.2963980436325073, 0.013266902416944504, 0.014700384810566902, 0.15410511195659637, 0.19264031946659088, 0.2789681851863861, 0.25248193740844727, 0.019205689430236816], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0ed5a8258b68f22861bb0fa3bf5a98c3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 240, 240], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2989310026168823, 0.3125799000263214, 0.26867589354515076, 0.3055455684661865, 0.37704676389694214, 0.32770824432373047, 0.48479416966438293, 0.3011208176612854, 0.3929636478424072, 0.1938694715499878, 0.12786252796649933, 0.052677664905786514, 0.054179929196834564, 0.008156371302902699, 0.4380294382572174, 0.012229179963469505, 0.1209029033780098, 0.162896990776062, 0.08386851102113724, 0.21094603836536407, 0.1813669055700302, 0.44861987233161926, 0.09999725967645645, 0.25075647234916687], dtype='float32').reshape([24]),
            paddle.to_tensor([0.49526461958885193, 0.13333994150161743, 0.17040091753005981, 0.0427236445248127, 0.24739691615104675, 0.3787170946598053, 0.4712286591529846, 0.2581985890865326, 0.23012253642082214, 0.2714328169822693, 0.06719967722892761, 0.14414852857589722, 0.28527915477752686, 0.01178709976375103, 0.32179489731788635, 0.2829473912715912, 0.15931980311870575, 0.12207698076963425, 0.1122756227850914, 0.38667529821395874, 0.42866548895835876, 0.2833808362483978, 0.14880473911762238, 0.35767966508865356], dtype='float32').reshape([24]),
            paddle.to_tensor([0.48656609654426575, 0.4678081274032593, 0.4484251141548157, 0.27053773403167725, 0.0006573019200004637, 0.3823550045490265, 0.37364494800567627, 0.4641377031803131, 0.11901265382766724, 0.35075807571411133, 0.10418736189603806, 0.43366825580596924, 0.4623287320137024, 0.2843692898750305, 0.14011873304843903, 0.4815276861190796, 0.10263621807098389, 0.4751259982585907, 0.156972736120224, 0.005412272643297911, 0.323327898979187, 0.3424151837825775, 0.18049034476280212, 0.12510345876216888], dtype='float32').reshape([24]),
            paddle.to_tensor([0.30561089515686035, 0.3606179654598236, 0.2086467742919922, 0.3519546091556549, 0.10175686329603195, 0.24619148671627045, 0.3287395238876343, 0.17887265980243683, 0.25453442335128784, 0.027788881212472916, 0.3266051113605499, 0.48968783020973206, 0.46983805298805237, 0.03331317380070686, 0.07523249089717865, 0.4743795394897461, 0.2961810231208801, 0.4434318542480469, 0.3409186005592346, 0.10459280014038086, 0.011153478175401688, 0.26783663034439087, 0.3190518915653229, 0.3828868269920349], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ee17e949c6a13b2147fd97a54b2836d5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 480, 480], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4608033001422882, 0.2771528661251068, 0.3077853322029114, 0.15882745385169983, 0.4403676390647888, 0.3014174997806549, 0.1368720382452011, 0.27374228835105896], dtype='float32').reshape([8]),
            paddle.to_tensor([0.2591215968132019, 0.1274474859237671, 0.4532667398452759, 0.005309162195771933, 0.12114439159631729, 0.012504739686846733, 0.2690131366252899, 0.12508773803710938], dtype='float32').reshape([8]),
            paddle.to_tensor([0.08220501989126205, 0.22560767829418182, 0.38317596912384033, 0.4800109267234802, 0.26623284816741943, 0.0627586767077446, 0.14451073110103607, 0.3211542069911957], dtype='float32').reshape([8]),
            paddle.to_tensor([0.24062630534172058, 0.030694391578435898, 0.2296541929244995, 0.004231294617056847, 0.3611728847026825, 0.05855594202876091, 0.16648952662944794, 0.49386683106422424], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f9a29fc5540803b40d321321edd1446a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3460647761821747, 0.03759608417749405, 0.06449561566114426, 0.2523132562637329, 0.30712705850601196, 0.45732933282852173, 0.26027679443359375, 0.2340448498725891, 0.4621221721172333, 0.02397165261209011, 0.373898983001709, 0.3564256429672241, 0.2532840967178345, 0.4607970118522644, 0.32642921805381775, 0.4604637026786804, 0.4560723602771759, 0.2640434503555298, 0.299813836812973, 0.3561883866786957, 0.13296134769916534, 0.45173779129981995, 0.4419045150279999, 0.07310266047716141, 0.3879484236240387, 0.07187185436487198, 0.40412020683288574, 0.4452918469905853, 0.377390056848526, 0.40808379650115967], dtype='float32').reshape([30]),
            paddle.to_tensor([0.18992407619953156, 0.05184319615364075, 0.3905681073665619, 0.09988638758659363, 0.2306569665670395, 0.4225534498691559, 0.03419887647032738, 0.09458023309707642, 0.318652868270874, 0.3027535676956177, 0.4991031587123871, 0.1027761772274971, 0.49667006731033325, 0.37838733196258545, 0.07434552907943726, 0.2689858078956604, 0.09909548610448837, 0.2312014251947403, 0.4973025321960449, 0.1879919171333313, 0.3929124176502228, 0.4950309693813324, 0.04688161611557007, 0.008752677589654922, 0.13936738669872284, 0.47212135791778564, 0.3029521703720093, 0.027955777943134308, 0.0581451877951622, 0.20299361646175385], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3235654830932617, 0.2865695357322693, 0.06946488469839096, 0.4005510210990906, 0.2995714247226715, 0.0019844048656523228, 0.49169549345970154, 0.1430440992116928, 0.4521975517272949, 0.26392704248428345, 0.24371206760406494, 0.419881671667099, 0.130268856883049, 0.21223469078540802, 0.13656964898109436, 0.10456281155347824, 0.45819273591041565, 0.3312126100063324, 0.3541792631149292, 0.49784785509109497, 0.3690558671951294, 0.19152820110321045, 0.3082337975502014, 0.46199536323547363, 0.2628200948238373, 0.10213805735111237, 0.22892776131629944, 0.4373834431171417, 0.0934545248746872, 0.2674286365509033], dtype='float32').reshape([30]),
            paddle.to_tensor([0.0981908068060875, 0.17165561020374298, 0.376038134098053, 0.3062860369682312, 0.26093751192092896, 0.3697115182876587, 0.42273736000061035, 0.12710291147232056, 0.30471938848495483, 0.11306865513324738, 0.04312586039304733, 0.07273796945810318, 0.4010941982269287, 0.35855913162231445, 0.07167503237724304, 0.15590696036815643, 0.3982660174369812, 0.23340003192424774, 0.0883154571056366, 0.19619745016098022, 0.4979792833328247, 0.4606116712093353, 0.047913528978824615, 0.05176907405257225, 0.18047462403774261, 0.2920037806034088, 0.427933931350708, 0.4232257008552551, 0.14980438351631165, 0.2360006421804428], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c36ba18e77462c0a827bc6c454f266dc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 8, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9519d736d1cb06178a76d775c683023d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_925515d29d60fc29be68dc0c3b464edd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b340f80c367196e43f70603799bfa159(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 30, 30], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dbb694cbdeae271f3d00f13f7629bfb0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_145ad20e7a3b3c8aa31f6b40d4fe3d45
    def get_inputs(self):
        return [
            paddle.uniform([1, 384], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_db1890183f90469b6470f2eb30d82595(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 16, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cedd74ee0f0ee9d73c1569accd1e80b0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 2560, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([2560], dtype='float32', min=0, max=0.5),
            paddle.uniform([2560], dtype='float32', min=0, max=0.5),
            paddle.uniform([2560], dtype='float32', min=0, max=0.5),
            paddle.uniform([2560], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d0714531debb02254542d6af9d5d87d6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 320, 320], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e08a467cbf68d452f9404daccfaecf04(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 15, 15], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_88f594b94ed79c90a3762907063b8ca6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 50, 84], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6159b4ea3940844fcc323f50c946d485(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 728, 19, 19], dtype='float16', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ccba862fa9c2ecd5256194d5f8ff2b8d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 2, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_081ec06c1992f4db821958fae785ca9b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.1351810097694397, 0.1635494828224182, 0.0337597094476223, 0.4065770208835602, 0.15577396750450134, 0.3392881453037262, 0.17892687022686005, 0.31887975335121155, 0.48156431317329407, 0.1663709282875061, 0.28975898027420044, 0.45238491892814636, 0.46331456303596497, 0.04598017781972885, 0.3750263750553131, 0.44411709904670715, 0.29748010635375977, 0.12488120794296265, 0.24400585889816284, 0.1448914110660553, 0.3700668215751648, 0.3758578300476074, 0.19362638890743256, 0.00290108029730618, 0.23585402965545654, 0.002573784440755844, 0.3652951419353485, 0.2984117865562439], dtype='float32').reshape([28]),
            paddle.to_tensor([0.03418072685599327, 0.49115949869155884, 0.34172332286834717, 0.21629871428012848, 0.3086155652999878, 0.10387406498193741, 0.47795751690864563, 0.26884332299232483, 0.20519831776618958, 0.4539199769496918, 0.08807236701250076, 0.2212730497121811, 0.13484077155590057, 0.4469260275363922, 0.032825302332639694, 0.366197794675827, 0.46828800439834595, 0.26161083579063416, 0.22813984751701355, 0.2372477948665619, 0.08744924515485764, 0.2857089340686798, 0.07458776980638504, 0.24207502603530884, 0.21060480177402496, 0.4240378439426422, 0.48475298285484314, 0.09557843953371048], dtype='float32').reshape([28]),
            paddle.to_tensor([0.4447677433490753, 0.4098854064941406, 0.022839464247226715, 0.3575095236301422, 0.10696834325790405, 0.3371276557445526, 0.23021918535232544, 0.46950405836105347, 0.07422807067632675, 0.016218356788158417, 0.49430274963378906, 0.02802181988954544, 0.16640503704547882, 0.23008261620998383, 0.24354444444179535, 0.12079253792762756, 0.25156456232070923, 0.17766320705413818, 0.006960160564631224, 0.2760879099369049, 0.17440874874591827, 0.4750673770904541, 0.0029332577250897884, 0.020883983001112938, 0.26806366443634033, 0.28611627221107483, 0.10329800844192505, 0.02356964349746704], dtype='float32').reshape([28]),
            paddle.to_tensor([0.3131377100944519, 0.02118144929409027, 0.046630702912807465, 0.4399823248386383, 0.09595318883657455, 0.2515423893928528, 0.3527558743953705, 0.29180237650871277, 0.009566660039126873, 0.3160376250743866, 0.08838097006082535, 0.4736478626728058, 0.03997400775551796, 0.3738372027873993, 0.1793864518404007, 0.3286462724208832, 0.37619727849960327, 0.1609308272600174, 0.45503631234169006, 0.3591760993003845, 0.003242186503484845, 0.04714960977435112, 0.1887046843767166, 0.12536583840847015, 0.290544331073761, 0.10319153964519501, 0.09659844636917114, 0.4189870059490204], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_000345d3fef3fda8308d6553c21137f6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_14250fb6dea63a62d3e88c31659def05(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 768, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_42706a870d1cfeac491fa245e0c819ef(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 704, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([704], dtype='float32', min=0, max=0.5),
            paddle.uniform([704], dtype='float32', min=0, max=0.5),
            paddle.uniform([704], dtype='float32', min=0, max=0.5),
            paddle.uniform([704], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fc69e580e1f5bf769ce9df1dbdadaf51(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 32, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_68c885aa9fd50f14fa1e73c99efaa7b6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 768, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_b4f4da3a8d44d1b5b3a2903a097df186(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5907b03828c722d77bcaf3695ae404cf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_b4f4da3a8d44d1b5b3a2903a097df186
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ec57bef53becd7e1be8e2e534c4733c1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 2, 6], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7e746e3013ad885c814ef8db594b1a65(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 16, 12], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2f2ec5544e17c5110d7a56a24e7f5d9f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 640, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_d589af777f5a8ae699611515c41499fb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_72a56ffb44843cc0c3b1967cb6ffde03(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_d589af777f5a8ae699611515c41499fb
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2f0f4e3fb9cf83309470291d071e813d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 8, 25], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bd92994130fae7a8d88b47b33d2cbef1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.25174039602279663, 0.11739535629749298, 0.13100211322307587, 0.24322499334812164, 0.36756277084350586, 0.4341895282268524, 0.29163289070129395, 0.14110633730888367, 0.15469364821910858, 0.3396559953689575, 0.11382929235696793, 0.394895076751709, 0.21743936836719513, 0.06704574078321457, 0.29101791977882385, 0.21445749700069427, 0.24210548400878906, 0.08776653558015823, 0.11347931623458862, 0.16972115635871887, 0.3522386848926544, 0.40021809935569763, 0.2492084503173828, 0.44852331280708313, 0.17886729538440704, 0.3417797088623047, 0.48639485239982605, 0.3494512140750885, 0.4570082724094391, 0.46023404598236084], dtype='float32').reshape([30]),
            paddle.to_tensor([0.20106975734233856, 0.12307345122098923, 0.2502707540988922, 0.3650415539741516, 0.24706509709358215, 0.17070910334587097, 0.28735801577568054, 0.23788638412952423, 0.4515824317932129, 0.015584612265229225, 0.21492290496826172, 0.4511776566505432, 0.2084062695503235, 0.18517540395259857, 0.008752621710300446, 0.12789909541606903, 0.2480669766664505, 0.15757206082344055, 0.14366942644119263, 0.24315674602985382, 0.33668163418769836, 0.014853809028863907, 0.26119598746299744, 0.3930724859237671, 0.44747671484947205, 0.15483571588993073, 0.2999269664287567, 0.06095181033015251, 0.21054686605930328, 0.48117128014564514], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3490549325942993, 0.17215712368488312, 0.06524725258350372, 0.28389471769332886, 0.17024165391921997, 0.17898224294185638, 0.4518522620201111, 0.16408273577690125, 0.31127235293388367, 0.2978357672691345, 0.2151452600955963, 0.35211998224258423, 0.0797078013420105, 0.22188955545425415, 0.484762966632843, 0.28660768270492554, 0.3968994617462158, 0.23009032011032104, 0.1527600735425949, 0.1221362054347992, 0.260830283164978, 0.4652387499809265, 0.12845848500728607, 0.47333210706710815, 0.2970476448535919, 0.25123900175094604, 0.40871956944465637, 0.2843573987483978, 0.3807591497898102, 0.1540471613407135], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4885633885860443, 0.4680238664150238, 0.034285008907318115, 0.016294218599796295, 0.19516369700431824, 0.21968553960323334, 0.28596368432044983, 0.4812932014465332, 0.24457208812236786, 0.1638537496328354, 0.05275462567806244, 0.3978063464164734, 0.3684965670108795, 0.07235720753669739, 0.393148273229599, 0.4590419828891754, 0.0968794971704483, 0.17603181302547455, 0.27005887031555176, 0.30190953612327576, 0.16279008984565735, 0.056453149765729904, 0.0048544155433773994, 0.1836521178483963, 0.4373916983604431, 0.4825965464115143, 0.41277897357940674, 0.019477078691124916, 0.19418776035308838, 0.39503321051597595], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_196d79199ccc17a811f7017282030c84(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.04298010095953941, 0.3444214165210724, 0.10635378211736679, 0.4480201303958893, 0.04840806499123573, 0.3848665654659271, 0.012357610277831554, 0.3102198541164398, 0.19841119647026062, 0.08710097521543503, 0.3028307259082794, 0.45587974786758423, 0.20193322002887726, 0.04079562798142433, 0.14424927532672882, 0.4602382183074951, 0.3575868308544159, 0.4020620286464691, 0.37683606147766113, 0.4796934127807617, 0.1661863476037979, 0.4467772841453552, 0.4347854554653168, 0.3055102229118347, 0.1198754608631134, 0.11866345256567001, 0.22064431011676788, 0.1853850930929184, 0.019657038152217865, 0.16763827204704285], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2599644660949707, 0.17445622384548187, 0.2697717845439911, 0.48312830924987793, 0.2184092402458191, 0.2286476492881775, 0.3385855257511139, 0.3351418972015381, 0.44210776686668396, 0.10799982398748398, 0.48029300570487976, 0.38722628355026245, 0.3415314555168152, 0.3983370065689087, 0.08018721640110016, 0.24471701681613922, 0.04654587432742119, 0.074385866522789, 0.287371963262558, 0.31438595056533813, 0.32808807492256165, 0.4614276885986328, 0.17016208171844482, 0.1541747897863388, 0.08005876839160919, 0.4712609052658081, 0.1945805698633194, 0.23091834783554077, 0.37310105562210083, 0.17707127332687378], dtype='float32').reshape([30]),
            paddle.to_tensor([0.22737513482570648, 0.4845822751522064, 0.1303611695766449, 0.020671421661973, 0.1910315901041031, 0.1482682079076767, 0.2948988676071167, 0.301993727684021, 0.0522489957511425, 0.0322427898645401, 0.19108332693576813, 0.4712885320186615, 0.4675386846065521, 0.4945421516895294, 0.43463462591171265, 0.22462047636508942, 0.12220985442399979, 0.3316037952899933, 0.016233546659350395, 0.18424755334854126, 0.4432294964790344, 0.21738824248313904, 0.4095432162284851, 0.2221631407737732, 0.4192267954349518, 0.18494679033756256, 0.12526080012321472, 0.40440526604652405, 0.05348682776093483, 0.08071497082710266], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4693225026130676, 0.12551249563694, 0.4931182861328125, 0.4461618661880493, 0.13409112393856049, 0.188151016831398, 0.4650100767612457, 0.3779096007347107, 0.04806745424866676, 0.31806161999702454, 0.28492897748947144, 0.4615963101387024, 0.2926301062107086, 0.06700389087200165, 0.3520868420600891, 0.09810318797826767, 0.23946082592010498, 0.005570099223405123, 0.21141090989112854, 0.24982839822769165, 0.26895561814308167, 0.13061566650867462, 0.22985239326953888, 0.1208517774939537, 0.35868027806282043, 0.2358982115983963, 0.10410875827074051, 0.2595551609992981, 0.30745482444763184, 0.15871858596801758], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_818fcec8a73a139b187129182d9957e8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 360, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([360], dtype='float32', min=0, max=0.5),
            paddle.uniform([360], dtype='float32', min=0, max=0.5),
            paddle.uniform([360], dtype='float32', min=0, max=0.5),
            paddle.uniform([360], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2cd27105ee608abea60139894cef0a00(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 480, 480], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2732510566711426, 0.33753329515457153, 0.25912201404571533, 0.26355689764022827, 0.17107781767845154, 0.41103845834732056, 0.4349501132965088, 0.16904373466968536], dtype='float32').reshape([8]),
            paddle.to_tensor([0.08745265007019043, 0.20511546730995178, 0.3565766215324402, 0.06818977743387222, 0.3367101848125458, 0.28089970350265503, 0.498790442943573, 0.45122238993644714], dtype='float32').reshape([8]),
            paddle.to_tensor([0.43694061040878296, 0.4615500867366791, 0.0010721497237682343, 0.3449670970439911, 0.1301037073135376, 0.08565960824489594, 0.1617436707019806, 0.4575599431991577], dtype='float32').reshape([8]),
            paddle.to_tensor([0.3761531114578247, 0.18377366662025452, 0.47066399455070496, 0.2689981460571289, 0.34159985184669495, 0.47114646434783936, 0.22740855813026428, 0.2989424169063568], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_524ed3569788c4f7144cecd30f44c32d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d40a0ce9c72056e3d65edf44a2fce08f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 200, 336], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5d414d342c5a4ff2a46c716078f7dd36(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e02bd9b223075dd2023e26edbe2e5a92(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_0edd578f48a07aa5d1d79f9f678b472e
    def get_inputs(self):
        return [
            paddle.uniform([16, 384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_5e7cb23119f65b33bdbad71952f6dd12(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a4c2cd58bd8d455823bed9cb9544eee5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_5e7cb23119f65b33bdbad71952f6dd12
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bc70696477abe5858d665b60f3258817(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4b5894e9bfbc62753f54e3f60ccddba0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 30, 30], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_52e43cc241afdd5927c058d793724e42(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 480, 480], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_98ef5852e579b431fd4b9d5574f9875f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.09024009853601456, 0.465846449136734, 0.4039321839809418, 0.4236215054988861, 0.31901365518569946, 0.1731182038784027, 0.07438316196203232, 0.28881317377090454, 0.13438938558101654, 0.35321244597435, 0.20769676566123962, 0.15827365219593048, 0.22191716730594635, 0.29613640904426575, 0.318195641040802, 0.3991709351539612, 0.38212016224861145, 0.2334761619567871, 0.03108512982726097, 0.13243944942951202, 0.010870865546166897, 0.13251827657222748, 0.31135568022727966, 0.4880335330963135], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3894738554954529, 0.35928410291671753, 0.14235395193099976, 0.132511168718338, 0.46691614389419556, 0.48741936683654785, 0.2473371922969818, 0.02319328486919403, 0.0037279294338077307, 0.05432742461562157, 0.31943419575691223, 0.16303350031375885, 0.24321603775024414, 0.34452226758003235, 0.38278329372406006, 0.10959542542695999, 0.07309255003929138, 0.09155357629060745, 0.1957874745130539, 0.38443097472190857, 0.23851056396961212, 0.3282078206539154, 0.3357170820236206, 0.2419939488172531], dtype='float32').reshape([24]),
            paddle.to_tensor([0.42436712980270386, 0.03238145262002945, 0.10421249270439148, 0.23634470999240875, 0.13760018348693848, 0.06379061937332153, 0.03863650932908058, 0.03236066922545433, 0.4746432304382324, 0.34293726086616516, 0.31333091855049133, 0.029497025534510612, 0.13784025609493256, 0.32946717739105225, 0.4933707118034363, 0.18481330573558807, 0.24104247987270355, 0.0984278991818428, 0.19514910876750946, 0.437812864780426, 0.24322955310344696, 0.4894300401210785, 0.45380276441574097, 0.4447157680988312], dtype='float32').reshape([24]),
            paddle.to_tensor([0.332231730222702, 0.2796057462692261, 0.012587555684149265, 0.05844932049512863, 0.11331982165575027, 0.24715158343315125, 0.35975581407546997, 0.13191573321819305, 0.3800874948501587, 0.3513725996017456, 0.01547982171177864, 0.26072245836257935, 0.434129536151886, 0.10089155286550522, 0.28467270731925964, 0.03984469175338745, 0.11335895210504532, 0.37671613693237305, 0.19489233195781708, 0.10221680998802185, 0.35630708932876587, 0.41636186838150024, 0.369504451751709, 0.2447752207517624], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_506f37d223f243152f90bc806a124c3a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_50d6ce2427ec9c3f1d9a88519a9ee400(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 16, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_04ee250d3f7b909faa7e4fb9b9bb33cc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2edec4140a2a86e6ac4cb090d72be9c4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 112, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_60ac304885dcb2d44ec7dc1a1edcf864(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1152, 6, 6], dtype='float16', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d71a41eb5230316ad3f401470cf4b7ed(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.27801814675331116, 0.4958604872226715, 0.4966631531715393, 0.3396948277950287, 0.2846277356147766, 0.4290798604488373, 0.22431565821170807, 0.2431817352771759, 0.4444367587566376, 0.48318609595298767, 0.2889133393764496, 0.02416726015508175, 0.16667242348194122, 0.37855926156044006, 0.02094898745417595, 0.2768813967704773, 0.277598112821579, 0.31967517733573914, 0.0035608592443168163, 0.25471967458724976, 0.44369614124298096, 0.32690906524658203, 0.09751325100660324, 0.4187201261520386, 0.36602309346199036, 0.2642526626586914, 0.2927958369255066, 0.4598219096660614], dtype='float32').reshape([28]),
            paddle.to_tensor([0.21297511458396912, 0.28681468963623047, 0.022235317155718803, 0.32158294320106506, 0.4476563334465027, 0.10797426104545593, 0.17712847888469696, 0.13119539618492126, 0.2682831585407257, 0.0666317418217659, 0.34846174716949463, 0.27785971760749817, 0.19262813031673431, 0.3534044623374939, 0.15567490458488464, 0.08519608527421951, 0.1527697890996933, 0.3203796148300171, 0.011618606746196747, 0.31450462341308594, 0.035283032804727554, 0.4165225625038147, 0.4802592098712921, 0.014821340329945087, 0.42890554666519165, 0.13126979768276215, 0.36750736832618713, 0.3743061125278473], dtype='float32').reshape([28]),
            paddle.to_tensor([0.47249525785446167, 0.20492026209831238, 0.07292933017015457, 0.13348707556724548, 0.412588894367218, 0.38414251804351807, 0.10240162163972855, 0.4412406384944916, 0.08668376505374908, 0.45217156410217285, 0.261232852935791, 0.4849063456058502, 0.3286502957344055, 0.3406994938850403, 0.2644314765930176, 0.16868840157985687, 0.42796647548675537, 0.37128159403800964, 0.4304390847682953, 0.45430219173431396, 0.11481664329767227, 0.18417184054851532, 0.21265967190265656, 0.32404354214668274, 0.11201968789100647, 0.18124835193157196, 0.15836314857006073, 0.40110549330711365], dtype='float32').reshape([28]),
            paddle.to_tensor([0.06303573399782181, 0.3291240632534027, 0.34991538524627686, 0.15443652868270874, 0.3616882264614105, 0.20127907395362854, 0.12589186429977417, 0.17083914577960968, 0.16711178421974182, 0.12429020553827286, 0.1374562680721283, 0.16844141483306885, 0.04737028852105141, 0.37319719791412354, 0.41597896814346313, 0.1972816437482834, 0.1714361608028412, 0.05661294609308243, 0.3293173015117645, 0.3765139579772949, 0.24420486390590668, 0.15936845541000366, 0.22378326952457428, 0.0201682411134243, 0.27032142877578735, 0.23119112849235535, 0.49815526604652405, 0.4525670111179352], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c0a42c4d0cd0acc18c313f59deac9fcf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a83c093fc4925508d4cbebbbe2a3e146(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9b4ddf112b4c78e73e1831e99073c066(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_af91295b59c5fca17f890a802deb001f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9c49c3d4cdd1a3e7e6672d44cea385a3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 4, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_99fe90696970908a799e2b36ff07c60f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_53ac826f6acab112ab6c50de8bfd4902(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 112, 199], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.058893900364637375, 0.14964863657951355, 0.061620160937309265, 0.32587867975234985, 0.09428708255290985, 0.4692589044570923, 0.08002722263336182, 0.07892780750989914, 0.1201753169298172, 0.03559194505214691, 0.39847084879875183, 0.37142887711524963, 0.1487618237733841, 0.45160213112831116, 0.35944437980651855, 0.09611020237207413, 0.2697199583053589, 0.2498069554567337], dtype='float32').reshape([18]),
            paddle.to_tensor([0.24743248522281647, 0.35971203446388245, 0.25626230239868164, 0.28089532256126404, 0.15801312029361725, 0.45648860931396484, 0.28818050026893616, 0.30308032035827637, 0.08395370841026306, 0.05984722822904587, 0.34567970037460327, 0.3672791123390198, 0.016936209052801132, 0.21407610177993774, 0.1339554637670517, 0.1496707648038864, 0.1823585033416748, 0.08067051321268082], dtype='float32').reshape([18]),
            paddle.to_tensor([0.41581523418426514, 0.43407613039016724, 0.33560076355934143, 0.3015395998954773, 0.057362087070941925, 0.15793825685977936, 0.23619820177555084, 0.3647141754627228, 0.27613455057144165, 0.1151646077632904, 0.1544065922498703, 0.39701682329177856, 0.17978796362876892, 0.19840708374977112, 0.27274808287620544, 0.025362957268953323, 0.24659936130046844, 0.04795357212424278], dtype='float32').reshape([18]),
            paddle.to_tensor([0.1939917355775833, 0.07383373379707336, 0.41533419489860535, 0.27156201004981995, 0.48942533135414124, 0.09268561750650406, 0.28733688592910767, 0.17751610279083252, 0.1874682903289795, 0.0026424312964081764, 0.38240957260131836, 0.06821756809949875, 0.4142327308654785, 0.18393681943416595, 0.476742684841156, 0.07736065238714218, 0.3702981770038605, 0.36159437894821167], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_45a61101c044f7bf6622cdd807c27f03(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 30, 30], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_df6de4496db184ce32004c0451a3f274(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1856, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1856], dtype='float32', min=0, max=0.5),
            paddle.uniform([1856], dtype='float32', min=0, max=0.5),
            paddle.uniform([1856], dtype='float32', min=0, max=0.5),
            paddle.uniform([1856], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f2d9bdf1d2964eadc3bf99e0b1aa186a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ad0d2524e0aedb228de6741698de9592(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 17, 17], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e117494db308a5339041ca22fb156a22(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_890602f2cb41e5a297265752c0009008(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 32, 64], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.10975776612758636, 0.4184778034687042, 0.4951764643192291, 0.44483527541160583, 0.14659206569194794, 0.32105711102485657, 0.12781167030334473, 0.11699218302965164, 0.020395858213305473, 0.40889185667037964, 0.037397146224975586, 0.1592552661895752, 0.3224446773529053, 0.46606311202049255, 0.27019739151000977, 0.4598022401332855, 0.13196802139282227, 0.3794037699699402], dtype='float32').reshape([18]),
            paddle.to_tensor([0.46134886145591736, 0.428850382566452, 0.27469319105148315, 0.1910398006439209, 0.16109664738178253, 0.4953792989253998, 0.2753244936466217, 0.2511668801307678, 0.14446866512298584, 0.1903054416179657, 0.40380993485450745, 0.4654881954193115, 0.204682856798172, 0.33767780661582947, 0.010078677907586098, 0.4183063805103302, 0.1714605689048767, 0.4833314120769501], dtype='float32').reshape([18]),
            paddle.to_tensor([0.0224243625998497, 0.2900558412075043, 0.06019086763262749, 0.16181066632270813, 0.3361230194568634, 0.38984325528144836, 0.40239760279655457, 0.1681811511516571, 0.3844480812549591, 0.30197957158088684, 0.34900176525115967, 0.2989369332790375, 0.3920721113681793, 0.2994302213191986, 0.06114959716796875, 0.4161347448825836, 0.05834970995783806, 0.2895062267780304], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2678831219673157, 0.05132513865828514, 0.28014862537384033, 0.3540476858615875, 0.08059459924697876, 0.31991833448410034, 0.031376227736473083, 0.030236339196562767, 0.08635202795267105, 0.11114473640918732, 0.02193281054496765, 0.4211057424545288, 0.03770394250750542, 0.028834065422415733, 0.371351033449173, 0.13441573083400726, 0.400128036737442, 0.03047122061252594], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_465c0017c1a52c557472087970776538(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a642943dd70ff5c9215943303be708bb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_145ad20e7a3b3c8aa31f6b40d4fe3d45
    def get_inputs(self):
        return [
            paddle.uniform([16, 768], dtype='float16', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1fa8320b5e3c67f9c47e5a1586a1dc75(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_449eae12aa22f6d61a74f8df1ea570d4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 480, 480], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.20791174471378326, 0.3512657880783081, 0.3935418426990509, 0.13404402136802673, 0.04338734969496727, 0.37876564264297485, 0.032742876559495926, 0.11517748981714249, 0.4778864085674286, 0.4155491888523102, 0.33497101068496704, 0.08036978542804718, 0.32318031787872314, 0.46492764353752136, 0.04032667726278305, 0.061985306441783905, 0.09224134683609009, 0.018456192687153816, 0.40445125102996826, 0.012227034196257591, 0.46824660897254944, 0.22292374074459076, 0.4217095971107483, 0.1604171246290207], dtype='float32').reshape([24]),
            paddle.to_tensor([0.18607044219970703, 0.45731377601623535, 0.3774895966053009, 0.09676501899957657, 0.10381603986024857, 0.4194267690181732, 0.24919305741786957, 0.2915085256099701, 0.15647611021995544, 0.19742068648338318, 0.34363651275634766, 0.21903371810913086, 0.049927692860364914, 0.44663724303245544, 0.45513978600502014, 0.034294892102479935, 0.32893118262290955, 0.13076473772525787, 0.37543344497680664, 0.25097641348838806, 0.049269501119852066, 0.19037820398807526, 0.47283825278282166, 0.49733564257621765], dtype='float32').reshape([24]),
            paddle.to_tensor([0.16897587478160858, 0.474031001329422, 0.07780526578426361, 0.33124983310699463, 0.4287818968296051, 0.3172225058078766, 0.3642904460430145, 0.004710359498858452, 0.29672038555145264, 0.2864389419555664, 0.42020365595817566, 0.02114741876721382, 0.008258390240371227, 0.30119964480400085, 0.22529952228069305, 0.17352011799812317, 0.39774924516677856, 0.07308121025562286, 0.278018593788147, 0.13933037221431732, 0.09050258994102478, 0.08813074231147766, 0.007692421320825815, 0.1435081511735916], dtype='float32').reshape([24]),
            paddle.to_tensor([0.05008459836244583, 0.24669161438941956, 0.3256688714027405, 0.2474510669708252, 0.24380438029766083, 0.31946229934692383, 0.07434140145778656, 0.06792160868644714, 0.08386670053005219, 0.07324044406414032, 0.226415753364563, 0.41941460967063904, 0.4793802499771118, 0.2434098869562149, 0.15246428549289703, 0.4979732930660248, 0.029273437336087227, 0.02561231143772602, 0.23647138476371765, 0.195579394698143, 0.1762688159942627, 0.11588647961616516, 0.44899389147758484, 0.36086711287498474], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5ea361d6d8809742e490968da7e6f82e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 512, 1024], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_91a9e2d82dac75a22a91fd1a7718312b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d7a8d6faa735c9663483a6e184463e36(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ba3a958c7670f1348a46df0dbcf7e71a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 16, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9c681ed43e80a75a4ce3eac352fc99b4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 256, 512], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_678ae6a1d631a8ee53b86b13895677b1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4569685757160187, 0.07188740372657776, 0.043797679245471954, 0.24932877719402313, 0.43175968527793884, 0.290310800075531, 0.37592610716819763, 0.3032418191432953, 0.10037091374397278, 0.47690027952194214, 0.1416560709476471, 0.3809279501438141, 0.30146217346191406, 0.28725263476371765, 0.03865322470664978, 0.24862004816532135], dtype='float32').reshape([16]),
            paddle.to_tensor([0.207688570022583, 0.26612287759780884, 0.2668226361274719, 0.4187876582145691, 0.47323718667030334, 0.16975419223308563, 0.3007955849170685, 0.20652517676353455, 0.012813800014555454, 0.4103839099407196, 0.30340516567230225, 0.03964802995324135, 0.23053507506847382, 0.20162464678287506, 0.2519223093986511, 0.4231475591659546], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4487173557281494, 0.18466417491436005, 0.4213258624076843, 0.24789616465568542, 0.3276798129081726, 0.12217843532562256, 0.39398181438446045, 0.14234411716461182, 0.04529000073671341, 0.31682923436164856, 0.3930613100528717, 0.1851726919412613, 0.2343517392873764, 0.30032938718795776, 0.4872993528842926, 0.31606778502464294], dtype='float32').reshape([16]),
            paddle.to_tensor([0.42097362875938416, 0.20549994707107544, 0.3784199059009552, 0.31551986932754517, 0.4597948491573334, 0.4236929714679718, 0.44721168279647827, 0.14350242912769318, 0.4114488661289215, 0.3993879556655884, 0.4538281559944153, 0.28360116481781006, 0.11932727694511414, 0.2393161952495575, 0.17806118726730347, 0.04905015230178833], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8a6a060c8f346231c0eb898a1e937ab3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 120, 120], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8e0409ce6f1d287aed1b82add7f827e6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f4c1e20b5612a5a915d7d48d7e7c126a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 768, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_56e2e055f345f5b1781847323966694c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1044, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1044], dtype='float32', min=0, max=0.5),
            paddle.uniform([1044], dtype='float32', min=0, max=0.5),
            paddle.uniform([1044], dtype='float32', min=0, max=0.5),
            paddle.uniform([1044], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9d30bb3f33c55292521493f2b45da89e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 30, 30], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a3ccb678001541d2502b2c9400458688(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f23763a3cace8ba81c8e2627ed031282(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 4, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d97e1437c3d4251789e3959e264d122b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_303e28bf8daa20964b62c99c19c50cd4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_39f301f0b7726800c3f64144f3a058ec(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 26], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6fc3d4cad8473c3e61d331713dd0fb7d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1280, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c50c1c92563e15641752ced1e35ed47b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a3216f17f3ebae06e93b357d6cf0c915(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e8b5381e773eda8e4516e16d6522384c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 432, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_955e1571a68c2039cd04a0e3f11eb212(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3876249f6b63dac34902e628ca392b86(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_b4f4da3a8d44d1b5b3a2903a097df186
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 256, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8f445cdd7f2f19d431b41c7a1578b52c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.19828525185585022, 0.38532644510269165, 0.13911065459251404, 0.4915873110294342, 0.3088332712650299, 0.005968507379293442, 0.3872829079627991, 0.04063931107521057, 0.301561564207077, 0.018569082021713257, 0.21407990157604218, 0.02890564501285553, 0.057219497859478, 0.1501350998878479, 0.06330052018165588, 0.3029191493988037, 0.2055734097957611, 0.17537470161914825, 0.3275960087776184, 0.16952526569366455, 0.22692234814167023, 0.25048312544822693, 0.1460447460412979, 0.4103824496269226], dtype='float32').reshape([24]),
            paddle.to_tensor([0.12273560464382172, 0.4304466247558594, 0.14862261712551117, 0.27651432156562805, 0.30783361196517944, 0.04825497418642044, 0.3880453109741211, 0.08915526419878006, 0.4358718991279602, 0.34670722484588623, 0.27423858642578125, 0.027819722890853882, 0.15837548673152924, 0.47496509552001953, 0.16843673586845398, 0.18080802261829376, 0.10365196317434311, 0.15703339874744415, 0.08868691325187683, 0.07418707758188248, 0.37600386142730713, 0.4650108814239502, 0.10507798194885254, 0.16418464481830597], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3555907607078552, 0.2574366629123688, 0.31292569637298584, 0.046465735882520676, 0.04452862963080406, 0.13383743166923523, 0.020702771842479706, 0.34107694029808044, 0.23569920659065247, 0.3683167099952698, 0.22138822078704834, 0.05691889300942421, 0.4691391587257385, 0.34388333559036255, 0.2124958634376526, 0.26468992233276367, 0.46936601400375366, 0.24206997454166412, 0.22754083573818207, 0.4118709862232208, 0.47460705041885376, 0.4890643358230591, 0.2708028554916382, 0.39585694670677185], dtype='float32').reshape([24]),
            paddle.to_tensor([0.45180264115333557, 0.26031431555747986, 0.20344243943691254, 0.36958757042884827, 0.43744096159935, 0.20925310254096985, 0.28166836500167847, 0.4235621690750122, 0.36251339316368103, 0.13465258479118347, 0.08904454112052917, 0.2695256173610687, 0.34409767389297485, 0.09805037826299667, 0.13885420560836792, 0.08812592923641205, 0.4263538420200348, 0.3743044137954712, 0.2755889594554901, 0.4193076193332672, 0.2815283536911011, 0.44076400995254517, 0.15625329315662384, 0.32327672839164734], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8c6d8a8c933f2a5c08148ba995f6599c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_be54e93bdd5284ccf261683f4fa893c4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_862256c62a1a22852c828c1826b16055(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 25, 42], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7a744866386aab0c7cba4745ce3875c8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_930f54bb59ffdb41f6f2f46dfbe49224(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.21873755753040314, 0.1369936615228653, 0.41277316212654114, 0.25952979922294617, 0.3970128893852234, 0.0012026338372379541, 0.44825369119644165, 0.03405817225575447, 0.3911667764186859, 0.23587848246097565, 0.3094791769981384, 0.22854691743850708, 0.00543967355042696, 0.49019181728363037, 0.11904488503932953, 0.4211347699165344], dtype='float32').reshape([16]),
            paddle.to_tensor([0.47116440534591675, 0.1463383585214615, 0.26743197441101074, 0.28083720803260803, 0.12042763084173203, 0.17665420472621918, 0.4664161801338196, 0.20922783017158508, 0.2892500162124634, 0.1469467729330063, 0.3997400104999542, 0.47400879859924316, 0.19571129977703094, 0.09080485999584198, 0.4807227551937103, 0.014933077618479729], dtype='float32').reshape([16]),
            paddle.to_tensor([0.37776753306388855, 0.24053999781608582, 0.18713393807411194, 0.16837769746780396, 0.050861526280641556, 0.44582244753837585, 0.2580665647983551, 0.4263980984687805, 0.0525459423661232, 0.3911420702934265, 0.05804755538702011, 0.4475651681423187, 0.08390729129314423, 0.11024675518274307, 0.14169876277446747, 0.47464045882225037], dtype='float32').reshape([16]),
            paddle.to_tensor([0.046508628875017166, 0.14801543951034546, 0.01913534477353096, 0.14417441189289093, 0.1192605271935463, 0.2350328117609024, 0.28184616565704346, 0.3498215675354004, 0.04288410022854805, 0.3333283066749573, 0.045919373631477356, 0.4277186095714569, 0.16971515119075775, 0.13039402663707733, 0.43921950459480286, 0.20691530406475067], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c6817780e887321a945ce91793f698a7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1504, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1504], dtype='float32', min=0, max=0.5),
            paddle.uniform([1504], dtype='float32', min=0, max=0.5),
            paddle.uniform([1504], dtype='float32', min=0, max=0.5),
            paddle.uniform([1504], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_58108d30d0de6cb7e7d13696ae52d2ba(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 30, 30], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5cbd2a5def56dcd3286531931b581b79(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 31, 31], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_630a0e162ff03fca2afad19f97d6aa2b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_19bfeb9d1390554a176a988d47dbbeec(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1280, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e7a9764e0434639b5aa0462bb4e10c00(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 1, 1], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_640f27fb14bf7d900d7337a4577bc756(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.006306534633040428, 0.06010526046156883, 0.1685640811920166, 0.31200385093688965, 0.35590922832489014, 0.236903116106987, 0.4622112810611725, 0.04143345728516579, 0.10227122902870178, 0.1633872389793396, 0.15401843190193176, 0.05261565372347832, 0.1017681211233139, 0.2462889552116394, 0.18605947494506836, 0.019864186644554138, 0.42945796251296997, 0.22218702733516693], dtype='float32').reshape([18]),
            paddle.to_tensor([0.340854287147522, 0.2907883822917938, 0.41540563106536865, 0.21406444907188416, 0.3374236524105072, 0.16215842962265015, 0.20179209113121033, 0.4209047853946686, 0.30029913783073425, 0.3552294373512268, 0.3435196876525879, 0.34250321984291077, 0.48436030745506287, 0.4516487419605255, 0.10948073863983154, 0.22220467031002045, 0.34494999051094055, 0.3637251853942871], dtype='float32').reshape([18]),
            paddle.to_tensor([0.468533456325531, 0.1541322022676468, 0.43344399333000183, 0.42409586906433105, 0.12706491351127625, 0.05721870809793472, 0.44643542170524597, 0.24857886135578156, 0.18825478851795197, 0.09206582605838776, 0.4523925483226776, 0.076528400182724, 0.3449276387691498, 0.3884512186050415, 0.09066414088010788, 0.4181179404258728, 0.19662553071975708, 0.18177533149719238], dtype='float32').reshape([18]),
            paddle.to_tensor([0.35917961597442627, 0.2989332675933838, 0.31132057309150696, 0.05922446399927139, 0.35357213020324707, 0.1649084985256195, 0.34027382731437683, 0.4995189309120178, 0.4482952952384949, 0.3942709267139435, 0.03012019209563732, 0.37323006987571716, 0.14240697026252747, 0.17338532209396362, 0.25011003017425537, 0.41303902864456177, 0.3800485134124756, 0.08826392889022827], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_13dea5b17369689beccc8a6adb5fc88e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.31781265139579773, 0.46042346954345703, 0.46815982460975647, 0.46753159165382385, 0.2696022391319275, 0.17651209235191345, 0.2596750855445862, 0.39742061495780945, 0.22993984818458557, 0.3448304235935211, 0.08477247506380081, 0.056120019406080246, 0.004266493022441864, 0.2293175756931305, 0.39809444546699524, 0.18207168579101562, 0.021074432879686356, 0.1744382679462433, 0.16864384710788727, 0.08452855050563812], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4689025282859802, 0.3976818025112152, 0.4392884075641632, 0.22443510591983795, 0.09521884471178055, 0.38827192783355713, 0.1393936425447464, 0.296610563993454, 0.3410671651363373, 0.484730988740921, 0.01544065959751606, 0.4835728108882904, 0.18586412072181702, 0.008707807399332523, 0.0550018809735775, 0.36373817920684814, 0.1836373507976532, 0.2848814129829407, 0.20966115593910217, 0.3773607611656189], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4162138104438782, 0.46346983313560486, 0.3493868112564087, 0.26085764169692993, 0.34044530987739563, 0.20909060537815094, 0.3095158338546753, 0.25898459553718567, 0.16951873898506165, 0.39552977681159973, 0.13245940208435059, 0.32339462637901306, 0.37595275044441223, 0.32774102687835693, 0.17722657322883606, 0.21042102575302124, 0.3610890209674835, 0.40275514125823975, 0.06688638031482697, 0.30423039197921753], dtype='float32').reshape([20]),
            paddle.to_tensor([0.32083824276924133, 0.13075245916843414, 0.2853200137615204, 0.23739533126354218, 0.2171306014060974, 0.2577517032623291, 0.04794187843799591, 0.26084256172180176, 0.43604063987731934, 0.1579410284757614, 0.1671709269285202, 0.2677322030067444, 0.49272245168685913, 0.2945636510848999, 0.2778500020503998, 0.12797877192497253, 0.20431381464004517, 0.34443730115890503, 0.2218340039253235, 0.40796002745628357], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_924480489ef6cd24267b8fa2bd147c72(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f4126f3aec0bb1b7771ee4a1b89c75ff(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 4, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d3bb317e2490c6a31555be1ecc45448d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 32, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0a001b2820be98ff44a03464682375ed(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1cca9b05a96a5910e0f54cac541a1bcb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 4, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_12f81f6371e95c028a404f8fe426a18b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_47119c76e1c2202661ccc1b2c85b049e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 4, 256, 512], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.08729707449674606, 0.128668874502182, 0.15873128175735474, 0.17685194313526154], dtype='float32').reshape([4]),
            paddle.to_tensor([0.4359659254550934, 0.3278149664402008, 0.43393054604530334, 0.06762024760246277], dtype='float32').reshape([4]),
            paddle.to_tensor([0.06138480454683304, 0.4590613842010498, 0.13320331275463104, 0.19874069094657898], dtype='float32').reshape([4]),
            paddle.to_tensor([0.33272695541381836, 0.2937277555465698, 0.2155747413635254, 0.3863617777824402], dtype='float32').reshape([4]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_419718d7255ac4d91daf74849782fa6d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 118, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([118], dtype='float32', min=0, max=0.5),
            paddle.uniform([118], dtype='float32', min=0, max=0.5),
            paddle.uniform([118], dtype='float32', min=0, max=0.5),
            paddle.uniform([118], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ec82e5e2ce86a9a9f4d3d425b500abdc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 88, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f755362f87af70121435839dfe30a02e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 31, 31], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9b5050678541797f213f49ac9e2d61f2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ace3316182e2218e246ee2ef6fae5a27(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.32629552483558655, 0.006515388377010822, 0.4591423273086548, 0.41661909222602844, 0.25262945890426636, 0.13868854939937592, 0.3942507207393646, 0.40838316082954407, 0.010081527754664421, 0.266544908285141, 0.4700586497783661, 0.1509753316640854, 0.05918952450156212, 0.41198045015335083, 0.30667972564697266, 0.1535380482673645, 0.06717906147241592, 0.040181733667850494, 0.35427039861679077, 0.05371464416384697, 0.2886834144592285, 0.16369974613189697, 0.3803042769432068, 0.4842609763145447, 0.19420281052589417, 0.1453847587108612, 0.2875727713108063, 0.1552681028842926, 0.1897047907114029, 0.48728838562965393], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3991856873035431, 0.38221275806427, 0.04845314845442772, 0.1606050282716751, 0.0476033054292202, 0.4522964358329773, 0.004545481875538826, 0.3923185467720032, 0.15882451832294464, 0.40594756603240967, 0.3195112347602844, 0.052723854780197144, 0.12470223754644394, 0.3263714909553528, 0.27953076362609863, 0.19763657450675964, 0.13979938626289368, 0.22241275012493134, 0.2938266396522522, 0.4790768027305603, 0.20883947610855103, 0.2715657651424408, 0.1582650989294052, 0.022917484864592552, 0.10767262428998947, 0.31460607051849365, 0.145554780960083, 0.1573055237531662, 0.4084002375602722, 0.2872653603553772], dtype='float32').reshape([30]),
            paddle.to_tensor([0.0408046692609787, 0.014970364980399609, 0.3333570063114166, 0.4475446939468384, 0.07783101499080658, 0.12703900039196014, 0.4893214702606201, 0.17884820699691772, 0.3861769437789917, 0.15201517939567566, 0.2647116184234619, 0.07042635977268219, 0.05147901177406311, 0.0261935256421566, 0.36130547523498535, 0.20461256802082062, 0.4089100956916809, 0.01642477512359619, 0.2478535920381546, 0.16331611573696136, 0.04726521670818329, 0.20839177072048187, 0.10857966542243958, 0.23427759110927582, 0.4494691789150238, 0.02245941199362278, 0.12102068215608597, 0.22433476150035858, 0.11593466997146606, 0.17782852053642273], dtype='float32').reshape([30]),
            paddle.to_tensor([0.47213008999824524, 0.35269224643707275, 0.09916530549526215, 0.22465041279792786, 0.12376764416694641, 0.43757766485214233, 0.4998578429222107, 0.06306862086057663, 0.12104646861553192, 0.2504916787147522, 0.28157666325569153, 0.22766803205013275, 0.11760247498750687, 0.3868653178215027, 0.3733384609222412, 0.11136538535356522, 0.11669378727674484, 0.13467563688755035, 0.29984256625175476, 0.40237516164779663, 0.4976344108581543, 0.28213798999786377, 0.17181947827339172, 0.04915772005915642, 0.3282492160797119, 0.20322728157043457, 0.33315742015838623, 0.12663282454013824, 0.14221879839897156, 0.1260106861591339], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4e4f64929be1c513728a974553c289fb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 112, 64, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1fa67a690e027b28b30fc5c77ed0a619(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 32, 64], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3323744237422943, 0.027811016887426376, 0.1261272430419922, 0.0345294252038002, 0.08120425045490265, 0.11432702839374542, 0.25323933362960815, 0.37560582160949707, 0.12216527760028839, 0.3823484182357788, 0.2681065499782562, 0.006762812379747629, 0.05985037237405777, 0.1389961987733841, 0.050091538578271866, 0.4619280695915222, 0.0807560533285141, 0.11350294202566147], dtype='float32').reshape([18]),
            paddle.to_tensor([0.34404948353767395, 0.46943241357803345, 0.37454548478126526, 0.25414592027664185, 0.07668706774711609, 0.07524801790714264, 0.0312175415456295, 0.3186728060245514, 0.44806352257728577, 0.15678133070468903, 0.2853159010410309, 0.42756882309913635, 0.043915119022130966, 0.4600273072719574, 0.0353141725063324, 0.34661200642585754, 0.3043830692768097, 0.028907548636198044], dtype='float32').reshape([18]),
            paddle.to_tensor([0.40265870094299316, 0.11172308027744293, 0.24462585151195526, 0.06992105394601822, 0.00856636743992567, 0.2506677806377411, 0.48872292041778564, 0.38013380765914917, 0.038683101534843445, 0.14646723866462708, 0.4666227102279663, 0.22696690261363983, 0.2827778458595276, 0.02045094408094883, 0.22144575417041779, 0.4671066105365753, 0.2842467725276947, 0.18441198766231537], dtype='float32').reshape([18]),
            paddle.to_tensor([0.0074250586330890656, 0.0527852363884449, 0.31407204270362854, 0.09787215292453766, 0.20633827149868011, 0.028571780771017075, 0.4680216908454895, 0.12373940646648407, 0.2886882722377777, 0.4352189004421234, 0.4407197833061218, 0.2881205081939697, 0.03326106071472168, 0.13434630632400513, 0.22680990397930145, 0.35203519463539124, 0.12854908406734467, 0.46126240491867065], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9c52fb9aa9c59110f4b270132ac8e55e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_90a7180ad3bb7f2d7dfa1790be1b792d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.35995060205459595, 0.08053317666053772, 0.08915162086486816, 0.36484017968177795, 0.10051075369119644, 0.457253634929657, 0.3297293484210968, 0.30017849802970886, 0.21508832275867462, 0.08905029296875, 0.07381020486354828, 0.47246482968330383, 0.45336124300956726, 0.06331083178520203, 0.06628896296024323, 0.34802213311195374], dtype='float32').reshape([16]),
            paddle.to_tensor([0.32422930002212524, 0.30770137906074524, 0.332584947347641, 0.449333518743515, 0.20362631976604462, 0.08629698306322098, 0.465323269367218, 0.18413236737251282, 0.4108368158340454, 0.43439817428588867, 0.17376577854156494, 0.41984251141548157, 0.3152490258216858, 0.36285775899887085, 0.26474806666374207, 0.00153326615691185], dtype='float32').reshape([16]),
            paddle.to_tensor([0.19778625667095184, 0.36791563034057617, 0.05822573974728584, 0.26677295565605164, 0.15443217754364014, 0.2906903624534607, 0.15382860600948334, 0.17675398290157318, 0.25704336166381836, 0.41870230436325073, 0.034706439822912216, 0.35767820477485657, 0.4985372722148895, 0.47768062353134155, 0.18432654440402985, 0.0352657288312912], dtype='float32').reshape([16]),
            paddle.to_tensor([0.1852279007434845, 0.3799317181110382, 0.055145248770713806, 0.1584884524345398, 0.004565061070024967, 0.3338842988014221, 0.26975536346435547, 0.11898128688335419, 0.000885381770785898, 0.3323083221912384, 0.258226215839386, 0.014628708362579346, 0.3910198509693146, 0.030094154179096222, 0.3495904803276062, 0.47241514921188354], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8ca51cbf600a2490b7ca40b7d02a9741(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fde2b4dd08efa7bcbdf3d088fe09e9e3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 144, 16, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3a8f5211399a84f10a158bea8c6903b2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 576, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cffbd8b3caf0d757da888be4dc1e5c4b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 52, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7a7c96533bdfd334b76337018c68dfeb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_416f514070c8425962b5cfd514483b60(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 704, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([704], dtype='float32', min=0, max=0.5),
            paddle.uniform([704], dtype='float32', min=0, max=0.5),
            paddle.uniform([704], dtype='float32', min=0, max=0.5),
            paddle.uniform([704], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fb391b4e872ed80757daec97cf4b4449(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f45a9f65212f712d627b53a023240e28(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2bc99dcc5500f7a29a5584225e16aa1c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e17ccab8683234328cdfc2eb162c541c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 162, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([162], dtype='float32', min=0, max=0.5),
            paddle.uniform([162], dtype='float32', min=0, max=0.5),
            paddle.uniform([162], dtype='float32', min=0, max=0.5),
            paddle.uniform([162], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b6579daf6a7ea8852537f1ebafd127f1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 17, 17], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8b60bbfee1fd574c6ea0d4d654a72ce3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_85086989d63185813228fb58ce28c230(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1152, 6, 6], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c7630a88e2a202958ab22812091cd0b4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 240, 240], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3590686023235321, 0.27330461144447327, 0.041421208530664444, 0.3131088614463806, 0.24285252392292023, 0.24924133718013763, 0.22111524641513824, 0.3517766296863556, 0.19158051908016205, 0.06037825345993042, 0.3132764399051666, 0.2058398574590683, 0.32474449276924133, 0.49454182386398315, 0.10247433930635452, 0.3765350580215454], dtype='float32').reshape([16]),
            paddle.to_tensor([0.04845793917775154, 0.3048231899738312, 0.07640615850687027, 0.03377165272831917, 0.11610323190689087, 0.37943822145462036, 0.20573857426643372, 0.0633629858493805, 0.4264945983886719, 0.24347074329853058, 0.46671363711357117, 0.42039039731025696, 0.4480225145816803, 0.3120047152042389, 0.33664318919181824, 0.42728352546691895], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3228788375854492, 0.34061622619628906, 0.30090370774269104, 0.17306973040103912, 0.3918195962905884, 0.47567665576934814, 0.2507275342941284, 0.0016491618007421494, 0.45522162318229675, 0.2272285670042038, 0.4956982433795929, 0.27306148409843445, 0.23106980323791504, 0.48545268177986145, 0.2274065464735031, 0.2459520846605301], dtype='float32').reshape([16]),
            paddle.to_tensor([0.11003084480762482, 0.14487868547439575, 0.3952023684978485, 0.4225406348705292, 0.3991195559501648, 0.12381494045257568, 0.3851245641708374, 0.3986818194389343, 0.15238983929157257, 0.19721803069114685, 0.010888593271374702, 0.3816608190536499, 0.267495721578598, 0.4116542339324951, 0.027261219918727875, 0.48958513140678406], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7ea6a4c390ee132dbb7b325909e348b3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_38ee47d84c502f0ea2de34186f9e0654(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e6d4ed98eec7e5ef681e9203c2877cf2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 352, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([352], dtype='float32', min=0, max=0.5),
            paddle.uniform([352], dtype='float32', min=0, max=0.5),
            paddle.uniform([352], dtype='float32', min=0, max=0.5),
            paddle.uniform([352], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_889f7e03798cab8767f6e2c1a0829b84(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 35, 35], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_59c331c58665c6a4141b180e59dd9f7e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 56, 112], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.16981585323810577, 0.3858683407306671, 0.4989103078842163, 0.43354684114456177, 0.002618646016344428, 0.1712602823972702, 0.39311346411705017, 0.18105575442314148], dtype='float32').reshape([8]),
            paddle.to_tensor([0.21790826320648193, 0.38687190413475037, 0.08128465712070465, 0.44072872400283813, 0.32554325461387634, 0.41940224170684814, 0.39097800850868225, 0.4518229067325592], dtype='float32').reshape([8]),
            paddle.to_tensor([0.307861864566803, 0.2138674408197403, 0.23463457822799683, 0.04652808979153633, 0.4155355989933014, 0.3267265856266022, 0.30848726630210876, 0.05490470305085182], dtype='float32').reshape([8]),
            paddle.to_tensor([0.15355980396270752, 0.33233344554901123, 0.4540427625179291, 0.4667191803455353, 0.03843003883957863, 0.07989250123500824, 0.3648870587348938, 0.16654755175113678], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_653ac50244b65b36b94d5b1c6debb79a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 720, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a0e002b18d39f95c5e93199a0e88016f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 44, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([44], dtype='float32', min=0, max=0.5),
            paddle.uniform([44], dtype='float32', min=0, max=0.5),
            paddle.uniform([44], dtype='float32', min=0, max=0.5),
            paddle.uniform([44], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d681682d8cd99b38c1cfe3605e3c21dd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 19, 19], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1bf66c7105f3099c0b458ae455682bf3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 208, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a62766a8b419dd50ff93483076c4c8ea(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 60, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_738daedfb3a4fb3f6bf032c7d7f24af2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f9d33b9dc807f261066a5e06d4a9b09e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 416, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0b928db8947d5408371c88061e0626ec(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.28652697801589966, 0.07468386739492416, 0.43501555919647217, 0.21310225129127502, 0.26445838809013367, 0.007904714904725552, 0.181561678647995, 0.44974520802497864, 0.26670512557029724, 0.39821866154670715, 0.23799468576908112, 0.17364941537380219, 0.20431163907051086, 0.45674774050712585, 0.40541860461235046, 0.4926474392414093, 0.2642543911933899, 0.40173324942588806, 0.07606314867734909, 0.20354601740837097, 0.4443182945251465, 0.11393383890390396, 0.07824694365262985, 0.1428830772638321, 0.3127400875091553, 0.010588200762867928, 0.17837315797805786, 0.49181804060935974], dtype='float32').reshape([28]),
            paddle.to_tensor([0.3983268737792969, 0.0348038375377655, 0.3025032877922058, 0.16889818012714386, 0.4681685268878937, 0.016055472195148468, 0.4477422535419464, 0.4366884231567383, 0.11975722759962082, 0.3117808997631073, 0.0033996631391346455, 0.4288847744464874, 0.24830274283885956, 0.3831201195716858, 0.2425321340560913, 0.14691388607025146, 0.147027388215065, 0.1303548365831375, 0.142782062292099, 0.41206175088882446, 0.3844112157821655, 0.3285047113895416, 0.4184017777442932, 0.22718901932239532, 0.3607359826564789, 0.4916374683380127, 0.24485163390636444, 0.26649561524391174], dtype='float32').reshape([28]),
            paddle.to_tensor([0.3127240836620331, 0.27141863107681274, 0.2553703784942627, 0.13544607162475586, 0.34488576650619507, 0.1757804900407791, 0.11196209490299225, 0.4008195996284485, 0.3271908164024353, 0.46589845418930054, 0.1533801257610321, 0.2725363075733185, 0.2206193506717682, 0.26583409309387207, 0.11989568918943405, 0.2622936964035034, 0.3511427342891693, 0.39460238814353943, 0.071742482483387, 0.33217230439186096, 0.274118036031723, 0.07918227463960648, 0.24909871816635132, 0.38613250851631165, 0.38712772727012634, 0.42799097299575806, 0.0784982219338417, 0.4191294312477112], dtype='float32').reshape([28]),
            paddle.to_tensor([0.14145904779434204, 0.26284128427505493, 0.4373592734336853, 0.2117053121328354, 0.3465670943260193, 0.48410847783088684, 0.44506368041038513, 0.24735403060913086, 0.18368537724018097, 0.42515015602111816, 0.04342268779873848, 0.10676130652427673, 0.24255526065826416, 0.02846504934132099, 0.04293414205312729, 0.001036464935168624, 0.35748186707496643, 0.4895666837692261, 0.033966101706027985, 0.048399828374385834, 0.18551477789878845, 0.44196152687072754, 0.06484910845756531, 0.26739656925201416, 0.3839625418186188, 0.47527867555618286, 0.16340403258800507, 0.4997381567955017], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_adbf939a1b5e3c3f5a447ec9ee78813f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f0ae0f87f76b233219df6346b38c9132(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1248, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1248], dtype='float32', min=0, max=0.5),
            paddle.uniform([1248], dtype='float32', min=0, max=0.5),
            paddle.uniform([1248], dtype='float32', min=0, max=0.5),
            paddle.uniform([1248], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c72ca6b7bf6dfe647f72f954b138fe92(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 175, 25], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cfca0abcb28d56c468721ebd0aeedf64(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b49aec4aa927ccdd2b00b7d7f38d4515(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 4, 50], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.45714887976646423, 0.17998400330543518, 0.3446011543273926, 0.08873313665390015, 0.040259674191474915, 0.41178464889526367, 0.36827340722084045, 0.3584778606891632, 0.4505523443222046, 0.11385189741849899, 0.2726587653160095, 0.3625568151473999, 0.11164143681526184, 0.0706503689289093, 0.3219701945781708, 0.34404242038726807, 0.1178579106926918, 0.0695817768573761, 0.2571917176246643, 0.18246258795261383, 0.2001158744096756, 0.4876033067703247, 0.46172791719436646, 0.2978406250476837], dtype='float32').reshape([24]),
            paddle.to_tensor([0.22623658180236816, 0.45925626158714294, 0.04422236979007721, 0.2095160037279129, 0.09875348210334778, 0.3343846797943115, 0.25886768102645874, 0.41673117876052856, 0.15405775606632233, 0.15852583944797516, 0.20413993299007416, 0.04822397232055664, 0.04963207617402077, 0.47288262844085693, 0.39414969086647034, 0.22370080649852753, 0.051359228789806366, 0.2853526473045349, 0.4215828776359558, 0.254644513130188, 0.3001146614551544, 0.012938054278492928, 0.05410315841436386, 0.2922961115837097], dtype='float32').reshape([24]),
            paddle.to_tensor([0.13354167342185974, 0.2079668939113617, 0.02574717253446579, 0.2861126661300659, 0.19663484394550323, 0.24782975018024445, 0.2519080340862274, 0.18874958157539368, 0.2803879976272583, 0.4280880391597748, 0.048780135810375214, 0.22623977065086365, 0.3567647337913513, 0.08735012263059616, 0.38199806213378906, 0.29221391677856445, 0.06155075132846832, 0.2527599334716797, 0.41852542757987976, 0.03874135762453079, 0.2657051086425781, 0.02265848219394684, 0.4879087805747986, 0.17517712712287903], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3766612112522125, 0.24433906376361847, 0.2999916970729828, 0.15966564416885376, 0.37350550293922424, 0.03743640333414078, 0.43124428391456604, 0.20093509554862976, 0.07712049037218094, 0.33519622683525085, 0.47196605801582336, 0.07426916062831879, 0.004756541922688484, 0.4680234491825104, 0.18189777433872223, 0.3623107969760895, 0.1487678587436676, 0.31032484769821167, 0.48239126801490784, 0.29609283804893494, 0.03783982992172241, 0.08267703652381897, 0.21585127711296082, 0.09373164176940918], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9e7c17f9ba7391848adf4d3942ea6d30(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_abc41fed2b404fcc6993a1a1a9cd35c2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 64, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8f59aa7663625a1bfb2ea9232682d630(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a2d8200ddfcf959c43beee85f0be5a81(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 32, 100], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3639446496963501, 0.44630739092826843, 0.4875672161579132, 0.4815711975097656, 0.1675761640071869, 0.33181941509246826, 0.092637799680233, 0.42798930406570435, 0.2203843742609024, 0.3810792863368988, 0.3157501816749573, 0.347345769405365, 0.3342176079750061, 0.44129666686058044, 0.4134676456451416, 0.33655011653900146], dtype='float32').reshape([16]),
            paddle.to_tensor([0.48010995984077454, 0.005860483273863792, 0.041946183890104294, 0.15371042490005493, 0.22952397167682648, 0.3256133496761322, 0.05917467549443245, 0.109436996281147, 0.3858061134815216, 0.4187720715999603, 0.3107219934463501, 0.37721648812294006, 0.16307339072227478, 0.48570311069488525, 0.23363204300403595, 0.27929115295410156], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4582924544811249, 0.1954330950975418, 0.3080494999885559, 0.025208398699760437, 0.18088982999324799, 0.046103887259960175, 0.4769670367240906, 0.23134845495224, 0.1862906962633133, 0.1752397119998932, 0.21417027711868286, 0.028933895751833916, 0.3240373134613037, 0.22469623386859894, 0.019701479002833366, 0.4339306950569153], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2081129550933838, 0.15998578071594238, 0.2639227509498596, 0.25475698709487915, 0.03613065555691719, 0.31506502628326416, 0.056895360350608826, 0.31258273124694824, 0.30724722146987915, 0.12440997362136841, 0.48209962248802185, 0.43971487879753113, 0.30424201488494873, 0.08214623481035233, 0.3589417040348053, 0.4921249747276306], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_42d6c669e8abafbc3177dad7b0feed49(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 50, 84], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b9701a2fcddbf1a292d4499882b279be(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 38, 38], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2db70108a35b227f958400f94692fcf1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.28465232253074646, 0.1664884388446808, 0.39676210284233093, 0.00552236195653677, 0.20029142498970032, 0.28743043541908264, 0.3077086806297302, 0.40899667143821716, 0.4032612442970276, 0.08442316204309464, 0.11694775521755219, 0.26153531670570374, 0.14337554574012756, 0.22564835846424103, 0.461946964263916, 0.09024318307638168, 0.3887537717819214, 0.2550981938838959, 0.1845010668039322, 0.17174100875854492], dtype='float32').reshape([20]),
            paddle.to_tensor([0.2735138237476349, 0.2425416260957718, 0.03267049044370651, 0.15462154150009155, 0.3885572850704193, 0.26569491624832153, 0.24348340928554535, 0.01691618375480175, 0.26752492785453796, 0.30441948771476746, 0.2497009038925171, 0.34376025199890137, 0.36008507013320923, 0.3768288195133209, 0.07569651305675507, 0.3718174397945404, 0.3364155888557434, 0.007162866648286581, 0.11896339058876038, 0.38497480750083923], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3596392571926117, 0.09070472419261932, 0.2700980305671692, 0.2096058428287506, 0.2618691623210907, 0.20681223273277283, 0.296412855386734, 0.21646779775619507, 0.4263989329338074, 0.36361628770828247, 0.03662830591201782, 0.15078993141651154, 0.11410385370254517, 0.22322584688663483, 0.13133496046066284, 0.018337983638048172, 0.27251720428466797, 0.18194280564785004, 0.008132506161928177, 0.02263001538813114], dtype='float32').reshape([20]),
            paddle.to_tensor([0.29164597392082214, 0.4323820173740387, 0.35772496461868286, 0.06927222013473511, 0.4655476212501526, 0.3495396375656128, 0.49854588508605957, 0.09425871819257736, 0.0973801538348198, 0.11827370524406433, 0.053121741861104965, 0.09706506878137589, 0.09473539888858795, 0.13112398982048035, 0.4133416414260864, 0.18174651265144348, 0.07922215759754181, 0.38357430696487427, 0.41091403365135193, 0.3901364505290985], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c39274b51611c3487a46588573ee57c5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3b096f7743dfa54831b5c4d0127ec9f2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 16, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_34519629d2b01e9728f353054bfb1426(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7394f52e4bf8060542dda7ee160d5113(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 64, 128], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.24119345843791962, 0.07792458683252335, 0.239655539393425, 0.17356543242931366, 0.20243419706821442, 0.25493067502975464, 0.09343390166759491, 0.3774202764034271, 0.11814193427562714, 0.395541787147522, 0.3344169557094574, 0.335011750459671, 0.4881640374660492, 0.2383228987455368, 0.11981242895126343, 0.1654917299747467, 0.1514638066291809, 0.39996397495269775], dtype='float32').reshape([18]),
            paddle.to_tensor([0.413677453994751, 0.36221280694007874, 0.3633121848106384, 0.3384620249271393, 0.34873902797698975, 0.4554400146007538, 0.4872244894504547, 0.09740432351827621, 0.04609445855021477, 0.36096811294555664, 0.3053288161754608, 0.056551139801740646, 0.49639785289764404, 0.46540871262550354, 0.3741611838340759, 0.31044307351112366, 0.316138356924057, 0.27422255277633667], dtype='float32').reshape([18]),
            paddle.to_tensor([0.09850219637155533, 0.08138862997293472, 0.408031165599823, 0.13384339213371277, 0.43764448165893555, 0.04264310002326965, 0.1111658364534378, 0.47986772656440735, 0.18539801239967346, 0.4250667095184326, 0.11868439614772797, 0.2498508095741272, 0.4747413694858551, 0.46123990416526794, 0.08692076802253723, 0.1832679808139801, 0.48101431131362915, 0.08238977938890457], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2718753516674042, 0.25390639901161194, 0.16383719444274902, 0.47459107637405396, 0.17591524124145508, 0.3848516345024109, 0.2805531919002533, 0.14284811913967133, 0.2409595400094986, 0.4017901122570038, 0.41718414425849915, 0.16643057763576508, 0.2863444685935974, 0.45057788491249084, 0.1626821905374527, 0.024260712787508965, 0.4178762435913086, 0.08988852798938751], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ff56fc19f43e0ef7553dc71c00465a61(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_145ad20e7a3b3c8aa31f6b40d4fe3d45
    def get_inputs(self):
        return [
            paddle.uniform([16, 384], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_00cd46a92889af15bf78508a30ec915a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 8, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1ffa5384829b2f3d9f3a83be7d363910(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 31, 31], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_158c2caadbf86dda7935196c61bbd9e7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 61, 61], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9653a8c9658758f169e155e1ff65ac71(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9806e63237171a6d0501b8d59308387f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6b2fd6be1e31c22ca1d801c922e67b05(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 26, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4119229018688202, 0.13692106306552887, 0.47857579588890076, 0.04276455566287041, 0.21981900930404663, 0.40723299980163574, 0.341121107339859, 0.3017699122428894, 0.11277847737073898, 0.38874441385269165, 0.34302374720573425, 0.20471975207328796, 0.2590494751930237, 0.1732279509305954, 0.02430584467947483, 0.018097134307026863, 0.4914034307003021, 0.1632234752178192, 0.3954808712005615, 0.1544979065656662, 0.05266870558261871, 0.24384833872318268, 0.44886696338653564, 0.2779102325439453, 0.17087441682815552, 0.013358299620449543], dtype='float32').reshape([26]),
            paddle.to_tensor([0.4651048481464386, 0.2695205509662628, 0.3771602213382721, 0.14945906400680542, 0.3810385763645172, 0.0033369886223226786, 0.4856360852718353, 0.43435513973236084, 0.005782925058156252, 0.016110898926854134, 0.030306579545140266, 0.4523671269416809, 0.4875675439834595, 0.25061798095703125, 0.22592978179454803, 0.06892327964305878, 0.3615841567516327, 0.20618365705013275, 0.22398991882801056, 0.05122319981455803, 0.07416405528783798, 0.36728307604789734, 0.1876782327890396, 0.36415648460388184, 0.2616709768772125, 0.2766695022583008], dtype='float32').reshape([26]),
            paddle.to_tensor([0.4156983196735382, 0.42686882615089417, 0.2913857102394104, 0.35789865255355835, 0.12756137549877167, 0.30048444867134094, 0.005906525533646345, 0.22712506353855133, 0.06272749602794647, 0.04521495848894119, 0.23870813846588135, 0.011597290635108948, 0.2964945435523987, 0.1223747581243515, 0.22545009851455688, 0.38512322306632996, 0.06040600687265396, 0.07827577739953995, 0.42736372351646423, 0.27570822834968567, 0.29161757230758667, 0.3004719913005829, 0.09815230965614319, 0.12125236541032791, 0.06928762793540955, 0.07961627840995789], dtype='float32').reshape([26]),
            paddle.to_tensor([0.4719191789627075, 0.10482947528362274, 0.42119231820106506, 0.47884759306907654, 0.09517330676317215, 0.2881617546081543, 0.2502247095108032, 0.06361720710992813, 0.4034343957901001, 0.30553382635116577, 0.3540724813938141, 0.34033772349357605, 0.07107333838939667, 0.11141739785671234, 0.27498435974121094, 0.45901942253112793, 0.23292171955108643, 0.06407015770673752, 0.2005985826253891, 0.2617802619934082, 0.2953152060508728, 0.35334354639053345, 0.36240488290786743, 0.11590073257684708, 0.018569733947515488, 0.22164003551006317], dtype='float32').reshape([26]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1dbb4ed8b7829b95c143cc7c768ed467(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2861080467700958, 0.31306084990501404, 0.4230341613292694, 0.04636417701840401, 0.08915860205888748, 0.3225381076335907, 0.4053455889225006, 0.13255184888839722, 0.2993258833885193, 0.4558182656764984, 0.12358453869819641, 0.23127718269824982, 0.2357683628797531, 0.25341930985450745, 0.2480536699295044, 0.22195005416870117, 0.024064071476459503, 0.38141676783561707, 0.4481373131275177, 0.09254603087902069, 0.26115843653678894, 0.15873168408870697, 0.2588846683502197, 0.35809874534606934, 0.45747053623199463, 0.3599175214767456, 0.22309660911560059, 0.07348732650279999, 0.2783803939819336, 0.0740606039762497], dtype='float32').reshape([30]),
            paddle.to_tensor([0.356891393661499, 0.44457128643989563, 0.2012975513935089, 0.03308282792568207, 0.1801387518644333, 0.4775896370410919, 0.3629821240901947, 0.04433205723762512, 0.0656941756606102, 0.05577066168189049, 0.34695762395858765, 0.4348088204860687, 0.35843104124069214, 0.08402801305055618, 0.3853345513343811, 0.002112355548888445, 0.3694564998149872, 0.4394203722476959, 0.07666981220245361, 0.3479931652545929, 0.3243582248687744, 0.34001144766807556, 0.03131166100502014, 0.42798638343811035, 0.015436242334544659, 0.37523919343948364, 0.12509891390800476, 0.08668092638254166, 0.07029691338539124, 0.3712226450443268], dtype='float32').reshape([30]),
            paddle.to_tensor([0.03905371576547623, 0.4672933518886566, 0.03392644226551056, 0.4979528784751892, 0.4988909363746643, 0.24482430517673492, 0.44580569863319397, 0.40517064929008484, 0.3689102828502655, 0.28000205755233765, 0.24208860099315643, 0.08625088632106781, 0.3815148174762726, 0.42341485619544983, 0.10579520463943481, 0.3246363401412964, 0.17197737097740173, 0.47108325362205505, 0.2826770544052124, 0.4771311283111572, 0.38513147830963135, 0.20540867745876312, 0.05073670297861099, 0.005242104642093182, 0.23285774886608124, 0.3817170560359955, 0.3113715350627899, 0.05917630344629288, 0.4318576157093048, 0.014380082488059998], dtype='float32').reshape([30]),
            paddle.to_tensor([0.09140847623348236, 0.2896258234977722, 0.03577396273612976, 0.41209128499031067, 0.034201428294181824, 0.14694707095623016, 0.15863846242427826, 0.21021784842014313, 0.2324257493019104, 0.12087571620941162, 0.40278422832489014, 0.363120436668396, 0.3016050159931183, 0.2695232629776001, 0.28329864144325256, 0.04543370380997658, 0.3017275035381317, 0.3969355821609497, 0.48605674505233765, 0.19122159481048584, 0.19295786321163177, 0.3846483528614044, 0.3901699185371399, 0.35411322116851807, 0.021296998485922813, 0.2030070573091507, 0.08550994098186493, 0.0020217907149344683, 0.424710214138031, 0.04322744533419609], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_a02f1e2888bea69fc79b31a122ed24eb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_97bc49daa8835af62d7ad6c08b10d810(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_a02f1e2888bea69fc79b31a122ed24eb
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 49], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6311937a32c65a39e7ecdb865d1effb3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_84bce6ace8919fc20de8034a5ce09b0b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 300, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([300], dtype='float32', min=0, max=0.5),
            paddle.uniform([300], dtype='float32', min=0, max=0.5),
            paddle.uniform([300], dtype='float32', min=0, max=0.5),
            paddle.uniform([300], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_851610f427c8b281219a52af41346ca0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 24, 24], dtype='float16', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ae6afcd80efef23964894217e21ed6a3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 1, 1], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e4bfe680fe98b4715b413f0a652d4cc3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a55e3b5f07afb933a82493b7f65d9409(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 1152, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_16b3f84f42f4a8263accbda1d3cf5793(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_165ed706feb1399b6287dc7e160f3839(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_24f23bf3e825e53c0f9a189bdae892ea(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 528, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([528], dtype='float32', min=0, max=0.5),
            paddle.uniform([528], dtype='float32', min=0, max=0.5),
            paddle.uniform([528], dtype='float32', min=0, max=0.5),
            paddle.uniform([528], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0e049d9edc00013acd9fd64e30efe2b0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 61, 61], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_590b938bdacfcee33d0a4259d23a37a6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_71c7b1fe753fafee6d664abb64416340(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 352, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([352], dtype='float32', min=0, max=0.5),
            paddle.uniform([352], dtype='float32', min=0, max=0.5),
            paddle.uniform([352], dtype='float32', min=0, max=0.5),
            paddle.uniform([352], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3ec84a40b4e717c998034e708f2db01d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.20078617334365845, 0.3828866183757782, 0.18675941228866577, 0.2229689508676529, 0.1178683415055275, 0.23124735057353973, 0.1094498261809349, 0.43508604168891907, 0.033306218683719635, 0.2121971696615219, 0.4035418629646301, 0.3144623637199402, 0.3189258277416229, 0.27870938181877136, 0.2725867033004761, 0.44646263122558594, 0.06915515661239624, 0.026910297572612762], dtype='float32').reshape([18]),
            paddle.to_tensor([0.26337599754333496, 0.23614081740379333, 0.4813079237937927, 0.16700200736522675, 0.41540369391441345, 0.49123769998550415, 0.1903316229581833, 0.33772414922714233, 0.2830564081668854, 0.23472270369529724, 0.4049678444862366, 0.33268386125564575, 0.08684608340263367, 0.37315428256988525, 0.2687087059020996, 0.021148979663848877, 0.035784799605607986, 0.3878226578235626], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3387492299079895, 0.3714679479598999, 0.2514861524105072, 0.41495609283447266, 0.4000299274921417, 0.3558276295661926, 0.0866713672876358, 0.21191631257534027, 0.4298839867115021, 0.05640983581542969, 0.1551654040813446, 0.11737474799156189, 0.23875124752521515, 0.013840530067682266, 0.028226381167769432, 0.4736320376396179, 0.28437334299087524, 0.40034282207489014], dtype='float32').reshape([18]),
            paddle.to_tensor([0.268609881401062, 0.3668605089187622, 0.02217773348093033, 0.0926038846373558, 0.09074033051729202, 0.27190637588500977, 0.09790845215320587, 0.4341706335544586, 0.37073010206222534, 0.12607789039611816, 0.2451815903186798, 0.09874434024095535, 0.31666073203086853, 0.007701733149588108, 0.08455884456634521, 0.4658470153808594, 0.31958815455436707, 0.4433635175228119], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fe357bd2132e30fdfeb88040c3888d08(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5f9b44244e4a66db980429f6c981bb22(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9b263a8980bc82a3d0b270f6660888cd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 512, 1024], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.17309962213039398, 0.12586693465709686, 0.051698822528123856, 0.0903661847114563, 0.21480132639408112, 0.2257397174835205, 0.054270610213279724, 0.45760247111320496, 0.40215229988098145, 0.3037509620189667, 0.15110884606838226, 0.09529615193605423, 0.15998293459415436, 0.26957711577415466, 0.10516266524791718, 0.005591997876763344], dtype='float32').reshape([16]),
            paddle.to_tensor([0.39745694398880005, 0.07504505664110184, 0.3088572025299072, 0.22471532225608826, 0.1725989133119583, 0.03189980983734131, 0.08871262520551682, 0.18016064167022705, 0.2527386546134949, 0.3911970853805542, 0.2870738208293915, 0.3561650514602661, 0.4980582594871521, 0.26666000485420227, 0.31089654564857483, 0.24358266592025757], dtype='float32').reshape([16]),
            paddle.to_tensor([0.16013263165950775, 0.3015562891960144, 0.10161962360143661, 0.38559621572494507, 0.1705847680568695, 0.4915679395198822, 0.4705629050731659, 0.1500975787639618, 0.03465097397565842, 0.33730751276016235, 0.10780379921197891, 0.2166300117969513, 0.35213735699653625, 0.1909899264574051, 0.4069804549217224, 0.26882460713386536], dtype='float32').reshape([16]),
            paddle.to_tensor([0.1172625869512558, 0.2400456666946411, 0.02695801481604576, 0.46902939677238464, 0.4909965693950653, 0.44583094120025635, 0.17804548144340515, 0.2434786558151245, 0.1632828563451767, 0.08550861477851868, 0.06628318876028061, 0.4363638460636139, 0.4984699487686157, 0.28057336807250977, 0.17565757036209106, 0.18884241580963135], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f87a6c6ea0d67316896e4c49a13ad1fd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 120, 120], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7e58733432019e545cb4a40927925308(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d132145e142c2df216d881929bdf1b19(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 200, 336], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_063dc7e01f236c8604db4f7a20cdb29e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 61, 61], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d158c362eda895472cbb7071b8e730d4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bf6bb842a03f16570ef598b7fc789a32(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 1, 1], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e4dada82336f848d47d879b66705e648(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 52, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a05e2f30ac6fac3cc1ccb22d88405d95(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_00fe16de063510ec07aad52a136c8fdf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 122, 122], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_067890abfb83f8910a777be9c42a6152(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 4, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_48604b98b1458fb0e9218ec4a4763146(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_230d2719f862e7eb2b0d6a05fd139f65(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 4, 3], dtype='float16', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f2f4ebae1e4886e48c34ba5c8a99b823(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_09e7061f89f8e6e5faa8aa2d08e3358e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_94889e251c81646ff70adc771777b2cc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 30, 30], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_22d23902942018d264ba497b8f9d2376(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.18730181455612183, 0.23307771980762482, 0.17630256712436676, 0.29299288988113403, 0.242271289229393, 0.08367964625358582, 0.0125067587941885, 0.20502428710460663, 0.028274374082684517, 0.4594322144985199, 0.23824581503868103, 0.17018276453018188, 0.404817134141922, 0.42386212944984436, 0.4978368282318115, 0.06830745935440063, 0.42080363631248474, 0.0721331313252449, 0.4301634132862091, 0.4705786406993866, 0.009495430625975132, 0.07885618507862091, 0.32431671023368835, 0.4751889407634735], dtype='float32').reshape([24]),
            paddle.to_tensor([0.0006298256921581924, 0.3026184141635895, 0.13963735103607178, 0.19297648966312408, 0.08962641656398773, 0.39094430208206177, 0.3730701506137848, 0.27777934074401855, 0.46445751190185547, 0.3935842514038086, 0.39034950733184814, 0.07824148237705231, 0.10386638343334198, 0.07359739392995834, 0.11551712453365326, 0.11515440791845322, 0.0043480005115270615, 0.04277338087558746, 0.39452478289604187, 0.42675116658210754, 0.2765178084373474, 0.20673280954360962, 0.280654639005661, 0.24121956527233124], dtype='float32').reshape([24]),
            paddle.to_tensor([0.1709660291671753, 0.31713733077049255, 0.21158865094184875, 0.3861190974712372, 0.07952336221933365, 0.4175413250923157, 0.3003668487071991, 0.37392109632492065, 0.2314063012599945, 0.41970887780189514, 0.17409610748291016, 0.10269704461097717, 0.0770406574010849, 0.1422094851732254, 0.10786449909210205, 0.09773317724466324, 0.3166901767253876, 0.4542829692363739, 0.12413585931062698, 0.2668018043041229, 0.26327356696128845, 0.06790284812450409, 0.2084202766418457, 0.13145296275615692], dtype='float32').reshape([24]),
            paddle.to_tensor([0.48641741275787354, 0.07369574159383774, 0.45504364371299744, 0.29292693734169006, 0.42157334089279175, 0.09001129120588303, 0.3687332570552826, 0.08536787331104279, 0.46170327067375183, 0.4574449360370636, 0.21336132287979126, 0.21322599053382874, 0.26817262172698975, 0.4664328992366791, 0.153187558054924, 0.3628390431404114, 0.4748881161212921, 0.3648054301738739, 0.16216924786567688, 0.4881514608860016, 0.0467020608484745, 0.17194919288158417, 0.4302220046520233, 0.39984047412872314], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e15276b31c10e67b61121ff3648d8c8e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9169a6687477bfda78bb7973d02820e1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 6, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d21c00921a93a65be184c28d7a3150f7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 320, 320], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.01137365773320198, 0.38522666692733765, 0.10063696652650833, 0.058494165539741516, 0.10555094480514526, 0.2928360104560852, 0.3707442879676819, 0.43333882093429565, 0.2503679394721985, 0.013551031239330769, 0.1955893635749817, 0.09946940094232559, 0.13328391313552856, 0.33754318952560425, 0.2741020619869232, 0.19206354022026062, 0.1578497290611267, 0.37222057580947876, 0.03761570155620575, 0.26162731647491455, 0.23843099176883698, 0.041021332144737244, 0.2942941188812256, 0.4520476162433624], dtype='float32').reshape([24]),
            paddle.to_tensor([0.30517205595970154, 0.05680977180600166, 0.48963144421577454, 0.11189775168895721, 0.0357164591550827, 0.08454011380672455, 0.4704761505126953, 0.24401555955410004, 0.3503054976463318, 0.1411372870206833, 0.2377498745918274, 0.019139280542731285, 0.47806844115257263, 0.0911903902888298, 0.15440009534358978, 0.3161429166793823, 0.046756286174058914, 0.10327658802270889, 0.42602622509002686, 0.16606123745441437, 0.23969590663909912, 0.10682854801416397, 0.04041367024183273, 0.1594770848751068], dtype='float32').reshape([24]),
            paddle.to_tensor([0.18339486420154572, 0.2641093134880066, 0.41934606432914734, 0.4574018120765686, 0.19358785450458527, 0.03968414291739464, 0.05725698173046112, 0.450303852558136, 0.06022460758686066, 0.057444751262664795, 0.2666792869567871, 0.40604135394096375, 0.21800599992275238, 0.3645622432231903, 0.39308038353919983, 0.38737791776657104, 0.057129956781864166, 0.20151862502098083, 0.27541276812553406, 0.40646395087242126, 0.11380894482135773, 0.14312417805194855, 0.08020429313182831, 0.03836160898208618], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3949969410896301, 0.25456076860427856, 0.3421071171760559, 0.03410522639751434, 0.38864031434059143, 0.04514233022928238, 0.28229862451553345, 0.11509303003549576, 0.45685333013534546, 0.022777320817112923, 0.40147483348846436, 0.4343748688697815, 0.131071999669075, 0.40619930624961853, 0.08902722597122192, 0.1584560126066208, 0.1703004688024521, 0.4862334728240967, 0.3029150366783142, 0.1610654592514038, 0.03794682025909424, 0.3799874186515808, 0.09699161350727081, 0.1369968205690384], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d250e99e11ab868a27cc44f37b47b2a0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 15, 15], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2afed2188fc5658e23815e9de49cc7fd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dfa7293be3e85a800fec9ef01c5f2284(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fbe86416b600ba76164e7603dc6b5205(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c0eeefefc0b8b59802798f7eda3591e5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 8, 6], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fa5b028763acf8883398433006dfcc90(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 200, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0cb619b3c87fd20e766b170517f46a22(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9e855878e0c3e2d87c143f63acb77104(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_23eb6c11ae7ffebf5efbb1b8545f47ef(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5df9ec34282d229187056c10339a1c32(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2bcefde44639e3e32ddeb3fb6c2dff1b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5290341b4fb141a26961a863ab965f75(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.15982744097709656, 0.23933398723602295, 0.3498312532901764, 0.49889400601387024, 0.14047980308532715, 0.24399414658546448, 0.13544367253780365, 0.02632862515747547, 0.34594929218292236, 0.3920927941799164, 0.25765442848205566, 0.4497760832309723, 0.4759552478790283, 0.3130870461463928, 0.05862788110971451, 0.4510805308818817, 0.08635927736759186, 0.3683956265449524, 0.058399979025125504, 0.09616336226463318], dtype='float32').reshape([20]),
            paddle.to_tensor([0.27126365900039673, 0.2959660291671753, 0.10597100108861923, 0.4930868446826935, 0.27503502368927, 0.4699018597602844, 0.3514899015426636, 0.3444860577583313, 0.44068053364753723, 0.24236565828323364, 0.41731011867523193, 0.040473226457834244, 0.06507725268602371, 0.47915518283843994, 0.40117111802101135, 0.30418744683265686, 0.34144657850265503, 0.27342092990875244, 0.2126375138759613, 0.29874059557914734], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3603457510471344, 0.33234524726867676, 0.1416282057762146, 0.10562803596258163, 0.16513356566429138, 0.18359176814556122, 0.31824541091918945, 0.2079591453075409, 0.3846471309661865, 0.4736134111881256, 0.25859540700912476, 0.44617927074432373, 0.10347873717546463, 0.34267306327819824, 0.2760200798511505, 0.3521715998649597, 0.2696036398410797, 0.23303233087062836, 0.18691067397594452, 0.4264785945415497], dtype='float32').reshape([20]),
            paddle.to_tensor([0.39955827593803406, 0.4599599540233612, 0.39555904269218445, 0.4682416617870331, 0.3387352526187897, 0.4720209538936615, 0.10424847155809402, 0.1799745112657547, 0.11788666248321533, 0.2620531916618347, 0.11718633025884628, 0.025000307708978653, 0.06905266642570496, 0.4571138918399811, 0.0883881226181984, 0.37121468782424927, 0.16109973192214966, 0.45322385430336, 0.17316046357154846, 0.08963523060083389], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3360394b06561ec13981756e0ed1aeb2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2232513129711151, 0.2771296501159668, 0.03509576991200447, 0.0711033046245575, 0.034122318029403687, 0.35054048895835876, 0.46856698393821716, 0.19297784566879272, 0.313290536403656, 0.4635263681411743, 0.2418413907289505, 0.45483067631721497, 0.07755475491285324, 0.4538331627845764, 0.27456945180892944, 0.07474878430366516, 0.3545325696468353, 0.16182421147823334, 0.15702596306800842, 0.26207253336906433, 0.09407307952642441, 0.2528032958507538, 0.31000468134880066, 0.11358892172574997, 0.46700769662857056, 0.3471280336380005, 0.08051124215126038, 0.3038466274738312, 0.015282400883734226, 0.1656889021396637], dtype='float32').reshape([30]),
            paddle.to_tensor([0.044117484241724014, 0.21279965341091156, 0.3890630900859833, 0.3584551215171814, 0.3918802738189697, 0.11041537672281265, 0.05929327383637428, 0.010431170463562012, 0.052692100405693054, 0.19694297015666962, 0.4044891595840454, 0.3837413489818573, 0.22056463360786438, 0.3225787281990051, 0.31363698840141296, 0.46165555715560913, 0.3067341148853302, 0.3204464316368103, 0.20370879769325256, 0.06440016627311707, 0.38707900047302246, 0.0108824223279953, 0.10114755481481552, 0.022267447784543037, 0.035006485879421234, 0.3192022740840912, 0.013304565101861954, 0.49420687556266785, 0.08314932882785797, 0.39359644055366516], dtype='float32').reshape([30]),
            paddle.to_tensor([0.42591920495033264, 0.3693685531616211, 0.0012195247691124678, 0.053380705416202545, 0.09497882425785065, 0.372163861989975, 0.030503399670124054, 0.4078408181667328, 0.025259748101234436, 0.13723769783973694, 0.45006921887397766, 0.273053377866745, 0.38647401332855225, 0.12639646232128143, 0.037865158170461655, 0.22624124586582184, 0.39856964349746704, 0.3910902440547943, 0.15212328732013702, 0.23772190511226654, 0.38033390045166016, 0.3083074688911438, 0.1804068237543106, 0.4423503875732422, 0.44076812267303467, 0.3502888083457947, 0.05188140645623207, 0.17849595844745636, 0.2831770181655884, 0.49319523572921753], dtype='float32').reshape([30]),
            paddle.to_tensor([0.46862196922302246, 0.23650610446929932, 0.2080754190683365, 0.06511610746383667, 0.36536359786987305, 0.3789174258708954, 0.3270758092403412, 0.01573813334107399, 0.42792314291000366, 0.023445285856723785, 0.17311877012252808, 0.2906539738178253, 0.35936033725738525, 0.4813034236431122, 0.3108227849006653, 0.33554646372795105, 0.2676756680011749, 0.30605605244636536, 0.18455976247787476, 0.14397820830345154, 0.04068417102098465, 0.21081726253032684, 0.24451889097690582, 0.19467054307460785, 0.3682428300380707, 0.3114941120147705, 0.38896381855010986, 0.058332040905952454, 0.029968436807394028, 0.49314039945602417], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_08db05122c1b633a0530508462126b52(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a16b2bdf1df903228b5ef33a8469352b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_50d223a0100c42923eb0df076c8c3721(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 100, 168], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2422ed98d073bf1757fc19e9f5a4af0d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.08638158440589905, 0.020856650546193123, 0.12865857779979706, 0.13018347322940826, 0.13283999264240265, 0.45333728194236755, 0.10856392234563828, 0.027789708226919174, 0.48188477754592896, 0.12248582392930984, 0.36940720677375793, 0.2512243986129761, 0.4899469316005707, 0.04627141356468201, 0.20849156379699707, 0.4856370687484741, 0.055785518139600754, 0.14446009695529938], dtype='float32').reshape([18]),
            paddle.to_tensor([0.0923064723610878, 0.41265958547592163, 0.2088777720928192, 0.15126143395900726, 0.1670749932527542, 0.4627508819103241, 0.46842697262763977, 0.06628488004207611, 0.4180559515953064, 0.4220610558986664, 0.17664247751235962, 0.03926229476928711, 0.10264177620410919, 0.02928614802658558, 0.48814061284065247, 0.2562544047832489, 0.17369326949119568, 0.36611834168434143], dtype='float32').reshape([18]),
            paddle.to_tensor([0.12262850254774094, 0.34095829725265503, 0.23770356178283691, 0.23461782932281494, 0.4162290394306183, 0.46978309750556946, 0.09550948441028595, 0.31003162264823914, 0.21264894306659698, 0.49214673042297363, 0.45473942160606384, 0.14277972280979156, 0.46145346760749817, 0.18316435813903809, 0.2062436044216156, 0.23713506758213043, 0.09765365719795227, 0.4141151010990143], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3110619783401489, 0.41485530138015747, 0.33314579725265503, 0.02165820077061653, 0.051287487149238586, 0.0645974650979042, 0.206197127699852, 0.41440919041633606, 0.004866673611104488, 0.418423056602478, 0.3409823775291443, 0.12151478976011276, 0.09758316725492477, 0.07310257107019424, 0.19188086688518524, 0.3735851049423218, 0.26378318667411804, 0.48463088274002075], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_433c0397ab823465c5848d084a4a7858(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([8, 512, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bac329050ce353d50389c16d6ba69d7f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 720, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c420cb22de4bdb1a97a1b15d3c0a573e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 30, 30], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dbcc2c47ef76280e1d10826731204cc9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 75, 75], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d8c735f7710bb5543088561920d59fe0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 122, 122], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_01bb87ddd71d77e98abcc9bb4fd36830(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.31375551223754883, 0.43732327222824097, 0.42460471391677856, 0.21421930193901062, 0.025974005460739136, 0.4436231553554535, 0.22599925100803375, 0.05935118347406387, 0.1289302259683609, 0.26434922218322754, 0.1738833487033844, 0.43482398986816406, 0.49897265434265137, 0.09281998872756958, 0.08324581384658813, 0.21150526404380798, 0.2572091817855835, 0.07024720311164856, 0.3652189373970032, 0.40137773752212524, 0.45680469274520874, 0.3443151116371155, 0.3458822965621948, 0.45412346720695496], dtype='float32').reshape([24]),
            paddle.to_tensor([0.23314246535301208, 0.20737704634666443, 0.4619573950767517, 0.23099364340305328, 0.3871771991252899, 0.4088211953639984, 0.10374253243207932, 0.1640443652868271, 0.1875278353691101, 0.21532927453517914, 0.03833431750535965, 0.33613425493240356, 0.3476967215538025, 0.34161797165870667, 0.34998026490211487, 0.4581681489944458, 0.35892048478126526, 0.2360299676656723, 0.0164573322981596, 0.2720327079296112, 0.20790231227874756, 0.3828434348106384, 0.45060020685195923, 0.0928630381822586], dtype='float32').reshape([24]),
            paddle.to_tensor([0.10172440856695175, 0.39850568771362305, 0.33364567160606384, 0.48681554198265076, 0.4466666579246521, 0.2332872450351715, 0.2356196939945221, 0.00854732096195221, 0.3749579191207886, 0.34241485595703125, 0.3862018585205078, 0.2965307831764221, 0.36686384677886963, 0.28068363666534424, 0.0032135904766619205, 0.026708301156759262, 0.4904252588748932, 0.17589561641216278, 0.01734449341893196, 0.04617825523018837, 0.44733360409736633, 0.35303929448127747, 0.3026081621646881, 0.1356898844242096], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2489609569311142, 0.07382749766111374, 0.149064838886261, 0.1498488336801529, 0.06218031793832779, 0.09655474126338959, 0.02243519201874733, 0.11950260400772095, 0.12721137702465057, 0.27669602632522583, 0.1704770028591156, 0.07808340340852737, 0.34905245900154114, 0.46058389544487, 0.12185851484537125, 0.21588759124279022, 0.15533949434757233, 0.16690358519554138, 0.3267425000667572, 0.4639809727668762, 0.07898597419261932, 0.4851463735103607, 0.2921624481678009, 0.2843032479286194], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_22054cfe1b40d036c9394b86a6369ea8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 100], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f0d004a9f801f540989da8f415a851ee(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 3, 3], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4d6c20296abcbd194a083c865494692c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1184, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1184], dtype='float32', min=0, max=0.5),
            paddle.uniform([1184], dtype='float32', min=0, max=0.5),
            paddle.uniform([1184], dtype='float32', min=0, max=0.5),
            paddle.uniform([1184], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a5ab9e67816d12695fe668a698f0232e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 1152, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c684ef3a7aac6d02f56312759c5d8cc3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_03058d14e912fd6847449ad535bc356d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 88, 25], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_17ae718d53546d0de454fbd0f64b08bb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 128, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cd190daaaf836f21385cf3a7fc435599(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_413cd0c7f6a7232225e7c39f7af0f02d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 232, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_157a00729c898c8994289990d1cee6ab(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_29218ac5e70af3a1d01cec72377ac96f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.32456308603286743, 0.45667243003845215, 0.018145998939871788, 0.38655349612236023, 0.31773945689201355, 0.30079716444015503, 0.09609443694353104, 0.2572072744369507, 0.48979562520980835, 0.44177311658859253, 0.007696258835494518, 0.41518881916999817, 0.2913427948951721, 0.1348801851272583, 0.270465224981308, 0.28007039427757263, 0.3213162422180176, 0.2839370369911194], dtype='float32').reshape([18]),
            paddle.to_tensor([0.15308547019958496, 0.3441517651081085, 0.35946476459503174, 0.24247543513774872, 0.4483085870742798, 0.4259561002254486, 0.11211112886667252, 0.29398030042648315, 0.48762252926826477, 0.4923969805240631, 0.09198196977376938, 0.44585591554641724, 0.09815419465303421, 0.1312224566936493, 0.4849037826061249, 0.14502012729644775, 0.23687757551670074, 0.4046018421649933], dtype='float32').reshape([18]),
            paddle.to_tensor([0.48297327756881714, 0.42949163913726807, 0.376076877117157, 0.37390029430389404, 0.06427840888500214, 0.21157950162887573, 0.3161257207393646, 0.3491285443305969, 0.009605230763554573, 0.15965312719345093, 0.4633672535419464, 0.07193481922149658, 0.42076054215431213, 0.0018324947450309992, 0.37895238399505615, 0.023746229708194733, 0.3039190471172333, 0.4998819828033447], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4855814576148987, 0.20225828886032104, 0.28640052676200867, 0.2768668234348297, 0.08158165961503983, 0.3363302946090698, 0.2646394371986389, 0.30588579177856445, 0.10166459530591965, 0.08760040253400803, 0.14617550373077393, 0.1551923304796219, 0.09956807643175125, 0.2633482813835144, 0.12841056287288666, 0.09771579504013062, 0.38487479090690613, 0.3614726662635803], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0553766af004ab2053e7e77bda0f041d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 120, 120], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a4636f80649e28a2f733de92da21b956(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 32, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bab79f640fb000e25aaff7996cc0a8de(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.013621801510453224, 0.1673230230808258, 0.34058651328086853, 0.40809106826782227, 0.3349522650241852, 0.03291800245642662, 0.2569127082824707, 0.31144797801971436, 0.06570937484502792, 0.17368900775909424, 0.25318604707717896, 0.32001057267189026, 0.14010454714298248, 0.11900366842746735, 0.033717092126607895, 0.42644262313842773, 0.18983912467956543, 0.04652954638004303, 0.2000603824853897, 0.13849811255931854, 0.07573606073856354, 0.07309720665216446, 0.36683177947998047, 0.03303992748260498], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3238038122653961, 0.2783220708370209, 0.016940930858254433, 0.10394567996263504, 0.03230823948979378, 0.059995897114276886, 0.25416889786720276, 0.39208829402923584, 0.009291228838264942, 0.3011750876903534, 0.4581834673881531, 0.3069489896297455, 0.38926783204078674, 0.17199356853961945, 0.3414265513420105, 0.056276995688676834, 0.07665175199508667, 0.48417142033576965, 0.14973387122154236, 0.3919435143470764, 0.10618004202842712, 0.3458999693393707, 0.423569917678833, 0.35788214206695557], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3417704701423645, 0.424550861120224, 0.38924866914749146, 0.022555112838745117, 0.3830406367778778, 0.25057104229927063, 0.4686320722103119, 0.43485599756240845, 0.4861680567264557, 0.4043029844760895, 0.03900199756026268, 0.34839531779289246, 0.3779589831829071, 0.14432546496391296, 0.3047649562358856, 0.047390058636665344, 0.18741033971309662, 0.45093560218811035, 0.04478074610233307, 0.18001557886600494, 0.20996157824993134, 0.19013500213623047, 0.4114394783973694, 0.1616668999195099], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4022065997123718, 0.09356541186571121, 0.00299245398491621, 0.23975200951099396, 0.46567922830581665, 0.14149442315101624, 0.43955427408218384, 0.23443010449409485, 0.08328276872634888, 0.1567169427871704, 0.21474534273147583, 0.28998473286628723, 0.298508882522583, 0.32458972930908203, 0.1825297623872757, 0.47278398275375366, 0.158482626080513, 0.00833277590572834, 0.278533011674881, 0.16774097084999084, 0.10963483154773712, 0.3406594693660736, 0.16138844192028046, 0.3902488350868225], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_16baf02b4527b73c17598177911ff1b4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_01c289ac9d05072efc778c1a98e677cd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_afc44de902ee03ba5c19876f8a289e25(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.09500528872013092, 0.4599989950656891, 0.2500194311141968, 0.13899937272071838, 0.09717491269111633, 0.1439344882965088, 0.4764464199542999, 0.13536302745342255, 0.05475180223584175, 0.4515674114227295, 0.33636748790740967, 0.3790466785430908, 0.4043758809566498, 0.28204411268234253, 0.014333569444715977, 0.0072655756957829], dtype='float32').reshape([16]),
            paddle.to_tensor([0.14110955595970154, 0.3992067277431488, 0.10992544889450073, 0.4913131892681122, 0.23684678971767426, 0.29623401165008545, 0.4429195523262024, 0.38639846444129944, 0.44734007120132446, 0.26237258315086365, 0.20189808309078217, 0.33697763085365295, 0.04075528681278229, 0.35061365365982056, 0.2385869175195694, 0.4228206276893616], dtype='float32').reshape([16]),
            paddle.to_tensor([0.35705825686454773, 0.1790739893913269, 0.16639384627342224, 0.49533313512802124, 0.20637212693691254, 0.38554832339286804, 0.4003099501132965, 0.37959757447242737, 0.3790501058101654, 0.20012614130973816, 0.03440804034471512, 0.16932615637779236, 0.3857615888118744, 0.37849757075309753, 0.44775259494781494, 0.30139169096946716], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3913479745388031, 0.48616698384284973, 0.11271185427904129, 0.22336871922016144, 0.2304307520389557, 0.09008105099201202, 0.34622302651405334, 0.011155281215906143, 0.3545544743537903, 0.2577771246433258, 0.09746824204921722, 0.47412070631980896, 0.46096110343933105, 0.13419082760810852, 0.35025590658187866, 0.08045302331447601], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_888ec5c876702bf58531aabf51065126(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1536, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_12508d97f7d3141664354d30645211f1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.178071066737175, 0.4284948706626892, 0.296810507774353, 0.2737804651260376, 0.41065001487731934, 0.4137074649333954, 0.08598481118679047, 0.03622172772884369, 0.09343349188566208, 0.18657273054122925, 0.496991902589798, 0.38589730858802795, 0.4120214879512787, 0.23260167241096497, 0.2656722068786621, 0.06846984475851059, 0.18411768972873688, 0.08945948630571365], dtype='float32').reshape([18]),
            paddle.to_tensor([0.26238733530044556, 0.1977885216474533, 0.17067952454090118, 0.1255275458097458, 0.17088761925697327, 0.07926961779594421, 0.2462921142578125, 0.45430612564086914, 0.14778964221477509, 0.28180038928985596, 0.09068179875612259, 0.23244237899780273, 0.09467225521802902, 0.3834719657897949, 0.2309102565050125, 0.25930681824684143, 0.3771932125091553, 0.10413563251495361], dtype='float32').reshape([18]),
            paddle.to_tensor([0.48163336515426636, 0.3936687111854553, 0.32913658022880554, 0.004344081971794367, 0.35460349917411804, 0.24480879306793213, 0.02012227103114128, 0.036229394376277924, 0.2313087284564972, 0.031398285180330276, 0.49568331241607666, 0.3268708288669586, 0.4888042211532593, 0.0783831924200058, 0.4368742108345032, 0.025305302813649178, 0.1453501433134079, 0.051743317395448685], dtype='float32').reshape([18]),
            paddle.to_tensor([0.21895107626914978, 0.47662365436553955, 0.03876733034849167, 0.01674123853445053, 0.23117227852344513, 0.44544681906700134, 0.24938564002513885, 0.45241647958755493, 0.3073723018169403, 0.14065928757190704, 0.06800522655248642, 0.28992941975593567, 0.07847175002098083, 0.08027470111846924, 0.4110400974750519, 0.3602530360221863, 0.3156723082065582, 0.22762690484523773], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_13d230d777b0b82757291afee8389fbf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 5, 5], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1a1f2849bb8b420b4f1f901c960beb59(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 14, 25], dtype='float16', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fac87eeef2544e98b83c55e4fbde5852(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.05718138441443443, 0.39122629165649414, 0.08013688027858734, 0.43273869156837463, 0.20807428658008575, 0.03414080664515495, 0.11931239813566208, 0.2093530148267746, 0.014209595508873463, 0.4377251863479614, 0.2029404491186142, 0.21465842425823212, 0.018366234377026558, 0.16647638380527496, 0.1530904471874237, 0.3000648617744446, 0.40619614720344543, 0.04335281625390053, 0.33406251668930054, 0.2082383632659912, 0.33232906460762024, 0.31334373354911804, 0.4024452269077301, 0.3025133013725281], dtype='float32').reshape([24]),
            paddle.to_tensor([0.018956152722239494, 0.346476674079895, 0.36694976687431335, 0.3427880108356476, 0.3895837068557739, 0.437728613615036, 0.04310787096619606, 0.04476151242852211, 0.31748372316360474, 0.24472548067569733, 0.2361549288034439, 0.2530132234096527, 0.1387278139591217, 0.3955644965171814, 0.4308384656906128, 0.08095734566450119, 0.05018581449985504, 0.06985943019390106, 0.1617896407842636, 0.27981430292129517, 0.04369422793388367, 0.17362062633037567, 0.16565588116645813, 0.2053123265504837], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3306882977485657, 0.3507307171821594, 0.4144345223903656, 0.04978819936513901, 0.2484506219625473, 0.07973793894052505, 0.031753476709127426, 0.43349531292915344, 0.26026618480682373, 0.3837102949619293, 0.07870787382125854, 0.26306137442588806, 0.209665909409523, 0.3512316048145294, 0.39244577288627625, 0.21716512739658356, 0.18629051744937897, 0.4686649441719055, 0.2757360339164734, 0.1921900510787964, 0.47142255306243896, 0.3157888352870941, 0.46969014406204224, 0.38293108344078064], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4078306257724762, 0.02235247939825058, 0.0854964554309845, 0.327958345413208, 0.4319191873073578, 0.4606173634529114, 0.19631259143352509, 0.4891527593135834, 0.22053837776184082, 0.261773020029068, 0.09072409570217133, 0.4538058340549469, 0.1345313936471939, 0.3248348832130432, 0.03211451694369316, 0.040537670254707336, 0.34816253185272217, 0.4421176016330719, 0.4051562249660492, 0.18903817236423492, 0.3572579324245453, 0.2406974732875824, 0.40037819743156433, 0.11555727571249008], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8469db2003f3c47ca9124c89d2abd9d9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 12, 12], dtype='float16', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1e4b6dab6efd6556630c4b3af5b37b47(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.17479322850704193, 0.37546804547309875, 0.37071529030799866, 0.006916502024978399, 0.13924559950828552, 0.09088113903999329, 0.37169650197029114, 0.41074514389038086, 0.31578049063682556, 0.32290953397750854, 0.47139883041381836, 0.10499747842550278, 0.08572161942720413, 0.32514283061027527, 0.21999822556972504, 0.1084292009472847, 0.31738075613975525, 0.4402740001678467, 0.2949274182319641, 0.1411667913198471], dtype='float32').reshape([20]),
            paddle.to_tensor([0.056952737271785736, 0.3960062861442566, 0.42521652579307556, 0.3364727795124054, 0.48832863569259644, 0.37295958399772644, 0.32149311900138855, 0.08659816533327103, 0.4880734980106354, 0.35334205627441406, 0.10938375443220139, 0.36403927206993103, 0.23519019782543182, 0.49289822578430176, 0.26263371109962463, 0.2985580265522003, 0.22441443800926208, 0.3223050832748413, 0.05153683200478554, 0.11854355037212372], dtype='float32').reshape([20]),
            paddle.to_tensor([0.2354859858751297, 0.010484607890248299, 0.2193935066461563, 0.060205668210983276, 0.01004320103675127, 0.20844528079032898, 0.3020429313182831, 0.3983898460865021, 0.30096474289894104, 0.3518983721733093, 0.13013681769371033, 0.07535968720912933, 0.3838537633419037, 0.11764157563447952, 0.23549261689186096, 0.06435860693454742, 0.49806860089302063, 0.24411776661872864, 0.14052192866802216, 0.1954462081193924], dtype='float32').reshape([20]),
            paddle.to_tensor([0.23910000920295715, 0.1712244302034378, 0.32318761944770813, 0.2958943843841553, 0.4127868711948395, 0.38512662053108215, 0.17257459461688995, 0.3926122486591339, 0.17864109575748444, 0.32040223479270935, 0.38604792952537537, 0.2885705530643463, 0.27185678482055664, 0.451504647731781, 0.20020025968551636, 0.03779185190796852, 0.43970489501953125, 0.01912662759423256, 0.2702316641807556, 0.05033065751194954], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_04c66ca1bad5da9dfa7ef06a38b23927(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 152, 152], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c192f1df6e00be53d785f21ab2f6a190(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 4, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.331987202167511, 0.06502979248762131, 0.29226699471473694, 0.1560075879096985], dtype='float32').reshape([4]),
            paddle.to_tensor([0.23891803622245789, 0.27609533071517944, 0.06661493331193924, 0.16009575128555298], dtype='float32').reshape([4]),
            paddle.to_tensor([0.004652801435440779, 0.21401967108249664, 0.0005928056198172271, 0.030856024473905563], dtype='float32').reshape([4]),
            paddle.to_tensor([0.12586542963981628, 0.4818449318408966, 0.19412778317928314, 0.1741701066493988], dtype='float32').reshape([4]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ce63ea67eec19983be45ed3f618e7a90(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.00958940014243126, 0.04581272602081299, 0.10357581824064255, 0.3479335606098175, 0.4414372146129608, 0.033868663012981415, 0.10920392721891403, 0.3917250335216522, 0.4276222586631775, 0.4432910978794098, 0.3565096855163574, 0.27627500891685486, 0.2297365665435791, 0.15803255140781403, 0.2666013240814209, 0.037690505385398865, 0.3570692539215088, 0.46250981092453003, 0.2901042699813843, 0.27432358264923096, 0.24456146359443665, 0.3610212206840515, 0.3695637583732605, 0.2980594038963318], dtype='float32').reshape([24]),
            paddle.to_tensor([0.1548948436975479, 0.11291296780109406, 0.48360675573349, 0.13999353349208832, 0.4587208926677704, 0.4616668224334717, 0.1617940366268158, 0.1986083686351776, 0.4257766306400299, 0.43842557072639465, 0.256293386220932, 0.4465152621269226, 0.10701204091310501, 0.009476500563323498, 0.041135024279356, 0.47755318880081177, 0.34551605582237244, 0.2530979514122009, 0.15752705931663513, 0.3706180155277252, 0.0677199587225914, 0.1631229817867279, 0.4549192190170288, 0.2862861752510071], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3221372067928314, 0.29382213950157166, 0.4482269287109375, 0.43200328946113586, 0.48324477672576904, 0.2043379843235016, 0.07957573980093002, 0.0455348826944828, 0.009968990460038185, 0.07217270135879517, 0.10331035405397415, 0.47973760962486267, 0.17816855013370514, 0.41451409459114075, 0.2890821695327759, 0.041037313640117645, 0.4370660185813904, 0.37640634179115295, 0.3632071316242218, 0.4680149555206299, 0.29705503582954407, 0.050369054079055786, 0.3006870746612549, 0.15581612288951874], dtype='float32').reshape([24]),
            paddle.to_tensor([0.02820339985191822, 0.28103184700012207, 0.36747750639915466, 0.045474350452423096, 0.40499594807624817, 0.4332963228225708, 0.30775490403175354, 0.2304995357990265, 0.4286110997200012, 0.28335538506507874, 0.32831522822380066, 0.08345317095518112, 0.1665657013654709, 0.1171000599861145, 0.32737669348716736, 0.004656753968447447, 0.37112924456596375, 0.2906888723373413, 0.4703747034072876, 0.03777752071619034, 0.27659985423088074, 0.3634543716907501, 0.4815713167190552, 0.43739548325538635], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b17ef474cd404a57fab38e2f4b7581e4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1968, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1968], dtype='float32', min=0, max=0.5),
            paddle.uniform([1968], dtype='float32', min=0, max=0.5),
            paddle.uniform([1968], dtype='float32', min=0, max=0.5),
            paddle.uniform([1968], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3da388fec608d0b53db3f2482c4c6e3f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1344, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1344], dtype='float32', min=0, max=0.5),
            paddle.uniform([1344], dtype='float32', min=0, max=0.5),
            paddle.uniform([1344], dtype='float32', min=0, max=0.5),
            paddle.uniform([1344], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8c028efd3f171936765f3e188f1e9e33(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1ad2b47f7bf039f89465d61626278397(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4c34af641aa3289578d1eac7a962a61e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 52, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5f0980e1843ba60f8bdac9094422493e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3932502865791321, 0.28075966238975525, 0.2885586619377136, 0.438371479511261, 0.4974319338798523, 0.40778374671936035, 0.21639330685138702, 0.34325897693634033, 0.3968610465526581, 0.11496227979660034, 0.46255064010620117, 0.4772462546825409, 0.2475796639919281, 0.3844235837459564, 0.13563795387744904, 0.32632654905319214], dtype='float32').reshape([16]),
            paddle.to_tensor([0.307966023683548, 0.4286383092403412, 0.1326601654291153, 0.029866186901926994, 0.10649745911359787, 0.49847662448883057, 0.35690444707870483, 0.17792470753192902, 0.379226416349411, 0.350768119096756, 0.10134398937225342, 0.14694097638130188, 0.10520697385072708, 0.13621556758880615, 0.20658931136131287, 0.4474652409553528], dtype='float32').reshape([16]),
            paddle.to_tensor([0.40517792105674744, 0.24684366583824158, 0.13303951919078827, 0.05151645839214325, 0.22744373977184296, 0.02703171595931053, 0.12478012591600418, 0.14435425400733948, 0.09598233550786972, 0.3268893361091614, 0.1992911845445633, 0.20852801203727722, 0.3437710404396057, 0.20989453792572021, 0.1878105103969574, 0.11774542927742004], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3495384454727173, 0.11997925490140915, 0.18680241703987122, 0.2659364342689514, 0.27871814370155334, 0.4693526327610016, 0.4256368577480316, 0.37180912494659424, 0.3786839544773102, 0.053639382123947144, 0.3017856478691101, 0.21112249791622162, 0.13888438045978546, 0.37345102429389954, 0.3880215585231781, 0.46950918436050415], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_47025a9d4e87f018677db5b418c84c08(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 16, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_069e7cf64d651f596cba675b352cd136(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_528a5ae510305ff315773bd0311e99c2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4882459342479706, 0.3891357183456421, 0.4791620373725891, 0.4360889792442322, 0.477166086435318, 0.4526396095752716, 0.47572749853134155, 0.33656033873558044, 0.2755078673362732, 0.20274172723293304, 0.2053758054971695, 0.2318994253873825, 0.12255098670721054, 0.19968406856060028, 0.34923627972602844, 0.46086448431015015, 0.3373200595378876, 0.26274943351745605], dtype='float32').reshape([18]),
            paddle.to_tensor([0.48373550176620483, 0.24740374088287354, 0.11639492213726044, 0.07014155387878418, 0.23372159898281097, 0.05701456218957901, 0.47141173481941223, 0.2107599377632141, 0.21882250905036926, 0.12553207576274872, 0.3883135914802551, 0.4846809208393097, 0.053047530353069305, 0.44310644268989563, 0.10961144417524338, 0.3396906852722168, 0.3636319041252136, 0.44006049633026123], dtype='float32').reshape([18]),
            paddle.to_tensor([0.08008091896772385, 0.058694563806056976, 0.4589923024177551, 0.2973892390727997, 0.36520060896873474, 0.16026246547698975, 0.08032816648483276, 0.41540491580963135, 0.03221104294061661, 0.24276287853717804, 0.3752138018608093, 0.27075061202049255, 0.03404274582862854, 0.4230491518974304, 0.07600466907024384, 0.3290550112724304, 0.2703382968902588, 0.14537972211837769], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4411880373954773, 0.07238559424877167, 0.4856095314025879, 0.010749217122793198, 0.4386647641658783, 0.021326808258891106, 0.16201983392238617, 0.0380399152636528, 0.28401151299476624, 0.1578473001718521, 0.28332415223121643, 0.3157113492488861, 0.03104572556912899, 0.29654553532600403, 0.21329152584075928, 0.3652876317501068, 0.2827608585357666, 0.1790020763874054], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cd1f3daa148513e794672a7e544e7d98(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 26, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3665306568145752, 0.019941195845603943, 0.4812191128730774, 0.225875124335289, 0.27539727091789246, 0.3799254298210144, 0.27320432662963867, 0.2093563824892044, 0.39993372559547424, 0.19444045424461365, 0.2762957811355591, 0.4756563901901245, 0.296645849943161, 0.4416310787200928, 0.4679509401321411, 0.43247100710868835, 0.48956626653671265, 0.14608831703662872, 0.4590334892272949, 0.06525532156229019, 0.15025703608989716, 0.34669995307922363, 0.22455357015132904, 0.10304012149572372, 0.1308106780052185, 0.1949707418680191], dtype='float32').reshape([26]),
            paddle.to_tensor([0.17596891522407532, 0.1303664594888687, 0.36752018332481384, 0.4773804247379303, 0.019706975668668747, 0.4678725004196167, 0.14680050313472748, 0.3935474157333374, 0.19600999355316162, 0.4979284107685089, 0.4947262704372406, 0.2161528766155243, 0.17727027833461761, 0.12321411073207855, 0.09076353162527084, 0.15731804072856903, 0.31348371505737305, 0.37588396668434143, 0.42234379053115845, 0.25580593943595886, 0.21926139295101166, 0.055074311792850494, 0.48668766021728516, 0.1082879826426506, 0.07611658424139023, 0.1497597098350525], dtype='float32').reshape([26]),
            paddle.to_tensor([0.23047791421413422, 0.1458645761013031, 0.4893653690814972, 0.12385005503892899, 0.057400938123464584, 0.26145434379577637, 0.15258477628231049, 0.4549175500869751, 0.27253830432891846, 0.10885436087846756, 0.3312360346317291, 0.19001439213752747, 0.3597966432571411, 0.24904130399227142, 0.10338718444108963, 0.17315152287483215, 0.007359273731708527, 0.3832094669342041, 0.3743549883365631, 0.3738918602466583, 0.37635117769241333, 0.2864035367965698, 0.0626903623342514, 0.1900608241558075, 0.054789938032627106, 0.017510322853922844], dtype='float32').reshape([26]),
            paddle.to_tensor([0.3140166103839874, 0.24221709370613098, 0.21947674453258514, 0.262393981218338, 0.4042430520057678, 0.4068795144557953, 0.07169097661972046, 0.291729211807251, 0.48975425958633423, 0.35775861144065857, 0.11940254271030426, 0.4888302981853485, 0.27436158061027527, 0.13987313210964203, 0.15203268826007843, 0.47098222374916077, 0.32645243406295776, 0.3979930877685547, 0.20389746129512787, 0.26492127776145935, 0.15009574592113495, 0.27044951915740967, 0.3223657011985779, 0.23436610400676727, 0.055669523775577545, 0.473763108253479], dtype='float32').reshape([26]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5bac1440a2a81d0b054372009a8fc922(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.43119871616363525, 0.34377503395080566, 0.20679235458374023, 0.28507357835769653, 0.37392324209213257, 0.4917404353618622, 0.42635953426361084, 0.408637136220932, 0.4051114618778229, 0.10059724003076553, 0.0861847996711731, 0.38320356607437134, 0.059812285006046295, 0.1972714364528656, 0.460193395614624, 0.44283217191696167, 0.4240436255931854, 0.16738058626651764, 0.18670152127742767, 0.3649710416793823, 0.18148736655712128, 0.08235512673854828, 0.21655037999153137, 0.04165138676762581, 0.250741571187973, 0.377481609582901, 0.20569537580013275, 0.06569546461105347, 0.10332237184047699, 0.3757078945636749], dtype='float32').reshape([30]),
            paddle.to_tensor([0.06146072968840599, 0.37296417355537415, 0.30946245789527893, 0.4193350672721863, 0.22944648563861847, 0.22304746508598328, 0.3428027331829071, 0.46591606736183167, 0.11399650573730469, 0.3243507444858551, 0.3722670078277588, 0.05441785976290703, 0.48438239097595215, 0.3784770965576172, 0.08122604340314865, 0.2375241070985794, 0.36532843112945557, 0.21793800592422485, 0.3558826148509979, 0.310475617647171, 0.24601157009601593, 0.2531026303768158, 0.467589408159256, 0.4854260981082916, 0.2093743532896042, 0.028815632686018944, 0.3529202342033386, 0.4903046786785126, 0.0029245319310575724, 0.4008927643299103], dtype='float32').reshape([30]),
            paddle.to_tensor([0.03534673899412155, 0.1345517486333847, 0.3558526039123535, 0.09151595830917358, 0.2339067906141281, 0.18522094190120697, 0.38806918263435364, 0.3803784251213074, 0.41540953516960144, 0.01895117573440075, 0.43982580304145813, 0.40240323543548584, 0.48840904235839844, 0.45454493165016174, 0.32836005091667175, 0.055413272231817245, 0.21985751390457153, 0.11281778663396835, 0.22270843386650085, 0.35409870743751526, 0.28681623935699463, 0.03156774118542671, 0.13497014343738556, 0.12159789353609085, 0.4541890621185303, 0.45215559005737305, 0.02171156369149685, 0.4970438778400421, 0.4933210015296936, 0.119468092918396], dtype='float32').reshape([30]),
            paddle.to_tensor([0.20846101641654968, 0.27423301339149475, 0.2664829194545746, 0.04422583431005478, 0.06084677577018738, 0.0958918035030365, 0.1694350242614746, 0.4451252818107605, 0.169070303440094, 0.09159573167562485, 0.23777011036872864, 0.1483011245727539, 0.10564322024583817, 0.1450616717338562, 0.02482195943593979, 0.2775121033191681, 0.11739032715559006, 0.09596851468086243, 0.16181065142154694, 0.14078779518604279, 0.4235704243183136, 0.27781277894973755, 0.250167578458786, 0.4718562364578247, 0.38474199175834656, 0.11491166800260544, 0.039986152201890945, 0.02474803477525711, 0.48995405435562134, 0.22671347856521606], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_92307ecbed030511174339039665bb95(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8834139b6e81cbef21eb6c154545d04e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bbe9af8f06b58db52b978ea1089b990d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e546b708569e8d38741e5ecf599d46fc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 61, 61], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_92f03dcfb4c66860d3e7f0daeb2f3360(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7dc78d09927be49b41da80b128bdaa49(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 232, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8c6dccf1c702ebe105911afd50e22556(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 4, 3], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_04d0000224fc6301c6a7326667b2cf90(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.29487672448158264, 0.31037047505378723, 0.047812145203351974, 0.005356001667678356, 0.46005770564079285, 0.47240403294563293, 0.15036681294441223, 0.21008731424808502, 0.3941618800163269, 0.02715287357568741, 0.3264729380607605, 0.4336228370666504, 0.05158329755067825, 0.04086536169052124, 0.3626789152622223, 0.2613224983215332, 0.039493996649980545, 0.25931796431541443, 0.12509021162986755, 0.16028614342212677], dtype='float32').reshape([20]),
            paddle.to_tensor([0.17031584680080414, 0.1705082803964615, 0.40450045466423035, 0.18633463978767395, 0.3992248475551605, 0.12196683883666992, 0.03694020211696625, 0.3584408462047577, 0.22000345587730408, 0.338961124420166, 0.33690154552459717, 0.4521956741809845, 0.3885057866573334, 0.2949892580509186, 0.10510018467903137, 0.23498982191085815, 0.4169871509075165, 0.012089742347598076, 0.1753474771976471, 0.19730935990810394], dtype='float32').reshape([20]),
            paddle.to_tensor([0.22376765310764313, 0.1345779150724411, 0.24134834110736847, 0.005894141271710396, 0.43002647161483765, 0.3762635588645935, 0.06562212109565735, 0.09382372349500656, 0.34260258078575134, 0.10472963750362396, 0.30669695138931274, 0.08361228555440903, 0.25923028588294983, 0.4014757573604584, 0.08536623418331146, 0.11418116092681885, 0.27800852060317993, 0.4667262136936188, 0.15695281326770782, 0.06445737183094025], dtype='float32').reshape([20]),
            paddle.to_tensor([0.2564709484577179, 0.36581555008888245, 0.12253091484308243, 0.118594229221344, 0.36718839406967163, 0.4371260404586792, 0.006531696766614914, 0.09143028408288956, 0.47117775678634644, 0.04304753988981247, 0.3775070309638977, 0.41963404417037964, 0.22470572590827942, 0.40351539850234985, 0.33900120854377747, 0.3144055902957916, 0.09005291014909744, 0.08945005387067795, 0.16913555562496185, 0.21205541491508484], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_52ee4603bb428c7f5afac32af49e0807(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2587029039859772, 0.28472158312797546, 0.06509862840175629, 0.44525229930877686, 0.3457499146461487, 0.10349888354539871, 0.46710890531539917, 0.018680810928344727, 0.4599526524543762, 0.2047422230243683, 0.0285392664372921, 0.39639851450920105, 0.2752368152141571, 0.4972791075706482, 0.30213654041290283, 0.21243125200271606, 0.2582123875617981, 0.1273716390132904, 0.03515166789293289, 0.4409679174423218, 0.3357165455818176, 0.3398081362247467, 0.2427019327878952, 0.19836339354515076], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3909623324871063, 0.07149229943752289, 0.36181432008743286, 0.371590793132782, 0.08081613481044769, 0.16263021528720856, 0.05370310693979263, 0.46617233753204346, 0.08209582418203354, 0.044518861919641495, 0.24229535460472107, 0.49731776118278503, 0.24583198130130768, 0.2402328997850418, 0.4100610315799713, 0.05361764132976532, 0.3723384737968445, 0.20366841554641724, 0.48363304138183594, 0.4159509837627411, 0.31619319319725037, 0.07883331924676895, 0.27154839038848877, 0.17659412324428558], dtype='float32').reshape([24]),
            paddle.to_tensor([0.09335507452487946, 0.03965147212147713, 0.317089319229126, 0.411414235830307, 0.3006564676761627, 0.04726498946547508, 0.3622704744338989, 0.36483046412467957, 0.21705622971057892, 0.1540246456861496, 0.05416484922170639, 0.15700477361679077, 0.09413797408342361, 0.030879536643624306, 0.25607433915138245, 0.19048109650611877, 0.4662023186683655, 0.0312349870800972, 0.4399159252643585, 0.2955814003944397, 0.053167492151260376, 0.21542136371135712, 0.2957158386707306, 0.11522423475980759], dtype='float32').reshape([24]),
            paddle.to_tensor([0.056924156844615936, 0.2981593906879425, 0.35270994901657104, 0.1964038908481598, 0.02297181822359562, 0.43009698390960693, 0.1244138702750206, 0.21104496717453003, 0.2826085090637207, 0.09137184172868729, 0.15021906793117523, 0.23213359713554382, 0.3763677775859833, 0.32082894444465637, 0.3785340487957001, 0.22445352375507355, 0.338492751121521, 0.07662922143936157, 0.03008420579135418, 0.4280660152435303, 0.009192871861159801, 0.23408269882202148, 0.02210076153278351, 0.4302382469177246], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_03eca6fb312f8659782bb2f9472c638c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a13df427aa87ff2deff137ee1506d010(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.043257735669612885, 0.16976286470890045, 0.462146520614624, 0.04404108598828316, 0.08072153478860855, 0.2775175869464874, 0.05112399905920029, 0.14553236961364746, 0.28622421622276306, 0.362289160490036, 0.2548457086086273, 0.23166337609291077, 0.06977226585149765, 0.3847857415676117, 0.38890278339385986, 0.42720890045166016, 0.19462203979492188, 0.07775553315877914, 0.4808318316936493, 0.08798105269670486, 0.152518630027771, 0.2672223150730133, 0.04860023036599159, 0.2572932541370392, 0.1781669557094574, 0.30906158685684204, 0.23516324162483215, 0.3756459951400757, 0.47416266798973083, 0.24927009642124176], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4433979392051697, 0.30507731437683105, 0.11392027139663696, 0.2348099648952484, 0.283876895904541, 0.39031049609184265, 0.209075927734375, 0.06763026118278503, 0.49989163875579834, 0.0378885418176651, 0.19109401106834412, 0.05969110503792763, 0.02590988762676716, 0.1010553240776062, 0.27286097407341003, 0.3497874140739441, 0.14444048702716827, 0.09371858090162277, 0.4958146810531616, 0.39771342277526855, 0.13751518726348877, 0.22088241577148438, 0.02542903460562229, 0.4287279546260834, 0.1415145844221115, 0.4811260998249054, 0.13439464569091797, 0.16886211931705475, 0.05969061702489853, 0.43030571937561035], dtype='float32').reshape([30]),
            paddle.to_tensor([0.05527319759130478, 0.17700828611850739, 0.11066260188817978, 0.4570172131061554, 0.27734890580177307, 0.1443411409854889, 0.22823385894298553, 0.026728777214884758, 0.030345335602760315, 0.06669163703918457, 0.38459905982017517, 0.4231535792350769, 0.282678484916687, 0.0965161919593811, 0.20420971512794495, 0.032661762088537216, 0.3648344874382019, 0.4001738131046295, 0.21585191786289215, 0.09231392294168472, 0.22694101929664612, 0.3839384913444519, 0.3962491452693939, 0.33574599027633667, 0.13793829083442688, 0.44442811608314514, 0.28939002752304077, 0.27946943044662476, 0.17093397676944733, 0.3820123076438904], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2833215296268463, 0.12230470031499863, 0.004682149272412062, 0.40175098180770874, 0.12830939888954163, 0.18243800103664398, 0.42114657163619995, 0.0501244030892849, 0.03962255269289017, 0.27050209045410156, 0.0639030933380127, 0.19837374985218048, 0.1680150181055069, 0.2983115613460541, 0.16045014560222626, 0.1130414828658104, 0.47964105010032654, 0.06258411705493927, 0.4945563077926636, 0.14586478471755981, 0.006153411231935024, 0.40440717339515686, 0.40138161182403564, 0.061130277812480927, 0.16464467346668243, 0.008586596697568893, 0.40954887866973877, 0.40734580159187317, 0.27061352133750916, 0.4906522035598755], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6604ca43e007a966a0b18784d6699c6e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 10, 10], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d023fc383f4c2160e6d1bd7a3050ed22(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_00ba18a9520593d13d9e51fba60e84ec(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.1340242624282837, 0.18533360958099365, 0.24286913871765137, 0.10257099568843842, 0.36595550179481506, 0.07216215878725052, 0.07667160034179688, 0.49850890040397644, 0.4742693603038788, 0.06241737678647041, 0.29986679553985596, 0.21394804120063782, 0.425262451171875, 0.4612298309803009, 0.24563443660736084, 0.29059475660324097, 0.04951372742652893, 0.2660825550556183, 0.4412974417209625, 0.37742578983306885, 0.009104476310312748, 0.2972693145275116, 0.21845994889736176, 0.18373821675777435, 0.12087428569793701, 0.37932002544403076, 0.1715737283229828, 0.3977504372596741, 0.03132195398211479, 0.2583775222301483], dtype='float32').reshape([30]),
            paddle.to_tensor([0.02447359450161457, 0.13536342978477478, 0.343310683965683, 0.09629804641008377, 0.13049528002738953, 0.03299843147397041, 0.023034922778606415, 0.38664743304252625, 0.25483113527297974, 0.48094436526298523, 0.006129211746156216, 0.25658535957336426, 0.12109657377004623, 0.08371888101100922, 0.32314950227737427, 0.1331702023744583, 0.3910363018512726, 0.16277286410331726, 0.4437740743160248, 0.48368752002716064, 0.1016182079911232, 0.02986493892967701, 0.21095837652683258, 0.4640405774116516, 0.37558767199516296, 0.26325929164886475, 0.30741849541664124, 0.1547505110502243, 0.4655509293079376, 0.47331422567367554], dtype='float32').reshape([30]),
            paddle.to_tensor([0.19388070702552795, 0.4243389964103699, 0.31947317719459534, 0.18723490834236145, 0.0930572971701622, 0.15588387846946716, 0.2341431826353073, 0.3405288755893707, 0.33160609006881714, 0.46110501885414124, 0.23122501373291016, 0.36540016531944275, 0.11163117736577988, 0.18925221264362335, 0.2122478485107422, 0.21123677492141724, 0.2664753496646881, 0.3512721359729767, 0.19310444593429565, 0.20711180567741394, 0.041203126311302185, 0.3954031467437744, 0.09387379884719849, 0.030971428379416466, 0.3546021580696106, 0.33570197224617004, 0.16724303364753723, 0.25597134232521057, 0.10960707813501358, 0.0693311020731926], dtype='float32').reshape([30]),
            paddle.to_tensor([0.24684028327465057, 0.23846153914928436, 0.4436873495578766, 0.09491858631372452, 0.42553406953811646, 0.23994295299053192, 0.15015050768852234, 0.34077897667884827, 0.015541954897344112, 0.3689962327480316, 0.013836400583386421, 0.13066792488098145, 0.30519765615463257, 0.3897935450077057, 0.3865872621536255, 0.18577496707439423, 0.38415008783340454, 0.11780145019292831, 0.19341793656349182, 0.277095764875412, 0.20430482923984528, 0.13682107627391815, 0.13379201292991638, 0.26687994599342346, 0.06561745703220367, 0.009083089418709278, 0.27761271595954895, 0.29090797901153564, 0.0416296049952507, 0.4197145104408264], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_30877f4063a08621273c9c9a5be40654(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 120, 120], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2c4aa8d4548009c40bd4e050f5ce1e4f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1, 240, 240], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.1505517065525055], dtype='float32').reshape([1]),
            paddle.to_tensor([0.06433632224798203], dtype='float32').reshape([1]),
            paddle.to_tensor([0.2167629897594452], dtype='float32').reshape([1]),
            paddle.to_tensor([0.28393837809562683], dtype='float32').reshape([1]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d2fa0b4a1683b084cb2ce8e2297fc61c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 25, 42], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7c06599f6493bcbbe7e79a56051bc716(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 32, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_eb777c0c87b0930ad8ac08a1fcc8297f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_976dfb453104b7ba3fd18b597493cf6a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 64, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3bb059612ece0d7fb3eafe9c4788596f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_42fc67e405a3a1aee052ba4751d3651a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 10, 10], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dc6e95c8341343313cd460ca93d3650a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 112, 64, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a5541b615fc18b2de60c00e5d76110f2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 112, 12, 12], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c262c6de55bf24ce174438ad7b76de0b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_94507acd165b07b6baa4d8d0246e1900(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 26], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5aa1d301a9c0bc7f66585b24d6eed571(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1376215778460b54f5746b972901a08e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.39308130741119385, 0.3851650357246399, 0.06726648658514023, 0.1410035789012909, 0.3163894712924957, 0.01713317632675171, 0.2997833490371704, 0.18314167857170105, 0.03459955379366875, 0.07682178914546967, 0.4917106032371521, 0.06963883340358734, 0.3317239284515381, 0.07800894975662231, 0.4260315001010895, 0.48632338643074036, 0.2628212571144104, 0.10019591450691223], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2571125328540802, 0.447216659784317, 0.2528782784938812, 0.14719633758068085, 0.3062252700328827, 0.11327537894248962, 0.3967239558696747, 0.2919500470161438, 0.3846164047718048, 0.09086555242538452, 0.422333300113678, 0.012964901514351368, 0.3073723316192627, 0.3575960397720337, 0.3663567304611206, 0.09246599674224854, 0.3806248605251312, 0.2944600582122803], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3275298774242401, 0.1455637365579605, 0.3895203769207001, 0.2666483223438263, 0.3337334096431732, 0.01552925631403923, 0.08361347019672394, 0.3267512917518616, 0.056185007095336914, 0.07779799401760101, 0.19438737630844116, 0.06053439527750015, 0.473771333694458, 0.39293667674064636, 0.22063955664634705, 0.47311991453170776, 0.484448105096817, 0.27120158076286316], dtype='float32').reshape([18]),
            paddle.to_tensor([0.016672374680638313, 0.4022892713546753, 0.20133846998214722, 0.04926556348800659, 0.2922534942626953, 0.2689746916294098, 0.1211465448141098, 0.05678568035364151, 0.3329178988933563, 0.4984670579433441, 0.06604894250631332, 0.46181273460388184, 0.3800124228000641, 0.204580619931221, 0.1444421112537384, 0.18637949228286743, 0.04107048362493515, 0.495671808719635], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_66cfd6cc91423ef2442b1ac2c094fccd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 44, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([44], dtype='float32', min=0, max=0.5),
            paddle.uniform([44], dtype='float32', min=0, max=0.5),
            paddle.uniform([44], dtype='float32', min=0, max=0.5),
            paddle.uniform([44], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_df92d5fa8b1039730f9cc72e40d47f4c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 32, 64], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.01861286722123623, 0.06129227578639984, 0.1847861260175705, 0.4069421887397766, 0.06160274147987366, 0.03687048330903053, 0.1420254111289978, 0.37116101384162903, 0.07834694534540176, 0.18210367858409882, 0.12521308660507202, 0.2627224326133728, 0.4945214092731476, 0.45480111241340637, 0.08551511913537979, 0.029029270634055138, 0.4652884602546692, 0.3855205774307251], dtype='float32').reshape([18]),
            paddle.to_tensor([0.43638938665390015, 0.06889112293720245, 0.4278361201286316, 0.4905347228050232, 0.3637753427028656, 0.3261263370513916, 0.2378634363412857, 0.10535769909620285, 0.05624258890748024, 0.3578338027000427, 0.10818105190992355, 0.04543526470661163, 0.002731380984187126, 0.38485217094421387, 0.36841070652008057, 0.4623250663280487, 0.2466987818479538, 0.19005410373210907], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3302915692329407, 0.09198983758687973, 0.2841376066207886, 0.22934211790561676, 0.3700653314590454, 0.40422219038009644, 0.4567388594150543, 0.025538641959428787, 0.3439006507396698, 0.04549456015229225, 0.3190058171749115, 0.2122116982936859, 0.2779161036014557, 0.001165308291092515, 0.3556552827358246, 0.15431861579418182, 0.20129390060901642, 0.1863848716020584], dtype='float32').reshape([18]),
            paddle.to_tensor([0.18456651270389557, 0.47586509585380554, 0.4005277156829834, 0.19902774691581726, 0.05429273843765259, 0.07010126113891602, 0.22199563682079315, 0.42525237798690796, 0.2575010061264038, 0.2811567783355713, 0.15067708492279053, 0.17892804741859436, 0.4736659824848175, 0.18040581047534943, 0.1400841772556305, 0.39630016684532166, 0.20896537601947784, 0.293367862701416], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8368f0a01542e4c9534fd05080b8bb71(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 4, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b111c331e9bab02e6ac6eb5ca7dea2e4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_53c03582fdbe946c81856c99c10397e5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_942df8548708224c32da3c241bf5bc94(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 64, 128], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3465673625469208, 0.13680970668792725, 0.19969511032104492, 0.2188423126935959, 0.4539971947669983, 0.3533167541027069, 0.16598352789878845, 0.3828919529914856, 0.2946104407310486, 0.36824843287467957, 0.488727331161499, 0.27544093132019043, 0.11041363328695297, 0.4337086081504822, 0.4948588013648987, 0.38965660333633423, 0.09001778066158295, 0.027933470904827118], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4577604830265045, 0.036102596670389175, 0.24581065773963928, 0.2590942978858948, 0.1911761313676834, 0.13625764846801758, 0.23897679150104523, 0.42399150133132935, 0.34186699986457825, 0.26485010981559753, 0.1359459012746811, 0.25100043416023254, 0.1651962846517563, 0.3988061249256134, 0.08222191035747528, 0.2679392993450165, 0.11328557133674622, 0.31688228249549866], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2870112359523773, 0.33697882294654846, 0.3635994791984558, 0.347215473651886, 0.15186482667922974, 0.3053606152534485, 0.23468933999538422, 0.23900581896305084, 0.37335166335105896, 0.22880175709724426, 0.2930627763271332, 0.24658533930778503, 0.4596993029117584, 0.09196770936250687, 0.19696559011936188, 0.4065895080566406, 0.17327527701854706, 0.34942877292633057], dtype='float32').reshape([18]),
            paddle.to_tensor([0.42428356409072876, 0.18154771625995636, 0.07021597772836685, 0.06535654515028, 0.2311100959777832, 0.06651366502046585, 0.3950207829475403, 0.3926814794540405, 0.35164591670036316, 0.356646329164505, 0.4522719085216522, 0.4465567469596863, 0.15980443358421326, 0.46616697311401367, 0.2498251348733902, 0.09162396937608719, 0.051333699375391006, 0.2995108962059021], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_97b47dc24d5154d982ef8cc701e9017d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d5e97d6678c93966451711f0f8081558(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_81957a704c1947d4666f67228f9b85d1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e98ca2b3f7fbfd2040a237455a4a77a6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4f09acc5f559c2944a97d0802b0d7b08(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1f9467a3dadd9f4760532f14e907fa47(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([8, 512, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_54f5a721ce9366e2ba070e62b0a00190(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_d589af777f5a8ae699611515c41499fb
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9d7466094bb60af57f138453f06ea9d3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 832, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([832], dtype='float32', min=0, max=0.5),
            paddle.uniform([832], dtype='float32', min=0, max=0.5),
            paddle.uniform([832], dtype='float32', min=0, max=0.5),
            paddle.uniform([832], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e0eeaf2e8d772fd056a048239e47d22c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 2, 4], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0a7ada57f9c23b23ae45a28c3d2bc4cc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.13707591593265533, 0.44293487071990967, 0.3043515384197235, 0.1147424653172493, 0.4487118422985077, 0.1514793336391449, 0.24159538745880127, 0.43397969007492065, 0.10947109758853912, 0.297441691160202, 0.37330371141433716, 0.2167181670665741, 0.37784650921821594, 0.1161976084113121, 0.347759872674942, 0.38314345479011536, 0.3597128093242645, 0.45272400975227356, 0.225867360830307, 0.3105744421482086], dtype='float32').reshape([20]),
            paddle.to_tensor([0.13803261518478394, 0.2646609842777252, 0.13138188421726227, 0.278872549533844, 0.16364528238773346, 0.15053318440914154, 0.42584139108657837, 0.33462071418762207, 0.2388487458229065, 0.3231761157512665, 0.1679573953151703, 0.2657823860645294, 0.21839037537574768, 0.11797325313091278, 0.16312657296657562, 0.04231033846735954, 0.26143306493759155, 0.34102025628089905, 0.1863909363746643, 0.08294440805912018], dtype='float32').reshape([20]),
            paddle.to_tensor([0.189327672123909, 0.1734403371810913, 0.25676727294921875, 0.07743307203054428, 0.32850202918052673, 0.04875439777970314, 0.3613414168357849, 0.16687838733196259, 0.29854679107666016, 0.17167265713214874, 0.2851744294166565, 0.225416362285614, 0.1091674193739891, 0.015049288049340248, 0.3440385162830353, 0.41676467657089233, 0.09730956703424454, 0.10254751145839691, 0.3586271405220032, 0.21510016918182373], dtype='float32').reshape([20]),
            paddle.to_tensor([0.004746770486235619, 0.24609598517417908, 0.010588765144348145, 0.11598549038171768, 0.45187434554100037, 0.4625120162963867, 0.3781212270259857, 0.0025688335299491882, 0.15336909890174866, 0.29179617762565613, 0.26150503754615784, 0.20645222067832947, 0.22734788060188293, 0.18483564257621765, 0.196812242269516, 0.3834110200405121, 0.2733057737350464, 0.39582258462905884, 0.11359819024801254, 0.24122478067874908], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7c768321aa2dd0365749c3c67257aa46(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a9d995cd107bf2c85f74ba83b519d991(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ea00d92eb13b92b6930b53cc3ccc47a0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c35e0df8fa6d9af4c18295029f2fd979(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 75, 75], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_69f6c4c1c483a7ba9ca4e4741b56c450(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_59076b23a8a27d9d5e37f4a25a3ff217(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 8, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c05e3af1597b4a6ae7b92f7e3e77941e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ba4b4d158db2ff434780413db06bb378(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 512], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.39214929938316345, 0.3594464957714081, 0.4813198447227478, 0.11533644050359726, 0.13455826044082642, 0.33973056077957153, 0.30327606201171875, 0.4597305655479431, 0.1495801955461502, 0.22234946489334106, 0.165420264005661, 0.19790959358215332, 0.48335665464401245, 0.31885379552841187, 0.22956052422523499, 0.49993062019348145], dtype='float32').reshape([16]),
            paddle.to_tensor([0.32040318846702576, 0.08960671722888947, 0.10727263987064362, 0.06864788383245468, 0.3226521909236908, 0.33012261986732483, 0.3174669146537781, 0.11298121511936188, 0.06683415919542313, 0.09475349634885788, 0.14040575921535492, 0.23333880305290222, 0.24551376700401306, 0.4657629132270813, 0.39151254296302795, 0.17356862127780914], dtype='float32').reshape([16]),
            paddle.to_tensor([0.23731321096420288, 0.38003280758857727, 0.05617913231253624, 0.1103973314166069, 0.1677161306142807, 0.1299157738685608, 0.08880943059921265, 0.09077572822570801, 0.4436260759830475, 0.1611553430557251, 0.27820783853530884, 0.07457795739173889, 0.09859630465507507, 0.4455343186855316, 0.19599811732769012, 0.28530409932136536], dtype='float32').reshape([16]),
            paddle.to_tensor([0.19780664145946503, 0.29858100414276123, 0.07412897050380707, 0.09176100790500641, 0.4713221788406372, 0.4532065987586975, 0.49814528226852417, 0.003768917638808489, 0.41917306184768677, 0.2431238889694214, 0.2675628066062927, 0.06520193070173264, 0.15131226181983948, 0.396956205368042, 0.19190499186515808, 0.19246166944503784], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_00526e8d39b4e12afc9f95f0f6665000(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_020f9336caf7b5133731826497d1a6a0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1376, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1376], dtype='float32', min=0, max=0.5),
            paddle.uniform([1376], dtype='float32', min=0, max=0.5),
            paddle.uniform([1376], dtype='float32', min=0, max=0.5),
            paddle.uniform([1376], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9279f9ba48147242c8ed3e91414bbaa7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.030884722247719765, 0.2393377423286438, 0.42497900128364563, 0.3479892611503601, 0.2728855311870575, 0.24716168642044067, 0.00022784416796639562, 0.08363869041204453, 0.2793479263782501, 0.1579444706439972, 0.031792815774679184, 0.008685312233865261, 0.26518216729164124, 0.34543517231941223, 0.13625316321849823, 0.06528738886117935, 0.003915745299309492, 0.38148102164268494, 0.014181454665958881, 0.06795753538608551, 0.31125882267951965, 0.3632400333881378, 0.051435064524412155, 0.08916958421468735], dtype='float32').reshape([24]),
            paddle.to_tensor([0.25788864493370056, 0.09776932746171951, 0.22397251427173615, 0.2905374765396118, 0.15335556864738464, 0.34893420338630676, 0.1345701664686203, 0.26660609245300293, 0.39976438879966736, 0.4311007559299469, 0.3301750719547272, 0.3160299062728882, 0.24617695808410645, 0.32995104789733887, 0.11461113393306732, 0.18234366178512573, 0.1458568572998047, 0.48506370186805725, 0.2645436227321625, 0.2895684838294983, 0.22289299964904785, 0.0433487668633461, 0.46085992455482483, 0.087775319814682], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3374248743057251, 0.05047829821705818, 0.2050737887620926, 0.2328028827905655, 0.3232369124889374, 0.2524799704551697, 0.14385420083999634, 0.35974904894828796, 0.4103699326515198, 0.41772693395614624, 0.012377438135445118, 0.29639932513237, 0.3818318247795105, 0.403914213180542, 0.08843392133712769, 0.34201791882514954, 0.20612771809101105, 0.018876919522881508, 0.30092543363571167, 0.4773939251899719, 0.28398647904396057, 0.11477909237146378, 0.002084968378767371, 0.28351274132728577], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4508911967277527, 0.43468305468559265, 0.4685741662979126, 0.23308274149894714, 0.07877255231142044, 0.0008829745347611606, 0.07792098820209503, 0.4831254780292511, 0.3685145080089569, 0.4340665936470032, 0.3413310647010803, 0.31385403871536255, 0.062335167080163956, 0.020749298855662346, 0.4067462086677551, 0.26367366313934326, 0.18142656981945038, 0.01626923307776451, 0.4360088109970093, 0.35449597239494324, 0.261749267578125, 0.08555418998003006, 0.11330481618642807, 0.07956396788358688], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bf6cefba5706ce9b0c25fe1a9db9d11e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.0250885970890522, 0.17063434422016144, 0.1792716681957245, 0.3486851155757904, 0.46823421120643616, 0.15412916243076324, 0.02050413191318512, 0.4577804207801819, 0.18813800811767578, 0.06938431411981583, 0.2767624855041504, 0.29455018043518066, 0.4360750615596771, 0.49071505665779114, 0.49436813592910767, 0.05126649886369705, 0.12274076789617538, 0.3343523442745209, 0.2707573473453522, 0.4539303779602051, 0.373209148645401, 0.44869405031204224, 0.46276530623435974, 0.35620322823524475, 0.07931891828775406, 0.31152838468551636, 0.3838631510734558, 0.27955275774002075, 0.33844295144081116, 0.27990561723709106], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2240513265132904, 0.4650317132472992, 0.017656315118074417, 0.28710800409317017, 0.12171670794487, 0.3639732003211975, 0.3509770631790161, 0.07302487641572952, 0.2781778872013092, 0.07166912406682968, 0.4267059862613678, 0.3065461218357086, 0.4594753384590149, 0.14453749358654022, 0.1506614238023758, 0.15631012618541718, 0.10900500416755676, 0.22646287083625793, 0.09526051580905914, 0.04806467890739441, 0.15746508538722992, 0.4525190591812134, 0.4371066093444824, 0.41960862278938293, 0.4023919999599457, 0.30733978748321533, 0.2611405551433563, 0.2596716284751892, 0.26408228278160095, 0.16115985810756683], dtype='float32').reshape([30]),
            paddle.to_tensor([0.24710138142108917, 0.14745567739009857, 0.09230180829763412, 0.1615854799747467, 0.4772682189941406, 0.1302901655435562, 0.3762889802455902, 0.454544335603714, 0.22864322364330292, 0.41866281628608704, 0.19092383980751038, 0.08673983067274094, 0.22664779424667358, 0.3436826467514038, 0.2620433270931244, 0.04488435760140419, 0.18652360141277313, 0.06997697055339813, 0.2796883285045624, 0.3103947937488556, 0.22743648290634155, 0.19805699586868286, 0.08033385127782822, 0.07413804531097412, 0.056471433490514755, 0.2008267045021057, 0.35537341237068176, 0.010687867179512978, 0.24225561320781708, 0.26535508036613464], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3203561007976532, 0.1378270387649536, 0.40096020698547363, 0.3029840290546417, 0.4584178626537323, 0.043010007590055466, 0.03583699092268944, 0.02247752994298935, 0.1779813915491104, 0.4753474295139313, 0.38078242540359497, 0.4696238338947296, 0.36932697892189026, 0.2592746317386627, 0.05411156266927719, 0.04824009910225868, 0.3939889073371887, 0.059769582003355026, 0.34764519333839417, 0.08161678165197372, 0.4021344780921936, 0.3490123152732849, 0.09649989753961563, 0.28373953700065613, 0.012061739340424538, 0.1820734441280365, 0.3489634394645691, 0.07576481997966766, 0.036529477685689926, 0.04697128012776375], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e47bd296a1aa7690394e4a17e574a3b8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 38, 38], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c6bb5ff6fbadffe1bc7cbbdb76745b5a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 88, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9a35ee208a6d0d28e7e95bea9ba909d4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_db3307622c8de0068b4d8e56e0f4fe68(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3071aa822b1dff4b170591b363c6e2a8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 96, 96], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1e5c88dec7d89b4695b3eb421020eaeb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 64, 128], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3831917345523834, 0.07055261731147766, 0.2688360810279846, 0.43359512090682983, 0.2637661397457123, 0.12578517198562622, 0.03243587166070938, 0.24252375960350037, 0.13654901087284088, 0.44170698523521423, 0.10880371928215027, 0.03639949485659599, 0.37176135182380676, 0.41036346554756165, 0.45468953251838684, 0.49671080708503723, 0.49007242918014526, 0.22905051708221436], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4159727692604065, 0.11642132699489594, 0.24810871481895447, 0.20997537672519684, 0.230453222990036, 0.4950529932975769, 0.01737549714744091, 0.2659771740436554, 0.2938610911369324, 0.4494633376598358, 0.35959309339523315, 0.30038386583328247, 0.17554590106010437, 0.20173725485801697, 0.36967191100120544, 0.15952569246292114, 0.06167355552315712, 0.20429538190364838], dtype='float32').reshape([18]),
            paddle.to_tensor([0.028340119868516922, 0.29745587706565857, 0.10750014334917068, 0.4805249571800232, 0.042491670697927475, 0.09117880463600159, 0.46217790246009827, 0.4502519965171814, 0.16764990985393524, 0.21377667784690857, 0.103253573179245, 0.1559654325246811, 0.09087245911359787, 0.09032649546861649, 0.08793218433856964, 0.0583522766828537, 0.24819518625736237, 0.32860586047172546], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3922678828239441, 0.46455445885658264, 0.2772578001022339, 0.3875112533569336, 0.31569039821624756, 0.4259529411792755, 0.3191532492637634, 0.3838101327419281, 0.3449871838092804, 0.03536316752433777, 0.42329639196395874, 0.49535930156707764, 0.1129058226943016, 0.19623450934886932, 0.27191779017448425, 0.45682063698768616, 0.09797481447458267, 0.048108089715242386], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b114937a1009f2830939d865db725b1d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d74f2428d5aaca700697e625a07b10e2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.1078769639134407, 0.4838927984237671, 0.19310419261455536, 0.09208843111991882, 0.3589269816875458, 0.45035919547080994, 0.16936932504177094, 0.13733108341693878, 0.14607694745063782, 0.05366242304444313, 0.23013842105865479, 0.3532971739768982, 0.09382326900959015, 0.3055208921432495, 0.4280402362346649, 0.3357515335083008, 0.10043830424547195, 0.2266940474510193], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4867355525493622, 0.015011466108262539, 0.10610607266426086, 0.34008705615997314, 0.2948741316795349, 0.3387153148651123, 0.3591177761554718, 0.25861385464668274, 0.013328596949577332, 0.06321363151073456, 0.3634357154369354, 0.30599138140678406, 0.3066917955875397, 0.045359812676906586, 0.2272864580154419, 0.23500612378120422, 0.027487680315971375, 0.37311822175979614], dtype='float32').reshape([18]),
            paddle.to_tensor([0.22378908097743988, 0.2553390562534332, 0.17085041105747223, 0.23790785670280457, 0.4472845792770386, 0.45951107144355774, 0.48695290088653564, 0.24707932770252228, 0.26736634969711304, 0.22604893147945404, 0.016041357070207596, 0.10302070528268814, 0.1641441136598587, 0.32178056240081787, 0.3235184848308563, 0.008861803449690342, 0.2235068380832672, 0.16274763643741608], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2680957317352295, 0.22574076056480408, 0.3709506094455719, 0.10146523267030716, 0.47194984555244446, 0.3800457715988159, 0.2527022957801819, 0.22531463205814362, 0.016364548355340958, 0.29596078395843506, 0.2328866720199585, 0.2255753129720688, 0.4758599102497101, 0.4918816387653351, 0.43012621998786926, 0.15784387290477753, 0.11200643330812454, 0.18973308801651], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_39efe9e3488ffdc7984afef5584797fa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1440, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1440], dtype='float32', min=0, max=0.5),
            paddle.uniform([1440], dtype='float32', min=0, max=0.5),
            paddle.uniform([1440], dtype='float32', min=0, max=0.5),
            paddle.uniform([1440], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ef8383315e9fd5b39a2fc2ff184c0d51(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_306777e574f3f93e58a616168828cd4d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.005974784959107637, 0.4698679447174072, 0.2644193768501282, 0.4917706847190857, 0.2553942799568176, 0.07615014165639877, 0.4628480076789856, 0.3447533845901489, 0.11384176462888718, 0.4366420805454254, 0.004211827646940947, 0.28518879413604736, 0.21296975016593933, 0.4543359875679016, 0.38263189792633057, 0.0547090619802475, 0.368645042181015, 0.42672574520111084], dtype='float32').reshape([18]),
            paddle.to_tensor([0.19420845806598663, 0.29365986585617065, 0.26353007555007935, 0.38629618287086487, 0.029330283403396606, 0.133022740483284, 0.291999876499176, 0.43710824847221375, 0.09178312867879868, 0.11994489282369614, 0.45500433444976807, 0.03394070267677307, 0.06760703027248383, 0.2961203157901764, 0.37926819920539856, 0.08062024414539337, 0.16214217245578766, 0.23175247013568878], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3798956274986267, 0.3354668617248535, 0.3716161847114563, 0.2402869462966919, 0.022846903651952744, 0.08653979748487473, 0.35387080907821655, 0.3678523302078247, 0.3976112902164459, 0.050812240689992905, 0.2898218631744385, 0.07820799201726913, 0.3094465732574463, 0.05991654843091965, 0.16388238966464996, 0.201656773686409, 0.06865309178829193, 0.08356378972530365], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3628210425376892, 0.26485010981559753, 0.23259912431240082, 0.2597390413284302, 0.385273814201355, 0.190979465842247, 0.06908366829156876, 0.35802263021469116, 0.12293271720409393, 0.24881798028945923, 0.08878835290670395, 0.1679372638463974, 0.10883749276399612, 0.0970173254609108, 0.31754404306411743, 0.21201765537261963, 0.4086337089538574, 0.05268383398652077], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8e438b7c61d7db11454833157bac4a34(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 240, 240], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.1340763419866562, 0.34638527035713196, 0.46409618854522705, 0.37523847818374634, 0.2817813754081726, 0.3755131661891937, 0.1019979938864708, 0.4415130913257599], dtype='float32').reshape([8]),
            paddle.to_tensor([0.1976177841424942, 0.2487218976020813, 0.42398494482040405, 0.17462743818759918, 0.3651624321937561, 0.3529103994369507, 0.2313307821750641, 0.08947482705116272], dtype='float32').reshape([8]),
            paddle.to_tensor([0.3635404407978058, 0.4749946594238281, 0.2909347414970398, 0.4460679292678833, 0.27201300859451294, 0.3150060474872589, 0.11461541801691055, 0.3350023031234741], dtype='float32').reshape([8]),
            paddle.to_tensor([0.26951053738594055, 0.2025785595178604, 0.4549993574619293, 0.008032753132283688, 0.1397523730993271, 0.21552857756614685, 0.44805169105529785, 0.08512768894433975], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_80618ec09d85a360ca4bb9a6cdd5ddd5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 1536, 10, 10], dtype='float16', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_83c05042ed51ebfca06ef86feace4333(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 8, 25], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_610ce5b56b1d3f7c5bdbb6dd3f2b9c52(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3c7de9a2e7d41a4f6ae81e6c61f3459c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9b6b114530c77deed384d4704507aebc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4090040326118469, 0.2939794659614563, 0.42589229345321655, 0.10020535439252853, 0.46299365162849426, 0.4367101788520813, 0.38614559173583984, 0.03914061188697815, 0.13243699073791504, 0.07366253435611725, 0.4722416400909424, 0.3687119483947754, 0.3123621344566345, 0.08874420821666718, 0.3478107750415802, 0.4038470983505249, 0.028865808621048927, 0.11149980872869492, 0.1309080272912979, 0.004648692440241575, 0.13543157279491425, 0.19580000638961792, 0.38171058893203735, 0.48342519998550415, 0.406567245721817, 0.36009952425956726, 0.09841692447662354, 0.4860353171825409], dtype='float32').reshape([28]),
            paddle.to_tensor([0.3012494146823883, 0.12798067927360535, 0.3464169204235077, 0.373947411775589, 0.0006085341447032988, 0.27421605587005615, 0.29128873348236084, 0.4004787802696228, 0.2237284779548645, 0.3187437951564789, 0.49118152260780334, 0.21357977390289307, 0.1301323026418686, 0.024311602115631104, 0.3885400891304016, 0.26518315076828003, 0.018867336213588715, 0.022284284234046936, 0.2199830263853073, 0.03989200294017792, 0.13832131028175354, 0.2471599131822586, 0.24818718433380127, 0.30599549412727356, 0.28596049547195435, 0.09221221506595612, 0.43527576327323914, 0.25650259852409363], dtype='float32').reshape([28]),
            paddle.to_tensor([0.1359727531671524, 0.4061409831047058, 0.15487562119960785, 0.4130313992500305, 0.3093487620353699, 0.1694260537624359, 0.21230089664459229, 0.11470603942871094, 0.12925587594509125, 0.33315509557724, 0.24340273439884186, 0.40947481989860535, 0.27112308144569397, 0.178958922624588, 0.05943300202488899, 0.46525540947914124, 0.37075117230415344, 0.4940292537212372, 0.059574004262685776, 0.01505843922495842, 0.12880022823810577, 0.4210871160030365, 0.12434536218643188, 0.002875426784157753, 0.3395463824272156, 0.41612815856933594, 0.19586187601089478, 0.38899755477905273], dtype='float32').reshape([28]),
            paddle.to_tensor([0.16821549832820892, 0.17432714998722076, 0.19499453902244568, 0.28341951966285706, 0.4901637136936188, 0.31354957818984985, 0.28407880663871765, 0.10283910483121872, 0.4069986641407013, 0.07185361534357071, 0.3632122278213501, 0.14442142844200134, 0.20822229981422424, 0.13984620571136475, 0.4260112941265106, 0.4254988133907318, 0.05249966308474541, 0.3785189092159271, 0.4749225974082947, 0.18815253674983978, 0.1524294763803482, 0.13583703339099884, 0.3684752583503723, 0.2993733286857605, 0.3221757113933563, 0.2330058217048645, 0.3413548171520233, 0.22123152017593384], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b5a171b25df2806c897223ff75e90d2a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.36884596943855286, 0.1518525332212448, 0.36065736413002014, 0.31697478890419006, 0.07937367260456085, 0.0050560301169753075, 0.25927817821502686, 0.17385776340961456, 0.29344940185546875, 0.3377630412578583, 0.27297207713127136, 0.19266557693481445, 0.34383243322372437, 0.3720872402191162, 0.006071018986403942, 0.3844073712825775, 0.4344576299190521, 0.3878369927406311, 0.46246322989463806, 0.428741455078125], dtype='float32').reshape([20]),
            paddle.to_tensor([0.39559227228164673, 0.1890021711587906, 0.1843479871749878, 0.08259182423353195, 0.1603001207113266, 0.41721564531326294, 0.3587227463722229, 0.2017780989408493, 0.46447038650512695, 0.1353478878736496, 0.04370727017521858, 0.026402432471513748, 0.2460481971502304, 0.2051621824502945, 0.3393761217594147, 0.4273170530796051, 0.39134693145751953, 0.06751443445682526, 0.3410724997520447, 0.49342650175094604], dtype='float32').reshape([20]),
            paddle.to_tensor([0.10002201795578003, 0.05625469982624054, 0.29181745648384094, 0.37689587473869324, 0.24867846071720123, 0.3557720184326172, 0.2134891152381897, 0.15690749883651733, 0.21576350927352905, 0.07918050140142441, 0.04146018996834755, 0.4603489637374878, 0.33069518208503723, 0.306796133518219, 0.20468157529830933, 0.15673519670963287, 0.01366919931024313, 0.22521954774856567, 0.2001761794090271, 0.3172491490840912], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3580051362514496, 0.15374568104743958, 0.0937681794166565, 0.06003996357321739, 0.41597506403923035, 0.04185151308774948, 0.17295588552951813, 0.46761396527290344, 0.10482180863618851, 0.22552122175693512, 0.2152586430311203, 0.22211259603500366, 0.10372814536094666, 0.1779801994562149, 0.4915560185909271, 0.3091816008090973, 0.23583249747753143, 0.3353734314441681, 0.4679321348667145, 0.002953418530523777], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1557ec0894a2f8cfb08dfd5c77714928(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 26], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d912de4db965a038a168cc4eb11b4643(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.1703857183456421, 0.4396023750305176, 0.4911375641822815, 0.06929639726877213, 0.0013265653979033232, 0.4388246536254883, 0.2106117606163025, 0.19761040806770325, 0.2252865731716156, 0.32115238904953003, 0.24804279208183289, 0.08602054417133331, 0.38409847021102905, 0.30528560280799866, 0.23710188269615173, 0.08659715950489044, 0.31153637170791626, 0.12882710993289948, 0.45916062593460083, 0.47000423073768616, 0.39340850710868835, 0.27833136916160583, 0.07153055816888809, 0.34584200382232666, 0.12421388179063797, 0.24753142893314362, 0.07257993519306183, 0.04128805547952652, 0.1257386952638626, 0.33538323640823364], dtype='float32').reshape([30]),
            paddle.to_tensor([0.47476887702941895, 0.36683326959609985, 0.17947469651699066, 0.4213249981403351, 0.3756920099258423, 0.22343966364860535, 0.1771218627691269, 0.3794228136539459, 0.028520256280899048, 0.23862046003341675, 0.03350365534424782, 0.19729499518871307, 0.41408678889274597, 0.30286121368408203, 0.3345928490161896, 0.02383090928196907, 0.05701114609837532, 0.3947072923183441, 0.3700381815433502, 0.02684881165623665, 0.496652752161026, 0.24806074798107147, 0.2903931140899658, 0.24147017300128937, 0.050657738000154495, 0.12481570988893509, 0.19985760748386383, 0.4839712083339691, 0.2829592227935791, 0.21141399443149567], dtype='float32').reshape([30]),
            paddle.to_tensor([0.0639118179678917, 0.3977820873260498, 0.36088716983795166, 0.28597232699394226, 0.3132413923740387, 0.08775462210178375, 0.37059712409973145, 0.4774170219898224, 0.2984548509120941, 0.21950982511043549, 0.1888287216424942, 0.46717363595962524, 0.40195536613464355, 0.1666070967912674, 0.0794125497341156, 0.40761253237724304, 0.07365400344133377, 0.25695765018463135, 0.22612087428569794, 0.36035212874412537, 0.03621947392821312, 0.2558326721191406, 0.23729999363422394, 0.4462798535823822, 0.3317427635192871, 0.28897684812545776, 0.2554636001586914, 0.40132173895835876, 0.4312381446361542, 0.35705968737602234], dtype='float32').reshape([30]),
            paddle.to_tensor([0.27050918340682983, 0.2837984263896942, 0.07815680652856827, 0.12821811437606812, 0.2656894326210022, 0.29089874029159546, 0.13143619894981384, 0.1980290561914444, 0.1647133082151413, 0.44316571950912476, 0.38349878787994385, 0.35733562707901, 0.2989363968372345, 0.25840333104133606, 0.01160349790006876, 0.3538699150085449, 0.37696385383605957, 0.14843513071537018, 0.47294190526008606, 0.36729249358177185, 0.3328419327735901, 0.1244218572974205, 0.21774983406066895, 0.06975409388542175, 0.242746040225029, 0.4477783739566803, 0.3454380929470062, 0.4332228899002075, 0.324626088142395, 0.30435365438461304], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_faece81c88b8cb5cfee46d23390eb268(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1287766f5c24569daf2f86fcaec33208(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([8, 256, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_74e000e38627cc34c4e978f09456cf34(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2548147141933441, 0.012710467912256718, 0.4910750687122345, 0.4952603578567505, 0.054169878363609314, 0.1660088449716568, 0.3795929253101349, 0.028434179723262787, 0.34558436274528503, 0.3672962784767151, 0.2277841866016388, 0.02884959615767002, 0.12013016641139984, 0.4191443920135498, 0.42847785353660583, 0.2777475416660309, 0.3294813930988312, 0.33522462844848633, 0.4375913441181183, 0.3966083526611328, 0.46633610129356384, 0.3465999662876129, 0.3934471905231476, 0.3931828737258911, 0.4824756681919098, 0.05831262096762657, 0.37365084886550903, 0.3535197675228119, 0.1506386697292328, 0.46285301446914673], dtype='float32').reshape([30]),
            paddle.to_tensor([0.40563511848449707, 0.41896384954452515, 0.010709880851209164, 0.34229499101638794, 0.4684925973415375, 0.034179408103227615, 0.39481914043426514, 0.4451865553855896, 0.21441172063350677, 0.3654426336288452, 0.2940443456172943, 0.32120242714881897, 0.03161826729774475, 0.2588537931442261, 0.23151524364948273, 0.23647402226924896, 0.30405110120773315, 0.01323649100959301, 0.2774530053138733, 0.30544987320899963, 0.30181610584259033, 0.13321134448051453, 0.014124415814876556, 0.4818980097770691, 0.2979309558868408, 0.13036057353019714, 0.11606097966432571, 0.4086732268333435, 0.29462912678718567, 0.40733882784843445], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4348633885383606, 0.2627350389957428, 0.10623917728662491, 0.2548403739929199, 0.11457087844610214, 0.058244895190000534, 0.1834399551153183, 0.49124205112457275, 0.18841663002967834, 0.09932871907949448, 0.18162164092063904, 0.40494614839553833, 0.12017117440700531, 0.3601744472980499, 0.36076998710632324, 0.28193849325180054, 0.33403027057647705, 0.30188310146331787, 0.3173603117465973, 0.1487966775894165, 0.3084728419780731, 0.33131077885627747, 0.3261318802833557, 0.4458235502243042, 0.10541165620088577, 0.28717949986457825, 0.216405987739563, 0.06139473244547844, 0.3042645752429962, 0.46877315640449524], dtype='float32').reshape([30]),
            paddle.to_tensor([0.007540556136518717, 0.2910025417804718, 0.24132606387138367, 0.009503648616373539, 0.24277323484420776, 0.02721036598086357, 0.2479081153869629, 0.4483112692832947, 0.33096954226493835, 0.058181505650281906, 0.4315139353275299, 0.15949088335037231, 0.3000459372997284, 0.08442772179841995, 0.12566302716732025, 0.4409332275390625, 0.38923346996307373, 0.17797178030014038, 0.16673524677753448, 0.013627558946609497, 0.18376527726650238, 0.07658422738313675, 0.013505743816494942, 0.05192980915307999, 0.3256150186061859, 0.14171959459781647, 0.49149924516677856, 0.45577603578567505, 0.4844110608100891, 0.4140189290046692], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_65ca09aca286417b1e8ac6e47b1310c4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.09536547213792801, 0.4407323896884918, 0.2963758409023285, 0.48801854252815247, 0.2873130738735199, 0.4171900451183319, 0.13360831141471863, 0.35771477222442627, 0.10831326991319656, 0.38587531447410583, 0.15455086529254913, 0.4237043857574463, 0.02122020162642002, 0.49501264095306396, 0.15922710299491882, 0.3410111963748932, 0.03577248752117157, 0.4376169443130493, 0.23869141936302185, 0.4479224383831024, 0.2544530928134918, 0.47121661901474, 0.46716567873954773, 0.1316900998353958], dtype='float32').reshape([24]),
            paddle.to_tensor([0.48631736636161804, 0.03336355835199356, 0.44704774022102356, 0.07083512097597122, 0.3804100453853607, 0.2293376326560974, 0.11582037061452866, 0.11331837624311447, 0.1659371554851532, 0.08491832762956619, 0.1542922705411911, 0.05676194652915001, 0.07772853225469589, 0.2106488049030304, 0.18016968667507172, 0.28612011671066284, 0.13111595809459686, 0.4971495270729065, 0.20277553796768188, 0.03594398498535156, 0.05039839819073677, 0.2103869915008545, 0.32673004269599915, 0.3277328312397003], dtype='float32').reshape([24]),
            paddle.to_tensor([0.08158288896083832, 0.26395440101623535, 0.28913071751594543, 0.4471282660961151, 0.4663754403591156, 0.37189793586730957, 0.06428244709968567, 0.44757890701293945, 0.31029197573661804, 0.41695597767829895, 0.29759183526039124, 0.35821592807769775, 0.046508122235536575, 0.47485047578811646, 0.4499585032463074, 0.4689488112926483, 0.06635169684886932, 0.2924721837043762, 0.3033267855644226, 0.023866353556513786, 0.02899315021932125, 0.49145784974098206, 0.1885400116443634, 0.18540218472480774], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2837512195110321, 0.19629241526126862, 0.10055053979158401, 0.35677337646484375, 0.06671871989965439, 0.2944380044937134, 0.1393609195947647, 0.45288950204849243, 0.42634448409080505, 0.38883715867996216, 0.4254937171936035, 0.1646386831998825, 0.05723007768392563, 0.1587122529745102, 0.4894990921020508, 0.33928874135017395, 0.039526376873254776, 0.4278312027454376, 0.049066752195358276, 0.02174113318324089, 0.1807643175125122, 0.32795408368110657, 0.19914202392101288, 0.3464398980140686], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_22e16a2388ae9fc6ae9cde5950dab568(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 17, 17], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f458041cc897cb31e866812589ef6e82(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 4, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4c7d79093a6b598f47ee1b3be6842cd4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 240, 240], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_41dde5624aaa94a6e01c001574e93b08(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 240, 240], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f7ec1b8b617f931c748bda2efd7d2d11(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 128, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_74148127b6c5a0a0636ff67c3d86b379(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.31811708211898804, 0.38762858510017395, 0.34690526127815247, 0.473213791847229, 0.1624554991722107, 0.11131930351257324, 0.23219360411167145, 0.016482403501868248, 0.10208222270011902, 0.14555485546588898, 0.4506887197494507, 0.10968577116727829, 0.32027095556259155, 0.49275529384613037, 0.38624265789985657, 0.17153902351856232, 0.49649855494499207, 0.2756552994251251, 0.06660515815019608, 0.27574974298477173, 0.39336174726486206, 0.42536553740501404, 0.36715462803840637, 0.3417810797691345], dtype='float32').reshape([24]),
            paddle.to_tensor([0.014179875142872334, 0.08167776465415955, 0.2742747366428375, 0.30015748739242554, 0.13385872542858124, 0.24330785870552063, 0.45117664337158203, 0.26582106947898865, 0.02410781756043434, 0.20805761218070984, 0.2723022699356079, 0.40328526496887207, 0.3778410851955414, 0.3540126085281372, 0.27624791860580444, 0.28212714195251465, 0.4809001386165619, 0.30390962958335876, 0.1961396187543869, 0.3026995360851288, 0.28319934010505676, 0.006468881852924824, 0.4248034358024597, 0.16901607811450958], dtype='float32').reshape([24]),
            paddle.to_tensor([0.16535499691963196, 0.1580876260995865, 0.4887411594390869, 0.24240237474441528, 0.39702412486076355, 0.0010716295801103115, 0.3529900312423706, 0.06294535100460052, 0.44983944296836853, 0.2884150743484497, 0.41576650738716125, 0.3557601273059845, 0.3367170989513397, 0.016831649467349052, 0.17646799981594086, 0.12154728174209595, 0.1889319270849228, 0.07234591990709305, 0.4942794144153595, 0.1709814965724945, 0.4014563262462616, 0.2578737139701843, 0.11921533942222595, 0.4653019607067108], dtype='float32').reshape([24]),
            paddle.to_tensor([0.0207980964332819, 0.09146876633167267, 0.06574182957410812, 0.28664129972457886, 0.17115066945552826, 0.11474236100912094, 0.17948518693447113, 0.07779235392808914, 0.3759430944919586, 0.24199256300926208, 0.021443059667944908, 0.486553817987442, 0.13868243992328644, 0.3568880558013916, 0.36380264163017273, 0.49792709946632385, 0.11891033500432968, 0.009797004982829094, 0.4177776277065277, 0.0713437870144844, 0.07168837636709213, 0.3985244631767273, 0.3466508388519287, 0.3437962234020233], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6605aba3bb75e60f52dac68b721def15(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 4, 50], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.29857349395751953, 0.23374617099761963, 0.22838521003723145, 0.2844778001308441, 0.25163355469703674, 0.4947919547557831, 0.3518318831920624, 0.37745481729507446, 0.479526549577713, 0.2673197388648987, 0.04040111228823662, 0.4470755159854889, 0.48967528343200684, 0.04247092455625534, 0.4760277271270752, 0.4417939782142639, 0.11345038563013077, 0.1477535367012024, 0.08920340985059738, 0.10622753947973251, 0.40110838413238525, 0.4668017625808716, 0.1028636172413826, 0.27170100808143616], dtype='float32').reshape([24]),
            paddle.to_tensor([0.37880611419677734, 0.116279736161232, 0.42437243461608887, 0.3740521967411041, 0.15136635303497314, 0.14553974568843842, 0.042197905480861664, 0.12186162173748016, 0.26733583211898804, 0.3404671847820282, 0.09246227890253067, 0.2566078007221222, 0.21037448942661285, 0.44054147601127625, 0.19326679408550262, 0.24942165613174438, 0.4511621296405792, 0.3587498664855957, 0.3243728578090668, 0.37156885862350464, 0.3750477433204651, 0.15938147902488708, 0.026090821251273155, 0.4391743838787079], dtype='float32').reshape([24]),
            paddle.to_tensor([0.46600306034088135, 0.2946198582649231, 0.24183060228824615, 0.23793643712997437, 0.3847307562828064, 0.246955007314682, 0.26860663294792175, 0.38314521312713623, 0.19408372044563293, 0.019012102857232094, 0.45657557249069214, 0.4755716323852539, 0.19428837299346924, 0.2612937390804291, 0.32188618183135986, 0.030655913054943085, 0.36451104283332825, 0.4360639452934265, 0.35643982887268066, 0.466244101524353, 0.48726481199264526, 0.2613546848297119, 0.18863624334335327, 0.382032573223114], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4169256389141083, 0.33192503452301025, 0.33672621846199036, 0.2937155067920685, 0.4524087607860565, 0.40736132860183716, 0.37735751271247864, 0.40945518016815186, 0.05892457813024521, 0.17346128821372986, 0.2869941294193268, 0.33793094754219055, 0.41278544068336487, 0.3919565975666046, 0.31091946363449097, 0.052617136389017105, 0.39974960684776306, 0.23038510978221893, 0.06602378934621811, 0.422370970249176, 0.1696670800447464, 0.3700026571750641, 0.12346132844686508, 0.10495785623788834], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_629b95b31b619d40e03ec4883e073e11(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.27934613823890686, 0.07273554056882858, 0.2701297700405121, 0.23172833025455475, 0.08709453791379929, 0.4467584490776062, 0.35906824469566345, 0.2684991955757141, 0.04704941064119339, 0.06346981972455978, 0.16008014976978302, 0.02355876751244068, 0.3640036880970001, 0.4378910958766937, 0.03490237146615982, 0.3226408362388611, 0.0836329311132431, 0.08512488007545471, 0.3508150577545166, 0.26572149991989136, 0.22230248153209686, 0.229015052318573, 0.3759276866912842, 0.40503403544425964], dtype='float32').reshape([24]),
            paddle.to_tensor([0.014038033783435822, 0.3019331097602844, 0.2375859022140503, 0.045446448028087616, 0.0640474408864975, 0.22496956586837769, 0.21507227420806885, 0.45839059352874756, 0.4365460276603699, 0.44618940353393555, 0.16086556017398834, 0.3011915683746338, 0.09152603894472122, 0.23284462094306946, 0.1687520146369934, 0.14278574287891388, 0.43443799018859863, 0.3186870813369751, 0.014387419447302818, 0.14368721842765808, 0.21996523439884186, 0.42991483211517334, 0.11221744865179062, 0.38257673382759094], dtype='float32').reshape([24]),
            paddle.to_tensor([0.18778197467327118, 0.31135109066963196, 0.4342091679573059, 0.18657267093658447, 0.23969562351703644, 0.16539709270000458, 0.4155815541744232, 0.3951112926006317, 0.2597498893737793, 0.19301071763038635, 0.45742544531822205, 0.07545319199562073, 0.3628886044025421, 0.3833385407924652, 0.04931945726275444, 0.31964680552482605, 0.11239194869995117, 0.21904215216636658, 0.43007904291152954, 0.07044564187526703, 0.2541618347167969, 0.4036489427089691, 0.3177422881126404, 0.4665376543998718], dtype='float32').reshape([24]),
            paddle.to_tensor([0.07046601176261902, 0.31924018263816833, 0.4408905804157257, 0.23398324847221375, 0.3262757360935211, 0.4865095317363739, 0.30116426944732666, 0.20657704770565033, 0.40085864067077637, 0.4212070107460022, 0.21194620430469513, 0.4830240309238434, 0.21332551538944244, 0.18275736272335052, 0.4459232985973358, 0.1613743007183075, 0.29875946044921875, 0.42769163846969604, 0.08195427060127258, 0.4789966642856598, 0.20817629992961884, 0.49981629848480225, 0.11641471832990646, 0.16744831204414368], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_01d361c32a0ccc44897e6a1c3387465a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_83a513c035ef0f9d35a3e2b69487e779(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 25, 42], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_76fab50cdf5245bdbced2ef6c5022d95(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 8, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_17035d965451bf808aa250a904b34488(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 31, 31], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6315e2bc69c8f5a09a73703b83d134c3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 16, 50], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.11625514179468155, 0.22508828341960907, 0.05429867282509804, 0.2799451947212219, 0.14504474401474, 0.4943857491016388, 0.41173094511032104, 0.10880490392446518], dtype='float32').reshape([8]),
            paddle.to_tensor([0.28211820125579834, 0.2980429530143738, 0.09280365705490112, 0.2643878757953644, 0.2530843913555145, 0.28239917755126953, 0.33673763275146484, 0.4563143849372864], dtype='float32').reshape([8]),
            paddle.to_tensor([0.06611432880163193, 0.35044360160827637, 0.03580035641789436, 0.4961563050746918, 0.020749375224113464, 0.061072591692209244, 0.22952619194984436, 0.43115878105163574], dtype='float32').reshape([8]),
            paddle.to_tensor([0.47699034214019775, 0.357571005821228, 0.2798372507095337, 0.07030083239078522, 0.3999105393886566, 0.09199654310941696, 0.42163223028182983, 0.03679531067609787], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7270d05b9878aa316cc03e10379371e4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4949aff03c1cadda0d343bc73d7e294c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6af8b11ee273fec5fb88a57b890becde(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 56, 100], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_801c3bd15ee9bc04c8894b70cf10ec01(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 128, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6e6945ed562776cb8cb49b68bfd721fb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 100], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c245d50eed9a9621e15b577f2b43f4d1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3509295880794525, 0.13257892429828644, 0.334819495677948, 0.03175622969865799, 0.3108804523944855, 0.3444502651691437, 0.02757306396961212, 0.26872655749320984, 0.15321281552314758, 0.4520026743412018, 0.13659149408340454, 0.4466424584388733, 0.25931045413017273, 0.1643560826778412, 0.3030281960964203, 0.23713389039039612, 0.43074488639831543, 0.20052437484264374, 0.22977952659130096, 0.06264027208089828, 0.27555498480796814, 0.17660124599933624, 0.17324848473072052, 0.19450822472572327, 0.4322766363620758, 0.06517180055379868, 0.01702982746064663, 0.0907268300652504, 0.05391859635710716, 0.0024019444826990366], dtype='float32').reshape([30]),
            paddle.to_tensor([0.16447553038597107, 0.4809444546699524, 0.12821166217327118, 0.3147328794002533, 0.010546828620135784, 0.45225730538368225, 0.45840200781822205, 0.13486015796661377, 0.23686756193637848, 0.4765041172504425, 0.04041581600904465, 0.1836993545293808, 0.3501952290534973, 0.28235870599746704, 0.21927666664123535, 0.31145864725112915, 0.030498405918478966, 0.23352116346359253, 0.07460762560367584, 0.031027821823954582, 0.1371898651123047, 0.010908491909503937, 0.3990439772605896, 0.02093818224966526, 0.014910301193594933, 0.40024587512016296, 0.18035711348056793, 0.25394922494888306, 0.25104519724845886, 0.472616046667099], dtype='float32').reshape([30]),
            paddle.to_tensor([0.41225117444992065, 0.3112136721611023, 0.19377899169921875, 0.1106235533952713, 0.43213123083114624, 0.22236387431621552, 0.4370879530906677, 0.27298563718795776, 0.29655733704566956, 0.3486902415752411, 0.24284771084785461, 0.24133001267910004, 0.01142787840217352, 0.3678855895996094, 0.22631406784057617, 0.19555656611919403, 0.10904530435800552, 0.38518354296684265, 0.33519378304481506, 0.3070562779903412, 0.3285343647003174, 0.0327836275100708, 0.2997418940067291, 0.34752175211906433, 0.262317419052124, 0.023517929017543793, 0.4650430679321289, 0.16886787116527557, 0.3073676824569702, 0.19651363790035248], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4174928665161133, 0.07947343587875366, 0.40980175137519836, 0.2711547911167145, 0.3489481210708618, 0.3854246735572815, 0.4032283425331116, 0.38856616616249084, 0.0821535512804985, 0.2257111370563507, 0.24802784621715546, 0.48483872413635254, 0.20984555780887604, 0.3485463559627533, 0.38063889741897583, 0.34399333596229553, 0.38804513216018677, 0.1383611559867859, 0.051604244858026505, 0.405643105506897, 0.42760562896728516, 0.3090231716632843, 0.13901618123054504, 0.09652788192033768, 0.055063169449567795, 0.19958989322185516, 0.026220688596367836, 0.1539030373096466, 0.0689769983291626, 0.08152738213539124], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f33c5caa43ebac659d955e92ab38e797(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.27900052070617676, 0.17481966316699982, 0.4285304844379425, 0.0592818558216095, 0.07624823600053787, 0.010529959574341774, 0.28636351227760315, 0.34409376978874207, 0.12430370599031448, 0.11203303188085556, 0.24393346905708313, 0.07915207743644714, 0.22698666155338287, 0.29011043906211853, 0.03127698227763176, 0.09719990193843842, 0.44601163268089294, 0.18613256514072418, 0.4252042770385742, 0.28448063135147095, 0.4676077663898468, 0.11099251359701157, 0.34960344433784485, 0.20190882682800293], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3559640347957611, 0.19888317584991455, 0.1843794882297516, 0.27280235290527344, 0.3965621590614319, 0.06376155465841293, 0.26697707176208496, 0.10775943100452423, 0.01918172277510166, 0.02350444905459881, 0.19276131689548492, 0.44702133536338806, 0.1307205855846405, 0.3295305371284485, 0.3059808611869812, 0.08693339675664902, 0.15121681988239288, 0.33545979857444763, 0.02330128289759159, 0.17585881054401398, 0.1543949842453003, 0.4194546937942505, 0.015426261350512505, 0.050618384033441544], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3260844051837921, 0.45236891508102417, 0.14246124029159546, 0.3428250551223755, 0.3725108206272125, 0.2550053298473358, 0.3359628915786743, 0.007308762986212969, 0.15310555696487427, 0.47378531098365784, 0.17000901699066162, 0.28525641560554504, 0.1546860933303833, 0.03609844297170639, 0.03595946356654167, 0.30496954917907715, 0.27207982540130615, 0.12255964428186417, 0.33582642674446106, 0.17535626888275146, 0.2162865549325943, 0.3635067343711853, 0.2476053237915039, 0.007767523638904095], dtype='float32').reshape([24]),
            paddle.to_tensor([0.33582890033721924, 0.09281639754772186, 0.38043880462646484, 0.43252032995224, 0.49965745210647583, 0.2367066591978073, 0.36506056785583496, 0.32395029067993164, 0.2576156556606293, 0.09096159040927887, 0.1470962017774582, 0.07130443304777145, 0.07758828997612, 0.23990948498249054, 0.38960298895835876, 0.024171914905309677, 0.08524364233016968, 0.40227898955345154, 0.4671754539012909, 0.10805417597293854, 0.4664325714111328, 0.3465307950973511, 0.22862909734249115, 0.25920501351356506], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_413e9bf57a4f044069c86c92c72e60b6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5cf8893654aa6ed0d25983cfd73a1818(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([8, 512, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d1a836f6b4f6bdd2f959c62ba0ca3fde(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e89ee7751ca803a5eb88dff494e0c4ee(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.0062120649963617325, 0.05726325884461403, 0.45924112200737, 0.2968478202819824, 0.38011959195137024, 0.49609270691871643, 0.01719067245721817, 0.39203375577926636, 0.14283257722854614, 0.42663562297821045, 0.18050068616867065, 0.057333171367645264, 0.2060883790254593, 0.45845717191696167, 0.10640769451856613, 0.4868640601634979, 0.027498189359903336, 0.05498884618282318, 0.31708842515945435, 0.27078506350517273, 0.2705135941505432, 0.2816389501094818, 0.062498293817043304, 0.0822034478187561, 0.2170899361371994, 0.13865365087985992, 0.21677309274673462, 0.1449182629585266, 0.12961289286613464, 0.20722676813602448], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4715997576713562, 0.13521808385849, 0.10108262300491333, 0.461258202791214, 0.002807001117616892, 0.14233075082302094, 0.22761358320713043, 0.25988325476646423, 0.4175158143043518, 0.06698276102542877, 0.3351474404335022, 0.2525486350059509, 0.42220187187194824, 0.23267099261283875, 0.07596024870872498, 0.035411201417446136, 0.18051835894584656, 0.42835184931755066, 0.22431407868862152, 0.13605549931526184, 0.33867472410202026, 0.25121480226516724, 0.31253698468208313, 0.4411206841468811, 0.037522993981838226, 0.19095133244991302, 0.13267649710178375, 0.1492382287979126, 0.2339034378528595, 0.41639062762260437], dtype='float32').reshape([30]),
            paddle.to_tensor([0.16361775994300842, 0.35302287340164185, 0.4866655766963959, 0.45933249592781067, 0.06324168294668198, 0.13201072812080383, 0.33464112877845764, 0.14430184662342072, 0.16126315295696259, 0.4971269369125366, 0.14893199503421783, 0.4039028286933899, 0.3135257363319397, 0.30933865904808044, 0.49283573031425476, 0.419816792011261, 0.06662921607494354, 0.08534123003482819, 0.040846455842256546, 0.16181960701942444, 0.3382486402988434, 0.059265222400426865, 0.05421322584152222, 0.016181854531168938, 0.2022637128829956, 0.09721393138170242, 0.33349013328552246, 0.15115445852279663, 0.342474102973938, 0.45569470524787903], dtype='float32').reshape([30]),
            paddle.to_tensor([0.03260113298892975, 0.3608185946941376, 0.13683922588825226, 0.2765321433544159, 0.22147643566131592, 0.24637238681316376, 0.21035918593406677, 0.13202445209026337, 0.022745579481124878, 0.47648125886917114, 0.3088524341583252, 0.49846765398979187, 0.19400249421596527, 0.059791941195726395, 0.3042048513889313, 0.29338306188583374, 0.3010174632072449, 0.1722690463066101, 0.48074546456336975, 0.26468318700790405, 0.35382285714149475, 0.25246158242225647, 0.10816247761249542, 0.3680250644683838, 0.3851410150527954, 0.2965068221092224, 0.3333379328250885, 0.4873659610748291, 0.34941476583480835, 0.35610294342041016], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_696bd4e4e5cb5a173b3b1653904e9662(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 768, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_11e0bdaed2841d89b397581dce666cf7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_97c4070257cbb4c2a6ef119fbc133e23(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 416, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_554f5a573087e07cce72d818e0ec3493(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d46c0342cca3aab8350dced2e0460753(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_0edd578f48a07aa5d1d79f9f678b472e
    def get_inputs(self):
        return [
            paddle.uniform([49, 512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_98edef8dc64cc56432351b9e658c82c4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 64, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_268e5ec150e286b5fe567fb250988bad(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 32, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.026849398389458656, 0.2401716262102127, 0.32137176394462585, 0.3848821222782135, 0.22030894458293915, 0.05847916007041931, 0.2589884400367737, 0.3523906171321869, 0.31995123624801636, 0.11775752156972885, 0.05005446448922157, 0.48001518845558167, 0.12230458855628967, 0.04309207201004028, 0.43459638953208923, 0.1920161247253418], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3460189700126648, 0.13786283135414124, 0.13533051311969757, 0.056838758289813995, 0.2039034515619278, 0.2836530804634094, 0.31274253129959106, 0.22028665244579315, 0.4553212821483612, 0.28795909881591797, 0.07760034501552582, 0.46786442399024963, 0.03356597572565079, 0.36751267313957214, 0.392641544342041, 0.16876965761184692], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4840953052043915, 0.09946927428245544, 0.10216471552848816, 0.3983607590198517, 0.4919545650482178, 0.45708978176116943, 0.10564786195755005, 0.20992448925971985, 0.1422162652015686, 0.0004446457023732364, 0.4872306287288666, 0.17380759119987488, 0.02718348056077957, 0.2971136271953583, 0.003356374567374587, 0.14520363509655], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3739590346813202, 0.4004739820957184, 0.08436796814203262, 0.13209781050682068, 0.20456737279891968, 0.28554442524909973, 0.08712869882583618, 0.25361576676368713, 0.3062624931335449, 0.35713934898376465, 0.1451665461063385, 0.1339852660894394, 0.4157986342906952, 0.05286550894379616, 0.2836783230304718, 0.3246362507343292], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bf76a1b87c1e2487fcab86f047bbb1a8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.19475772976875305, 0.322025865316391, 0.12636958062648773, 0.24014374613761902, 0.06268876791000366, 0.31552648544311523, 0.23732511699199677, 0.33848491311073303, 0.48915180563926697, 0.37702515721321106, 0.44001075625419617, 0.3324829638004303, 0.24749855697155, 0.2310558408498764, 0.3741978406906128, 0.15580011904239655, 0.029331840574741364, 0.14015620946884155, 0.096562460064888, 0.08790753036737442, 0.23782801628112793, 0.2608020007610321, 0.4205044209957123, 0.37842634320259094, 0.3246482312679291, 0.17572204768657684, 0.2184322029352188, 0.2357867956161499], dtype='float32').reshape([28]),
            paddle.to_tensor([0.048478689044713974, 0.380930632352829, 0.15971273183822632, 0.23820798099040985, 0.19746051728725433, 0.1437363177537918, 0.02265312895178795, 0.023964231833815575, 0.15001118183135986, 0.2046801745891571, 0.21299532055854797, 0.200861856341362, 0.12768223881721497, 0.45995545387268066, 0.13552318513393402, 0.35112980008125305, 0.19938677549362183, 0.1512131541967392, 0.22903089225292206, 0.21888065338134766, 0.4351562261581421, 0.39828425645828247, 0.012117870151996613, 0.21590451896190643, 0.14966373145580292, 0.3260383605957031, 0.28005802631378174, 0.35095009207725525], dtype='float32').reshape([28]),
            paddle.to_tensor([0.10238053649663925, 0.004683962091803551, 0.181747168302536, 0.08620903640985489, 0.42049354314804077, 0.4226052761077881, 0.001155106583610177, 0.2069954127073288, 0.23914475739002228, 0.47203579545021057, 0.06163615733385086, 0.40040940046310425, 0.06973827630281448, 0.2110537588596344, 0.13404235243797302, 0.08057662099599838, 0.39829137921333313, 0.1865433305501938, 0.390386164188385, 0.3713712990283966, 0.3944813311100006, 0.27446144819259644, 0.3797609508037567, 0.05184108763933182, 0.3623373210430145, 0.17144708335399628, 0.38609811663627625, 0.17599262297153473], dtype='float32').reshape([28]),
            paddle.to_tensor([0.043796323239803314, 0.14360103011131287, 0.41192498803138733, 0.4168447256088257, 0.1471622884273529, 0.2356594353914261, 0.10036515444517136, 0.03742165118455887, 0.013299316167831421, 0.14239753782749176, 0.3999803364276886, 0.22015443444252014, 0.45273521542549133, 0.23311574757099152, 0.050124641507864, 0.40207475423812866, 0.26509663462638855, 0.47517526149749756, 0.27253589034080505, 0.0301430132240057, 0.29810383915901184, 0.11119569838047028, 0.062022190541028976, 0.26936593651771545, 0.40232905745506287, 0.2024211585521698, 0.18136729300022125, 0.4866503179073334], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b9f6a474f5cb148d14fbd14dbc7d9622(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 4, 3], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8290f5307cc0ab1963d8cb66795133c7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1536, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8ac116f40c7435c34e1ad933817dc0a5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_62983e3e636c281015f7849f52fb9594(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1968, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1968], dtype='float32', min=0, max=0.5),
            paddle.uniform([1968], dtype='float32', min=0, max=0.5),
            paddle.uniform([1968], dtype='float32', min=0, max=0.5),
            paddle.uniform([1968], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7caf83624592e8ee2103be64009187c6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.43932390213012695, 0.2723398208618164, 0.09366580098867416, 0.45614495873451233, 0.2331894040107727, 0.04191584512591362, 0.22342394292354584, 0.13960254192352295, 0.42964550852775574, 0.06658842414617538, 0.15500609576702118, 0.05222025886178017, 0.030823174864053726, 0.4400519132614136, 0.01773885451257229, 0.05514758080244064, 0.41639891266822815, 0.0236218124628067, 0.483346551656723, 0.4104868471622467, 0.1887194961309433, 0.06925615668296814, 0.07265854626893997, 0.4937480092048645], dtype='float32').reshape([24]),
            paddle.to_tensor([0.09133926779031754, 0.2257642298936844, 0.37377986311912537, 0.1779768466949463, 0.4260493218898773, 0.0774722471833229, 0.014781469479203224, 0.4300762116909027, 0.4376964569091797, 0.01629897952079773, 0.07847967743873596, 0.07577408850193024, 0.07079682499170303, 0.4012204706668854, 0.3236229419708252, 0.18538768589496613, 0.42388734221458435, 0.1554480493068695, 0.20536279678344727, 0.36194097995758057, 0.09850985556840897, 0.22724466025829315, 0.02639220654964447, 0.43153858184814453], dtype='float32').reshape([24]),
            paddle.to_tensor([0.09166955202817917, 0.18207134306430817, 0.009558642283082008, 0.03282320871949196, 0.04286828637123108, 0.425700843334198, 0.22179535031318665, 0.08102311193943024, 0.34576502442359924, 0.4448224902153015, 0.31844446063041687, 0.020907467231154442, 0.21020615100860596, 0.4517926275730133, 0.2676417827606201, 0.33072179555892944, 0.014754311181604862, 0.3067006766796112, 0.22631321847438812, 0.4255501627922058, 0.06362990289926529, 0.29067692160606384, 0.43909525871276855, 0.3241986036300659], dtype='float32').reshape([24]),
            paddle.to_tensor([0.06734711676836014, 0.38310766220092773, 0.06498940289020538, 0.0687951073050499, 0.4112640619277954, 0.27714410424232483, 0.3212517499923706, 0.09498651325702667, 0.20313435792922974, 0.12497203797101974, 0.1553022265434265, 0.21886461973190308, 0.33918431401252747, 0.4164890944957733, 0.22206035256385803, 0.18664346635341644, 0.2813756763935089, 0.27265897393226624, 0.49655774235725403, 0.14303305745124817, 0.15674515068531036, 0.47103989124298096, 0.2825181186199188, 0.07169584184885025], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ea9f98a236307734504362036f512508(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1200, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_f5f2213be92f20fda5378beeed72688f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_376c09b1c6d0d3d7b9522d222d9c0031(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_f5f2213be92f20fda5378beeed72688f
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ddb8c90b149730f9a03a490d2cbf0697(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 480, 480], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cec9bbcaa9659c0f6157fb587f3384bc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 42, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([42], dtype='float32', min=0, max=0.5),
            paddle.uniform([42], dtype='float32', min=0, max=0.5),
            paddle.uniform([42], dtype='float32', min=0, max=0.5),
            paddle.uniform([42], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_11e8a113028b0b90c736dffe07c4e3be(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_827c7ecc3a40367cc58a098f13ba69a8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 70, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([70], dtype='float32', min=0, max=0.5),
            paddle.uniform([70], dtype='float32', min=0, max=0.5),
            paddle.uniform([70], dtype='float32', min=0, max=0.5),
            paddle.uniform([70], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_be2634e3e0da946dca4cd0e651ae6105(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_922461e79cc1140e909fd0d887e435cc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 64, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_851cb4eebe53841cc73526feded6db07(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 60, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4842e4544bbbf6b4b528c0eab5716a5b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 4, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3019214b1a524867018977a9c8c32605(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 128, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_80ef68b9e057ba5923fcb2f49be167da(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_9d5dde09230a5209a57565a3b7304f49
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 49], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_954ef9a4a56ca7648d4753d892e58c43(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 512, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.24643653631210327, 0.08545564860105515, 0.06462950259447098, 0.06973426043987274, 0.4651513397693634, 0.06984379887580872, 0.1928129643201828, 0.11969669908285141, 0.37609943747520447, 0.2703169882297516, 0.31754744052886963, 0.33966025710105896, 0.4203825891017914, 0.07073214650154114, 0.28144851326942444, 0.48071175813674927], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4972046911716461, 0.30568766593933105, 0.45912623405456543, 0.21471424400806427, 0.049337584525346756, 0.3328264057636261, 0.4820614159107208, 0.07802830636501312, 0.3540525436401367, 0.06327396631240845, 0.23335236310958862, 0.1325492262840271, 0.12374605983495712, 0.030868107452988625, 0.28330469131469727, 0.13318054378032684], dtype='float32').reshape([16]),
            paddle.to_tensor([0.0942687913775444, 0.19696728885173798, 0.2638721168041229, 0.08522404730319977, 0.23186330497264862, 0.42827481031417847, 0.11460670828819275, 0.2759772539138794, 0.16880860924720764, 0.3248452842235565, 0.05810784175992012, 0.2586057186126709, 0.24616552889347076, 0.0034224751871079206, 0.3878635764122009, 0.31102800369262695], dtype='float32').reshape([16]),
            paddle.to_tensor([0.24213792383670807, 0.003651328617706895, 0.25670182704925537, 0.08366497606039047, 0.3544554114341736, 0.04472990334033966, 0.30240505933761597, 0.3684309124946594, 0.014892343431711197, 0.3229099214076996, 0.08364798128604889, 0.16551220417022705, 0.2304084151983261, 0.4585498571395874, 0.1137632355093956, 0.4474749267101288], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4489a16ce90ff3ce51b9f0c0abfa013b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5f35f5ad20a4138972922db14005ece2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 4, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c4698e8e66bf4ae39b260cdc515db379(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 100], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ca0e314a3a0b65ff777cce1240e4b4ce(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 144, 14, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_efa044b8c49bc663d99ae2b96ab11e8d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_d589af777f5a8ae699611515c41499fb
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_01a234bf8fb86b474035e84be1dd91cd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_13f5931cc81d4ed288e8d97b01b9297f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1296, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1296], dtype='float32', min=0, max=0.5),
            paddle.uniform([1296], dtype='float32', min=0, max=0.5),
            paddle.uniform([1296], dtype='float32', min=0, max=0.5),
            paddle.uniform([1296], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_72a55ffdf78ac401883234c72495128c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.16883234679698944, 0.21755921840667725, 0.40218064188957214, 0.16557680070400238, 0.2774640619754791, 0.4666329324245453, 0.464037150144577, 0.17132294178009033, 0.4231467843055725, 0.3836173415184021, 0.4112800657749176, 0.13192425668239594, 0.036156509071588516, 0.4295010268688202, 0.013747690245509148, 0.027616050094366074, 0.4615880846977234, 0.021442804485559464], dtype='float32').reshape([18]),
            paddle.to_tensor([0.11479631811380386, 0.037047695368528366, 0.26340436935424805, 0.3122694790363312, 0.44083574414253235, 0.005161696579307318, 0.37027424573898315, 0.001254888717085123, 0.33538419008255005, 0.09688074141740799, 0.34254810214042664, 0.22197097539901733, 0.38202768564224243, 0.38380756974220276, 0.36181050539016724, 0.3087419867515564, 0.010792652145028114, 0.3756890892982483], dtype='float32').reshape([18]),
            paddle.to_tensor([0.30408063530921936, 0.42915788292884827, 0.3105124831199646, 0.1099386215209961, 0.2988247275352478, 0.021201642230153084, 0.10119854658842087, 0.3627050220966339, 0.3684020936489105, 0.4862857460975647, 0.08999209105968475, 0.3886055648326874, 0.05702877417206764, 0.05289251729846001, 0.19062823057174683, 0.05346798524260521, 0.4605308771133423, 0.29015928506851196], dtype='float32').reshape([18]),
            paddle.to_tensor([0.28789639472961426, 0.3379826843738556, 0.07753878086805344, 0.23009897768497467, 0.22503633797168732, 0.13891969621181488, 0.4992651343345642, 0.3757018446922302, 0.42730093002319336, 0.43412137031555176, 0.06407325714826584, 0.45712095499038696, 0.059452857822179794, 0.13683468103408813, 0.41786032915115356, 0.1482677012681961, 0.12030625343322754, 0.006934723816812038], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6ca209a9f56c869ccae69b4106bbb854(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 240, 240], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.23080942034721375, 0.452243447303772, 0.4744970500469208, 0.29400894045829773, 0.3041287660598755, 0.057793524116277695, 0.026423487812280655, 0.41073963046073914, 0.4144246280193329, 0.08361664414405823, 0.2846926748752594, 0.38343632221221924, 0.13514654338359833, 0.34784454107284546, 0.024591615423560143, 0.3262021243572235], dtype='float32').reshape([16]),
            paddle.to_tensor([0.003447410650551319, 0.3379007875919342, 0.3176245391368866, 0.39322176575660706, 0.4303714334964752, 0.19622403383255005, 0.43059638142585754, 0.00828470103442669, 0.23175722360610962, 0.3597375154495239, 0.2138517051935196, 0.3671005070209503, 0.24557530879974365, 0.2470862716436386, 0.1052502915263176, 0.22629864513874054], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2911795377731323, 0.4292663633823395, 0.3252360224723816, 0.48684820532798767, 0.36000868678092957, 0.46329864859580994, 0.2966453433036804, 0.29614630341529846, 0.16296574473381042, 0.026961587369441986, 0.24210165441036224, 0.45203572511672974, 0.4383662939071655, 0.47050416469573975, 0.10554691404104233, 0.34421446919441223], dtype='float32').reshape([16]),
            paddle.to_tensor([0.45306482911109924, 0.1638462096452713, 0.4644436538219452, 0.26962703466415405, 0.03306020796298981, 0.38834938406944275, 0.3455716371536255, 0.09843266755342484, 0.06516604870557785, 0.019451137632131577, 0.23012778162956238, 0.310614675283432, 0.033963028341531754, 0.19235700368881226, 0.11924398690462112, 0.4075509011745453], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_89222663a9eec7bd6a21d63c2aa8e50f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1280, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_89ff2cd406d34d720d4ca2b66914247f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 8, 6], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_df08af11f1caaee4ba13996cff3f91b2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 32, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d9c34e6cdfef867934789908592b675a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 2, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b082f346cd2b4ebc03ecb8cc0c7a5419(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_70e645756dfcab6ff7e1dcd76f66c506(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.27558237314224243, 0.2617965042591095, 0.3269663453102112, 0.20108553767204285, 0.2540694773197174, 0.21468880772590637, 0.4653856158256531, 0.3221813440322876, 0.27546197175979614, 0.4445887506008148, 0.2974705100059509, 0.43239641189575195, 0.4392896592617035, 0.36555215716362, 0.33942314982414246, 0.2469245195388794, 0.05188506469130516, 0.11057313531637192, 0.19019362330436707, 0.15191113948822021, 0.4530055522918701, 0.4829186797142029, 0.2513810694217682, 0.40515702962875366], dtype='float32').reshape([24]),
            paddle.to_tensor([0.009831685572862625, 0.4509485960006714, 0.20282697677612305, 0.06528254598379135, 0.09952264279127121, 0.2750159204006195, 0.4953474700450897, 0.4795216917991638, 0.03040137328207493, 0.18456323444843292, 0.04105643928050995, 0.2525409758090973, 0.24077266454696655, 0.42871981859207153, 0.41175577044487, 0.08598783612251282, 0.4046153128147125, 0.16668282449245453, 0.496212363243103, 0.1536799967288971, 0.3410973846912384, 0.19312679767608643, 0.06255576014518738, 0.4269089996814728], dtype='float32').reshape([24]),
            paddle.to_tensor([0.11294222623109818, 0.04363624379038811, 0.02541949413716793, 0.0470612496137619, 0.07407744973897934, 0.1894647479057312, 0.25488200783729553, 0.007820429280400276, 0.1027250736951828, 0.3496283292770386, 0.1784886121749878, 0.02024482563138008, 0.1810373216867447, 0.08495782315731049, 0.28014618158340454, 0.33340150117874146, 0.030735135078430176, 0.007661298383027315, 0.11780857294797897, 0.47509267926216125, 0.034777283668518066, 0.08150050044059753, 0.1643824428319931, 0.12531466782093048], dtype='float32').reshape([24]),
            paddle.to_tensor([0.325443297624588, 0.0965534895658493, 0.2011503428220749, 0.2440861463546753, 0.016519682481884956, 0.3234054744243622, 0.24700011312961578, 0.2969414293766022, 0.4993930160999298, 0.09449927508831024, 0.03597312420606613, 0.3293658196926117, 0.10044466704130173, 0.1824495643377304, 0.13647575676441193, 0.41375479102134705, 0.2824093699455261, 0.10303328931331635, 0.13155150413513184, 0.05681725591421127, 0.48452314734458923, 0.3488893210887909, 0.42029890418052673, 0.10998766869306564], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fe2d6412a4949ed7695fbb7d210274d3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_af834e3adfbe93c6683e5d8605205ca1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 512], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.007592474110424519, 0.2616603672504425, 0.23508590459823608, 0.32560309767723083, 0.3788093626499176, 0.16468015313148499, 0.10855700820684433, 0.1589038223028183, 0.21420028805732727, 0.10411826521158218, 0.11396583169698715, 0.21862094104290009, 0.3932785093784332, 0.19001799821853638, 0.21815915405750275, 0.22262907028198242], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4870295226573944, 0.4862513840198517, 0.35958126187324524, 0.3668329417705536, 0.46674150228500366, 0.2293815314769745, 0.40942350029945374, 0.32812559604644775, 0.3449366092681885, 0.32979097962379456, 0.3361029028892517, 0.012639330700039864, 0.32418081164360046, 0.45410051941871643, 0.34663504362106323, 0.4902513921260834], dtype='float32').reshape([16]),
            paddle.to_tensor([0.32442575693130493, 0.06916364282369614, 0.13927097618579865, 0.4757714569568634, 0.45012450218200684, 0.35374748706817627, 0.34928783774375916, 0.12354445457458496, 0.2095431387424469, 0.27305883169174194, 0.02761898748576641, 0.1235022097826004, 0.2621581554412842, 0.2907956540584564, 0.003721080254763365, 0.4038766622543335], dtype='float32').reshape([16]),
            paddle.to_tensor([0.20360799133777618, 0.3721122443675995, 0.2455790936946869, 0.2097170352935791, 0.35058432817459106, 0.07441201061010361, 0.4769107401371002, 0.28579485416412354, 0.05953579396009445, 0.2376812845468521, 0.06861402839422226, 0.11971157044172287, 0.17697463929653168, 0.11806439608335495, 0.3926222622394562, 0.3053518831729889], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8171d7164b4887cfd5df2e4ad7e30cc4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 16, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b7e67a9ae86fbbe874ea4b5585b0a952(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 10, 10], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bcbc422043b1e98fe1533870f3face89(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8dd4c317cf5758b55a9053f37b9833c1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2bba1baf959b6e4ac8a282c1b3494e9c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 122, 122], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dfcb98967902055e830e95639fe9ecb1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 320, 320], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.48591986298561096, 0.3868829607963562, 0.30204862356185913, 0.23729027807712555, 0.4210376441478729, 0.19614927470684052, 0.09467331320047379, 0.2794146239757538], dtype='float32').reshape([8]),
            paddle.to_tensor([0.2773144245147705, 0.3542498052120209, 0.13884080946445465, 0.049979519098997116, 0.4975494146347046, 0.13677570223808289, 0.13023556768894196, 0.045112572610378265], dtype='float32').reshape([8]),
            paddle.to_tensor([0.44427987933158875, 0.2131778448820114, 0.16878080368041992, 0.04169272258877754, 0.3973926603794098, 0.29532313346862793, 0.040018413215875626, 0.36950668692588806], dtype='float32').reshape([8]),
            paddle.to_tensor([0.15581807494163513, 0.300971657037735, 0.26529461145401, 0.302202433347702, 0.14934398233890533, 0.16490551829338074, 0.2545519769191742, 0.25440022349357605], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1e20525a3cb301d41ca863ea7974942f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.21972180902957916, 0.12194415181875229, 0.33585888147354126, 0.4297078251838684, 0.3005286157131195, 0.007156282663345337, 0.08438292145729065, 0.47839149832725525, 0.3485497236251831, 0.3374350070953369, 0.2621353268623352, 0.405730664730072, 0.48565080761909485, 0.18365827202796936, 0.43613311648368835, 0.3452194333076477, 0.30127066373825073, 0.28425729274749756], dtype='float32').reshape([18]),
            paddle.to_tensor([0.38336268067359924, 0.3242602050304413, 0.19853071868419647, 0.09614689648151398, 0.48844361305236816, 0.3255009651184082, 0.3518758714199066, 0.328731894493103, 0.32800590991973877, 0.4974953234195709, 0.455000102519989, 0.38180363178253174, 0.34833601117134094, 0.41403499245643616, 0.3470551669597626, 0.25677117705345154, 0.3761500418186188, 0.36693981289863586], dtype='float32').reshape([18]),
            paddle.to_tensor([0.48754221200942993, 0.207680806517601, 0.35186219215393066, 0.21346794068813324, 0.16373665630817413, 0.42233818769454956, 0.058481041342020035, 0.4243479073047638, 0.09810640662908554, 0.48754751682281494, 0.03278760239481926, 0.17310483753681183, 0.06006203219294548, 0.1507343053817749, 0.4741980731487274, 0.015797175467014313, 0.2072087526321411, 0.09722009301185608], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3514431118965149, 0.4450763463973999, 0.03956936299800873, 0.1945229023694992, 0.37080875039100647, 0.44803425669670105, 0.0011011234018951654, 0.005975260399281979, 0.37716156244277954, 0.46238306164741516, 0.4648487865924835, 0.21984541416168213, 0.3791084587574005, 0.03746601566672325, 0.09362227469682693, 0.36425644159317017, 0.48033440113067627, 0.182157501578331], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6ab5a817ee1ab2d1385fd9d35445e9f9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 4, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cc071661ba2f6afe5bfb04c2830a3898(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 544, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([544], dtype='float32', min=0, max=0.5),
            paddle.uniform([544], dtype='float32', min=0, max=0.5),
            paddle.uniform([544], dtype='float32', min=0, max=0.5),
            paddle.uniform([544], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c9f9cfb572e73322e574f55b885eb7e4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 2, 4], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fb440921a2081dfef2c49c166e5ec6c8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 12, 12], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8a5c0dbd0d8b191f908d5d13e9414626(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 800, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([800], dtype='float32', min=0, max=0.5),
            paddle.uniform([800], dtype='float32', min=0, max=0.5),
            paddle.uniform([800], dtype='float32', min=0, max=0.5),
            paddle.uniform([800], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d80ad80c6536ec5615ec97e69d0daee3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_556ba203c0f6ab7c32b646ed49692769(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 128, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_57c239b680029edba97351ce835f5394(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 240, 240], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.15231971442699432, 0.12197840958833694, 0.35933905839920044, 0.4445956349372864, 0.40240204334259033, 0.1805611550807953, 0.09583619236946106, 0.4486713111400604, 0.4117589592933655, 0.4674169421195984, 0.011609969660639763, 0.3965643644332886, 0.1378655731678009, 0.11835578083992004, 0.29627251625061035, 0.3113543391227722], dtype='float32').reshape([16]),
            paddle.to_tensor([0.37591028213500977, 0.499387264251709, 0.42977750301361084, 0.018681520596146584, 0.4857724606990814, 0.34682852029800415, 0.2256629765033722, 0.404371052980423, 0.4646379053592682, 0.29448169469833374, 0.10398795455694199, 0.46958377957344055, 0.26183021068573, 0.34757834672927856, 0.13281020522117615, 0.31706464290618896], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3644624948501587, 0.2640021741390228, 0.3291114866733551, 0.26065126061439514, 0.2733596861362457, 0.15798135101795197, 0.4133773446083069, 0.09823990613222122, 0.34940943121910095, 0.49859872460365295, 0.4459371268749237, 0.3891264498233795, 0.07849385589361191, 0.12931406497955322, 0.05526283383369446, 0.4675990641117096], dtype='float32').reshape([16]),
            paddle.to_tensor([0.48456424474716187, 0.10252921283245087, 0.272684782743454, 0.47975295782089233, 0.07577472180128098, 0.07099699229001999, 0.4386759400367737, 0.2511592209339142, 0.1750008910894394, 0.43855151534080505, 0.4609592854976654, 0.022194476798176765, 0.2645706236362457, 0.3448985815048218, 0.242489293217659, 0.3610517084598541], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2554b4964ee5ce0a7b25c32d387cc108(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_0edd578f48a07aa5d1d79f9f678b472e
    def get_inputs(self):
        return [
            paddle.uniform([49, 1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bea51a6bf5718669324082f2900b0595(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 432, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7a6ddf2990b2f8277da716780b9a991a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 1, 1], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9be6db102a8dd919b7f6d098dcc8bcde(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0b96bd4ec3774e0e888293d54deda817(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 8, 50], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.34286803007125854, 0.44292783737182617, 0.33270981907844543, 0.2843298316001892, 0.15041649341583252, 0.27743974328041077, 0.24367868900299072, 0.07339438050985336, 0.05278869718313217, 0.12104891985654831, 0.011128254234790802, 0.3775385022163391, 0.37438884377479553, 0.13577499985694885, 0.43932729959487915, 0.3708462715148926], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4689676761627197, 0.35076022148132324, 0.1423458755016327, 0.3274367153644562, 0.023999923840165138, 0.02904980443418026, 0.38714805245399475, 0.2032279670238495, 0.04310183972120285, 0.12125510722398758, 0.41645348072052, 0.2221376597881317, 0.42564570903778076, 0.02507101371884346, 0.4711855351924896, 0.14447805285453796], dtype='float32').reshape([16]),
            paddle.to_tensor([0.35094842314720154, 0.37278759479522705, 0.19525188207626343, 0.09646471589803696, 0.4224412739276886, 0.06598799675703049, 0.1454872488975525, 0.006794262677431107, 0.4206635057926178, 0.028741875663399696, 0.32655295729637146, 0.4289935827255249, 0.4125179052352905, 0.0151999955996871, 0.4590354263782501, 0.16619136929512024], dtype='float32').reshape([16]),
            paddle.to_tensor([0.41390809416770935, 0.19397637248039246, 0.4641740322113037, 0.3345932364463806, 0.22948506474494934, 0.3720105290412903, 0.3138546049594879, 0.06319765746593475, 0.27414196729660034, 0.1365492343902588, 0.24058954417705536, 0.47988423705101013, 0.4876447916030884, 0.26035988330841064, 0.480378121137619, 0.04080117866396904], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bbba0c3e5d4473cb63c130d31c652f84(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 320, 320], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2304379642009735, 0.42074355483055115, 0.33229875564575195, 0.36580026149749756, 0.19264768064022064, 0.31293952465057373, 0.09245528280735016, 0.420510858297348, 0.4558483064174652, 0.3999534249305725, 0.206154003739357, 0.02567843720316887, 0.3488961458206177, 0.03420312702655792, 0.4629391133785248, 0.2578694224357605], dtype='float32').reshape([16]),
            paddle.to_tensor([0.06582501530647278, 0.36069849133491516, 0.15342938899993896, 0.29776546359062195, 0.36323243379592896, 0.3163794279098511, 0.07306910306215286, 0.11766183376312256, 0.3855639100074768, 0.43825244903564453, 0.16323061287403107, 0.2863951027393341, 0.4680740237236023, 0.33147555589675903, 0.4931882917881012, 0.28240060806274414], dtype='float32').reshape([16]),
            paddle.to_tensor([0.13272029161453247, 0.05554968863725662, 0.08416195958852768, 0.0978935956954956, 0.402107298374176, 0.37575364112854004, 0.3709297478199005, 0.4591919183731079, 0.3062332272529602, 0.13763616979122162, 0.08546487987041473, 0.4266364574432373, 0.047168537974357605, 0.17769969999790192, 0.11714334040880203, 0.4509468078613281], dtype='float32').reshape([16]),
            paddle.to_tensor([0.32629767060279846, 0.1118021309375763, 0.37697920203208923, 0.37762975692749023, 0.23255133628845215, 0.48804771900177, 0.34336960315704346, 0.42157888412475586, 0.35209527611732483, 0.09860721975564957, 0.2098589390516281, 0.0724465399980545, 0.39183682203292847, 0.08462925255298615, 0.3926818370819092, 0.09954146295785904], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_749a91677cd19a2d9b3ac11b4197c020(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_b4f4da3a8d44d1b5b3a2903a097df186
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e188bcc482351a5581983ba472c5bce5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 31, 31], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c2053a0dbcf7b6b996cfaa7e866a438e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 3, 3], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_31c9298b71a2f8cbded964ea5cf700a0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 512], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.018268797546625137, 0.35546043515205383, 0.4569370448589325, 0.1443842351436615, 0.019910406321287155, 0.33299675583839417, 0.3931117057800293, 0.1811758577823639, 0.31503918766975403, 0.4330016076564789, 0.3929845094680786, 0.4280622899532318, 0.02230546623468399, 0.3105880916118622, 0.4435177445411682, 0.3510604798793793], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3155252933502197, 0.2815503478050232, 0.13949958980083466, 0.10299021750688553, 0.38861021399497986, 0.010868304409086704, 0.42474108934402466, 0.23838606476783752, 0.023989930748939514, 0.020339112728834152, 0.15941017866134644, 0.4504266381263733, 0.4313330054283142, 0.4869805574417114, 0.2929050028324127, 0.11137565225362778], dtype='float32').reshape([16]),
            paddle.to_tensor([0.22104744613170624, 0.4914577603340149, 0.2878299653530121, 0.3733885884284973, 0.31568142771720886, 0.3374788165092468, 0.24304737150669098, 0.39690372347831726, 0.2714462876319885, 0.3042606711387634, 0.302359014749527, 0.057251542806625366, 0.3153148889541626, 0.03864545375108719, 0.14435528218746185, 0.1365565061569214], dtype='float32').reshape([16]),
            paddle.to_tensor([0.013665327802300453, 0.25550463795661926, 0.2781681418418884, 0.034540120512247086, 0.45986640453338623, 0.367922306060791, 0.4401842951774597, 0.3435804545879364, 0.25888973474502563, 0.31939569115638733, 0.44393211603164673, 0.13830645382404327, 0.3725493252277374, 0.3532378673553467, 0.15771687030792236, 0.4812861979007721], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5c5e6a64543906aa2b4c85d2e311d23c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 504, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([504], dtype='float32', min=0, max=0.5),
            paddle.uniform([504], dtype='float32', min=0, max=0.5),
            paddle.uniform([504], dtype='float32', min=0, max=0.5),
            paddle.uniform([504], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_eec3d9c83cd7cc778346a595a3eff137(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.15940694510936737, 0.4738842248916626, 0.025003211572766304, 0.3741895258426666, 0.08789823204278946, 0.16060781478881836, 0.06260523945093155, 0.11346928775310516, 0.3067798316478729, 0.3178505599498749, 0.3610692322254181, 0.3705819547176361, 0.3790612518787384, 0.11588653177022934, 0.14265449345111847, 0.4601595401763916, 0.23613037168979645, 0.21373985707759857, 0.22468680143356323, 0.48713555932044983, 0.1319865882396698, 0.3674969971179962, 0.16279008984565735, 0.09880731999874115], dtype='float32').reshape([24]),
            paddle.to_tensor([0.42724931240081787, 0.4309903383255005, 0.19987276196479797, 0.007580711971968412, 0.07651938498020172, 0.28804507851600647, 0.26294901967048645, 0.45177680253982544, 0.2600118815898895, 0.11600270867347717, 0.4577515721321106, 0.12095027416944504, 0.4667924642562866, 0.08602123707532883, 0.07517176866531372, 0.38334962725639343, 0.18851684033870697, 0.20154665410518646, 0.3624153137207031, 0.4792296588420868, 0.35624775290489197, 0.4335525631904602, 0.08264289796352386, 0.13314573466777802], dtype='float32').reshape([24]),
            paddle.to_tensor([0.03712989017367363, 0.47766897082328796, 0.44723549485206604, 0.055838555097579956, 0.49847710132598877, 0.18056799471378326, 0.01651323027908802, 0.17210523784160614, 0.051477789878845215, 0.03891754150390625, 0.37050116062164307, 0.18506991863250732, 0.2966499924659729, 0.3796064257621765, 0.2900735139846802, 0.4017079174518585, 0.39131584763526917, 0.46963658928871155, 0.022050369530916214, 0.11946642398834229, 0.008470028638839722, 0.27164533734321594, 0.0031440844759345055, 0.04662448540329933], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3413495719432831, 0.4145185947418213, 0.33307504653930664, 0.07532087713479996, 0.35501936078071594, 0.015001537278294563, 0.4195103645324707, 0.3319472372531891, 0.05631038174033165, 0.23651431500911713, 0.047341931611299515, 0.3035423159599304, 0.3548049330711365, 0.4932253956794739, 0.2812972962856293, 0.1713920682668686, 0.05901236832141876, 0.19596673548221588, 0.19729620218276978, 0.44389912486076355, 0.09206625819206238, 0.18137282133102417, 0.45496442914009094, 0.1723785698413849], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2837e5da37b29bf8935bca82c2d86ba7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.0022186655551195145, 0.38197067379951477, 0.3024742901325226, 0.26590797305107117, 0.33584898710250854, 0.480711966753006, 0.06698556244373322, 0.3548523187637329, 0.11866353452205658, 0.22738459706306458, 0.438220351934433, 0.47463318705558777, 0.10005398094654083, 0.38279855251312256, 0.08639336377382278, 0.06352989375591278, 0.3942168951034546, 0.17093485593795776], dtype='float32').reshape([18]),
            paddle.to_tensor([0.03444933891296387, 0.3183729946613312, 0.4940108060836792, 0.092369444668293, 0.37879490852355957, 0.38861921429634094, 0.13577818870544434, 0.44427117705345154, 0.252903550863266, 0.2977238893508911, 0.299693763256073, 0.36104658246040344, 0.00014355260645970702, 0.4672565758228302, 0.2779783606529236, 0.472444087266922, 0.2988751232624054, 0.0995512455701828], dtype='float32').reshape([18]),
            paddle.to_tensor([0.1555090695619583, 0.20801186561584473, 0.25034281611442566, 0.32146763801574707, 0.10375560075044632, 0.32650917768478394, 0.4557133913040161, 0.22918711602687836, 0.46925926208496094, 0.18083076179027557, 0.41892945766448975, 0.27223196625709534, 0.405914306640625, 0.10937745869159698, 0.19774103164672852, 0.30369916558265686, 0.14072385430335999, 0.167979434132576], dtype='float32').reshape([18]),
            paddle.to_tensor([0.47516676783561707, 0.17203953862190247, 0.18060730397701263, 0.2406432181596756, 0.28708457946777344, 0.11951392143964767, 0.3148266077041626, 0.23596572875976562, 0.10835569351911545, 0.06030925363302231, 0.40219900012016296, 0.4899819493293762, 0.37328851222991943, 0.18219076097011566, 0.1335693895816803, 0.1386941820383072, 0.458377480506897, 0.34006085991859436], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_afb6f78e30cd6662fe77b8b081c009e3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1696, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1696], dtype='float32', min=0, max=0.5),
            paddle.uniform([1696], dtype='float32', min=0, max=0.5),
            paddle.uniform([1696], dtype='float32', min=0, max=0.5),
            paddle.uniform([1696], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_64aec9621ba7b2e43e0f3a0bd25a6207(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2b37704396b8cd1224fb25cd49efa556(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2480790764093399, 0.1004282683134079, 0.444479376077652, 0.0813087671995163, 0.39327436685562134, 0.3883141279220581, 0.09592065215110779, 0.4222180247306824, 0.12216655910015106, 0.1099846288561821, 0.09036342054605484, 0.20572429895401, 0.48716750741004944, 0.03314599022269249, 0.32182934880256653, 0.3099895417690277, 0.19987688958644867, 0.4706360995769501, 0.2934567928314209, 0.2823096811771393, 0.2451809048652649, 0.11373943090438843, 0.3628349304199219, 0.3438984751701355, 0.034347157925367355, 0.2260059416294098, 0.366453617811203, 0.2615010142326355, 0.4365617334842682, 0.04242757707834244], dtype='float32').reshape([30]),
            paddle.to_tensor([0.04550321400165558, 0.46030083298683167, 0.32716357707977295, 0.21886923909187317, 0.454328328371048, 0.3007699251174927, 0.1829463243484497, 0.12363056093454361, 0.16248925030231476, 0.08493243902921677, 0.31834444403648376, 0.40791165828704834, 0.21249888837337494, 0.2998409867286682, 0.1946488320827484, 0.2289797067642212, 0.2691250145435333, 0.1121930330991745, 0.02062579244375229, 0.13082091510295868, 0.2755574584007263, 0.0936054140329361, 0.25827556848526, 0.49656644463539124, 0.47659069299697876, 0.3967150151729584, 0.2137613445520401, 0.019785311073064804, 0.4191531836986542, 0.38432249426841736], dtype='float32').reshape([30]),
            paddle.to_tensor([0.38329562544822693, 0.4138953387737274, 0.3005797863006592, 0.042315773665905, 0.33162450790405273, 0.16779693961143494, 0.41976940631866455, 0.09751686453819275, 0.378144234418869, 0.46316617727279663, 0.1639660745859146, 0.3825600743293762, 0.37093138694763184, 0.30220508575439453, 0.36060792207717896, 0.16888855397701263, 0.4591244161128998, 0.36678364872932434, 0.042195919901132584, 0.3597705662250519, 0.03688374161720276, 0.33459895849227905, 0.08948492258787155, 0.02715483121573925, 0.32140085101127625, 0.1176140308380127, 0.3779967129230499, 0.14157865941524506, 0.07209187000989914, 0.008787909522652626], dtype='float32').reshape([30]),
            paddle.to_tensor([0.08583174645900726, 0.2802176773548126, 0.30917325615882874, 0.24475586414337158, 0.18458876013755798, 0.25682684779167175, 0.4222089350223541, 0.16834500432014465, 0.2453370988368988, 0.2644977271556854, 0.420322448015213, 0.23063115775585175, 0.40686464309692383, 0.3958922326564789, 0.41439175605773926, 0.36827728152275085, 0.46666911244392395, 0.37197795510292053, 0.3456416726112366, 0.23697547614574432, 0.3093792796134949, 0.006390932947397232, 0.08996546268463135, 0.14379067718982697, 0.10191912949085236, 0.10558151453733444, 0.38533854484558105, 0.24570266902446747, 0.4652509391307831, 0.13680033385753632], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3246a3ac6b7180fe33f9e0064cbe6286(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 120, 120], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f3f6c96b143f87e0b0b424ecba71609b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.020448923110961914, 0.2048112452030182, 0.28075361251831055, 0.2814021110534668, 0.03196505457162857, 0.43660497665405273, 0.29490745067596436, 0.45765867829322815, 0.40486156940460205, 0.4200904667377472, 0.18676669895648956, 0.4399344325065613, 0.2369813621044159, 0.21835222840309143, 0.059967029839754105, 0.3776029944419861, 0.195813849568367, 0.16029544174671173], dtype='float32').reshape([18]),
            paddle.to_tensor([0.464581161737442, 0.1355644315481186, 0.28090009093284607, 0.16240495443344116, 0.24949681758880615, 0.04277385398745537, 0.2869139015674591, 0.3281254470348358, 0.427008718252182, 0.15002509951591492, 0.2543610632419586, 0.015888992697000504, 0.010070188902318478, 0.2790282964706421, 0.3458671569824219, 0.11265160143375397, 0.49462440609931946, 0.4854363799095154], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3874704837799072, 0.2703738510608673, 0.038746029138565063, 0.44849321246147156, 0.02072218619287014, 0.45425042510032654, 0.38159674406051636, 0.232822984457016, 0.001650339225307107, 0.49764516949653625, 0.030834896489977837, 0.07818182557821274, 0.44908004999160767, 0.3778238296508789, 0.022312413901090622, 0.436818391084671, 0.25774866342544556, 0.18959255516529083], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4458332657814026, 0.3232596218585968, 0.39201611280441284, 0.4152313768863678, 0.3142731487751007, 0.14465585350990295, 0.2695845663547516, 0.4505479633808136, 0.32650864124298096, 0.04731656238436699, 0.24590444564819336, 0.37009286880493164, 0.3493163287639618, 0.48010948300361633, 0.3401262164115906, 0.24849610030651093, 0.4708777368068695, 0.39425763487815857], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_88a8ee9d072122fda9e0bf496da8d48b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 64, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_959b2ca67282a9c7adeef590e1d96c15(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_87451730d2cb822ca1d3140d0209b2c8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2854004204273224, 0.18828964233398438, 0.1637091487646103, 0.47349727153778076, 0.39209914207458496, 0.45975202322006226, 0.34511882066726685, 0.2838236689567566, 0.12028523534536362, 0.34847623109817505, 0.4398897588253021, 0.18453732132911682, 0.17998194694519043, 0.16099156439304352, 0.4324599504470825, 0.18891142308712006, 0.3907780051231384, 0.2982591688632965, 0.40256237983703613, 0.12644711136817932, 0.23959766328334808, 0.11414358764886856, 0.46599912643432617, 0.13464315235614777], dtype='float32').reshape([24]),
            paddle.to_tensor([0.27807146310806274, 0.026066835969686508, 0.3119122087955475, 0.36412590742111206, 0.2749992609024048, 0.05526860058307648, 0.19253934919834137, 0.4772399961948395, 0.07461585849523544, 0.4850483238697052, 0.418695867061615, 0.4246477782726288, 0.23661550879478455, 0.25843173265457153, 0.1404682844877243, 0.4894144535064697, 0.2903245985507965, 0.48116201162338257, 0.31017574667930603, 0.2740022540092468, 0.395796537399292, 0.06737420707941055, 0.42271313071250916, 0.16494520008563995], dtype='float32').reshape([24]),
            paddle.to_tensor([0.26784712076187134, 0.2869563102722168, 0.2522556185722351, 0.05661337450146675, 0.18933093547821045, 0.027637658640742302, 0.11906326562166214, 0.41951969265937805, 0.3637144863605499, 0.17098909616470337, 0.10480748862028122, 0.10773088037967682, 0.2911556661128998, 0.12405931204557419, 0.14432696998119354, 0.1677810102701187, 0.29059502482414246, 0.4742397665977478, 0.4020801782608032, 0.29623258113861084, 0.4656302034854889, 0.2554573118686676, 0.39741405844688416, 0.16154466569423676], dtype='float32').reshape([24]),
            paddle.to_tensor([0.21778729557991028, 0.09977909922599792, 0.39460620284080505, 0.46380847692489624, 0.2025262415409088, 0.2586837410926819, 0.17509301006793976, 0.012738010846078396, 0.397941529750824, 0.2208000272512436, 0.2523929476737976, 0.40117183327674866, 0.339544415473938, 0.4618389904499054, 0.2670344412326813, 0.3482533395290375, 0.008868166245520115, 0.3527756929397583, 0.05651400610804558, 0.14240342378616333, 0.008844864554703236, 0.4510621130466461, 0.40566086769104004, 0.2466210573911667], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3650ad2b4f4d06890e2b8beb13f20640(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a81f8e974259482675dca744840ce209(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 62, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([62], dtype='float32', min=0, max=0.5),
            paddle.uniform([62], dtype='float32', min=0, max=0.5),
            paddle.uniform([62], dtype='float32', min=0, max=0.5),
            paddle.uniform([62], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b80ae4e260cda84a803e2503ecb4ef36(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 12, 12], dtype='float16', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_49a7acdba7c9f8217b8714059af12dae(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 26, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.02461903914809227, 0.42086055874824524, 0.43261027336120605, 0.12786583602428436, 0.22915984690189362, 0.1739252656698227, 0.1238892674446106, 0.1287832260131836, 0.4724400043487549, 0.3460504412651062, 0.4614929258823395, 0.09865536540746689, 0.008693902753293514, 0.22951893508434296, 0.22985199093818665, 0.20192959904670715, 0.10484085232019424, 0.20316345989704132, 0.13691124320030212, 0.15096619725227356, 0.04558394476771355, 0.18416360020637512, 0.26209285855293274, 0.4219284653663635, 0.2590503990650177, 0.15732328593730927], dtype='float32').reshape([26]),
            paddle.to_tensor([0.08697282522916794, 0.3328983783721924, 0.13953009247779846, 0.19402506947517395, 0.3096450865268707, 0.2910786271095276, 0.3217875063419342, 0.41010475158691406, 0.4531802237033844, 0.1978972852230072, 0.37239697575569153, 0.2372812181711197, 0.0049493154510855675, 0.4285922944545746, 0.10963094979524612, 0.009569250047206879, 0.2283031940460205, 0.4751283526420593, 0.34675145149230957, 0.20692747831344604, 0.46105921268463135, 0.3706545829772949, 0.22623564302921295, 0.09364817291498184, 0.4749802350997925, 0.24639557301998138], dtype='float32').reshape([26]),
            paddle.to_tensor([0.3361856937408447, 0.45097339153289795, 0.030318433418869972, 0.08417442440986633, 0.4452468156814575, 0.49062463641166687, 0.1439662128686905, 0.3362952768802643, 0.41491270065307617, 0.08490042388439178, 0.04882644861936569, 0.3280012607574463, 0.21730811893939972, 0.24427756667137146, 0.010758639313280582, 0.4754917025566101, 0.15461458265781403, 0.4817451536655426, 0.2973019480705261, 0.05441194027662277, 0.49850115180015564, 0.23203535377979279, 0.4072701930999756, 0.4870329797267914, 0.4305717349052429, 0.41616201400756836], dtype='float32').reshape([26]),
            paddle.to_tensor([0.30262133479118347, 0.45706507563591003, 0.3317515254020691, 0.3478795886039734, 0.3575659394264221, 0.17917616665363312, 0.28309422731399536, 0.3524860441684723, 0.21322230994701385, 0.44957682490348816, 0.3404575288295746, 0.43533003330230713, 0.29353460669517517, 0.40026214718818665, 0.0060175685212016106, 0.21979278326034546, 0.14067527651786804, 0.41970178484916687, 0.04284786060452461, 0.26811927556991577, 0.4057276248931885, 0.3017290234565735, 0.15947432816028595, 0.11140493303537369, 0.4858309030532837, 0.35675570368766785], dtype='float32').reshape([26]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0d546292a9e6e11534d0a40d4fee1436(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.14546138048171997, 0.31612879037857056, 0.047790370881557465, 0.16686905920505524, 0.1810610443353653, 0.2776558995246887, 0.2915889024734497, 0.27222713828086853, 0.40250200033187866, 0.2442094385623932, 0.43250298500061035, 0.3756241500377655, 0.33469676971435547, 0.04890412837266922, 0.2621222734451294, 0.0671716257929802, 0.41898319125175476, 0.20809535682201385, 0.3968232572078705, 0.379779189825058, 0.43698829412460327, 0.03590956702828407, 0.10764720290899277, 0.33653444051742554, 0.04111948609352112, 0.07068689912557602, 0.3932201564311981, 0.4500427544116974, 0.45090413093566895, 0.28823021054267883], dtype='float32').reshape([30]),
            paddle.to_tensor([0.0056714992970228195, 0.0906287282705307, 0.3753158152103424, 0.35118529200553894, 0.04794679582118988, 0.20443010330200195, 0.2419189214706421, 0.26214149594306946, 0.3076992928981781, 0.029539158567786217, 0.2514982223510742, 0.34819671511650085, 0.23640982806682587, 0.33435913920402527, 0.027373291552066803, 0.3232675790786743, 0.08640384674072266, 0.15832918882369995, 0.4418087303638458, 0.01987459883093834, 0.3841298222541809, 0.27891966700553894, 0.21936051547527313, 0.3782585859298706, 0.053520917892456055, 0.32869473099708557, 0.3275797665119171, 0.3070680499076843, 0.2707492411136627, 0.34944406151771545], dtype='float32').reshape([30]),
            paddle.to_tensor([0.19969743490219116, 0.2590426504611969, 0.14827188849449158, 0.4806121289730072, 0.22005011141300201, 0.4589380621910095, 0.37876102328300476, 0.27040615677833557, 0.469865083694458, 0.24367877840995789, 0.1925014853477478, 0.47926634550094604, 0.06857559084892273, 0.39427047967910767, 0.06504876911640167, 0.2794498801231384, 0.27514299750328064, 0.2510640323162079, 0.37981781363487244, 0.4891168773174286, 0.25098615884780884, 0.2578732371330261, 0.3073441982269287, 0.17435641586780548, 0.2648399770259857, 0.18451562523841858, 0.17964933812618256, 0.38609781861305237, 0.21179820597171783, 0.39252257347106934], dtype='float32').reshape([30]),
            paddle.to_tensor([0.02691897377371788, 0.42131757736206055, 0.11707395315170288, 0.4427196681499481, 0.17734093964099884, 0.1177918016910553, 0.03462966904044151, 0.1296049803495407, 0.35341012477874756, 0.4027259349822998, 0.44710689783096313, 0.2480306625366211, 0.002516822423785925, 0.021200934424996376, 0.3809060752391815, 0.14573296904563904, 0.017466697841882706, 0.32544219493865967, 0.3676910102367401, 0.09913672506809235, 0.12497304379940033, 0.4907895624637604, 0.41514500975608826, 0.1933000683784485, 0.41958358883857727, 0.06249653920531273, 0.46704286336898804, 0.23378026485443115, 0.25918570160865784, 0.10114261507987976], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_516912a9a3c4d9429ae562b758632c72(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([8, 128, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_68f2cd7a65d59005d894f319802fd6ab(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d1900f2e229a8c61388c6b41417f6e76(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 416, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9ae67bb515a7cd29d14650fe4a1492f2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1344, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1344], dtype='float32', min=0, max=0.5),
            paddle.uniform([1344], dtype='float32', min=0, max=0.5),
            paddle.uniform([1344], dtype='float32', min=0, max=0.5),
            paddle.uniform([1344], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_54a62a2b6084718ed110d0fd0c7d4061(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_304d885984149afbda27f1e5e49a3b9d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e54a6a35d70cac51a3d14aa2063b9f61(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 50, 84], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f7dc28fd944d58d79ee122c5ef98dcfb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 73, 73], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c0876997be363f1cdc0d5dbb3d88d4c2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5c875bb135463a4a7b81c16186a39c20(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4972613751888275, 0.47452324628829956, 0.3185339868068695, 0.08721967786550522, 0.35736480355262756, 0.3557851016521454, 0.16753536462783813, 0.09232474863529205, 0.11163076758384705, 0.39798423647880554, 0.48888394236564636, 0.12098349630832672, 0.14297258853912354, 0.29669222235679626, 0.2539498209953308, 0.3155530095100403, 0.010323449037969112, 0.43910983204841614, 0.42783233523368835, 0.4218880832195282, 0.35438209772109985, 0.1008312776684761, 0.2794000804424286, 0.034518301486968994], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3618564009666443, 0.3606865704059601, 0.08292853832244873, 0.04374063014984131, 0.3330572545528412, 0.04408472776412964, 0.4155109226703644, 0.1546269953250885, 0.47753092646598816, 0.2813098430633545, 0.033532705157995224, 0.3030685484409332, 0.4999738037586212, 0.0690637156367302, 0.1338869035243988, 0.2641092836856842, 0.23333929479122162, 0.21923020482063293, 0.36691048741340637, 0.4487248957157135, 0.1035970151424408, 0.30198442935943604, 0.44151055812835693, 0.32859256863594055], dtype='float32').reshape([24]),
            paddle.to_tensor([0.11035864800214767, 0.09958246350288391, 0.08523128926753998, 0.27213868498802185, 0.483985036611557, 0.2691710293292999, 0.35297244787216187, 0.39384201169013977, 0.42182013392448425, 0.34223610162734985, 0.14757652580738068, 0.07251206785440445, 0.22022193670272827, 0.011579161509871483, 0.1633484959602356, 0.18379244208335876, 0.13996432721614838, 0.22439248859882355, 0.1355779469013214, 0.3081853985786438, 0.4528125822544098, 0.446316123008728, 0.20932653546333313, 0.3297332525253296], dtype='float32').reshape([24]),
            paddle.to_tensor([0.28999555110931396, 0.17167888581752777, 0.3440361022949219, 0.16451339423656464, 0.35049453377723694, 0.27222132682800293, 0.40084537863731384, 0.06337563693523407, 0.12264728546142578, 0.44093018770217896, 0.4683937430381775, 0.2674066722393036, 0.17435118556022644, 0.1022004559636116, 0.07038921117782593, 0.11869978159666061, 0.43737390637397766, 0.15234652161598206, 0.3368654251098633, 0.08139289170503616, 0.041608650237321854, 0.207283154129982, 0.4943639934062958, 0.29691600799560547], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9dc3a67bc5ef1a1d7159954ae6c2b011(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 120, 120], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.027542419731616974, 0.3106735050678253, 0.3078296482563019, 0.1932656466960907, 0.2534685730934143, 0.26369526982307434, 0.12213154882192612, 0.2119065523147583, 0.0406128466129303, 0.06180012226104736, 0.49164384603500366, 0.48992860317230225, 0.08994297683238983, 0.4947603940963745, 0.3952505588531494, 0.18528935313224792, 0.4788302779197693, 0.1650593876838684, 0.4953077733516693, 0.3953242897987366, 0.40073874592781067, 0.3477518558502197, 0.4715059995651245, 0.1566247195005417], dtype='float32').reshape([24]),
            paddle.to_tensor([0.31404024362564087, 0.4539252817630768, 0.2110254317522049, 0.24287253618240356, 0.04415140673518181, 0.29369816184043884, 0.1908908486366272, 0.4484647810459137, 0.217947855591774, 0.1319340020418167, 0.2254106104373932, 0.1548568308353424, 0.081580251455307, 0.4424830973148346, 0.1019444465637207, 0.35964253544807434, 0.11824290454387665, 0.24132001399993896, 0.36239758133888245, 0.4253261387348175, 0.004952354356646538, 0.17962947487831116, 0.45843249559402466, 0.17749522626399994], dtype='float32').reshape([24]),
            paddle.to_tensor([0.09928315132856369, 0.29089149832725525, 0.4230223596096039, 0.33881238102912903, 0.022937318310141563, 0.42606550455093384, 0.4848536550998688, 0.4209841191768646, 0.32962581515312195, 0.21265444159507751, 0.426656037569046, 0.35741549730300903, 0.1321379691362381, 0.013217772357165813, 0.12326473742723465, 0.00040205050026997924, 0.2710137367248535, 0.2717877924442291, 0.36016741394996643, 0.28774693608283997, 0.05441838130354881, 0.2578405439853668, 0.3307074010372162, 0.274434894323349], dtype='float32').reshape([24]),
            paddle.to_tensor([0.48795583844184875, 0.3617652952671051, 0.12098300457000732, 0.32822170853614807, 0.09526366740465164, 0.28319108486175537, 0.1822321116924286, 0.2385750561952591, 0.3523627817630768, 0.44053882360458374, 0.48809412121772766, 0.45655566453933716, 0.11327411234378815, 0.486035019159317, 0.22192534804344177, 0.2189452052116394, 0.47469502687454224, 0.4431390166282654, 0.34017693996429443, 0.3089161217212677, 0.019541090354323387, 0.34816646575927734, 0.14568568766117096, 0.0788436233997345], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c329366d556152a023a72eee42310459(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.38783127069473267, 0.4587535560131073, 0.40166282653808594, 0.4723281264305115, 0.2480607032775879, 0.37616223096847534, 0.39296406507492065, 0.49136096239089966, 0.20590800046920776, 0.24323613941669464, 0.40281033515930176, 0.3375692367553711, 0.3371099829673767, 0.48387080430984497, 0.013963252305984497, 0.37210792303085327, 0.16020438075065613, 0.06701719015836716], dtype='float32').reshape([18]),
            paddle.to_tensor([0.0220587570220232, 0.46523743867874146, 0.24832405149936676, 0.21052400767803192, 0.05881328880786896, 0.30215907096862793, 0.39944955706596375, 0.11906129866838455, 0.32484322786331177, 0.08930902183055878, 0.26703646779060364, 0.2678030729293823, 0.3282381296157837, 0.4836645722389221, 0.4408758580684662, 0.46981650590896606, 0.3681778907775879, 0.21044951677322388], dtype='float32').reshape([18]),
            paddle.to_tensor([0.11350798606872559, 0.2655423879623413, 0.17257289588451385, 0.2888380289077759, 0.441450297832489, 0.27696311473846436, 0.001221556682139635, 0.11335045099258423, 0.2770225405693054, 0.022184718400239944, 0.1111873984336853, 0.01396515779197216, 0.19566631317138672, 0.4574737250804901, 0.04701157659292221, 0.07645062357187271, 0.2672935128211975, 0.2822067141532898], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3524734675884247, 0.49657636880874634, 0.3607279360294342, 0.09891883283853531, 0.36874452233314514, 0.439111590385437, 0.06467840075492859, 0.09002735465765, 0.07299450784921646, 0.11377103626728058, 0.14021749794483185, 0.304890513420105, 0.45771774649620056, 0.18593692779541016, 0.4178408682346344, 0.09548494964838028, 0.44165101647377014, 0.4848783314228058], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aca2d9e506d372ce10a921bb7b1efe39(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f2292afb0c54437f367224fa6ac5b5af(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a6ed405cf659c4787d83ff95f1485357(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ef898b02c7d645fc360c1daf96998b13(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 60, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_be777f27892e6dd2a8b65ed5d1f7f23f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.08779142051935196, 0.4998299181461334, 0.39954906702041626, 0.19534529745578766, 0.12928913533687592, 0.300979346036911, 0.38059815764427185, 0.2669924795627594, 0.37476006150245667, 0.4285140037536621, 0.04537809267640114, 0.4133063554763794, 0.18729613721370697, 0.26749446988105774, 0.34063389897346497, 0.447396844625473, 0.4230077862739563, 0.353587806224823, 0.10297924280166626, 0.14978159964084625, 0.30889803171157837, 0.47684070467948914, 0.49579131603240967, 0.3181775212287903, 0.3923523724079132, 0.09893667697906494, 0.49316704273223877, 0.4906292259693146, 0.19416244328022003, 0.4991823136806488], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3789932131767273, 0.0729014053940773, 0.24636395275592804, 0.1896245926618576, 0.06708073616027832, 0.43959227204322815, 0.34932398796081543, 0.33018723130226135, 0.4049590528011322, 0.08195944875478745, 0.4196246862411499, 0.3973764479160309, 0.14863330125808716, 0.15791650116443634, 0.3663044571876526, 0.17293578386306763, 0.322348415851593, 0.07341685146093369, 0.09020832926034927, 0.23709991574287415, 0.3715599775314331, 0.10403847694396973, 0.2944178879261017, 0.10547183454036713, 0.19233591854572296, 0.01434971485286951, 0.13203054666519165, 0.31192803382873535, 0.25720757246017456, 0.17816846072673798], dtype='float32').reshape([30]),
            paddle.to_tensor([0.011639355681836605, 0.3089585304260254, 0.06349502503871918, 0.30833274126052856, 0.3700224757194519, 0.06272266805171967, 0.02442893385887146, 0.06600745767354965, 0.42138001322746277, 0.43847212195396423, 0.09502442926168442, 0.2582194209098816, 0.2868044376373291, 0.4882644712924957, 0.2387264221906662, 0.24280034005641937, 0.03265930712223053, 0.30468520522117615, 0.02516251988708973, 0.027559099718928337, 0.3082428276538849, 0.27862241864204407, 0.019606217741966248, 0.28208503127098083, 0.09220117330551147, 0.23007827997207642, 0.31915539503097534, 0.35555580258369446, 0.25694119930267334, 0.3836580812931061], dtype='float32').reshape([30]),
            paddle.to_tensor([0.07299397885799408, 0.4125114381313324, 0.32465144991874695, 0.3268900513648987, 0.04887765645980835, 0.3314109146595001, 0.4518875181674957, 0.3103989362716675, 0.07011920213699341, 0.2428426742553711, 0.20774489641189575, 0.3332127332687378, 0.07619993388652802, 0.1789465695619583, 0.13894613087177277, 0.3779745101928711, 0.09486398100852966, 0.19183038175106049, 0.22714173793792725, 0.08884140104055405, 0.2642912268638611, 0.045637425035238266, 0.24160918593406677, 0.010692537762224674, 0.22616393864154816, 0.3076825737953186, 0.36963292956352234, 0.4645408093929291, 0.41681966185569763, 0.04574815556406975], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_de893945915a21b8552902a46df26c64(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.47642454504966736, 0.3843069076538086, 0.26128560304641724, 0.3967403471469879, 0.07627438753843307, 0.2688705027103424, 0.3105244040489197, 0.3194432854652405, 0.47744134068489075, 0.12844142317771912, 0.33645546436309814, 0.4274417757987976, 0.12344878911972046, 0.39287522435188293, 0.0675618126988411, 0.4698594808578491, 0.1914316713809967, 0.06930914521217346, 0.188674658536911, 0.33775222301483154, 0.4147154986858368, 0.06627783924341202, 0.4513569176197052, 0.48653995990753174, 0.06081810221076012, 0.15570658445358276, 0.03803786262869835, 0.04319063574075699, 0.2876277267932892, 0.10794652998447418], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3774990141391754, 0.18462178111076355, 0.22711050510406494, 0.19853579998016357, 0.0939556360244751, 0.13964426517486572, 0.32802850008010864, 0.03300559148192406, 0.3564373552799225, 0.26025497913360596, 0.37238168716430664, 0.27941814064979553, 0.01664802059531212, 0.2086709886789322, 0.24587255716323853, 0.3168933391571045, 0.46291500329971313, 0.492944598197937, 0.2198963165283203, 0.44388434290885925, 0.0959101989865303, 0.3214467465877533, 0.1622922420501709, 0.033184271305799484, 0.4515220820903778, 0.11618221551179886, 0.2768895626068115, 0.07947956770658493, 0.3142755329608917, 0.14602908492088318], dtype='float32').reshape([30]),
            paddle.to_tensor([0.053513940423727036, 0.27837806940078735, 0.2407517433166504, 0.22614628076553345, 0.28477126359939575, 0.45884037017822266, 0.4494498670101166, 0.3546009957790375, 0.4139555096626282, 0.3098005950450897, 0.33478042483329773, 0.3297714293003082, 0.49483761191368103, 0.4028536081314087, 0.1417466700077057, 0.42756983637809753, 0.28876134753227234, 0.17495819926261902, 0.25828662514686584, 0.41094329953193665, 0.46602192521095276, 0.16207174956798553, 0.43060174584388733, 0.3590244948863983, 0.13257959485054016, 0.41217824816703796, 0.28659749031066895, 0.28526216745376587, 0.2661205530166626, 0.3243154287338257], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4332844316959381, 0.1700391173362732, 0.21660028398036957, 0.43739399313926697, 0.407402902841568, 0.16651172935962677, 0.01239914633333683, 0.42259714007377625, 0.1359664350748062, 0.12734439969062805, 0.17780324816703796, 0.07204530388116837, 0.44559240341186523, 0.07528829574584961, 0.4033139944076538, 0.16188295185565948, 0.4000277817249298, 0.35957884788513184, 0.29201799631118774, 0.31629467010498047, 0.4580695629119873, 0.43724384903907776, 0.15580275654792786, 0.45662155747413635, 0.31239500641822815, 0.3783590495586395, 0.4635009467601776, 0.4467927813529968, 0.4341062307357788, 0.342555433511734], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_be815a2b0ac5251ae6194b6067b61244(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 112, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cdee1e03ba4d7666484c28dd5e647930(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 480, 480], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.0795273631811142, 0.11113493889570236, 0.4787251055240631, 0.3696681261062622, 0.039674047380685806, 0.4718158543109894, 0.3723421096801758, 0.27295371890068054], dtype='float32').reshape([8]),
            paddle.to_tensor([0.24970883131027222, 0.060478053987026215, 0.31049349904060364, 0.3722791373729706, 0.09998645633459091, 0.33255499601364136, 0.4710150957107544, 0.08429224789142609], dtype='float32').reshape([8]),
            paddle.to_tensor([0.24106557667255402, 0.220005065202713, 0.031236831098794937, 0.4171915054321289, 0.35671013593673706, 0.21360385417938232, 0.4801636040210724, 0.156559556722641], dtype='float32').reshape([8]),
            paddle.to_tensor([0.16782313585281372, 0.4510577917098999, 0.09642624855041504, 0.029223667457699776, 0.4182383418083191, 0.20531737804412842, 0.10336810350418091, 0.10279162228107452], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4bfda9cd51f564a6caac57ee6d5345eb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 144, 48, 48], dtype='float16', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cfe80bd7385ed31a3e4a6af7dff90fa0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3031339645385742, 0.13860097527503967, 0.11144803464412689, 0.4234522581100464, 0.004500387702137232, 0.3829321563243866, 0.4160613715648651, 0.08900746703147888, 0.10827374458312988, 0.2238244116306305, 0.31632015109062195, 0.2845032215118408, 0.4907018542289734, 0.30544501543045044, 0.21662048995494843, 0.4066753685474396, 0.30671942234039307, 0.05625557899475098, 0.3140740990638733, 0.22042924165725708, 0.3651198148727417, 0.06152705103158951, 0.23558874428272247, 0.38697144389152527], dtype='float32').reshape([24]),
            paddle.to_tensor([0.468165785074234, 0.18668358027935028, 0.11536182463169098, 0.02798302471637726, 0.4154796898365021, 0.020217014476656914, 0.33104580640792847, 0.3256261646747589, 0.33297744393348694, 0.18285498023033142, 0.2346099615097046, 0.1177658885717392, 0.1569570153951645, 0.04988858476281166, 0.05010605230927467, 0.47240060567855835, 0.13016654551029205, 0.25493311882019043, 0.14777645468711853, 0.3702820837497711, 0.201829195022583, 0.012273083440959454, 0.3997962772846222, 0.22065284848213196], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4196085035800934, 0.07594601064920425, 0.1731671690940857, 0.17184174060821533, 0.2647983133792877, 0.24687844514846802, 0.049256037920713425, 0.0804668515920639, 0.23270751535892487, 0.191560760140419, 0.1907079666852951, 0.06950134038925171, 0.30033233761787415, 0.3589155673980713, 0.0333201065659523, 0.0728735625743866, 0.09204448759555817, 0.3875676095485687, 0.26022401452064514, 0.40727362036705017, 0.25303915143013, 0.08660241216421127, 0.26459378004074097, 0.39533013105392456], dtype='float32').reshape([24]),
            paddle.to_tensor([0.014886106364428997, 0.24366793036460876, 0.368619829416275, 0.07769954204559326, 0.10045995563268661, 0.42816177010536194, 0.35689041018486023, 0.016403723508119583, 0.4883497655391693, 0.1584380567073822, 0.39052727818489075, 0.3503842055797577, 0.1633126437664032, 0.1278715282678604, 0.10400933027267456, 0.40314754843711853, 0.1013537049293518, 0.2994646430015564, 0.0923011302947998, 0.4532600939273834, 0.06041519343852997, 0.33166199922561646, 0.15230993926525116, 0.2403322011232376], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8cf7316702ee2c8c243bc7088e656a3e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_802888ebeb83c657512a18e38bcc362a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 38, 38], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_546d57d58adbde1715af6dce37e0207a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 2, 6], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_272123507c34111166d6f9d764fcb4b8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cdc3b0a148b551f86897e4668e0bdb6b
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 4], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_eac502bad7ee4bc47a14963db719cbb0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 12, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2c2a09a24d44c2ceeb45aa131c75c877(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 32, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.11139935255050659, 0.4123745858669281, 0.05717657506465912, 0.27314919233322144, 0.04474325105547905, 0.01560918241739273, 0.1052924245595932, 0.06051238626241684], dtype='float32').reshape([8]),
            paddle.to_tensor([0.17016687989234924, 0.28595849871635437, 0.32385990023612976, 0.3244142532348633, 0.23813651502132416, 0.21497194468975067, 0.1460774838924408, 0.3771185576915741], dtype='float32').reshape([8]),
            paddle.to_tensor([0.33703625202178955, 0.18261341750621796, 0.239778071641922, 0.05711693316698074, 0.2626039683818817, 0.2510426640510559, 0.08560764789581299, 0.0038055330514907837], dtype='float32').reshape([8]),
            paddle.to_tensor([0.19601762294769287, 0.46727263927459717, 0.21489344537258148, 0.41313737630844116, 0.4368182420730591, 0.41465017199516296, 0.4183128774166107, 0.32882770895957947], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b2ae34095d578abb1415e0733faed9d7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_f5f2213be92f20fda5378beeed72688f
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 2, 2], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a77dec3bd112bdea3d88d797b6668f18(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 768, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a7e8bbdb3f70252741167c32b13c8367(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 150, 150], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_754cf3adee4fd03f0fe313736991c4ee(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 2, 2], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fca3640fa9744c6d4761462fcdd3046a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 1, 2], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6c7085e4f7dbbbc469ad4da2dd49bbd3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 320, 320], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fce65fb236704f8df35b21778860d531(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 17, 17], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a144257015dac998fd38cf24e8061650(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 256, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c4bee5819c3d00a59574ced23ae964ae(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 12, 12], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_189560c36d66c4e23ef6668aed2a6292(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 304, 304], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f5a63c8367b4ebf3ff6c3b47ed78a531(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_510485e7284d5ea8505c5705178e8df8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3350035548210144, 0.09299838542938232, 0.31389352679252625, 0.49774831533432007, 0.04279283434152603, 0.056830357760190964, 0.05520748719573021, 0.1937422901391983, 0.47293806076049805, 0.3718375861644745, 0.2276928871870041, 0.12342880666255951, 0.0025406810455024242, 0.4579903781414032, 0.20783096551895142, 0.41531577706336975, 0.4349029064178467, 0.09982666373252869], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3707464337348938, 0.08606749773025513, 0.3173885643482208, 0.3220241069793701, 0.0011807384435087442, 0.37040582299232483, 0.3399348855018616, 0.3072671890258789, 0.08926578611135483, 0.22531534731388092, 0.1509171575307846, 0.4352571368217468, 0.35144054889678955, 0.3678898215293884, 0.3709315061569214, 0.11633513122797012, 0.09842461347579956, 0.3971705138683319], dtype='float32').reshape([18]),
            paddle.to_tensor([0.1845512092113495, 0.3816230595111847, 0.18574918806552887, 0.11356648802757263, 0.25979530811309814, 0.09479640424251556, 0.30093592405319214, 0.28184354305267334, 0.04625933989882469, 0.34316712617874146, 0.34442833065986633, 0.21286381781101227, 0.3869290053844452, 0.19413495063781738, 0.397237092256546, 0.42221733927726746, 0.4027528464794159, 0.02401771768927574], dtype='float32').reshape([18]),
            paddle.to_tensor([0.01827838458120823, 0.3598572313785553, 0.06018012389540672, 0.2604024410247803, 0.49550825357437134, 0.4652923345565796, 0.23433007299900055, 0.35179245471954346, 0.11172886937856674, 0.17574049532413483, 0.10752595961093903, 0.4412018060684204, 0.014347745105624199, 0.24632981419563293, 0.35824722051620483, 0.05532157048583031, 0.14554932713508606, 0.17685946822166443], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7d7fc51e65ccbd9a2c21e22a7c425b62(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 4, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_13492e0533079f4e9df4399a547c648c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 608, 608], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8cffce9648203988a5f986e3ac4c4f15(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 320, 320], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_df1cfa7c96f703bf8903e4c3958e05ca(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1760, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1760], dtype='float32', min=0, max=0.5),
            paddle.uniform([1760], dtype='float32', min=0, max=0.5),
            paddle.uniform([1760], dtype='float32', min=0, max=0.5),
            paddle.uniform([1760], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b595bf894a8205c40f28878331e17792(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b5b8e9ff916c554ce7265e7c8a766543(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 4, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_93525d94ea551542a1ce1b3867134a97(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_66dd600bf74cd6e470df63df0bd5aa83(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 432, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_392f345d39880094263f542fc25200c0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6e3dce464c34a0d9ce2b67e65e0d071e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.41563186049461365, 0.11207275837659836, 0.2949613630771637, 0.07533783465623856, 0.12435571104288101, 0.3020845353603363, 0.3885877728462219, 0.24148985743522644], dtype='float32').reshape([8]),
            paddle.to_tensor([0.20026785135269165, 0.14838331937789917, 0.27045878767967224, 0.09875363111495972, 0.21759602427482605, 0.15146851539611816, 0.10476715117692947, 0.33092570304870605], dtype='float32').reshape([8]),
            paddle.to_tensor([0.4102899432182312, 0.11399076133966446, 0.3586408197879791, 0.35331660509109497, 0.11815668642520905, 0.318318247795105, 0.474206805229187, 0.1631876826286316], dtype='float32').reshape([8]),
            paddle.to_tensor([0.21815690398216248, 0.4309990406036377, 0.16142432391643524, 0.4034181833267212, 0.45277681946754456, 0.4376911520957947, 0.011676653288304806, 0.11542173475027084], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9fd003444bad580c146954150c333c1d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e5515205ff6279da181e30520651049c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d4d7b7f27bc1ec501550c5de43cb33b1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 10, 10], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b6c67ebce72c9370858852a910970c0c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 42, 1, 1], dtype='float32', min=0, max=0.5),
            paddle.uniform([42], dtype='float32', min=0, max=0.5),
            paddle.uniform([42], dtype='float32', min=0, max=0.5),
            paddle.uniform([42], dtype='float32', min=0, max=0.5),
            paddle.uniform([42], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_09767093dac4a55887e850076c5f6e05(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f536b4656537b8c9fafa34f5be9cc133(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c91106b3a10dca6aa63c7948fca33bd3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c438e75b76a49b0f4e750d515c27a56c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1584, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1584], dtype='float32', min=0, max=0.5),
            paddle.uniform([1584], dtype='float32', min=0, max=0.5),
            paddle.uniform([1584], dtype='float32', min=0, max=0.5),
            paddle.uniform([1584], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0d0d368441f890e35e0b943273997d24(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0f23ce17efe7cf88a607a2f30fb8693a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 480, 480], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.049769047647714615, 0.4538155198097229, 0.4214932322502136, 0.056680645793676376, 0.289039671421051, 0.17625753581523895, 0.2375190556049347, 0.2449282556772232], dtype='float32').reshape([8]),
            paddle.to_tensor([0.006214232183992863, 0.23206381499767303, 0.11749991029500961, 0.17281344532966614, 0.03168090060353279, 0.4094850420951843, 0.3216913938522339, 0.22033065557479858], dtype='float32').reshape([8]),
            paddle.to_tensor([0.478493869304657, 0.262562096118927, 0.2284967303276062, 0.05433718115091324, 0.23720602691173553, 0.40368902683258057, 0.3842984735965729, 0.2023792266845703], dtype='float32').reshape([8]),
            paddle.to_tensor([0.26316842436790466, 0.24154244363307953, 0.05554436519742012, 0.4508073031902313, 0.07974535971879959, 0.003320112358778715, 0.4676670432090759, 0.2541852593421936], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_518c63d08a0a618b675e0240f4a2b37a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 240, 240], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e5991472f2896dda80674a6de20b6ead(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 512, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5f7d4591ab0b12280abc4cb98151ee1b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ead4e9297cfea0c20f4b600cf2ae66d7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_31c35f788635ef8457295daabc7fdd21(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bf66b1c5b32f7fc6bda3d211a9848a83(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 256, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d9ea0719a3161af562549ff5d6b9c451(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_33e96beb36c8f4ea1d7968a89a59e12b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.04860871285200119, 0.47327637672424316, 0.27734804153442383, 0.4145191013813019, 0.3995008170604706, 0.3452295660972595, 0.37910446524620056, 0.3556990325450897, 0.45805883407592773, 0.4356819689273834, 0.49224621057510376, 0.15916313230991364, 0.09771028161048889, 0.38037121295928955, 0.3398989140987396, 0.2680998146533966, 0.3742370903491974, 0.43741104006767273, 0.19776952266693115, 0.3201477825641632], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4261849820613861, 0.007117441855370998, 0.3069949448108673, 0.29014042019844055, 0.09921535104513168, 0.020874476060271263, 0.4967038631439209, 0.2666604816913605, 0.39914655685424805, 0.18968959152698517, 0.3797762393951416, 0.3199397623538971, 0.4684675633907318, 0.21983595192432404, 0.05949723720550537, 0.45794183015823364, 0.33804482221603394, 0.4177098572254181, 0.33076873421669006, 0.27582618594169617], dtype='float32').reshape([20]),
            paddle.to_tensor([0.1535036414861679, 0.3644997477531433, 0.18700292706489563, 0.38374167680740356, 0.2598721981048584, 0.3758178651332855, 0.23676888644695282, 0.43062344193458557, 0.21174168586730957, 0.3838135004043579, 0.0958828330039978, 0.18389523029327393, 0.2118738293647766, 0.3629392683506012, 0.11933835595846176, 0.03983624652028084, 0.1148252859711647, 0.3050476610660553, 0.27625322341918945, 0.2981572151184082], dtype='float32').reshape([20]),
            paddle.to_tensor([0.12661230564117432, 0.39888808131217957, 0.3876330256462097, 0.42630666494369507, 0.1311868578195572, 0.17859593033790588, 0.08144524693489075, 0.20896196365356445, 0.3478817939758301, 0.15033015608787537, 0.4731173515319824, 0.011769495904445648, 0.3339359760284424, 0.2035197764635086, 0.026680193841457367, 0.3214057981967926, 0.10093709826469421, 0.09969199448823929, 0.17811647057533264, 0.05728210508823395], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_565a3604b7f82964097be11ffb38d124(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_60f8b95aba499b85b6b22561e5c747f1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2732b484daaf07b6fce2deb250de2a2a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1200, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_934ad465a492909700d4b4e94475bb82(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 320, 320], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_729b5693df111aa3cb8c20dfd7e0f370(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 320, 320], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.45560574531555176, 0.11176196485757828, 0.23053525388240814, 0.35333600640296936, 0.3891109526157379, 0.22341319918632507, 0.4086264371871948, 0.48125892877578735, 0.1963760256767273, 0.18021446466445923, 0.398040235042572, 0.4326294958591461, 0.2937089204788208, 0.07978580892086029, 0.2382284551858902, 0.03865577653050423], dtype='float32').reshape([16]),
            paddle.to_tensor([0.047353826463222504, 0.20998020470142365, 0.2097318172454834, 0.10045219957828522, 0.40863698720932007, 0.21877172589302063, 0.062044233083724976, 0.4350493848323822, 0.18859902024269104, 0.4784887135028839, 0.34669119119644165, 0.31707996129989624, 0.0833483338356018, 0.06330916285514832, 0.004107224754989147, 0.3559228181838989], dtype='float32').reshape([16]),
            paddle.to_tensor([0.22011229395866394, 0.2737288177013397, 0.14429351687431335, 0.3376181423664093, 0.4398023188114166, 0.4110814034938812, 0.03262761980295181, 0.41932785511016846, 0.24926835298538208, 0.015649255365133286, 0.47711560130119324, 0.18714210391044617, 0.46209612488746643, 0.04253441467881203, 0.30584293603897095, 0.23237313330173492], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3524637818336487, 0.27097219228744507, 0.3711303174495697, 0.2709904909133911, 0.16912660002708435, 0.19663676619529724, 0.372124046087265, 0.11557014286518097, 0.18315422534942627, 0.22968976199626923, 0.36172589659690857, 0.06727693974971771, 0.089671291410923, 0.33558011054992676, 0.4480047821998596, 0.23726089298725128], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fa947a845c73dacbdac05771f3ac2dfa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1584, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1584], dtype='float32', min=0, max=0.5),
            paddle.uniform([1584], dtype='float32', min=0, max=0.5),
            paddle.uniform([1584], dtype='float32', min=0, max=0.5),
            paddle.uniform([1584], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d6fa81d7c482fdaec7b4e6d1f2bb127a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 16, 32], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3896304965019226, 0.043298061937093735, 0.2981126010417938, 0.16352559626102448, 0.11974181234836578, 0.11624480038881302, 0.4118940234184265, 0.28828006982803345, 0.11662048101425171, 0.15882226824760437, 0.011905156075954437, 0.3087155818939209, 0.024601101875305176, 0.06757113337516785, 0.27476945519447327, 0.32212144136428833, 0.4808935225009918, 0.08312084525823593], dtype='float32').reshape([18]),
            paddle.to_tensor([0.03904689848423004, 0.28342437744140625, 0.4447690546512604, 0.32570111751556396, 0.3169843554496765, 0.4978921413421631, 0.4171904921531677, 0.31142181158065796, 0.10776109248399734, 0.3709220290184021, 0.32538846135139465, 0.1758747547864914, 0.2131073921918869, 0.0971715971827507, 0.2246934026479721, 0.0704929381608963, 0.33720144629478455, 0.0710332989692688], dtype='float32').reshape([18]),
            paddle.to_tensor([0.11694718152284622, 0.40339842438697815, 0.252634733915329, 0.1042252704501152, 0.2823042571544647, 0.35383859276771545, 0.40185558795928955, 0.029756976291537285, 0.22854958474636078, 0.3230944275856018, 0.37090983986854553, 0.04367918521165848, 0.19113785028457642, 0.05373667553067207, 0.44583213329315186, 0.16873550415039062, 0.4518727660179138, 0.010540690273046494], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3582039773464203, 0.022904008626937866, 0.3261069357395172, 0.46243852376937866, 0.19478023052215576, 0.2711371183395386, 0.3081350326538086, 0.2770988941192627, 0.10020414739847183, 0.25640785694122314, 0.44215840101242065, 0.46483322978019714, 0.010237242095172405, 0.41931700706481934, 0.41086438298225403, 0.2987391948699951, 0.03355736285448074, 0.38199084997177124], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3d60d788c7e855efd68abdc8824fc071(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.35817188024520874, 0.4082985520362854, 0.3891002833843231, 0.4555891752243042, 0.4879056215286255, 0.2578999698162079, 0.17798824608325958, 0.31632742285728455, 0.050440870225429535, 0.3549026846885681, 0.31588929891586304, 0.025868158787488937, 0.05383786931633949, 0.27012577652931213, 0.16270901262760162, 0.016541799530386925, 0.2627268433570862, 0.18028073012828827, 0.0576520599424839, 0.47645777463912964, 0.4622928202152252, 0.06249315291643143, 0.07799925655126572, 0.05531274899840355, 0.1589588224887848, 0.43818673491477966, 0.48533278703689575, 0.43330827355384827, 0.04600633308291435, 0.2537596523761749], dtype='float32').reshape([30]),
            paddle.to_tensor([0.33347073197364807, 0.17147445678710938, 0.04114319756627083, 0.14018261432647705, 0.3822431266307831, 0.32920363545417786, 0.3280927240848541, 0.30485308170318604, 0.4192424714565277, 0.10234016925096512, 0.15601320564746857, 0.20275959372520447, 0.14927390217781067, 0.055628612637519836, 0.1475018709897995, 0.3014001250267029, 0.46851056814193726, 0.32479608058929443, 0.01679147221148014, 0.4463457763195038, 0.04061012715101242, 0.02981981448829174, 0.19096121191978455, 0.33459439873695374, 0.15019762516021729, 0.07138971984386444, 0.19360710680484772, 0.4091546833515167, 0.10178005695343018, 0.13846439123153687], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2134794443845749, 0.10630913823843002, 0.21724243462085724, 0.45324191451072693, 0.1924588978290558, 0.2093057781457901, 0.30453476309776306, 0.48214295506477356, 0.03845975548028946, 0.053148623555898666, 0.46465352177619934, 0.045820266008377075, 0.16672931611537933, 0.11638405919075012, 0.4270455241203308, 0.4445870518684387, 0.08605054765939713, 0.3368335962295532, 0.4280528128147125, 0.16456753015518188, 0.4132763743400574, 0.15008574724197388, 0.45807039737701416, 0.29357773065567017, 0.06739121675491333, 0.3509026765823364, 0.2697173058986664, 0.09299878776073456, 0.4242611825466156, 0.0008463484118692577], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3958642780780792, 0.32683131098747253, 0.20287925004959106, 0.043584708124399185, 0.2532821297645569, 0.39832547307014465, 0.03593699634075165, 0.45378294587135315, 0.4075140357017517, 0.1827603429555893, 0.07133834809064865, 0.07872722297906876, 0.3777507543563843, 0.48190823197364807, 0.03429297357797623, 0.2362842708826065, 0.34306007623672485, 0.07515012472867966, 0.3443259298801422, 0.18682336807250977, 0.3453412652015686, 0.025920338928699493, 0.13232199847698212, 0.04607901722192764, 0.4253842830657959, 0.0562933087348938, 0.40367791056632996, 0.2893286645412445, 0.28035739064216614, 0.15176090598106384], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e9e03ce9eb9db3725cf5453210511178(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c6c59d5f619a2ab9a0109c016626db6d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5421538e781890ee61cf6601fa2192e9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1e777253f8d8f1d4cf511be76c17d2dd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 144, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3ed55101396cef1aebedb1b2f7de279b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a936ebbaeefe74ddbe5ac4f4d0f6f028(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 82, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_379682914a50f73a362f544097390dab(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.43830224871635437, 0.16110990941524506, 0.4648418724536896, 0.06981060653924942, 0.36618772149086, 0.3584263026714325, 0.26326510310173035, 0.1461438089609146, 0.18844819068908691, 0.06428364664316177, 0.3308596909046173, 0.2040615826845169, 0.24911029636859894, 0.02217710018157959, 0.1632426530122757, 0.33422237634658813], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4998878836631775, 0.06558892875909805, 0.006149707827717066, 0.4867491126060486, 0.43813037872314453, 0.2977840006351471, 0.4363701343536377, 0.06833373010158539, 0.04287801682949066, 0.37661781907081604, 0.24703246355056763, 0.4968550205230713, 0.08104199171066284, 0.28615009784698486, 0.08024467527866364, 0.004745127633213997], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2536034882068634, 0.2465071976184845, 0.17452208697795868, 0.46261319518089294, 0.48617565631866455, 0.14850910007953644, 0.27001115679740906, 0.30542999505996704, 0.14529307186603546, 0.1397794783115387, 0.28722190856933594, 0.31399303674697876, 0.2764957547187805, 0.026056600734591484, 0.21864232420921326, 0.4580531716346741], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3883523941040039, 0.2532498836517334, 0.19351424276828766, 0.30413514375686646, 0.09438362717628479, 0.2299153208732605, 0.44717171788215637, 0.02836773917078972, 0.1711803674697876, 0.3553308844566345, 0.04378075897693634, 0.022352131083607674, 0.17368295788764954, 0.14664193987846375, 0.3708082139492035, 0.3059610426425934], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_81c17e36b9c6eefa66b6ef59a7b7bebc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 10, 10], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fe53feab6295160b163666f79f5bcc04(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 88, 61, 61], dtype='float16', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2c9785c31f54986e115c311a9f1c64ab(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.42050421237945557, 0.4721742272377014, 0.005781600251793861, 0.354946106672287, 0.07178562134504318, 0.13878384232521057, 0.11000168323516846, 0.16029874980449677, 0.34506121277809143, 0.2944631278514862, 0.19161897897720337, 0.01658821478486061], dtype='float32').reshape([12]),
            paddle.to_tensor([0.07151760160923004, 0.21738943457603455, 0.18771056830883026, 0.05806198716163635, 0.06798235327005386, 0.3741544783115387, 0.16242805123329163, 0.06694992631673813, 0.39773160219192505, 0.010929767973721027, 0.14957089722156525, 0.4120323359966278], dtype='float32').reshape([12]),
            paddle.to_tensor([0.3389200270175934, 0.16846545040607452, 0.2424820065498352, 0.02285705879330635, 0.1519252359867096, 0.38279953598976135, 0.09152041375637054, 0.26142728328704834, 0.2836301624774933, 0.4243457019329071, 0.1742284893989563, 0.4210890531539917], dtype='float32').reshape([12]),
            paddle.to_tensor([0.4350106418132782, 0.19826100766658783, 0.3350091278553009, 0.4956262707710266, 0.10551168769598007, 0.10325878113508224, 0.2877415716648102, 0.08993422240018845, 0.478706955909729, 0.22614282369613647, 0.11781137436628342, 0.11062006652355194], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_26efaf8d5bcabc8f8b9ee03ab166b082(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 16, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1f046f263702d5a3cce2c5248aef2cc6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.19714513421058655, 0.3823363482952118, 0.14822611212730408, 0.21775054931640625, 0.06418587267398834, 0.22701434791088104, 0.00867227092385292, 0.01233001984655857, 0.15884603559970856, 0.3409154713153839, 0.006313581485301256, 0.49638083577156067, 0.29785147309303284, 0.23686979711055756, 0.42146971821784973, 0.19818417727947235, 0.23703770339488983, 0.08961614221334457, 0.18428604304790497, 0.15763601660728455, 0.1397249400615692, 0.3241387903690338, 0.2533585727214813, 0.012499559670686722], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2829122245311737, 0.16199703514575958, 0.4628830552101135, 0.24441960453987122, 0.4656645953655243, 0.34076741337776184, 0.40144526958465576, 0.378135085105896, 0.29869046807289124, 0.24432800710201263, 0.2345404475927353, 0.3695029616355896, 0.4232783019542694, 0.3315163850784302, 0.3352010250091553, 0.23807111382484436, 0.23839575052261353, 0.0958755612373352, 0.15601055324077606, 0.3563121259212494, 0.29064199328422546, 0.35010072588920593, 0.16504999995231628, 0.20100684463977814], dtype='float32').reshape([24]),
            paddle.to_tensor([0.30301353335380554, 0.2926294207572937, 0.38726356625556946, 0.36879199743270874, 0.009666107594966888, 0.37432289123535156, 0.15431071817874908, 0.35475054383277893, 0.32010939717292786, 0.3880869150161743, 0.054662302136421204, 0.12446334958076477, 0.13538743555545807, 0.3660459518432617, 0.24730443954467773, 0.043788257986307144, 0.3405194878578186, 0.08305467665195465, 0.41606271266937256, 0.34623846411705017, 0.045374151319265366, 0.08822724968194962, 0.39357540011405945, 0.1500757485628128], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4398519992828369, 0.15949884057044983, 0.33358854055404663, 0.3112134337425232, 0.48253580927848816, 0.3943292796611786, 0.04862548038363457, 0.32142776250839233, 0.3378260135650635, 0.24152663350105286, 0.08548352867364883, 0.4699834883213043, 0.2034381479024887, 0.0173058919608593, 0.09178853780031204, 0.20377641916275024, 0.49553024768829346, 0.3429281711578369, 0.17926931381225586, 0.08701635897159576, 0.1534474790096283, 0.37978485226631165, 0.22618453204631805, 0.17477846145629883], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6390db9e941d4b2588b0f93d072cdc44(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_db21dbeb3a4c11619e62c13cf8555ba6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 480, 480], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.17760048806667328, 0.16558881103992462, 0.014332612045109272, 0.0865204706788063, 0.02436278946697712, 0.2992015779018402, 0.15600769221782684, 0.07663706690073013], dtype='float32').reshape([8]),
            paddle.to_tensor([0.3180259168148041, 0.2912239730358124, 0.06499075144529343, 0.47715523838996887, 0.2630048394203186, 0.1481064110994339, 0.09710544347763062, 0.19247359037399292], dtype='float32').reshape([8]),
            paddle.to_tensor([0.2631453573703766, 0.024827174842357635, 0.27558326721191406, 0.3583705723285675, 0.23689740896224976, 0.3217276632785797, 0.17132015526294708, 0.090987429022789], dtype='float32').reshape([8]),
            paddle.to_tensor([0.2369135171175003, 0.07652095705270767, 0.22820819914340973, 0.2734222114086151, 0.08063525706529617, 0.46153146028518677, 0.06101979687809944, 0.1955530047416687], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9bcf356110d246c8d7fb3a578628f16f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 112, 199], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4342d2501d2cd0876870d79d2ab66e74(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fb3adf16d3f9d3d744ae5d8fd754f72c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([8, 1024, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bb1e7b68aeb68ed60e99149084e26259(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_f5f2213be92f20fda5378beeed72688f
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9a5e4fd533f3ef908a0a0957f57690ca(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 32, 8, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_917cc8a4b10b86d5c2c65cf264ee37c2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e5eeb78d95074cfe82c876ece07775ff(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_145ad20e7a3b3c8aa31f6b40d4fe3d45
    def get_inputs(self):
        return [
            paddle.uniform([16, 512], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b8684cfc38dc9f207c530bc59d7c6dfc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1728, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1728], dtype='float32', min=0, max=0.5),
            paddle.uniform([1728], dtype='float32', min=0, max=0.5),
            paddle.uniform([1728], dtype='float32', min=0, max=0.5),
            paddle.uniform([1728], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_49d82d5dd02f27a9e4678d7bb78c07bf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.22273151576519012, 0.23833248019218445, 0.11281756311655045, 0.37448349595069885, 0.4650901257991791, 0.25962570309638977, 0.2065524458885193, 0.38891610503196716, 0.3723548650741577, 0.3939061462879181, 0.151585653424263, 0.16021806001663208, 0.18880194425582886, 0.02719215303659439, 0.0880589410662651, 0.15670567750930786, 0.31226277351379395, 0.055836934596300125, 0.03286733850836754, 0.1930626630783081], dtype='float32').reshape([20]),
            paddle.to_tensor([0.2511211931705475, 0.4999711215496063, 0.3307742774486542, 0.3377109467983246, 0.059004560112953186, 0.2731640934944153, 0.27904602885246277, 0.2861524224281311, 0.36139896512031555, 0.293613076210022, 0.1641344428062439, 0.011983223259449005, 0.49716848134994507, 0.3721431791782379, 0.18401294946670532, 0.12128856778144836, 0.44107434153556824, 0.3015795052051544, 0.09924943000078201, 0.207818403840065], dtype='float32').reshape([20]),
            paddle.to_tensor([0.01515748631209135, 0.319563090801239, 0.07882866263389587, 0.30108675360679626, 0.4453408420085907, 0.39998099207878113, 0.42089346051216125, 0.05608535185456276, 0.4485303461551666, 0.14192737638950348, 0.4307990074157715, 0.0002566764014773071, 0.27202317118644714, 0.4897879362106323, 0.12349385023117065, 0.17460110783576965, 0.06811211258172989, 0.21581250429153442, 0.47162383794784546, 0.4146254360675812], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3840142488479614, 0.4702986180782318, 0.4131304621696472, 0.12326553463935852, 0.26438918709754944, 0.30460038781166077, 0.3080123960971832, 0.39654600620269775, 0.32712990045547485, 0.2602642774581909, 0.20769786834716797, 0.47900667786598206, 0.48590609431266785, 0.08121119439601898, 0.2592640221118927, 0.027071453630924225, 0.29515475034713745, 0.43314462900161743, 0.33027932047843933, 0.16644825041294098], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a189650ad2cf49138ddade1316ee5cb6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 4, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4eaca6add0871c0eabf356a29291b9fb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.24003815650939941, 0.41979244351387024, 0.10188073664903641, 0.34710726141929626, 0.35497990250587463, 0.3657246232032776, 0.38734155893325806, 0.35479772090911865, 0.002100039040669799, 0.45640090107917786, 0.36742231249809265, 0.3849048316478729, 0.010091356001794338, 0.2888382077217102, 0.010444259271025658, 0.2293577641248703, 0.2826721966266632, 0.39980897307395935], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2590535879135132, 0.002295960206538439, 0.32081443071365356, 0.27208411693573, 0.46902427077293396, 0.1649555265903473, 0.30254822969436646, 0.12187661975622177, 0.2745596170425415, 0.2748868763446808, 0.30900564789772034, 0.2897182106971741, 0.07988374680280685, 0.47974908351898193, 0.17965887486934662, 0.271379679441452, 0.004784747492522001, 0.2229674756526947], dtype='float32').reshape([18]),
            paddle.to_tensor([0.020176751539111137, 0.16799145936965942, 0.4402709901332855, 0.17899785935878754, 0.032188646495342255, 0.05952523276209831, 0.44339486956596375, 0.10800197720527649, 0.257823646068573, 0.23456352949142456, 0.2318829894065857, 0.20030857622623444, 0.3661470115184784, 0.09521176666021347, 0.38411906361579895, 0.2453763782978058, 0.16053655743598938, 0.34915947914123535], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3006727397441864, 0.03947592154145241, 0.09426333755254745, 0.2774968147277832, 0.23547197878360748, 0.4503854215145111, 0.44366490840911865, 0.47804903984069824, 0.4329027533531189, 0.22078044712543488, 0.15371641516685486, 0.05953141674399376, 0.32961586117744446, 0.3915005028247833, 0.1556355357170105, 0.19995318353176117, 0.2544180750846863, 0.28809547424316406], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c182d0d414eff0cdd3eea0a8f368c4e0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cdc3b0a148b551f86897e4668e0bdb6b
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_16cd58428c7c2d21413c39411fe84cb8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.38421785831451416, 0.49233531951904297, 0.4699110686779022, 0.3747326731681824, 0.2617549002170563, 0.23820553719997406, 0.07261112332344055, 0.4887304902076721, 0.06595171988010406, 0.23573090136051178, 0.06442786753177643, 0.26564091444015503], dtype='float32').reshape([12]),
            paddle.to_tensor([0.47807008028030396, 0.2776629328727722, 0.16754083335399628, 0.02813914604485035, 0.46087172627449036, 0.4068272113800049, 0.2665857672691345, 0.29737240076065063, 0.4665673077106476, 0.45638489723205566, 0.17780672013759613, 0.09237784892320633], dtype='float32').reshape([12]),
            paddle.to_tensor([0.05892206355929375, 0.09157782047986984, 0.16665364801883698, 0.1972789466381073, 0.04883917421102524, 0.11043696105480194, 0.4463222026824951, 0.08168816566467285, 0.213820219039917, 0.48806166648864746, 0.229207843542099, 0.2880784273147583], dtype='float32').reshape([12]),
            paddle.to_tensor([0.41529110074043274, 0.08918095380067825, 0.118984155356884, 0.07645975798368454, 0.2858685851097107, 0.34746289253234863, 0.4595067501068115, 0.11426994204521179, 0.014340235851705074, 0.27817869186401367, 0.2559531629085541, 0.1339949518442154], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ba999752cd0dcc288c7e6e6412ef683f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 960, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fd92697fefa213b8937faa8ceb16a2fa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 480, 480], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.16844980418682098, 0.21654705703258514, 0.06228332594037056, 0.4542114734649658, 0.31197893619537354, 0.3685908317565918, 0.4250195622444153, 0.017222696915268898], dtype='float32').reshape([8]),
            paddle.to_tensor([0.3561665713787079, 0.47327688336372375, 0.4169546961784363, 0.2883651852607727, 0.4082874357700348, 0.4227832853794098, 0.09203682839870453, 0.41147077083587646], dtype='float32').reshape([8]),
            paddle.to_tensor([0.11554256081581116, 0.34504690766334534, 0.4623142182826996, 0.2939022481441498, 0.4909454584121704, 0.09170198440551758, 0.3650362491607666, 0.24597129225730896], dtype='float32').reshape([8]),
            paddle.to_tensor([0.04292869567871094, 0.0520353689789772, 0.30435121059417725, 0.2003113031387329, 0.38740044832229614, 0.3391737937927246, 0.10639751702547073, 0.1253972202539444], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8c3d2fd7b017a4b6546fbeae3a5ad69a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_932bdc22f3c5cf405ec0d4df0befe3f3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([8, 1024, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_78a3a656a23a3289999b8cd36546af40(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 82, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8e29e5274dd02c9c2b872fef41b776a8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_af286fb9e8d72c5d0d8067c9f396eff1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 2, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6d8c07e56140d23051f0db1021d960fd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 64, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ec70c7c394f42a08d00d717446e5c53f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_55ea71e9885c1555403a7ad470b92623(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1872, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1872], dtype='float32', min=0, max=0.5),
            paddle.uniform([1872], dtype='float32', min=0, max=0.5),
            paddle.uniform([1872], dtype='float32', min=0, max=0.5),
            paddle.uniform([1872], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_89eade693c25932d04c67f3c7ba6845d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 50, 84], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8d524bc3a72041d016abab1faac5dd33(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 31, 31], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b29e14e849a778bc022e8f9bb574cb31(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 2, 112, 224], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.41459858417510986, 0.02498791553080082], dtype='float32').reshape([2]),
            paddle.to_tensor([0.28813037276268005, 0.4527205228805542], dtype='float32').reshape([2]),
            paddle.to_tensor([0.2200256735086441, 0.37116727232933044], dtype='float32').reshape([2]),
            paddle.to_tensor([0.22313396632671356, 0.49587008357048035], dtype='float32').reshape([2]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e6b553aee25a14095fcaea1a114717a9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_236d86d5f0638c40a3f1874a57050b35(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_56ba344d63318f77354f22823329049c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 10, 10], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_32206034041d11162f5135a98a4245a0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 512, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.21653901040554047, 0.1005636602640152, 0.06052328646183014, 0.04682661220431328, 0.15058153867721558, 0.1364588737487793, 0.25754034519195557, 0.3507068157196045, 0.05782259255647659, 0.16689705848693848, 0.08773890137672424, 0.3556857705116272, 0.11206302791833878, 0.39772844314575195, 0.17153142392635345, 0.41717419028282166], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3556843400001526, 0.12007606029510498, 0.06614191830158234, 0.3060067594051361, 0.10049154609441757, 0.4720175266265869, 0.13935139775276184, 0.3317267894744873, 0.03532109782099724, 0.20223909616470337, 0.2887692451477051, 0.05266009271144867, 0.16792573034763336, 0.39854350686073303, 0.19273728132247925, 0.06406796723604202], dtype='float32').reshape([16]),
            paddle.to_tensor([0.14835862815380096, 0.2761700749397278, 0.3045435845851898, 0.23699331283569336, 0.3937701880931854, 0.04615186154842377, 0.03959619626402855, 0.3766486346721649, 0.15921151638031006, 0.4923039376735687, 0.13937796652317047, 0.1240500882267952, 0.005761442705988884, 0.36607223749160767, 0.43079909682273865, 0.06115760654211044], dtype='float32').reshape([16]),
            paddle.to_tensor([0.41810721158981323, 0.20115529000759125, 0.47842466831207275, 0.10121416300535202, 0.28936463594436646, 0.03799203038215637, 0.12185437232255936, 0.4433594346046448, 0.041069187223911285, 0.4205269515514374, 0.2759726643562317, 0.293725848197937, 0.4426000416278839, 0.45799538493156433, 0.4959634244441986, 0.4845725893974304], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_43001cf74787ab078e0faeab7a3b3603(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8e44bfaa47c8865d9fbeb4aa0a90a59f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 4, 240, 240], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.023395732045173645, 0.4191160202026367, 0.30394190549850464, 0.3405238389968872], dtype='float32').reshape([4]),
            paddle.to_tensor([0.08922045677900314, 0.31130096316337585, 0.3553767204284668, 0.18312831223011017], dtype='float32').reshape([4]),
            paddle.to_tensor([0.4527716040611267, 0.06614293158054352, 0.15611572563648224, 0.32939496636390686], dtype='float32').reshape([4]),
            paddle.to_tensor([0.10308404266834259, 0.3192172944545746, 0.2620554566383362, 0.4311582148075104], dtype='float32').reshape([4]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ce3bcc05ffadd9db6e19ebaf73e70672(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 100, 168], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ad6fef59927eb057a0271ca5dae7e9b7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.40388360619544983, 0.43270057439804077, 0.21574057638645172, 0.40472283959388733, 0.3994116187095642, 0.20194073021411896, 0.15067759156227112, 0.3069167137145996, 0.011895569041371346, 0.23802819848060608, 0.34839725494384766, 0.18625155091285706, 0.2417588084936142, 0.08304858207702637, 0.4270196259021759, 0.25739142298698425, 0.3791499435901642, 0.2640147805213928, 0.4104587137699127, 0.2798093855381012, 0.31895682215690613, 0.24047909677028656, 0.38958120346069336, 0.10790534317493439], dtype='float32').reshape([24]),
            paddle.to_tensor([0.35266703367233276, 0.28815895318984985, 0.05073634535074234, 0.12921030819416046, 0.434751957654953, 0.042867470532655716, 0.17804250121116638, 0.1389617919921875, 0.17160208523273468, 0.15660682320594788, 0.015474552288651466, 0.18419134616851807, 0.02283918298780918, 0.047688283026218414, 0.3367418944835663, 0.27182573080062866, 0.42899778485298157, 0.08243602514266968, 0.2387218028306961, 0.06985673308372498, 0.4435698091983795, 0.2888104021549225, 0.4131558835506439, 0.16765132546424866], dtype='float32').reshape([24]),
            paddle.to_tensor([0.46962302923202515, 0.44211745262145996, 0.14688143134117126, 0.016707921400666237, 0.04564942792057991, 0.35431933403015137, 0.42538633942604065, 0.05357629433274269, 0.03418009355664253, 0.1207670345902443, 0.4220162332057953, 0.39463087916374207, 0.14397656917572021, 0.31774336099624634, 0.3659915626049042, 0.22729139029979706, 0.3641476035118103, 0.3335786759853363, 0.16334550082683563, 0.3759380877017975, 0.00026936392532661557, 0.1891450434923172, 0.017028965055942535, 0.05847432464361191], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2580418288707733, 0.04280476272106171, 0.4258606433868408, 0.4437493085861206, 0.20000433921813965, 0.2046521157026291, 0.3067191243171692, 0.03369826823472977, 0.13865569233894348, 0.12783659994602203, 0.05882805213332176, 0.4709582030773163, 0.1011224165558815, 0.11075155436992645, 0.2849743366241455, 0.0926651582121849, 0.30961906909942627, 0.26818203926086426, 0.28077980875968933, 0.43349236249923706, 0.27140170335769653, 0.18098866939544678, 0.04128897935152054, 0.4556928277015686], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d3659f024956290ab910c58f1526246e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1f98b629e4d004948afa68909aa35127(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 320, 320], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.019122183322906494, 0.440935343503952, 0.4930415451526642, 0.3430418074131012, 0.08475738018751144, 0.14849057793617249, 0.24436485767364502, 0.3912147879600525], dtype='float32').reshape([8]),
            paddle.to_tensor([0.49243152141571045, 0.4081438183784485, 0.04026234522461891, 0.07032115012407303, 0.2199859321117401, 0.1285148411989212, 0.028213368728756905, 0.17750030755996704], dtype='float32').reshape([8]),
            paddle.to_tensor([0.4155184328556061, 0.4142703711986542, 0.3305407166481018, 0.3524945080280304, 0.32433032989501953, 0.24814480543136597, 0.32147806882858276, 0.2898158133029938], dtype='float32').reshape([8]),
            paddle.to_tensor([0.07132101058959961, 0.39730557799339294, 0.09992805123329163, 0.45391061902046204, 0.32648175954818726, 0.0012924540787935257, 0.12769120931625366, 0.11431442946195602], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_016d4eeb23cb9bc1c473c9ac0d90e95e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3519d8afbfa813f0d406d023d060a1fa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.29776525497436523, 0.09909208863973618, 0.18310664594173431, 0.16390562057495117, 0.2628183960914612, 0.173479825258255, 0.4969957172870636, 0.18891264498233795, 0.0696190595626831, 0.043992675840854645, 0.2298564463853836, 0.054399702697992325, 0.0806388333439827, 0.16360001266002655, 0.04383918642997742, 0.014727660454809666, 0.2950608432292938, 0.4486338794231415, 0.4974256157875061, 0.3188571631908417], dtype='float32').reshape([20]),
            paddle.to_tensor([0.23801599442958832, 0.14307470619678497, 0.31382226943969727, 0.13660851120948792, 0.3884182870388031, 0.2635068893432617, 0.3151019811630249, 0.0581759549677372, 0.348169207572937, 0.10382291674613953, 0.283498615026474, 0.024186907336115837, 0.08117201924324036, 0.3557108938694, 0.040808480232954025, 0.20900504291057587, 0.19387267529964447, 0.28407272696495056, 0.21358291804790497, 0.4786589741706848], dtype='float32').reshape([20]),
            paddle.to_tensor([0.49792125821113586, 0.07778789848089218, 0.1054418608546257, 0.11172188073396683, 0.3298739790916443, 0.10898732393980026, 0.2010180503129959, 0.48811838030815125, 0.02977995201945305, 0.004690445959568024, 0.28793299198150635, 0.46544018387794495, 0.2574205696582794, 0.32221370935440063, 0.1251247376203537, 0.2811752259731293, 0.062318023294210434, 0.14751867949962616, 0.0035173818469047546, 0.3730453848838806], dtype='float32').reshape([20]),
            paddle.to_tensor([0.07599636912345886, 0.4356904625892639, 0.3488145172595978, 0.4994785785675049, 0.386369526386261, 0.35117480158805847, 0.015729425475001335, 0.4175163805484772, 0.28811073303222656, 0.1823020577430725, 0.01566368341445923, 0.13624408841133118, 0.2672392427921295, 0.4730584919452667, 0.09288680553436279, 0.19819678366184235, 0.4083978533744812, 0.4103423058986664, 0.331571102142334, 0.33389225602149963], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7fd90c6cbe00e3e26bf0b5f4f4c9c160(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1920, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1920], dtype='float32', min=0, max=0.5),
            paddle.uniform([1920], dtype='float32', min=0, max=0.5),
            paddle.uniform([1920], dtype='float32', min=0, max=0.5),
            paddle.uniform([1920], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_760ed7d32ac47ad32d6055eaae31c64b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cbc0a2eb931dd44aa01d998240cc8119(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.28798621892929077, 0.2300778329372406, 0.45738038420677185, 0.30348968505859375, 0.02012191154062748, 0.42911386489868164, 0.15575186908245087, 0.35952186584472656, 0.1405414640903473, 0.23638057708740234, 0.344252347946167, 0.29506656527519226, 0.48986104130744934, 0.46313148736953735, 0.11948800832033157, 0.3015458285808563, 0.06462647020816803, 0.28962457180023193, 0.206837996840477, 0.23781384527683258, 0.37497392296791077, 0.2773429751396179, 0.1748524308204651, 0.3292854130268097], dtype='float32').reshape([24]),
            paddle.to_tensor([0.201998770236969, 0.3824220299720764, 0.3061377704143524, 0.15067367255687714, 0.25938013195991516, 0.31529369950294495, 0.3948073089122772, 0.07562487572431564, 0.21171726286411285, 0.19149208068847656, 0.4280250370502472, 0.09255567193031311, 0.2382478415966034, 0.3614857792854309, 0.45900315046310425, 0.2604166269302368, 0.16995160281658173, 0.36636459827423096, 0.39978480339050293, 0.4960998296737671, 0.4975524842739105, 0.3210623264312744, 0.16726312041282654, 0.16969989240169525], dtype='float32').reshape([24]),
            paddle.to_tensor([0.21976718306541443, 0.048824530094861984, 0.49277153611183167, 0.4338652491569519, 0.2902284264564514, 0.2830670475959778, 0.14834699034690857, 0.34769243001937866, 0.4901113212108612, 0.349911093711853, 0.14310553669929504, 0.4071624279022217, 0.446206271648407, 0.309017539024353, 0.31497979164123535, 0.018199559301137924, 0.227855384349823, 0.21596193313598633, 0.03990280628204346, 0.11801714450120926, 0.06059042364358902, 0.41180896759033203, 0.16622820496559143, 0.4951495826244354], dtype='float32').reshape([24]),
            paddle.to_tensor([0.1445520669221878, 0.19587816298007965, 0.09459572285413742, 0.38595879077911377, 0.25654056668281555, 0.3425537943840027, 0.3455178141593933, 0.03688124194741249, 0.24659301340579987, 0.09391311556100845, 0.4618237018585205, 0.3943019211292267, 0.23974160850048065, 0.2954312860965729, 0.27667877078056335, 0.2082718163728714, 0.3138341009616852, 0.13048055768013, 0.40692445635795593, 0.4406764507293701, 0.4857742488384247, 0.022456616163253784, 0.37352830171585083, 0.2598336637020111], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5a76c33b113291dd32275a4971a389d6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 224, 224], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.008680556900799274, 0.348122775554657, 0.22036926448345184, 0.20373786985874176, 0.1404336392879486, 0.1533655971288681, 0.47613292932510376, 0.4086172580718994, 0.10304800420999527, 0.2430763691663742, 0.3710278570652008, 0.40184035897254944, 0.15087924897670746, 0.2434312254190445, 0.06144662946462631, 0.06740064918994904], dtype='float32').reshape([16]),
            paddle.to_tensor([0.14074411988258362, 0.3569350242614746, 0.1060052290558815, 0.2657613754272461, 0.3456861078739166, 0.052556827664375305, 0.45029863715171814, 0.4062696695327759, 0.1861843317747116, 0.09660191833972931, 0.2857630252838135, 0.4438928961753845, 0.09790794551372528, 0.20451325178146362, 0.1787445992231369, 0.3461536765098572], dtype='float32').reshape([16]),
            paddle.to_tensor([0.21468298137187958, 0.1186622679233551, 0.20204266905784607, 0.24063844978809357, 0.25511467456817627, 0.06693873554468155, 0.4636377692222595, 0.18101908266544342, 0.09418985247612, 0.3090687394142151, 0.2602042257785797, 0.038301218301057816, 0.1924002319574356, 0.2849644422531128, 0.2673795521259308, 0.01850917749106884], dtype='float32').reshape([16]),
            paddle.to_tensor([0.05018128454685211, 0.15671177208423615, 0.17538756132125854, 0.35178813338279724, 0.1402353048324585, 0.31668293476104736, 0.17818062007427216, 0.18648777902126312, 0.012130136601626873, 0.0402078777551651, 0.30293720960617065, 0.3693760633468628, 0.3704788386821747, 0.23508700728416443, 0.41495636105537415, 0.3602302074432373], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_df250feca2d1420b00e1bbd3a374d953(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_176ec3a270238334358328d07a041566(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 240, 240], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6abbf19e372505c2dd2ec7570a7da81b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.20523564517498016, 0.4426819682121277, 0.009887507185339928, 0.20431123673915863, 0.40269872546195984, 0.3242991268634796, 0.20351436734199524, 0.28587719798088074, 0.4694729745388031, 0.45752859115600586, 0.22778292000293732, 0.08718748390674591, 0.25365105271339417, 0.49321606755256653, 0.45541831851005554, 0.25243374705314636, 0.13559289276599884, 0.22624549269676208, 0.332432359457016, 0.40240660309791565, 0.06251838058233261, 0.4330860674381256, 0.09098910540342331, 0.20523548126220703, 0.43386319279670715, 0.20633098483085632, 0.0323980487883091, 0.4392319917678833], dtype='float32').reshape([28]),
            paddle.to_tensor([0.10071088373661041, 0.4905592203140259, 0.023668646812438965, 0.2779620587825775, 0.1532338708639145, 0.4542378783226013, 0.28745728731155396, 0.02935526706278324, 0.04968724027276039, 0.3811890780925751, 0.2978552579879761, 0.49349600076675415, 0.22416676580905914, 0.1675165593624115, 0.06947197020053864, 0.15426743030548096, 0.17685624957084656, 0.2521823048591614, 0.39179593324661255, 0.3293714225292206, 0.435068279504776, 0.05643780902028084, 0.23934820294380188, 0.33791452646255493, 0.39597463607788086, 0.3233834207057953, 0.28739526867866516, 0.1649392992258072], dtype='float32').reshape([28]),
            paddle.to_tensor([0.11374527961015701, 0.4070321321487427, 0.37715524435043335, 0.27806758880615234, 0.04954648017883301, 0.024863358587026596, 0.2342044562101364, 0.22337110340595245, 0.1160154715180397, 0.07247450947761536, 0.14947962760925293, 0.0351608470082283, 0.08127126097679138, 0.313663512468338, 0.4381113052368164, 0.4693659245967865, 0.09181660413742065, 0.24276015162467957, 0.09579387307167053, 0.20964516699314117, 0.3841034471988678, 0.13142290711402893, 0.16258440911769867, 0.4839833676815033, 0.13476432859897614, 0.33340904116630554, 0.20990464091300964, 0.012775321491062641], dtype='float32').reshape([28]),
            paddle.to_tensor([0.06867668777704239, 0.25583532452583313, 0.395504891872406, 0.26071998476982117, 0.27564218640327454, 0.08579307049512863, 0.3264594078063965, 0.32945331931114197, 0.263858824968338, 0.40560615062713623, 0.13774800300598145, 0.34300628304481506, 0.10460082441568375, 0.31109654903411865, 0.08495974540710449, 0.3135850727558136, 0.20684128999710083, 0.20043787360191345, 0.11067435145378113, 0.33282092213630676, 0.040625639259815216, 0.16495253145694733, 0.42100852727890015, 0.33761337399482727, 0.3101647198200226, 0.3340561091899872, 0.2370734065771103, 0.34660661220550537], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9afdcee1a6c2a6a8463c1713054be8d6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.07746419310569763, 0.06443144381046295, 0.0577966570854187, 0.05360066145658493, 0.4552902281284332, 0.3183266222476959, 0.13479939103126526, 0.3077179491519928, 0.19732147455215454, 0.1899377405643463, 0.1798572838306427, 0.24462684988975525, 0.09923198074102402, 0.23424360156059265, 0.18512430787086487, 0.2706057131290436, 0.387191504240036, 0.48610687255859375, 0.03171912953257561, 0.2122190147638321, 0.181878000497818, 0.28338223695755005, 0.012340137735009193, 0.0624900758266449], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4913068413734436, 0.1950739175081253, 0.13306476175785065, 0.008991598151624203, 0.05168453976511955, 0.37808042764663696, 0.2564666271209717, 0.48505228757858276, 0.2930111587047577, 0.49253979325294495, 0.4059487581253052, 0.05658666789531708, 0.0729057714343071, 0.1567739099264145, 0.10446426272392273, 0.4684819281101227, 0.12671668827533722, 0.18865685164928436, 0.29630571603775024, 0.4202861487865448, 0.4980219602584839, 0.4687711298465729, 0.19184954464435577, 0.31797417998313904], dtype='float32').reshape([24]),
            paddle.to_tensor([0.19990000128746033, 0.12537692487239838, 0.04235595837235451, 0.21143552660942078, 0.04906999319791794, 0.2775542736053467, 0.024814357981085777, 0.2768779993057251, 0.20461490750312805, 0.4756760001182556, 0.2403978556394577, 0.18191376328468323, 0.22306714951992035, 0.40690529346466064, 0.3247360289096832, 0.1480339616537094, 0.4958784282207489, 0.3295791447162628, 0.3500519096851349, 0.09624417871236801, 0.07502997666597366, 0.4360288083553314, 0.4884257912635803, 0.11743628233671188], dtype='float32').reshape([24]),
            paddle.to_tensor([0.35710036754608154, 0.07177761197090149, 0.42755115032196045, 0.1752229779958725, 0.3093004822731018, 0.17643111944198608, 0.3165782690048218, 0.2132783830165863, 0.41277047991752625, 0.3433355391025543, 0.18180641531944275, 0.09221842139959335, 0.22881677746772766, 0.15757372975349426, 0.30553510785102844, 0.4268212616443634, 0.33417242765426636, 0.40526679158210754, 0.23006759583950043, 0.4417337477207184, 0.18669982254505157, 0.2832251489162445, 0.023964405059814453, 0.07500945776700974], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ccccb9caa4787400b003a725155c463f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 150, 150], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_33925fb080c5734a4280abb41bc09b41(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.43158844113349915, 0.26824334263801575, 0.09898067265748978, 0.4367294907569885, 0.3684494197368622, 0.025820525363087654, 0.2585369646549225, 0.10036888718605042, 0.4152166545391083, 0.377379834651947, 0.2317497283220291, 0.07075947523117065, 0.06890018284320831, 0.21976661682128906, 0.11480340361595154, 0.30342790484428406, 0.3423187732696533, 0.07594703882932663], dtype='float32').reshape([18]),
            paddle.to_tensor([0.48304277658462524, 0.1650511473417282, 0.1987602412700653, 0.3837931752204895, 0.44059765338897705, 0.040620356798172, 0.17326746881008148, 0.01142684742808342, 0.3210408389568329, 0.035652562975883484, 0.45650315284729004, 0.23715364933013916, 0.173312708735466, 0.46555325388908386, 0.0316336564719677, 0.18906933069229126, 0.21215710043907166, 0.13481931388378143], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3115721344947815, 0.05382002517580986, 0.1845669150352478, 0.42209362983703613, 0.2716871201992035, 0.46611279249191284, 0.39296793937683105, 0.48828113079071045, 0.3168770372867584, 0.1358388513326645, 0.4567250609397888, 0.16775472462177277, 0.18357688188552856, 0.10967127978801727, 0.005905332509428263, 0.4691113829612732, 0.3288479447364807, 0.12788313627243042], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3127175271511078, 0.3816452622413635, 0.46114662289619446, 0.29900577664375305, 0.3002374470233917, 0.1122308298945427, 0.11127465963363647, 0.41297078132629395, 0.02659868262708187, 0.08535964041948318, 0.4811539947986603, 0.2934785485267639, 0.383881151676178, 0.43957221508026123, 0.2602621018886566, 0.03159403055906296, 0.2964784801006317, 0.40840765833854675], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1c7a40841346cda627a00ecbba89e618(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 16, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_34bd9fea540da17a48d9a3f426b1b296(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_5e7cb23119f65b33bdbad71952f6dd12
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4b4c245a497429544bdf4bf2d693a4f5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 16, 50], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.0954677015542984, 0.06615833193063736, 0.33645933866500854, 0.17772191762924194, 0.43480196595191956, 0.14359050989151, 0.006258043460547924, 0.3593859374523163], dtype='float32').reshape([8]),
            paddle.to_tensor([0.08875451982021332, 0.2597481608390808, 0.1796627640724182, 0.44706398248672485, 0.39438971877098083, 0.20755703747272491, 0.03014618344604969, 0.10406940430402756], dtype='float32').reshape([8]),
            paddle.to_tensor([0.2731740176677704, 0.14811819791793823, 0.43845877051353455, 0.49642127752304077, 0.035640183836221695, 0.35217440128326416, 0.43880534172058105, 0.00818672776222229], dtype='float32').reshape([8]),
            paddle.to_tensor([0.23517151176929474, 0.3825598657131195, 0.007191574200987816, 0.3092356324195862, 0.3604370057582855, 0.43900007009506226, 0.11619634181261063, 0.07298722118139267], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f93bd1934fdc261a1ae1fc663f1bf3d1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_0edd578f48a07aa5d1d79f9f678b472e
    def get_inputs(self):
        return [
            paddle.uniform([1, 640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aa47231b4bb80ea3e6aea444b79a2cbc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 152, 152], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_86da30f7829434ed595e9f5d588b0929(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f04931d0e94d92ec95a8440ae91fe6f0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_0edd578f48a07aa5d1d79f9f678b472e
    def get_inputs(self):
        return [
            paddle.uniform([196, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a35b056ff6b22e643a44e99fbe23ecbf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_be8d3fea350bc861b70af6aecdd3924e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2679781913757324, 0.3228271007537842, 0.07531953603029251, 0.07024170458316803, 0.32507839798927307, 0.25462040305137634, 0.31905537843704224, 0.48334458470344543, 0.2269769310951233, 0.37739449739456177, 0.2967555820941925, 0.3414953052997589, 0.49138450622558594, 0.353643536567688, 0.028652938082814217, 0.2728622257709503, 0.053784098476171494, 0.0229191891849041, 0.13483580946922302, 0.1366039216518402, 0.18846876919269562, 0.4161904752254486, 0.3890421390533447, 0.17674891650676727], dtype='float32').reshape([24]),
            paddle.to_tensor([0.18838855624198914, 0.02041500248014927, 0.3584669232368469, 0.3657403886318207, 0.2511354386806488, 0.24652697145938873, 0.15802942216396332, 0.4264752268791199, 0.27576595544815063, 0.33580338954925537, 0.3043631315231323, 0.010746520943939686, 0.4580937623977661, 0.24227042496204376, 0.44928663969039917, 0.26589542627334595, 0.09842412173748016, 0.4867129325866699, 0.31185534596443176, 0.26068177819252014, 0.2796756625175476, 0.23178830742835999, 0.09322960674762726, 0.49427855014801025], dtype='float32').reshape([24]),
            paddle.to_tensor([0.46403273940086365, 0.08008072525262833, 0.18318359553813934, 0.14144152402877808, 0.412349671125412, 0.3689609467983246, 0.21041926741600037, 0.13928058743476868, 0.1531754583120346, 0.06589165329933167, 0.040946073830127716, 0.3879829943180084, 0.4636225700378418, 0.15732821822166443, 0.013949128799140453, 0.4493958055973053, 0.15697163343429565, 0.06828442960977554, 0.33883821964263916, 0.04744488745927811, 0.08225442469120026, 0.4739632308483124, 0.10368023812770844, 0.07012981176376343], dtype='float32').reshape([24]),
            paddle.to_tensor([0.32783883810043335, 0.01887565292418003, 0.15580545365810394, 0.20358748733997345, 0.31428197026252747, 0.23101867735385895, 0.11006869375705719, 0.28136730194091797, 0.21939732134342194, 0.4929039776325226, 0.034493640065193176, 0.13364392518997192, 0.23980802297592163, 0.38667333126068115, 0.23496030271053314, 0.3689151406288147, 0.4804844558238983, 0.11619863659143448, 0.23645497858524323, 0.05195761099457741, 0.0207035131752491, 0.11188924312591553, 0.01852991431951523, 0.30329984426498413], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8ad6206a5135403d69bbf467dd03e3f6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f908c9930276ccb5c0a265eaab12af57(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.00963907316327095, 0.4952716529369354, 0.4298664927482605, 0.1898406445980072, 0.09366508573293686, 0.22005803883075714, 0.09544357657432556, 0.4099632799625397, 0.10906834155321121, 0.3631872236728668, 0.10855133831501007, 0.07764644175767899, 0.383882075548172, 0.4136222004890442, 0.16006900370121002, 0.45305028557777405, 5.282094934955239e-05, 0.25554874539375305, 0.3602435886859894, 0.3348648250102997, 0.2983684837818146, 0.30115029215812683, 0.1923469603061676, 0.31077080965042114], dtype='float32').reshape([24]),
            paddle.to_tensor([0.19193211197853088, 0.20530351996421814, 0.29504451155662537, 0.0791202113032341, 0.3969559073448181, 0.30510783195495605, 0.47212767601013184, 0.2411944717168808, 0.4894741177558899, 0.2651132345199585, 0.3872438371181488, 0.07113046199083328, 0.4509156346321106, 0.13936014473438263, 0.2208334058523178, 0.055066633969545364, 0.4304090738296509, 0.13155527412891388, 0.16349200904369354, 0.39094066619873047, 0.04404998943209648, 0.46518296003341675, 0.006557495333254337, 0.10919535160064697], dtype='float32').reshape([24]),
            paddle.to_tensor([0.05450006201863289, 0.3982734978199005, 0.05617506429553032, 0.11740963160991669, 0.1392359882593155, 0.16406205296516418, 0.2876022756099701, 0.2510375380516052, 0.03513907268643379, 0.12879565358161926, 0.007613968104124069, 0.2476000338792801, 0.10909519344568253, 0.48613035678863525, 0.17029590904712677, 0.22623467445373535, 0.2966732084751129, 0.4956979751586914, 0.25022125244140625, 0.3559741675853729, 0.370802640914917, 0.05038881674408913, 0.22164352238178253, 0.07103568315505981], dtype='float32').reshape([24]),
            paddle.to_tensor([0.08098900318145752, 0.3215682804584503, 0.12781865894794464, 0.4035007655620575, 0.441372811794281, 0.4919942617416382, 0.1776186227798462, 0.26314300298690796, 0.4025031328201294, 0.3414134979248047, 0.05998397246003151, 0.2167266458272934, 0.429374098777771, 0.1401931345462799, 0.3931504189968109, 0.36554649472236633, 0.21730168163776398, 0.2958480417728424, 0.008897439576685429, 0.23111370205879211, 0.0361039862036705, 0.004783615469932556, 0.4361465275287628, 0.06803616136312485], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_43ebdddd7cf19351a1d937c54892bb6c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 960, 32, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_604036787027e03917a83ac61be3c492(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.06381262838840485, 0.45318543910980225, 0.2804327607154846, 0.4058617651462555, 0.24423499405384064, 0.46036750078201294, 0.1658225953578949, 0.34569066762924194, 0.0313868522644043, 0.3011142909526825, 0.48094162344932556, 0.25284847617149353], dtype='float32').reshape([12]),
            paddle.to_tensor([0.3952401578426361, 0.24121858179569244, 0.15054529905319214, 0.10707014799118042, 0.33466407656669617, 0.48185354471206665, 0.42595410346984863, 0.1980632245540619, 0.2874648869037628, 0.4829528033733368, 0.20142444968223572, 0.1594938039779663], dtype='float32').reshape([12]),
            paddle.to_tensor([0.38596710562705994, 0.24314641952514648, 0.33016347885131836, 0.287341833114624, 0.10186438262462616, 0.46608901023864746, 0.21135589480400085, 0.1785285472869873, 0.16953805088996887, 0.02884753607213497, 0.22976772487163544, 0.34726482629776], dtype='float32').reshape([12]),
            paddle.to_tensor([0.225415900349617, 0.07314510643482208, 0.401439905166626, 0.43791186809539795, 0.21048450469970703, 0.2274932563304901, 0.4118502736091614, 0.07529069483280182, 0.3551419675350189, 0.276999831199646, 0.3591594398021698, 0.3704681396484375], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b2286ecc58aae8c27c5dc5ab2b12f31a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 28, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ae9787d28041986f5a10926f79461a14(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 320, 320], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5ed6de9e531cc6cc951a718a979a5d3c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_017ec56b902df5417299af4e9fcf0fe1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2ababadff4cb59ffdbe91ec36b5f5176(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.09295497834682465, 0.025722548365592957, 0.06422818452119827, 0.03898372873663902, 0.32906895875930786, 0.4219232201576233, 0.31956803798675537, 0.20048783719539642, 0.06442268937826157, 0.1930951327085495, 0.22685863077640533, 0.23906907439231873, 0.4656972289085388, 0.24735644459724426, 0.29132142663002014, 0.21089132130146027, 0.0666373074054718, 0.36685633659362793], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3031371533870697, 0.09579239785671234, 0.0932476818561554, 0.33111366629600525, 0.3489837646484375, 0.02176978811621666, 0.47510188817977905, 0.044223204255104065, 0.25316157937049866, 0.2125585675239563, 0.1971932053565979, 0.1974344104528427, 0.20993001759052277, 0.06971094757318497, 0.32229912281036377, 0.4381139576435089, 0.150462806224823, 0.2691768407821655], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3109012842178345, 0.1134110763669014, 0.17224037647247314, 0.18357376754283905, 0.4528023600578308, 0.16150140762329102, 0.4152207374572754, 0.2368774265050888, 0.007859867997467518, 0.30276575684547424, 0.02677152119576931, 0.2254311740398407, 0.3919932246208191, 0.4250199496746063, 0.4071047306060791, 0.2067701518535614, 0.07729945331811905, 0.42972132563591003], dtype='float32').reshape([18]),
            paddle.to_tensor([0.16573317348957062, 0.003276535775512457, 0.3673282861709595, 0.4551817774772644, 0.4349232614040375, 0.24182935059070587, 0.42949485778808594, 0.20142096281051636, 0.05779968574643135, 0.026521315798163414, 0.002453960943967104, 0.01308456715196371, 0.1449110358953476, 0.07367560267448425, 0.4976945221424103, 0.2558709383010864, 0.05985128879547119, 0.035011887550354004], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b62888419ace5e97051364cca9bf9d50(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ded7a00ee14fde3b549baaf66f48dcc2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 88, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2c6cf599e1350650a646833ea9a1fe87(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 6, 6], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7ba53cdbed8af5d97053baf1faa5d1c7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 31, 31], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e21dd11a1e032c2ad6a2b742b9396df5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 38, 38], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f96290e1355076775534b2d7875b0c1a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([8, 32, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d28ed095c9e7bfa24842b50df21517c3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 240, 240], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_712738bae5c16b00301ad566ea4ec6a7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 4, 50], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3994264304637909, 0.001811078516766429, 0.11123137176036835, 0.12214881181716919, 0.398196280002594, 0.23727549612522125, 0.15042279660701752, 0.22085843980312347, 0.23467807471752167, 0.09755048155784607, 0.45932215452194214, 0.17276474833488464, 0.4302747845649719, 0.4348624348640442, 0.1571096032857895, 0.2714834213256836, 0.16188719868659973, 0.013295610435307026, 0.4578738510608673, 0.4575388729572296, 0.10127703100442886, 0.3098101317882538, 0.4037233889102936, 0.09879935532808304], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2792396545410156, 0.42996692657470703, 0.1275864541530609, 0.17959849536418915, 0.017213523387908936, 0.03869633004069328, 0.1604703664779663, 0.28652262687683105, 0.14912094175815582, 0.4213097095489502, 0.14795728027820587, 0.4137546420097351, 0.49330636858940125, 0.2994592487812042, 0.011947648599743843, 0.30775898694992065, 0.4537426829338074, 0.4912576377391815, 0.3720588982105255, 0.101382777094841, 0.11316654086112976, 0.03775807097554207, 0.43866902589797974, 0.22442984580993652], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2133324146270752, 0.24525655806064606, 0.06963995844125748, 0.12366009503602982, 0.1600666046142578, 0.31966644525527954, 0.06623146682977676, 0.1419581025838852, 0.20743264257907867, 0.2191365361213684, 0.1755918264389038, 0.24150672554969788, 0.31763437390327454, 0.13403813540935516, 0.2695746421813965, 0.13960030674934387, 0.34173864126205444, 0.197392538189888, 0.42750290036201477, 0.31079596281051636, 0.19539016485214233, 0.033597931265830994, 0.16210737824440002, 0.20695607364177704], dtype='float32').reshape([24]),
            paddle.to_tensor([0.16395340859889984, 0.19341705739498138, 0.2541945278644562, 0.006457354873418808, 0.2929895222187042, 0.2952654957771301, 0.015866847708821297, 0.372844398021698, 0.38439926505088806, 0.07402893155813217, 0.49818506836891174, 0.29442453384399414, 0.2821105420589447, 0.16819001734256744, 0.09886094182729721, 0.18115995824337006, 0.11106820404529572, 0.3033897280693054, 0.16950133442878723, 0.36332735419273376, 0.16061079502105713, 0.03946617990732193, 0.464615136384964, 0.2525327801704407], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ea59fee9c7c149fa357c2417072d5a0f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 32, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.005332523491233587, 0.42055144906044006, 0.47846508026123047, 0.4856955409049988, 0.2691401243209839, 0.1877598613500595, 0.14972597360610962, 0.20070600509643555, 0.4030279219150543, 0.04358386993408203, 0.09566815942525864, 0.2999059557914734, 0.3828025460243225, 0.16402195394039154, 0.439590722322464, 0.4663538336753845], dtype='float32').reshape([16]),
            paddle.to_tensor([0.18550774455070496, 0.055122245103120804, 0.4010617733001709, 0.4731360673904419, 0.057763028889894485, 0.2309863567352295, 0.3130665123462677, 0.20687255263328552, 0.20500151813030243, 0.01375337690114975, 0.4175119400024414, 0.35516926646232605, 0.30428779125213623, 0.4861375093460083, 0.2741830348968506, 0.3443208932876587], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4171229302883148, 0.40793201327323914, 0.20369964838027954, 0.13228566944599152, 0.4961870014667511, 0.3829328417778015, 0.06219085305929184, 0.033804330974817276, 0.4902692139148712, 0.3040298521518707, 0.27096623182296753, 0.1182364821434021, 0.1679266393184662, 0.1446339339017868, 0.4959772825241089, 0.34818950295448303], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2900729775428772, 0.2246043086051941, 0.002433032263070345, 0.22811931371688843, 0.2062092423439026, 0.16337953507900238, 0.014063272625207901, 0.19898295402526855, 0.3172280788421631, 0.2288057953119278, 0.10767652839422226, 0.3164619505405426, 0.35778337717056274, 0.09581393748521805, 0.2737148702144623, 0.3958094120025635], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f255e1c80fa1db1a630c94efc732b327(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 256, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6e8ce7a87b34dc4223045bb47d1230d4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6698206ccd58209134b1df29b48d85d0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_fc6b6917e04b7b7cf9fdb39dde97ec40
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8ff0ae41db8f55bc93b7c7070c98b569(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_b4f4da3a8d44d1b5b3a2903a097df186
    def get_inputs(self):
        return [
            paddle.uniform([1, 640, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5c6e41c4657380870b728d5945863cee(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.36922645568847656, 0.3495626151561737, 0.47293609380722046, 0.1951693445444107, 0.014423197135329247, 0.46647047996520996, 0.28399452567100525, 0.18782836198806763, 0.3686716854572296, 0.34062111377716064, 0.3315023183822632, 0.3397650718688965, 0.3673509359359741, 0.06840889900922775, 0.07562717795372009, 0.16871024668216705, 0.17946067452430725, 0.24958541989326477], dtype='float32').reshape([18]),
            paddle.to_tensor([0.42899805307388306, 0.4944852888584137, 0.15874695777893066, 0.21257975697517395, 0.13313668966293335, 0.4065675735473633, 0.17068348824977875, 0.05636321008205414, 0.38006675243377686, 0.30809327960014343, 0.4149008095264435, 0.15023362636566162, 0.3165740370750427, 0.49102583527565, 0.48225143551826477, 0.31250330805778503, 0.13869167864322662, 0.3012615740299225], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4427095055580139, 0.16619646549224854, 0.08302313834428787, 0.027689997106790543, 0.16554105281829834, 0.40640050172805786, 0.36087435483932495, 0.2953093647956848, 0.31890323758125305, 0.021970592439174652, 0.20801718533039093, 0.4683695137500763, 0.10943777859210968, 0.31444188952445984, 0.07928819954395294, 0.49747034907341003, 0.05356334149837494, 0.2996782958507538], dtype='float32').reshape([18]),
            paddle.to_tensor([0.26469069719314575, 0.08245356380939484, 0.377384752035141, 0.1620115339756012, 0.30041152238845825, 0.3210308849811554, 0.28270599246025085, 0.16363631188869476, 0.09000463783740997, 0.41072243452072144, 0.28242847323417664, 0.29101741313934326, 0.14294731616973877, 0.30674245953559875, 0.006890044081956148, 0.2549629807472229, 0.296077698469162, 0.33472615480422974], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ac87eaab3a094ba78d55a8398b9fbbd3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.25638866424560547, 0.1658860296010971, 0.4017789959907532, 0.18613633513450623, 0.07462523132562637, 0.20435473322868347, 0.08785209059715271, 0.007821167819201946, 0.11530051380395889, 0.4854789972305298, 0.21540464460849762, 0.21928054094314575, 0.23381663858890533, 0.14796355366706848, 0.4220860004425049, 0.08378773927688599, 0.09810946881771088, 0.0036581438034772873, 0.4094467759132385, 0.12658795714378357, 0.1154491975903511, 0.17335599660873413, 0.11468929052352905, 0.2787286937236786, 0.24123011529445648, 0.26493212580680847, 0.30613958835601807, 0.12157651036977768], dtype='float32').reshape([28]),
            paddle.to_tensor([0.18297292292118073, 0.15671595931053162, 0.14094899594783783, 0.29996368288993835, 0.2881063222885132, 0.047189462929964066, 0.08272796869277954, 0.011134210042655468, 0.16441483795642853, 0.08841980248689651, 0.08070426434278488, 0.3011088967323303, 0.40583351254463196, 0.07208459824323654, 0.33378124237060547, 0.3613925576210022, 0.3948356807231903, 0.3527202606201172, 0.3923737704753876, 0.12996424734592438, 0.3186510503292084, 0.36164116859436035, 0.42627274990081787, 0.14000922441482544, 0.31304460763931274, 0.4135626256465912, 0.0767584890127182, 0.08648249506950378], dtype='float32').reshape([28]),
            paddle.to_tensor([0.21413879096508026, 0.43453747034072876, 0.16907839477062225, 0.35189947485923767, 0.16713206470012665, 0.18372176587581635, 0.03048045188188553, 0.07347283512353897, 0.16470025479793549, 0.32231438159942627, 0.44371625781059265, 0.20763936638832092, 0.4193250238895416, 0.11860676109790802, 0.2739194929599762, 0.07911001145839691, 0.007721604313701391, 0.08248189836740494, 0.283875435590744, 0.3119998574256897, 0.18235327303409576, 0.002009949879720807, 0.0330706350505352, 0.40481746196746826, 0.1495068371295929, 0.45345431566238403, 0.3980529308319092, 0.2357337921857834], dtype='float32').reshape([28]),
            paddle.to_tensor([0.12405792623758316, 0.1950102150440216, 0.10731049627065659, 0.1822555661201477, 0.2419614940881729, 0.0897345021367073, 0.10237854719161987, 0.355510413646698, 0.3588543236255646, 0.10265734791755676, 0.29615411162376404, 0.1263541728258133, 0.15481220185756683, 0.10742159932851791, 0.0030773943290114403, 0.2744813561439514, 0.009586289525032043, 0.10744132101535797, 0.13037560880184174, 0.2534988224506378, 0.031471312046051025, 0.27978020906448364, 0.4210316240787506, 0.3673045337200165, 0.4288535714149475, 0.11892824620008469, 0.35432127118110657, 0.0037087774835526943], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_47a54859993a2c95a5784d8025b22ff5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 144, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_af7555471a5fa47dd9d2d25adeb36ed1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 30, 30], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7d2968d0b978accf150d88385827fc2f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_49cf40789861080fc4e5f2e5e4757013(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1792, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1792], dtype='float32', min=0, max=0.5),
            paddle.uniform([1792], dtype='float32', min=0, max=0.5),
            paddle.uniform([1792], dtype='float32', min=0, max=0.5),
            paddle.uniform([1792], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_47cafdf047cd3893c16699258564dcd2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_34b0099e1201eee8b349702b71428da4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1c1809567be7789c657be7168a9fe5fc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 30, 30], dtype='float16', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aa3fef3847d51121d13158505e8c9956(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.10869183391332626, 0.30978989601135254, 0.3658161759376526, 0.373618483543396, 0.16743409633636475, 0.11209133267402649, 0.3741781413555145, 0.30994850397109985, 0.41650712490081787, 0.28985342383384705, 0.3075851798057556, 0.07236962765455246, 0.07590130716562271, 0.2930222749710083, 0.27465787529945374, 0.1526165008544922, 0.4676412343978882, 0.2743188142776489, 0.1582435816526413, 0.33633187413215637, 0.06565548479557037, 0.3602913022041321, 0.06596657633781433, 0.009190930053591728], dtype='float32').reshape([24]),
            paddle.to_tensor([0.27290573716163635, 0.40164700150489807, 0.38162654638290405, 0.3018812835216522, 0.20830431580543518, 0.19885669648647308, 0.29059576988220215, 0.17981179058551788, 0.125578835606575, 0.3870548605918884, 0.16969417035579681, 0.023766599595546722, 0.18242087960243225, 0.2822342813014984, 0.43976929783821106, 0.44730255007743835, 0.042184825986623764, 0.059642139822244644, 0.1738988161087036, 0.06147799268364906, 0.350166380405426, 0.3904533088207245, 0.19332492351531982, 0.05787679925560951], dtype='float32').reshape([24]),
            paddle.to_tensor([0.39953845739364624, 0.26289916038513184, 0.35819917917251587, 0.0613572932779789, 0.07585755735635757, 0.06656549125909805, 0.20896466076374054, 0.34801924228668213, 0.2756377160549164, 0.09087258577346802, 0.4301649332046509, 0.33536943793296814, 0.1393927037715912, 0.26824092864990234, 0.4420112371444702, 0.029333287850022316, 0.4625343680381775, 0.37253835797309875, 0.261057585477829, 0.12665459513664246, 0.04991081729531288, 0.41631126403808594, 0.4797864556312561, 0.24135486781597137], dtype='float32').reshape([24]),
            paddle.to_tensor([0.11373645812273026, 0.12524114549160004, 0.32543817162513733, 0.13505977392196655, 0.39989563822746277, 0.41999033093452454, 0.3112691640853882, 0.39025017619132996, 0.0029502357356250286, 0.23880252242088318, 0.3087962865829468, 0.07281949371099472, 0.1897309124469757, 0.2197963446378708, 0.16001974046230316, 0.42210090160369873, 0.43269723653793335, 0.322031706571579, 0.43821683526039124, 0.1346103698015213, 0.03900003433227539, 0.045653365552425385, 0.18662914633750916, 0.43220454454421997], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_76283ff3599282bca63ad6c9911aba99(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 2, 112, 199], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.25037682056427, 0.37291815876960754], dtype='float32').reshape([2]),
            paddle.to_tensor([0.07843264192342758, 0.0931035503745079], dtype='float32').reshape([2]),
            paddle.to_tensor([0.4978650212287903, 0.4207112789154053], dtype='float32').reshape([2]),
            paddle.to_tensor([0.03968729451298714, 0.10475129634141922], dtype='float32').reshape([2]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c149bad194d58a7cf519e4772571a1c2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 12, 12], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_533b48e0a2606fa61cfb2a15727e4d92(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 82, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6af922ab14acc3c11d8287c3c65e3cce(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 102, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([102], dtype='float32', min=0, max=0.5),
            paddle.uniform([102], dtype='float32', min=0, max=0.5),
            paddle.uniform([102], dtype='float32', min=0, max=0.5),
            paddle.uniform([102], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d19765f7cf34c15de7a500fd50a90dfc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.436860054731369, 0.3739330768585205, 0.42349615693092346, 0.36396172642707825, 0.1267104297876358, 0.29293403029441833, 0.098443403840065, 0.4120364785194397, 0.02684645727276802, 0.18810486793518066, 0.004385298117995262, 0.23515628278255463, 0.06802568584680557, 0.342936635017395, 0.3058969974517822, 0.36302581429481506, 0.19983524084091187, 0.15440364181995392, 0.16254578530788422, 0.30131232738494873], dtype='float32').reshape([20]),
            paddle.to_tensor([0.03798171132802963, 0.054855797439813614, 0.4844759702682495, 0.26358306407928467, 0.2145432084798813, 0.403802752494812, 0.2209421992301941, 0.44454091787338257, 0.3325815498828888, 0.27651119232177734, 0.24906176328659058, 0.1837702840566635, 0.3300878703594208, 0.2917044758796692, 0.4842681586742401, 0.38289639353752136, 0.25682929158210754, 0.3116863965988159, 0.3152053952217102, 0.3313342332839966], dtype='float32').reshape([20]),
            paddle.to_tensor([0.29077792167663574, 0.4711114168167114, 0.022691525518894196, 0.36034882068634033, 0.23633597791194916, 0.3561917543411255, 0.06265587359666824, 0.4308275580406189, 0.29402104020118713, 0.10051830857992172, 0.13285602629184723, 0.15259484946727753, 0.0899028480052948, 0.3483429551124573, 0.300533652305603, 0.31328362226486206, 0.3657846748828888, 0.3584064841270447, 0.06398306787014008, 0.34671762585639954], dtype='float32').reshape([20]),
            paddle.to_tensor([0.39169254899024963, 0.478073388338089, 0.42675498127937317, 0.4192875325679779, 0.19017159938812256, 0.2522801458835602, 0.18614310026168823, 0.33038511872291565, 0.19305038452148438, 0.2705075442790985, 0.30297204852104187, 0.3045877516269684, 0.19494156539440155, 0.33751097321510315, 0.3556782901287079, 0.26258301734924316, 0.19287852942943573, 0.05270613729953766, 0.44387924671173096, 0.30451640486717224], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c850b4951faaffbbdf4b1633e68d161f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 256, 128], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2296615093946457, 0.16428998112678528, 0.2505505383014679, 0.38713932037353516, 0.16393505036830902, 0.165256068110466, 0.030101684853434563, 0.4176899492740631, 0.007090810686349869, 0.25676068663597107, 0.4244028627872467, 0.15911442041397095, 0.36329618096351624, 0.16988149285316467, 0.31613680720329285, 0.15384860336780548, 0.02285182662308216, 0.0016197399236261845, 0.25992345809936523, 0.4662388563156128, 0.4932410418987274, 0.17761985957622528, 0.19106444716453552, 0.39213624596595764], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2253836840391159, 0.18392783403396606, 0.22931374609470367, 0.3202677369117737, 0.4167332649230957, 0.4072113037109375, 0.4446175694465637, 0.18521061539649963, 0.4559737741947174, 0.14275118708610535, 0.12172432243824005, 0.3456909954547882, 0.4939294755458832, 0.04078920930624008, 0.06402935832738876, 0.3516104817390442, 0.09527108818292618, 0.14874805510044098, 0.00013973662862554193, 0.4156690537929535, 0.2680128812789917, 0.3164524734020233, 0.03668172284960747, 0.35892361402511597], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3399914503097534, 0.2004563957452774, 0.33388063311576843, 0.48217177391052246, 0.3779219388961792, 0.10396712273359299, 0.3827466070652008, 0.13520129024982452, 0.05833633616566658, 0.05991495028138161, 0.07288498431444168, 0.2031525820493698, 0.29302093386650085, 0.0909329429268837, 0.19653423130512238, 0.20401743054389954, 0.46471118927001953, 0.4836195111274719, 0.36149120330810547, 0.4935210645198822, 0.17411309480667114, 0.3011774718761444, 0.4889330267906189, 0.15466861426830292], dtype='float32').reshape([24]),
            paddle.to_tensor([0.06885223835706711, 0.16208629310131073, 0.16247540712356567, 0.29364144802093506, 0.3424377143383026, 0.45293372869491577, 0.4847785234451294, 0.15725888311862946, 0.3847636282444, 0.3194640278816223, 0.13970525562763214, 0.12175702303647995, 0.15374763309955597, 0.14749886095523834, 0.06570424139499664, 0.0780722126364708, 0.08419222384691238, 0.15180273354053497, 0.33356815576553345, 0.3571048676967621, 0.22338519990444183, 0.3423826992511749, 0.16398833692073822, 0.33082133531570435], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_229c342c4d2a42c93b5574cac4614fe2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_018c90d2406f056ca90ca40699938dc9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 12, 12], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_026a1368cd4f20a985083c719213675a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3c22e99a19551864e4c2c53dc9e25197(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_b4f4da3a8d44d1b5b3a2903a097df186
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c03958cf605d342bba33c8d5bf10fca2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 512, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9d1872f6c24dd670c0b05d7d06f2e933(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1440, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1440], dtype='float32', min=0, max=0.5),
            paddle.uniform([1440], dtype='float32', min=0, max=0.5),
            paddle.uniform([1440], dtype='float32', min=0, max=0.5),
            paddle.uniform([1440], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0d6a7b530067c8ad6d560fdc0964a134(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.1292482614517212, 0.06776802986860275, 0.0927787572145462, 0.4466434121131897, 0.3116549253463745, 0.2907249331474304, 0.10730628669261932, 0.22036993503570557, 0.053071316331624985, 0.12220186740159988, 0.185895636677742, 0.49478331208229065, 0.09308894723653793, 0.46619755029678345, 0.33550000190734863, 0.2978460192680359, 0.3201691210269928, 0.14272576570510864, 0.06594954431056976, 0.010951821692287922, 0.4100996255874634, 0.02284424565732479, 0.23982805013656616, 0.40806350111961365], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4537056088447571, 0.3098032474517822, 0.3336114287376404, 0.22133010625839233, 0.1554185450077057, 0.3876495659351349, 0.058700863271951675, 0.2661203444004059, 0.31975632905960083, 0.3773419260978699, 0.0520135797560215, 0.47295162081718445, 0.31037089228630066, 0.3596043288707733, 0.24007999897003174, 0.11848866194486618, 0.31852370500564575, 0.15288430452346802, 0.45936983823776245, 0.12146034091711044, 0.033740296959877014, 0.41080954670906067, 0.306088387966156, 0.2612093687057495], dtype='float32').reshape([24]),
            paddle.to_tensor([0.16630221903324127, 0.017264852300286293, 0.3640444576740265, 0.2521769106388092, 0.15099680423736572, 0.4357534348964691, 0.0007059191702865064, 0.19223634898662567, 0.2205209881067276, 0.28691181540489197, 0.0033475011587142944, 0.004814666695892811, 0.15608997642993927, 0.03506725654006004, 0.4912382662296295, 0.44790783524513245, 0.3116547763347626, 0.1427919715642929, 0.058355484157800674, 0.38585764169692993, 0.3631594479084015, 0.1545720100402832, 0.2250385731458664, 0.396137535572052], dtype='float32').reshape([24]),
            paddle.to_tensor([0.209705650806427, 0.2664836347103119, 0.3186867833137512, 0.05885623022913933, 0.2217806726694107, 0.3848559260368347, 0.4745299518108368, 0.4820902347564697, 0.2101515680551529, 0.4924868047237396, 0.30066996812820435, 0.04120126739144325, 0.3111526370048523, 0.44737520813941956, 0.08002989739179611, 0.4026249051094055, 0.4481842517852783, 0.0787087082862854, 0.020852727815508842, 0.49760115146636963, 0.052163396030664444, 0.24424396455287933, 0.11327707022428513, 0.3408263623714447], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4895b06e4cbd5b787836470329a05327(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 12, 12], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_62bf3a0a0c2ff399b45a5a03384da9a0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_847ccdfb38fbdcf20620631f1e4cc076(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_0edd578f48a07aa5d1d79f9f678b472e
    def get_inputs(self):
        return [
            paddle.uniform([1, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8568e1d59d2f6ac3a93c647c8d1b9ece(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 512, 1024], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.475342333316803, 0.013035351410508156, 0.4093072712421417, 0.2620989680290222, 0.17896735668182373, 0.4844193756580353, 0.04859365522861481, 0.36043781042099, 0.14855483174324036, 0.36464977264404297, 0.09030605107545853, 0.4704442322254181, 0.1686011105775833, 0.05736618861556053, 0.39656829833984375, 0.01659935899078846], dtype='float32').reshape([16]),
            paddle.to_tensor([0.40986302495002747, 0.40552929043769836, 0.04724440723657608, 0.49032410979270935, 0.36973705887794495, 0.23223333060741425, 0.4180510640144348, 0.3250437378883362, 0.4773520231246948, 0.3132112920284271, 0.1368769407272339, 0.24129743874073029, 0.09309516847133636, 0.3557901680469513, 0.44665056467056274, 0.12593893706798553], dtype='float32').reshape([16]),
            paddle.to_tensor([0.38624274730682373, 0.12297410517930984, 0.12202860414981842, 0.08668660372495651, 0.26165056228637695, 0.29231026768684387, 0.348996102809906, 0.14175574481487274, 0.11984319984912872, 0.030154993757605553, 0.22238358855247498, 0.07498611509799957, 0.4212714731693268, 0.3366958796977997, 0.27627941966056824, 0.27844229340553284], dtype='float32').reshape([16]),
            paddle.to_tensor([0.45788389444351196, 0.29609906673431396, 0.3172542154788971, 0.22868414223194122, 0.4411267936229706, 0.2577226758003235, 0.32533445954322815, 0.40261155366897583, 0.1599903106689453, 0.3604581952095032, 0.07916613668203354, 0.4172859489917755, 0.1484689563512802, 0.33597683906555176, 0.1312432587146759, 0.1922946572303772], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_965e2a2522d52953badc65c3838c1730(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 44, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([44], dtype='float32', min=0, max=0.5),
            paddle.uniform([44], dtype='float32', min=0, max=0.5),
            paddle.uniform([44], dtype='float32', min=0, max=0.5),
            paddle.uniform([44], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4fe2f7e1f9e5f7542105523a4dc622be(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e0f15764a58e7ecbce9119f3eafcd630(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 4, 3], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7f2feb9b3a49be63e7a49d89add65895(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d417a85ad18af683b0a1756e8a992acb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 122, 122], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b460e92cc28c76639181a341ff547da2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.00531267374753952, 0.18650662899017334, 0.18780282139778137, 0.3666766285896301, 0.20239338278770447, 0.4459967017173767, 0.2508823871612549, 0.016783351078629494, 0.15631583333015442, 0.49192923307418823, 0.09062615036964417, 0.3411778211593628, 0.16419818997383118, 0.38907667994499207, 0.10492625087499619, 0.3344670832157135, 0.32321229577064514, 0.4306185841560364, 0.3625466227531433, 0.39574113488197327, 0.29742908477783203, 0.10479198396205902, 0.19754230976104736, 0.22625695168972015, 0.2342762053012848, 0.07056420296430588, 0.37024447321891785, 0.07903499901294708], dtype='float32').reshape([28]),
            paddle.to_tensor([0.2967415452003479, 0.16266588866710663, 0.3241727948188782, 0.13171958923339844, 0.2329738289117813, 0.04017610102891922, 0.15256012976169586, 0.48601382970809937, 0.2721374034881592, 0.03608732670545578, 0.31777486205101013, 0.10645551979541779, 0.07459644973278046, 0.4039956033229828, 0.4724128246307373, 0.4862838685512543, 0.37998390197753906, 0.22402095794677734, 0.2868311107158661, 0.3841543197631836, 0.186050683259964, 0.2210068255662918, 0.07085375487804413, 0.3272358477115631, 0.19183781743049622, 0.23381735384464264, 0.014222044497728348, 0.44231003522872925], dtype='float32').reshape([28]),
            paddle.to_tensor([0.3991570472717285, 0.237679123878479, 0.203359916806221, 0.2713569402694702, 0.1834510713815689, 0.14753736555576324, 0.2077128291130066, 0.09372496604919434, 0.2353167086839676, 0.42655181884765625, 0.04149996116757393, 0.22570201754570007, 0.29409360885620117, 0.46627187728881836, 0.10060427337884903, 0.004937421530485153, 0.16141492128372192, 0.2567051947116852, 0.38905465602874756, 0.2264404296875, 0.3676288425922394, 0.015415252186357975, 0.36416423320770264, 0.041348960250616074, 0.2145417481660843, 0.3403734266757965, 0.26307427883148193, 0.26126986742019653], dtype='float32').reshape([28]),
            paddle.to_tensor([0.32759329676628113, 0.37257033586502075, 0.260578453540802, 0.22081926465034485, 0.22466187179088593, 0.4386725127696991, 0.4095238745212555, 0.33121952414512634, 0.472417950630188, 0.33594805002212524, 0.44168081879615784, 0.04926232993602753, 0.16972051560878754, 0.12837345898151398, 0.08868634700775146, 0.21772785484790802, 0.33019810914993286, 0.04395667836070061, 0.354300856590271, 0.20991936326026917, 0.30993908643722534, 0.2533026933670044, 0.4611063301563263, 0.05440929904580116, 0.17423094809055328, 0.013401348143815994, 0.3985656797885895, 0.23808786273002625], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_972534663799a28f3f84ce6eac5c4db2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dce3509ba976fc2d6b4c9810e780f1c8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.003539975732564926, 0.47109538316726685, 0.2130454182624817, 0.12522441148757935, 0.020335564389824867, 0.4244157373905182, 0.11700434982776642, 0.4111025035381317, 0.4021749198436737, 0.09054108709096909, 0.21204788982868195, 0.49211984872817993, 0.273527592420578, 0.3038733899593353, 0.2515495717525482, 0.25743311643600464, 0.2707161605358124, 0.07665958255529404, 0.38707950711250305, 0.49971887469291687, 0.4526251554489136, 0.07002892345190048, 0.19170866906642914, 0.24783505499362946, 0.21566633880138397, 0.09526437520980835, 0.4449334740638733, 0.2448604851961136], dtype='float32').reshape([28]),
            paddle.to_tensor([0.4005391299724579, 0.16289196908473969, 0.49506041407585144, 0.04179757088422775, 0.1341296285390854, 0.23423850536346436, 0.11208070069551468, 0.07707931846380234, 0.1392558515071869, 0.42770057916641235, 0.030199941247701645, 0.27862077951431274, 0.38540443778038025, 0.36390307545661926, 0.2599022090435028, 0.30908745527267456, 0.3719676434993744, 0.45580747723579407, 0.3296101689338684, 0.11627998948097229, 0.4211382269859314, 0.2458232194185257, 0.15936248004436493, 0.34604504704475403, 0.041090577840805054, 0.3455861806869507, 0.42975321412086487, 0.3497726321220398], dtype='float32').reshape([28]),
            paddle.to_tensor([0.23852838575839996, 0.10260504484176636, 0.3969772160053253, 0.2950393557548523, 0.2582506835460663, 0.46878868341445923, 0.08982063829898834, 0.12592943012714386, 0.2536523640155792, 0.09862875938415527, 0.1548941284418106, 0.19935455918312073, 0.4471423029899597, 0.17118053138256073, 0.38482409715652466, 0.18027865886688232, 0.46128326654434204, 0.06310594081878662, 0.24982678890228271, 0.29253992438316345, 0.449662983417511, 0.1713700294494629, 0.4350953698158264, 0.3109647035598755, 0.12002439051866531, 0.3744555115699768, 0.3213508427143097, 0.034124091267585754], dtype='float32').reshape([28]),
            paddle.to_tensor([0.39023342728614807, 0.18502376973628998, 0.29311367869377136, 0.21521052718162537, 0.39303267002105713, 0.09056303650140762, 0.27628275752067566, 0.4783266484737396, 0.15165545046329498, 0.31862130761146545, 0.06285622715950012, 0.03758862614631653, 0.38947853446006775, 0.32681161165237427, 0.49154117703437805, 0.028121624141931534, 0.39311641454696655, 0.023567650467157364, 0.2589567005634308, 0.3004417419433594, 0.4773142337799072, 0.37754419445991516, 0.42513149976730347, 0.05843282490968704, 0.21582236886024475, 0.44059205055236816, 0.3977032005786896, 0.43943119049072266], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d38c95607ad83ba40f2b6865866f7d4a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 8, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8fe971f4b94d44b24321c7202c7bc056(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_145ad20e7a3b3c8aa31f6b40d4fe3d45
    def get_inputs(self):
        return [
            paddle.uniform([49, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cc1faf6f796f1bb90da08d6fb8b18275(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_79ce24998f3e759d86ab3b960574ab50(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_eef075e77cd405a28041ae3e91bcced7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.14585500955581665, 0.017270896583795547, 0.4221425950527191, 0.2854897081851959, 0.36174219846725464, 0.28488045930862427, 0.38960713148117065, 0.37952354550361633, 0.20425191521644592, 0.269528329372406, 0.27779504656791687, 0.013325054198503494, 0.28086692094802856, 0.30283093452453613, 0.36604416370391846, 0.4166794419288635, 0.16322411596775055, 0.06121331453323364, 0.12668579816818237, 0.4224090278148651], dtype='float32').reshape([20]),
            paddle.to_tensor([0.04070345312356949, 0.16515587270259857, 0.022858919575810432, 0.1297449767589569, 0.14362235367298126, 0.3447956442832947, 0.2745751738548279, 0.1736283004283905, 0.11016788333654404, 0.18979763984680176, 0.4946822226047516, 0.15243230760097504, 0.1210179477930069, 0.1450933814048767, 0.17961126565933228, 0.4376625120639801, 0.22661837935447693, 0.27894487977027893, 0.07074464112520218, 0.1371316760778427], dtype='float32').reshape([20]),
            paddle.to_tensor([0.20808465778827667, 0.1562686413526535, 0.3178386390209198, 0.49543923139572144, 0.3526775538921356, 0.3610720932483673, 0.34087154269218445, 0.23303978145122528, 0.27757957577705383, 0.40104398131370544, 0.2505250573158264, 0.27562040090560913, 0.23276546597480774, 0.16197048127651215, 0.16038303077220917, 0.48074671626091003, 0.13901767134666443, 0.22839675843715668, 0.2591753304004669, 0.18108640611171722], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3792310059070587, 0.38060981035232544, 0.37930506467819214, 0.4955160915851593, 0.25685909390449524, 0.05344896763563156, 0.029693912714719772, 0.43002304434776306, 0.13110747933387756, 0.3137838542461395, 0.05874563008546829, 0.2754287123680115, 0.07475177198648453, 0.4961361587047577, 0.23953458666801453, 0.4596402049064636, 0.14212974905967712, 0.36454853415489197, 0.3314458429813385, 0.39545905590057373], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4c1c77ca16ed7240c83da96cc4253e06(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_73daa6df1f54e84e0c0be2870b7469e1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 832, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([832], dtype='float32', min=0, max=0.5),
            paddle.uniform([832], dtype='float32', min=0, max=0.5),
            paddle.uniform([832], dtype='float32', min=0, max=0.5),
            paddle.uniform([832], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9df6fdcec9c76bec2ba37e4c38b4ba11(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 6, 6], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_51d4383fbf12e184cee6696049d34f6c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 35, 35], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8605a7ad91ba24a347c33ec42576885c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.12973597645759583, 0.3466455638408661, 0.465355783700943, 0.26301395893096924, 0.23099134862422943, 0.2558664083480835, 0.33205080032348633, 0.05077546834945679, 0.25621798634529114, 0.3567339777946472, 0.13654732704162598, 0.3394179940223694, 0.20776060223579407, 0.20949576795101166, 0.27999114990234375, 0.044823966920375824, 0.29366448521614075, 0.2783081531524658, 0.37908637523651123, 0.28625723719596863], dtype='float32').reshape([20]),
            paddle.to_tensor([0.43422892689704895, 0.39721667766571045, 0.07867579907178879, 0.05797819420695305, 0.21577616035938263, 0.4717293381690979, 0.013587226159870625, 0.030118735507130623, 0.3711923360824585, 0.1759992092847824, 0.24398215115070343, 0.3805261552333832, 0.44056859612464905, 0.2646302878856659, 0.07930810004472733, 0.005199246108531952, 0.42502346634864807, 0.09902391582727432, 0.4574241638183594, 0.08516482263803482], dtype='float32').reshape([20]),
            paddle.to_tensor([0.26649725437164307, 0.21915775537490845, 0.3349819481372833, 0.4459736943244934, 0.07286358624696732, 0.0029483800753951073, 0.2556791305541992, 0.12178785353899002, 0.48283854126930237, 0.20261721312999725, 0.012395351193845272, 0.3722115159034729, 0.4713199734687805, 0.1849459707736969, 0.43789374828338623, 0.09651030600070953, 0.33820849657058716, 0.1934535950422287, 0.1873042732477188, 0.3831605911254883], dtype='float32').reshape([20]),
            paddle.to_tensor([0.2544196546077728, 0.2013077288866043, 0.33744555711746216, 0.2727356553077698, 0.27532270550727844, 0.05417743697762489, 0.072478286921978, 0.1453205943107605, 0.0348905585706234, 0.18373903632164001, 0.36274099349975586, 0.20725633203983307, 0.2567991614341736, 0.15668362379074097, 0.4124685525894165, 0.23859480023384094, 0.22610606253147125, 0.17744293808937073, 0.41852501034736633, 0.4926513135433197], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c8255f67f9cf5c116c8d83bbab8c58c7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_42e57ee1257b131bd52a8a01c8bd9b54(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 138, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([138], dtype='float32', min=0, max=0.5),
            paddle.uniform([138], dtype='float32', min=0, max=0.5),
            paddle.uniform([138], dtype='float32', min=0, max=0.5),
            paddle.uniform([138], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c0be56c9e55ad7c7068d1c27b836fdaa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 19, 19], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cf4ea245311f72133754ecf7b8bc356e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f9426457ddaa4bec37e0c4e98a4feaa4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.1433931291103363, 0.2563546299934387, 0.3360213339328766, 0.3194129765033722, 0.32579293847084045, 0.07143230736255646, 0.10111640393733978, 0.27149295806884766, 0.16549155116081238, 0.36789706349372864, 0.28406792879104614, 0.17721699178218842, 0.2884477972984314, 0.32363635301589966, 0.08608511835336685, 0.47451668977737427, 0.2516292333602905, 0.047896333038806915, 0.023550255224108696, 0.39363938570022583, 0.41163334250450134, 0.08048783242702484, 0.2147110253572464, 0.15333105623722076, 0.2023053616285324, 0.12593907117843628, 0.36320292949676514, 0.052716560661792755, 0.49402204155921936, 0.29656437039375305], dtype='float32').reshape([30]),
            paddle.to_tensor([0.271896094083786, 0.37436890602111816, 0.3351062536239624, 0.26956167817115784, 0.47591397166252136, 0.32235994935035706, 0.041185785084962845, 0.3431040942668915, 0.047310829162597656, 0.251352459192276, 0.29120033979415894, 0.3719666600227356, 0.2862745225429535, 0.4211589992046356, 0.2645203173160553, 0.3210738003253937, 0.49785158038139343, 0.003909653052687645, 0.1903674155473709, 0.3755921721458435, 0.00505833188071847, 0.15263395011425018, 0.1620720624923706, 0.08275549113750458, 0.44339698553085327, 0.15616701543331146, 0.3699936866760254, 0.22090964019298553, 0.27683940529823303, 0.4654589295387268], dtype='float32').reshape([30]),
            paddle.to_tensor([0.32214826345443726, 0.24697180092334747, 0.2843131124973297, 0.3345697522163391, 0.3251940906047821, 0.14569900929927826, 0.2403610646724701, 0.12169455736875534, 0.097746342420578, 0.024288704618811607, 0.24723346531391144, 0.16634997725486755, 0.2966877520084381, 0.3290383815765381, 0.433228999376297, 0.32587966322898865, 0.2731887996196747, 0.3431321084499359, 0.0655234232544899, 0.06270594894886017, 0.3833634853363037, 0.2672130763530731, 0.1953718364238739, 0.18348748981952667, 0.3433130979537964, 0.4229240417480469, 0.13952384889125824, 0.4733172357082367, 0.10741037130355835, 0.3558371663093567], dtype='float32').reshape([30]),
            paddle.to_tensor([0.45096951723098755, 0.23774582147598267, 0.3994019627571106, 0.06326638907194138, 0.24520352482795715, 0.333037793636322, 0.26343944668769836, 0.3288612961769104, 0.3629220128059387, 0.3797658383846283, 0.270704448223114, 0.008326519280672073, 0.26868292689323425, 0.4975796937942505, 0.16752566397190094, 0.4002695381641388, 0.18851330876350403, 0.3464159369468689, 0.47807881236076355, 0.38927727937698364, 0.22943837940692902, 0.2471836805343628, 0.4977286458015442, 0.3386068344116211, 0.3175867199897766, 0.4932926893234253, 0.012789290398359299, 0.4283691942691803, 0.4347940981388092, 0.40999460220336914], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fbcbd3d4f0406faf82bab4bea833b459(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 480, 480], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3281659781932831, 0.2957717478275299, 0.344228059053421, 0.07787920534610748, 0.2417534738779068, 0.1374155431985855, 0.2564123570919037, 0.16014762222766876], dtype='float32').reshape([8]),
            paddle.to_tensor([0.17059439420700073, 0.04452399164438248, 0.3108731806278229, 0.3380909860134125, 0.08854854851961136, 0.1282915323972702, 0.2399338185787201, 0.13016153872013092], dtype='float32').reshape([8]),
            paddle.to_tensor([0.22092552483081818, 0.005781997926533222, 0.028722841292619705, 0.09350348263978958, 0.4626440107822418, 0.2870311141014099, 0.15664246678352356, 0.3369508683681488], dtype='float32').reshape([8]),
            paddle.to_tensor([0.31259286403656006, 0.05237532779574394, 0.08272959291934967, 0.26823702454566956, 0.033693473786115646, 0.2552991211414337, 0.0054334718734025955, 0.058733418583869934], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2f59c85e869fb4852fea81a201762ce1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.28135818243026733, 0.09452663362026215, 0.059469614177942276, 0.09456363320350647, 0.24683180451393127, 0.18404020369052887, 0.3464462161064148, 0.29913219809532166, 0.014790826477110386, 0.2954126000404358, 0.2596546709537506, 0.13904643058776855, 0.34853753447532654, 0.01990852691233158, 0.42747336626052856, 0.2562831938266754, 0.3699077367782593, 0.3067910075187683, 0.061448704451322556, 0.31444457173347473], dtype='float32').reshape([20]),
            paddle.to_tensor([0.1116415485739708, 0.2826012074947357, 0.09553561359643936, 0.3200022280216217, 0.22621798515319824, 0.19059240818023682, 0.3719755709171295, 0.2174014151096344, 0.05500062182545662, 0.07441891729831696, 0.13112445175647736, 0.10272153466939926, 0.48400914669036865, 0.26278477907180786, 0.15560294687747955, 0.05502333864569664, 0.08403294533491135, 0.23283587396144867, 0.49462875723838806, 0.11870177835226059], dtype='float32').reshape([20]),
            paddle.to_tensor([0.47756803035736084, 0.3629527986049652, 0.005916710011661053, 0.43896937370300293, 0.37698930501937866, 0.09559181332588196, 0.42619508504867554, 0.27102014422416687, 0.17662709951400757, 0.0523710660636425, 0.2873404324054718, 0.25966909527778625, 0.36016520857810974, 0.04462702199816704, 0.3894978165626526, 0.3139325678348541, 0.07548348605632782, 0.06622016429901123, 0.44198933243751526, 0.2960803806781769], dtype='float32').reshape([20]),
            paddle.to_tensor([0.1512138843536377, 0.4455955922603607, 0.4413823187351227, 0.298801988363266, 0.3775598704814911, 0.2681357264518738, 0.0008778083720244467, 0.016788557171821594, 0.46535423398017883, 0.34008243680000305, 0.38966673612594604, 0.22023695707321167, 0.4448797106742859, 0.46283894777297974, 0.15683108568191528, 0.12495823949575424, 0.4583742916584015, 0.22034721076488495, 0.47004806995391846, 0.4707679748535156], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a5a3048c2f779d9725753401037bcde8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 32, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_64539ccba33e39bdfbf4314d18fb9833(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.39916229248046875, 0.20920102298259735, 0.0013255842495709658, 0.45233386754989624, 0.06346447020769119, 0.4672015607357025, 0.334583580493927, 0.41241517663002014, 0.24840116500854492, 0.3823050856590271, 0.2588188946247101, 0.07079114019870758, 0.4579292833805084, 0.24020564556121826, 0.009561887010931969, 0.03448076173663139, 0.2800474762916565, 0.4230565130710602], dtype='float32').reshape([18]),
            paddle.to_tensor([0.31425896286964417, 0.04100053012371063, 0.33216607570648193, 0.284596711397171, 0.03618745505809784, 0.3287186324596405, 0.14365339279174805, 0.2074621617794037, 0.25996270775794983, 0.0176344346255064, 0.3497951030731201, 0.2654738426208496, 0.24148248136043549, 0.46138232946395874, 0.4272707402706146, 0.42504385113716125, 0.3204213082790375, 0.4927487373352051], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4846111536026001, 0.11064859479665756, 0.46172192692756653, 0.211225688457489, 0.4231316149234772, 0.30972859263420105, 0.33986422419548035, 0.31457218527793884, 0.1353100836277008, 0.4465256333351135, 0.3591419458389282, 0.03976017236709595, 0.386088103055954, 0.1022736057639122, 0.29769882559776306, 0.12489540129899979, 0.16836360096931458, 0.1252867430448532], dtype='float32').reshape([18]),
            paddle.to_tensor([0.07563640177249908, 0.15945985913276672, 0.32662245631217957, 0.20997214317321777, 0.3584713339805603, 0.37008190155029297, 0.13900259137153625, 0.24534906446933746, 0.05080954730510712, 0.08867698162794113, 0.11765462160110474, 0.02850579097867012, 0.4540349841117859, 0.19525308907032013, 0.017514286562800407, 0.4091114103794098, 0.020703835412859917, 0.0739147886633873], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6be3a5d5032d77d89ea93f6cdfc9a5fa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 50, 84], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_94335ef4e5897047138df29683c959f1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_00fe46f5d0ab5a58990863f62abf9894(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cdc3b0a148b551f86897e4668e0bdb6b
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9668e41c5601fb0a6edfdeb3ed5cbbfb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_d589af777f5a8ae699611515c41499fb
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_60d97c6ba1391dc0c0dc72235997d16e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_617ad5578853eb646b87c414d330eb3d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.0867689698934555, 0.263782262802124, 0.0024293786846101284, 0.30059120059013367, 0.07478311657905579, 0.036859944462776184, 0.04947369545698166, 0.14042851328849792, 0.1769578903913498, 0.41994428634643555, 0.369582861661911, 0.2241259515285492, 0.28195852041244507, 0.01758921518921852, 0.36847561597824097, 0.27123749256134033, 0.2964341342449188, 0.14901894330978394, 0.21844139695167542, 0.33804410696029663], dtype='float32').reshape([20]),
            paddle.to_tensor([0.30557411909103394, 0.3717119097709656, 0.24306577444076538, 0.33762216567993164, 0.23457014560699463, 0.296254962682724, 0.18123772740364075, 0.16261664032936096, 0.2238103151321411, 0.22405001521110535, 0.3386358916759491, 0.4254676401615143, 0.15625165402889252, 0.4989171326160431, 0.000511910708155483, 0.048191819339990616, 0.3859730064868927, 0.19386853277683258, 0.4767901599407196, 0.4728297293186188], dtype='float32').reshape([20]),
            paddle.to_tensor([0.2087090015411377, 0.34822583198547363, 0.46605053544044495, 0.44679969549179077, 0.16044142842292786, 0.21840400993824005, 0.47840428352355957, 0.4053443968296051, 0.1784474104642868, 0.23462346196174622, 0.11321539431810379, 0.42093193531036377, 0.23099744319915771, 0.465829998254776, 0.48514315485954285, 0.4434916377067566, 0.06251054257154465, 0.11778563261032104, 0.36249902844429016, 0.47652366757392883], dtype='float32').reshape([20]),
            paddle.to_tensor([0.457986056804657, 0.19257819652557373, 0.004063892178237438, 0.28525829315185547, 0.041682321578264236, 0.30759698152542114, 0.39001384377479553, 0.14756424725055695, 0.3430522084236145, 0.45872417092323303, 0.3230314552783966, 0.24310711026191711, 0.2674834430217743, 0.29999855160713196, 0.04940848425030708, 0.4121844172477722, 0.046536196023225784, 0.013379726558923721, 0.21197816729545593, 0.2786138355731964], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_12de28554061a33e3c070c052e56e32b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7f491496970c567562a5dc2ca893bf2f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_11bea1026744e3807e4c3ecdfbc2d62e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 256, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d29aa8aec2fcf454e2fa0a4beebfa4e4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 112, 199], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_836a8fd80e78af64c427fe7f3e8319ea(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.42562898993492126, 0.30726948380470276, 0.1404137760400772, 0.13534115254878998, 0.23004961013793945, 0.31025224924087524, 0.37094640731811523, 0.4608825743198395, 0.10828719288110733, 0.4081839621067047, 0.04795439541339874, 0.4906134307384491, 0.457353413105011, 0.26271501183509827, 0.4017186760902405, 0.4517083764076233, 0.08666765689849854, 0.24481593072414398, 0.43049564957618713, 0.29380184412002563, 0.010452625341713428, 0.2823995053768158, 0.4723806083202362, 0.08986440300941467], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2665727138519287, 0.06261109560728073, 0.4870413541793823, 0.3954557180404663, 0.12145537883043289, 0.026494981721043587, 0.49896132946014404, 0.08756984025239944, 0.22758540511131287, 0.06424820423126221, 0.27421921491622925, 0.23178082704544067, 0.08950001001358032, 0.22999203205108643, 0.33270299434661865, 0.09981165081262589, 0.18885508179664612, 0.4169679582118988, 0.3838236331939697, 0.3074103593826294, 0.04793410003185272, 0.1573253571987152, 0.06141505762934685, 0.46676936745643616], dtype='float32').reshape([24]),
            paddle.to_tensor([0.23787590861320496, 0.3275638818740845, 0.4877736568450928, 0.3624683618545532, 0.3614274859428406, 0.24483072757720947, 0.09789174050092697, 0.296121209859848, 0.2339019775390625, 0.131143257021904, 0.22157172858715057, 0.050285980105400085, 0.25090599060058594, 0.3923497796058655, 0.29841428995132446, 0.27018266916275024, 0.2859755754470825, 0.05821404233574867, 0.49914368987083435, 0.29436159133911133, 0.37535256147384644, 0.258292555809021, 0.2785468101501465, 0.0045034028589725494], dtype='float32').reshape([24]),
            paddle.to_tensor([0.03546718508005142, 0.22293244302272797, 0.45708009600639343, 0.27072179317474365, 0.46968311071395874, 0.21461397409439087, 0.16290828585624695, 0.3507770299911499, 0.2734585702419281, 0.23471114039421082, 0.25963348150253296, 0.17705565690994263, 0.05355672538280487, 0.13434958457946777, 0.35945892333984375, 0.4230216145515442, 0.08380647003650665, 0.12157513946294785, 0.15963441133499146, 0.34067925810813904, 0.20972692966461182, 0.01772601343691349, 0.3656896948814392, 0.3325793147087097], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_51ba1825a7a31f615f2b3eeb6fc06458(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 10, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.05570099502801895, 0.2002655565738678, 0.15683822333812714, 0.1981631964445114, 0.2945002615451813, 0.40253692865371704, 0.18278391659259796, 0.3524797558784485, 0.2892066538333893, 0.12220945954322815], dtype='float32').reshape([10]),
            paddle.to_tensor([0.33385932445526123, 0.055069684982299805, 0.11880467087030411, 0.3438282608985901, 0.46065664291381836, 0.22406908869743347, 0.11968794465065002, 0.10316900908946991, 0.1558060497045517, 0.3132895529270172], dtype='float32').reshape([10]),
            paddle.to_tensor([0.1931486427783966, 0.2130492627620697, 0.4639378786087036, 0.37253105640411377, 0.49852874875068665, 0.08803045004606247, 0.05259779468178749, 0.16601236164569855, 0.2879156172275543, 0.20789316296577454], dtype='float32').reshape([10]),
            paddle.to_tensor([0.07186833024024963, 0.19248591363430023, 0.11894605308771133, 0.19743557274341583, 0.3806762993335724, 0.34601595997810364, 0.4740736186504364, 0.2877886891365051, 0.20289048552513123, 0.45408084988594055], dtype='float32').reshape([10]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b6c33a7e1e8c7a776a21c959e0bce6bf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_34f6b4a6020b5ad9847b83a777b6cb11(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_145ad20e7a3b3c8aa31f6b40d4fe3d45
    def get_inputs(self):
        return [
            paddle.uniform([49, 384], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_78c44b67df6f24c47dc8056f8c5e4646(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 200, 336], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_942b8df679ebb3cfd16faf7acebaa474(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_0edd578f48a07aa5d1d79f9f678b472e
    def get_inputs(self):
        return [
            paddle.uniform([16, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_775d0d09940f6f652b7193572281ccaf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1044, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1044], dtype='float32', min=0, max=0.5),
            paddle.uniform([1044], dtype='float32', min=0, max=0.5),
            paddle.uniform([1044], dtype='float32', min=0, max=0.5),
            paddle.uniform([1044], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_df55b266d2926634b6e27ba8fd4e2751(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 120, 120], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_95b5016f483150de6dd04d96375a0416(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ac5af6b8c605f90890634f630d200f27(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 2112, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([2112], dtype='float32', min=0, max=0.5),
            paddle.uniform([2112], dtype='float32', min=0, max=0.5),
            paddle.uniform([2112], dtype='float32', min=0, max=0.5),
            paddle.uniform([2112], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dc43dd0015eecaab463b62a64d5fa105(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 19, 19], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3080664b8098301fc3e00b67e97bef04(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3620198667049408, 0.44858622550964355, 0.1790362149477005, 0.4539709985256195, 0.3423577845096588, 0.4310566186904907, 0.4521419107913971, 0.008878606371581554, 0.19768524169921875, 0.33927905559539795, 0.38093239068984985, 0.46197301149368286, 0.30883267521858215, 0.4192940890789032, 0.42476141452789307, 0.09633605182170868, 0.25753703713417053, 0.08324595540761948, 0.054653316736221313, 0.010822751559317112, 0.410578191280365, 0.16577790677547455, 0.33578893542289734, 0.1667322963476181, 0.013040076941251755, 0.09613371640443802, 0.4701748192310333, 0.10330460965633392, 0.1776164025068283, 0.11892175674438477], dtype='float32').reshape([30]),
            paddle.to_tensor([0.22141335904598236, 0.381913959980011, 0.41732823848724365, 0.17698580026626587, 0.03181956708431244, 0.25974124670028687, 0.21741798520088196, 0.2684497833251953, 0.3426246643066406, 0.25125643610954285, 0.029351936653256416, 0.15394239127635956, 0.03464203327894211, 0.2463773638010025, 0.16523326933383942, 0.06084145978093147, 0.35974740982055664, 0.08505898714065552, 0.030491763725876808, 0.2638434171676636, 0.31920456886291504, 0.21584446728229523, 0.3724455237388611, 0.007323205936700106, 0.39312925934791565, 0.1879900097846985, 0.4317227303981781, 0.3413273096084595, 0.43772903084754944, 0.4947586953639984], dtype='float32').reshape([30]),
            paddle.to_tensor([0.07350601255893707, 0.4466821551322937, 0.12468034029006958, 0.22109808027744293, 0.04287838935852051, 0.38676366209983826, 0.2870153784751892, 0.45830172300338745, 0.08954387903213501, 0.27724117040634155, 0.004533528350293636, 0.4136104881763458, 0.05258103832602501, 0.3105928301811218, 0.18375438451766968, 0.4863438010215759, 0.2345840036869049, 0.2590974271297455, 0.2114705592393875, 0.36018988490104675, 0.03618765249848366, 0.1288672834634781, 0.1980607956647873, 0.024193665012717247, 0.3670920133590698, 0.19942839443683624, 0.3692001700401306, 0.09247978031635284, 0.11590667068958282, 0.32735931873321533], dtype='float32').reshape([30]),
            paddle.to_tensor([0.43103229999542236, 0.16427640616893768, 0.30972811579704285, 0.08973851054906845, 0.02774197980761528, 0.004600072279572487, 0.0660393089056015, 0.11041901260614395, 0.12569960951805115, 0.20907926559448242, 0.4794027805328369, 0.4510173201560974, 0.42557772994041443, 0.2764105200767517, 0.1204758957028389, 0.41134172677993774, 0.41166168451309204, 0.49860405921936035, 0.26075735688209534, 0.20188546180725098, 0.4967016279697418, 0.44209790229797363, 0.0466855987906456, 0.14836442470550537, 0.37956640124320984, 0.0995330810546875, 0.49493664503097534, 0.26391223073005676, 0.01482074148952961, 0.0029085928108543158], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dcd4d369d4222203377cf2c9b6580ea7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 10, 10], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_89792713fe37a120ae7a473278cd0404(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 232, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_04d9f50998862810eaf29d37241fade8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ac57af21732dbdfb80431ca78f7f3b6f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6388d1d8168a9be1d445432eff24429f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.49033817648887634, 0.4468939006328583, 0.07786738872528076, 0.038680803030729294, 0.33178818225860596, 0.3443855047225952, 0.31470999121665955, 0.43401241302490234, 0.11955505609512329, 0.17075622081756592, 0.36873650550842285, 0.3973391652107239, 0.2607978880405426, 0.2853052616119385, 0.13657674193382263, 0.1903858482837677, 0.21716737747192383, 0.11714993417263031, 0.3910994827747345, 0.29628273844718933, 0.3215067982673645, 0.414480984210968, 0.21070711314678192, 0.22617697715759277], dtype='float32').reshape([24]),
            paddle.to_tensor([0.30136117339134216, 0.17471420764923096, 0.4092835485935211, 0.15197071433067322, 0.17540982365608215, 0.17194752395153046, 0.14509908854961395, 0.42956575751304626, 0.4235096871852875, 0.15399643778800964, 0.3573184311389923, 0.08579304069280624, 0.3281317353248596, 0.30805328488349915, 0.18977200984954834, 0.022226380184292793, 0.28823989629745483, 0.08797576278448105, 0.06832011789083481, 0.00018617307068780065, 0.4224974513053894, 0.22499457001686096, 0.3406901955604553, 0.034311890602111816], dtype='float32').reshape([24]),
            paddle.to_tensor([0.24540294706821442, 0.43626052141189575, 0.3458554446697235, 0.11234830319881439, 0.39881420135498047, 0.44214409589767456, 0.32530659437179565, 0.23235537111759186, 0.323739618062973, 0.016231779009103775, 0.24341891705989838, 0.24320179224014282, 0.41825130581855774, 0.0010052802972495556, 0.21351748704910278, 0.365682989358902, 0.11949697881937027, 0.3680226504802704, 0.3358433246612549, 0.15290112793445587, 0.04701884090900421, 0.41701972484588623, 0.3081105649471283, 0.041532788425683975], dtype='float32').reshape([24]),
            paddle.to_tensor([0.15729308128356934, 0.2634469270706177, 0.15964747965335846, 0.030762232840061188, 0.46233052015304565, 0.37390708923339844, 0.2818176746368408, 0.10763312876224518, 0.23324935138225555, 0.09836321324110031, 0.19756600260734558, 0.09951039403676987, 0.2582285404205322, 0.19485287368297577, 0.24406756460666656, 0.21134327352046967, 0.3482523262500763, 0.24315913021564484, 0.27115240693092346, 0.09527847915887833, 0.2634628415107727, 0.43527480959892273, 0.3856998682022095, 0.11162394285202026], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9e39930d497ac2ba4d8910a8a3b7a454(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 576, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e680bd4e1c5da6c997f661e2537ac250(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1792, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1792], dtype='float32', min=0, max=0.5),
            paddle.uniform([1792], dtype='float32', min=0, max=0.5),
            paddle.uniform([1792], dtype='float32', min=0, max=0.5),
            paddle.uniform([1792], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_df8e1f9ea8d21ab9f2cccdbe698593e8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 64, 48], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e1c984bc7e4322cf21a1844397da5b8d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 224, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0d26ad9c6ad3ef4e638cd9d5548db3c8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 10, 10], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c750cbc8bf5dc7e47fa2bd9214ec567a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f2b24eb78fdb0b1da8118e8b9bbb2534(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 112, 64, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_62cd1f6ee2bacaa66c2c5f4824744173(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_145ad20e7a3b3c8aa31f6b40d4fe3d45
    def get_inputs(self):
        return [
            paddle.uniform([196, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9ec3947b599c84b5e222e8fdaf1f1e07(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_07e65756e32cdacabbf7fdc73c01ecee(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d2e0fc47bf68fd24771b08758fb50861(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 4, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cfbd2542cd9208d3e02c7bd450d83f95(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.37042444944381714, 0.48798832297325134, 0.32646530866622925, 0.20391686260700226, 0.14580772817134857, 0.4739017188549042, 0.2964082360267639, 0.2762841582298279, 0.3429056406021118, 0.3511194586753845, 0.0638086125254631, 0.21319983899593353, 0.3584657609462738, 0.04318571463227272, 0.2514267861843109, 0.21491070091724396, 0.1596851348876953, 0.20467661321163177, 0.22926969826221466, 0.4827543795108795, 0.49958398938179016, 0.4941858649253845, 0.40606123208999634, 0.473943829536438], dtype='float32').reshape([24]),
            paddle.to_tensor([0.020246472209692, 0.3783518373966217, 0.45521438121795654, 0.004086692351847887, 0.33491379022598267, 0.014301822520792484, 0.3105586767196655, 0.4249754250049591, 0.014557849615812302, 0.3159695863723755, 0.1606801450252533, 0.03499724715948105, 0.21813057363033295, 0.3521457314491272, 0.4064209461212158, 0.08184552192687988, 0.2385028451681137, 0.1354418247938156, 0.2795407176017761, 0.35062268376350403, 0.39923980832099915, 0.43480899930000305, 0.38207781314849854, 0.020524214953184128], dtype='float32').reshape([24]),
            paddle.to_tensor([0.07560915499925613, 0.027230046689510345, 0.2320951372385025, 0.35634535551071167, 0.4311842620372772, 0.16665397584438324, 0.05112561583518982, 0.09599526226520538, 0.07500780373811722, 0.09127948433160782, 0.2044287770986557, 0.3301699459552765, 0.19268548488616943, 0.11702835559844971, 0.2622299790382385, 0.31177306175231934, 0.23643268644809723, 0.3429502844810486, 0.2962503433227539, 0.20706309378147125, 0.4299812912940979, 0.2669690251350403, 0.20591206848621368, 0.20035281777381897], dtype='float32').reshape([24]),
            paddle.to_tensor([0.12602050602436066, 0.3136933445930481, 0.0473819300532341, 0.07437631487846375, 0.08544343709945679, 0.44059816002845764, 0.49251100420951843, 0.030382269993424416, 0.3860922157764435, 0.2772596478462219, 0.4207225441932678, 0.44190752506256104, 0.2864910662174225, 0.008508149534463882, 0.35244423151016235, 0.07664868235588074, 0.31514668464660645, 0.3211328983306885, 0.14011147618293762, 0.013669216074049473, 0.09716223925352097, 0.23860204219818115, 0.37767520546913147, 0.43324014544487], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_00ed45cec6ec96a57e26e7bcb9d27880(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 2560, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([2560], dtype='float32', min=0, max=0.5),
            paddle.uniform([2560], dtype='float32', min=0, max=0.5),
            paddle.uniform([2560], dtype='float32', min=0, max=0.5),
            paddle.uniform([2560], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_68c8d15dbdcd300204a66d505b5d7621(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 4, 3], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_02fc409db1efd356d649c7a1c95ceca0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9fbf45bc52a5e9effbf1909fdc573523(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 8, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_faa48a48d846cdbe9b992a121a0a9ee4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_12ec515a203ad4c6b5c9ab77f36b2a49(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.08382882177829742, 0.271268755197525, 0.4775814414024353, 0.159735769033432, 0.30142685770988464, 0.09888080507516861, 0.1027306616306305, 0.01409437321126461, 0.4309951663017273, 0.4305941164493561, 0.48032426834106445, 0.2108083963394165, 0.3164939284324646, 0.08646876364946365, 0.16928131878376007, 0.45170140266418457, 0.19485796988010406, 0.35466650128364563, 0.06470013409852982, 0.21518735587596893], dtype='float32').reshape([20]),
            paddle.to_tensor([0.19751213490962982, 0.21431343257427216, 0.03446652367711067, 0.13653050363063812, 0.3838684856891632, 0.3145467936992645, 0.21528927981853485, 0.19724835455417633, 0.3984483480453491, 0.09356648474931717, 0.26195964217185974, 0.07028540968894958, 0.012944129295647144, 0.3405545651912689, 0.21044468879699707, 0.19329477846622467, 0.4317445755004883, 0.3811359405517578, 0.10801365226507187, 0.017124194651842117], dtype='float32').reshape([20]),
            paddle.to_tensor([0.03524672985076904, 0.09885720163583755, 0.2891116440296173, 0.13621927797794342, 0.482625275850296, 0.35284850001335144, 0.13781249523162842, 0.1526133418083191, 0.02805270068347454, 0.10101401060819626, 0.2547764182090759, 0.4151661992073059, 0.0674835816025734, 0.45750296115875244, 0.22782327234745026, 0.39257359504699707, 0.33921393752098083, 0.21836642920970917, 0.49578097462654114, 0.320350706577301], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4448051452636719, 0.24904367327690125, 0.06503826379776001, 0.029621170833706856, 0.1692168265581131, 0.40816280245780945, 0.19593383371829987, 0.11241630464792252, 0.10843433439731598, 0.16438190639019012, 0.06761080026626587, 0.20289427042007446, 0.44906750321388245, 0.23204901814460754, 0.17381979525089264, 0.00393546000123024, 0.4668135643005371, 0.12725046277046204, 0.20708595216274261, 0.08454547822475433], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b2a353948af1ecdd657793221473e1fc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1152, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_89de40f7551f9a881b697fe0bea1859c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.07935332506895065, 0.25171542167663574, 0.004379626363515854, 0.20531915128231049, 0.24931186437606812, 0.3378892242908478, 0.19120371341705322, 0.3450110852718353, 0.36380141973495483, 0.12546581029891968, 0.3374553918838501, 0.48486328125, 0.09174605458974838, 0.3303040862083435, 0.019442183896899223, 0.38005316257476807], dtype='float32').reshape([16]),
            paddle.to_tensor([0.03415137901902199, 0.4323252737522125, 0.015158776193857193, 0.40569186210632324, 0.40378138422966003, 0.09441376477479935, 0.2704989016056061, 0.46357178688049316, 0.23896923661231995, 0.2840895354747772, 0.18732377886772156, 0.18117743730545044, 0.29507702589035034, 0.10370323807001114, 0.12053827941417694, 0.12097188085317612], dtype='float32').reshape([16]),
            paddle.to_tensor([0.09561432898044586, 0.025298206135630608, 0.48676797747612, 0.007830070331692696, 0.28396347165107727, 0.40325480699539185, 0.42382222414016724, 0.3857424557209015, 0.10402156412601471, 0.27874690294265747, 0.47988080978393555, 0.05139946937561035, 0.41623181104660034, 0.19110266864299774, 0.4675070345401764, 0.38960304856300354], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3159770965576172, 0.14101290702819824, 0.3734520971775055, 0.3361988663673401, 0.027088016271591187, 0.46766483783721924, 0.3114815354347229, 0.17882534861564636, 0.1288186013698578, 0.03302135318517685, 0.478351354598999, 0.3936879634857178, 0.45918816328048706, 0.020143378525972366, 0.45954686403274536, 0.31899160146713257], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6fa22984d305dc31090619ba02a64a93(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9fb9ebf2cf180d580c1ef689789dffaa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.14305970072746277, 0.29573971033096313, 0.15534156560897827, 0.37368372082710266, 0.20367994904518127, 0.49774089455604553, 0.38185030221939087, 0.4150305688381195, 0.38147810101509094, 0.3498384952545166, 0.4341604709625244, 0.07570616155862808, 0.11468564718961716, 0.4913562536239624, 0.2647499740123749, 0.3196078836917877], dtype='float32').reshape([16]),
            paddle.to_tensor([0.032804738730192184, 0.08288594335317612, 0.06407676637172699, 0.0540119893848896, 0.2243954837322235, 0.260466068983078, 0.13112662732601166, 0.41730982065200806, 0.12711314857006073, 0.2915763854980469, 0.06778216361999512, 0.03007536195218563, 0.18670637905597687, 0.3228065073490143, 0.24234211444854736, 0.28355157375335693], dtype='float32').reshape([16]),
            paddle.to_tensor([0.21843212842941284, 0.352132111787796, 0.16020648181438446, 0.07632336765527725, 0.22359609603881836, 0.0978923961520195, 0.39881348609924316, 0.005379914306104183, 0.14485570788383484, 0.46808937191963196, 0.44896525144577026, 0.26959550380706787, 0.05123709514737129, 0.24362611770629883, 0.21941299736499786, 0.4529263377189636], dtype='float32').reshape([16]),
            paddle.to_tensor([0.209869384765625, 0.03722776100039482, 0.37252408266067505, 0.20359967648983002, 0.047324806451797485, 0.06973590701818466, 0.33882901072502136, 0.3188629746437073, 0.05146637558937073, 0.26021289825439453, 0.24051915109157562, 0.07930555194616318, 0.19337373971939087, 0.4480392634868622, 0.07510683685541153, 0.49657678604125977], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_eac3e100808af6d439ddc2a94962d9d1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 64, 128], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.049281470477581024, 0.28063660860061646, 0.1829894781112671, 0.3896317481994629, 0.18549484014511108, 0.32331418991088867, 0.068147674202919, 0.315681129693985, 0.17004084587097168, 0.2520394027233124, 0.1962117850780487, 0.23571641743183136, 0.3616357743740082, 0.20692312717437744, 0.48370131850242615, 0.09432289004325867, 0.2351808249950409, 0.053356852382421494], dtype='float32').reshape([18]),
            paddle.to_tensor([0.22507227957248688, 0.45797160267829895, 0.27070948481559753, 0.4198213219642639, 0.1591167002916336, 0.4756527245044708, 0.35889333486557007, 0.4615010619163513, 0.22839084267616272, 0.11079158633947372, 0.03781414031982422, 0.3370024263858795, 0.034732986241579056, 0.36521828174591064, 0.03456088900566101, 0.41996604204177856, 0.4835473299026489, 0.42948395013809204], dtype='float32').reshape([18]),
            paddle.to_tensor([0.480228453874588, 0.0013102563098073006, 0.3402842581272125, 0.004655875731259584, 0.4904012680053711, 0.07835210859775543, 0.26255396008491516, 0.3377399146556854, 0.4773224890232086, 0.07786843180656433, 0.34068378806114197, 0.43618011474609375, 0.49116674065589905, 0.18048545718193054, 0.02259119041264057, 0.31199148297309875, 0.0069176205433905125, 0.05974000692367554], dtype='float32').reshape([18]),
            paddle.to_tensor([0.09922107309103012, 0.11196625977754593, 0.188277468085289, 0.08222091943025589, 0.3792722225189209, 0.4246666729450226, 0.2309534251689911, 0.23718219995498657, 0.25737911462783813, 0.291425883769989, 0.18376821279525757, 0.4182184636592865, 0.22142961621284485, 0.2935652434825897, 0.06884506344795227, 0.04066956788301468, 0.08830634504556656, 0.3686921000480652], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_098cb80c9c6db2c74dedeb1e47fcbc9e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3243524134159088, 0.26615166664123535, 0.23778989911079407, 0.3435227870941162, 0.29103147983551025, 0.31463244557380676, 0.4923415780067444, 0.3013976216316223, 0.1946132928133011, 0.4470106363296509, 0.14664119482040405, 0.13920219242572784, 0.4179104268550873, 0.25602856278419495, 0.024159051477909088, 0.34243735671043396, 0.15302495658397675, 0.35700130462646484], dtype='float32').reshape([18]),
            paddle.to_tensor([0.0166337713599205, 0.4943583309650421, 0.35333746671676636, 0.20174576342105865, 0.04046069458127022, 0.1004297211766243, 0.4755280911922455, 0.3622737526893616, 0.3649977147579193, 0.39226850867271423, 0.1746644675731659, 0.01213018037378788, 0.4150075614452362, 0.38071194291114807, 0.11856339126825333, 0.07629403471946716, 0.1948995590209961, 0.35274210572242737], dtype='float32').reshape([18]),
            paddle.to_tensor([0.13679622113704681, 0.10951422899961472, 0.2185284048318863, 0.025897813960909843, 0.4354202449321747, 0.3483433723449707, 0.28574690222740173, 0.06741016358137131, 0.4610922038555145, 0.4264258146286011, 0.06299833208322525, 0.05670071765780449, 0.20116958022117615, 0.21710577607154846, 0.20934289693832397, 0.43582069873809814, 0.36310744285583496, 0.4655722677707672], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2559470534324646, 0.09133414179086685, 0.04389168694615364, 0.03302735090255737, 0.3507305979728699, 0.2823224365711212, 0.11098060756921768, 0.49911391735076904, 0.0043636211194098, 0.2690267860889435, 0.4247370958328247, 0.1457682102918625, 0.46109429001808167, 0.16328291594982147, 0.04618142545223236, 0.08300106972455978, 0.4508136808872223, 0.3659336268901825], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cc588958300f8282b425b8320edbdffe(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.21057066321372986, 0.2433863878250122, 0.13036106526851654, 0.12144804000854492, 0.4009605050086975, 0.4799807071685791, 0.40053755044937134, 0.07124575227499008, 0.03619373217225075, 0.19475767016410828, 0.4564979374408722, 0.15200074017047882, 0.03648436814546585, 0.36666035652160645, 0.055851370096206665, 0.2421245574951172, 0.4159965515136719, 0.27331024408340454, 0.34158098697662354, 0.45385491847991943, 0.22007709741592407, 0.2825565040111542, 0.41772735118865967, 0.4265284538269043, 0.16064102947711945, 0.026618272066116333, 0.1202368438243866, 0.25158700346946716, 0.46938109397888184, 0.3037334978580475], dtype='float32').reshape([30]),
            paddle.to_tensor([0.44872957468032837, 0.17148293554782867, 0.1187930554151535, 0.027692090719938278, 0.3660873770713806, 0.42148149013519287, 0.21288537979125977, 0.2764425277709961, 0.02838396094739437, 0.36801019310951233, 0.3547309339046478, 0.4831942021846771, 0.2890712022781372, 0.3241446912288666, 0.142477884888649, 0.41795459389686584, 0.39669930934906006, 0.18150874972343445, 0.1565304547548294, 0.3041492700576782, 0.1718354970216751, 0.48169922828674316, 0.07973290234804153, 0.23020398616790771, 0.4000699818134308, 0.4960479736328125, 0.10503651201725006, 0.011347755789756775, 0.23486071825027466, 0.16246576607227325], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2599535584449768, 0.4593017101287842, 0.08974309265613556, 0.4915425181388855, 0.21072319149971008, 0.26300376653671265, 0.4062471389770508, 0.19619427621364594, 0.045610833913087845, 0.37178197503089905, 0.06237674877047539, 0.25918012857437134, 0.37766826152801514, 0.1460508108139038, 0.34053322672843933, 0.29643014073371887, 0.4452681243419647, 0.2761925458908081, 0.15395532548427582, 0.24797479808330536, 0.36846253275871277, 0.4651491940021515, 0.08546119928359985, 0.24541813135147095, 0.15278863906860352, 0.004632383584976196, 0.1454785317182541, 0.06790827214717865, 0.3413172662258148, 0.3098628520965576], dtype='float32').reshape([30]),
            paddle.to_tensor([0.335696280002594, 0.002464492106810212, 0.4931460916996002, 0.37654536962509155, 0.040547508746385574, 0.06360187381505966, 0.17109380662441254, 0.08426543325185776, 0.1111038327217102, 0.08872997015714645, 0.23445041477680206, 0.2958124577999115, 0.1313214898109436, 0.12202475965023041, 0.3460158407688141, 0.15885354578495026, 0.03222007304430008, 0.015750745311379433, 0.17402225732803345, 0.10173337906599045, 0.09449948370456696, 0.007090503815561533, 0.18651211261749268, 0.3069361448287964, 0.13267523050308228, 0.023159915581345558, 0.318928599357605, 0.2640860378742218, 0.23343804478645325, 0.3649977147579193], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0b559b8922a11993fe83bc1ec8944464(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0ec78bf32924ecc0a97b9b65d030ffdb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1920, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1920], dtype='float32', min=0, max=0.5),
            paddle.uniform([1920], dtype='float32', min=0, max=0.5),
            paddle.uniform([1920], dtype='float32', min=0, max=0.5),
            paddle.uniform([1920], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c7a613b0ef315bbeeaa57b903fc66489(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.07918675243854523, 0.26822832226753235, 0.1821460723876953, 0.20497092604637146, 0.3862767517566681, 0.01838398166000843, 0.47853460907936096, 0.017079127952456474, 0.06579862534999847, 0.25064292550086975, 0.3910471200942993, 0.012858308851718903, 0.3170005977153778, 0.3986634612083435, 0.001878694398328662, 0.3076784908771515, 0.4030120074748993, 0.24743753671646118, 0.2606681287288666, 0.07054062932729721, 0.020081035792827606, 0.3324480652809143, 0.49542540311813354, 0.0638716071844101, 0.37010398507118225, 0.03971528261899948, 0.3311935067176819, 0.44036704301834106, 0.49868446588516235, 0.39327213168144226], dtype='float32').reshape([30]),
            paddle.to_tensor([0.11797516793012619, 0.19534558057785034, 0.2637092173099518, 0.1948598027229309, 0.313019722700119, 0.421663373708725, 0.32534798979759216, 0.1858769953250885, 0.1363065540790558, 0.3771340847015381, 0.12374808639287949, 0.37995821237564087, 0.3602536916732788, 0.3115241229534149, 0.02638327330350876, 0.13123439252376556, 0.19900627434253693, 0.43956682085990906, 0.2898918688297272, 0.13191679120063782, 0.43929174542427063, 0.1231955885887146, 0.26213154196739197, 0.058900024741888046, 0.2978214621543884, 0.4109954833984375, 0.40245845913887024, 0.33063244819641113, 0.3358319401741028, 0.19855839014053345], dtype='float32').reshape([30]),
            paddle.to_tensor([0.1459229290485382, 0.39574599266052246, 0.022799931466579437, 0.3071049451828003, 0.3084653317928314, 0.2684774100780487, 0.16177330911159515, 0.06390038877725601, 0.175841823220253, 0.4439903795719147, 0.2419375777244568, 0.48898962140083313, 0.34740349650382996, 0.4397154450416565, 0.20273272693157196, 0.17408952116966248, 0.17674998939037323, 0.27250534296035767, 0.3302621841430664, 0.0010146822314709425, 0.08057273179292679, 0.15898096561431885, 0.16862021386623383, 0.366995632648468, 0.4223939776420593, 0.29342910647392273, 0.217984139919281, 0.4482690989971161, 0.3597964942455292, 0.44610631465911865], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4961030185222626, 0.46724992990493774, 0.07792960852384567, 0.019012168049812317, 0.20444490015506744, 0.4989708662033081, 0.34537971019744873, 0.4557821452617645, 0.061595041304826736, 0.23084771633148193, 0.016674388200044632, 0.1005309671163559, 0.13913963735103607, 0.19326791167259216, 0.3861686885356903, 0.47632595896720886, 0.21173922717571259, 0.43712282180786133, 0.40770596265792847, 0.22999617457389832, 0.22233790159225464, 0.017999552190303802, 0.01705045998096466, 0.15516382455825806, 0.44339272379875183, 0.4438440203666687, 0.34118807315826416, 0.4051145613193512, 0.2325010448694229, 0.31498900055885315], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3976c4eb08613cad08878c78a534385f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ce5f981fde5893b148f98e0804aaebb2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 512], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4335973858833313, 0.47225743532180786, 0.3766078054904938, 0.35560256242752075, 0.327108234167099, 0.07845991849899292, 0.16625083982944489, 0.46676915884017944, 0.18919599056243896, 0.22868211567401886, 0.4040645956993103, 0.32323378324508667, 0.06497960537672043, 0.2102212905883789, 0.21930032968521118, 0.3285965621471405], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2958270013332367, 0.4535989463329315, 0.40894120931625366, 0.4695647954940796, 0.3995068669319153, 0.38665100932121277, 0.2466086745262146, 0.19521328806877136, 0.06471502780914307, 0.1599835455417633, 0.05150808393955231, 0.24492469429969788, 0.1862146407365799, 0.20341423153877258, 0.07402495294809341, 0.28364914655685425], dtype='float32').reshape([16]),
            paddle.to_tensor([0.11471664160490036, 0.48647552728652954, 0.11506948620080948, 0.356844961643219, 0.4398745596408844, 0.32777538895606995, 0.21313540637493134, 0.15974712371826172, 0.4542790949344635, 0.35681626200675964, 0.4684161841869354, 0.300975501537323, 0.3931039869785309, 0.2002171277999878, 0.11715448647737503, 0.1631634384393692], dtype='float32').reshape([16]),
            paddle.to_tensor([0.41407454013824463, 0.01627095602452755, 0.07477220892906189, 0.3772812783718109, 0.29385116696357727, 0.13547798991203308, 0.4057929217815399, 0.4113248288631439, 0.12881486117839813, 0.06119295209646225, 0.3065226674079895, 0.40820175409317017, 0.1807010918855667, 0.14393290877342224, 0.4302578866481781, 0.35974642634391785], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_668b0f256b233db4cebf65215893265e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.21815478801727295, 0.06691042333841324, 0.483479380607605, 0.10582511126995087, 0.042351964861154556, 0.32355931401252747, 0.47600680589675903, 0.3724324405193329, 0.16024819016456604, 0.052586037665605545, 0.019009141251444817, 0.3365251421928406, 0.2143227905035019, 0.4155302345752716, 0.03957178071141243, 0.0006289288285188377], dtype='float32').reshape([16]),
            paddle.to_tensor([0.17614585161209106, 0.21233853697776794, 0.2102799415588379, 0.4129231572151184, 0.42682182788848877, 0.20347438752651215, 0.3048216998577118, 0.4239596724510193, 0.08899471908807755, 0.4770474433898926, 0.38700345158576965, 0.14524301886558533, 0.2531451880931854, 0.38111287355422974, 0.1885988563299179, 0.14344169199466705], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4687284231185913, 0.2839183509349823, 0.3583435118198395, 0.10615107417106628, 0.3565186858177185, 0.48730549216270447, 0.2333315908908844, 0.38389310240745544, 0.11744797974824905, 0.006419054232537746, 0.3399747908115387, 0.35062387585639954, 0.360381156206131, 0.09878847002983093, 0.23171697556972504, 0.4207191467285156], dtype='float32').reshape([16]),
            paddle.to_tensor([0.1443311870098114, 0.09939878433942795, 0.3290644586086273, 0.15427154302597046, 0.2949887812137604, 0.15406759083271027, 0.3858441114425659, 0.4577469527721405, 0.4431202709674835, 0.30889636278152466, 0.27447205781936646, 0.18284741044044495, 0.31085461378097534, 0.08790500462055206, 0.058634884655475616, 0.004606098402291536], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f94351e8057ddda42da72242498d97e2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 8, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_be38378a807c7e7750fea703815d21fb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3247254192829132, 0.30188795924186707, 0.4988363981246948, 0.03727499395608902, 0.11740756034851074, 0.371415376663208, 0.27315253019332886, 0.31556132435798645, 0.4056639075279236, 0.2830328941345215, 0.47580474615097046, 0.1287907361984253, 0.19019095599651337, 0.17393611371517181, 0.3274492621421814, 0.3976007103919983, 0.2936316430568695, 0.24969562888145447, 0.47394701838493347, 0.38735511898994446, 0.1259085088968277, 0.08045624196529388, 0.1020800992846489, 0.40627720952033997, 0.13518904149532318, 0.3666890561580658, 0.2119814157485962, 0.14946642518043518, 0.04899347200989723, 0.21584349870681763], dtype='float32').reshape([30]),
            paddle.to_tensor([0.0181763656437397, 0.11845342069864273, 0.4639076292514801, 0.2207445353269577, 0.13873638212680817, 0.21583642065525055, 0.17500054836273193, 0.39962154626846313, 0.23408760130405426, 0.027591833844780922, 0.0828780084848404, 0.14652109146118164, 0.39326366782188416, 0.3147282004356384, 0.4616442322731018, 0.28886452317237854, 0.0003794555668719113, 0.23173025250434875, 0.018561456352472305, 0.3763425052165985, 0.05656234174966812, 0.11849343776702881, 0.38450226187705994, 0.15457554161548615, 0.19432853162288666, 0.2913629710674286, 0.22849203646183014, 0.03684913367033005, 0.08432599157094955, 0.22667379677295685], dtype='float32').reshape([30]),
            paddle.to_tensor([0.16711680591106415, 0.09641019999980927, 0.0890788808465004, 0.28550252318382263, 0.27474138140678406, 0.05131318047642708, 0.4693432152271271, 0.1664205640554428, 0.010821731761097908, 0.30284619331359863, 0.29703813791275024, 0.3896382451057434, 0.30339863896369934, 0.4224218726158142, 0.4994892179965973, 0.08716219663619995, 0.3711755871772766, 0.428718239068985, 0.38304629921913147, 0.332973450422287, 0.40395981073379517, 0.034404415637254715, 0.25685545802116394, 0.40204116702079773, 0.12324642390012741, 0.35539761185646057, 0.006545349024236202, 0.2638784945011139, 0.36433181166648865, 0.27358928322792053], dtype='float32').reshape([30]),
            paddle.to_tensor([0.24597902595996857, 0.4725562632083893, 0.1449650228023529, 0.4989321827888489, 0.06073959916830063, 0.4970291256904602, 0.20961782336235046, 0.0406181663274765, 0.08260422945022583, 0.4696732759475708, 0.11021271347999573, 0.48297640681266785, 0.4613170921802521, 0.07010713219642639, 0.27924588322639465, 0.46894052624702454, 0.00443302234634757, 0.13793706893920898, 0.2410225123167038, 0.37688180804252625, 0.3024172782897949, 0.11537888646125793, 0.2997547388076782, 0.28840163350105286, 0.46058571338653564, 0.12997184693813324, 0.034383755177259445, 0.49542003870010376, 0.1988268494606018, 0.46832016110420227], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8e57906535e6ba5cf0fce2a867224374(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 32, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2683204412460327, 0.49883076548576355, 0.14666031301021576, 0.3991878926753998, 0.3080592453479767, 0.08943508565425873, 0.41569533944129944, 0.3200310170650482, 0.3860913813114166, 0.13074356317520142, 0.14291641116142273, 0.00792367197573185, 0.13857147097587585, 0.29830023646354675, 0.12571416795253754, 0.42666515707969666], dtype='float32').reshape([16]),
            paddle.to_tensor([0.46224021911621094, 0.26671791076660156, 0.15898141264915466, 0.39131981134414673, 0.04937935993075371, 0.27218034863471985, 0.44176748394966125, 0.42728638648986816, 0.486431747674942, 0.4892595112323761, 0.3512050211429596, 0.1120297759771347, 0.3513445258140564, 0.23388099670410156, 0.12248098850250244, 0.22232089936733246], dtype='float32').reshape([16]),
            paddle.to_tensor([0.0722537487745285, 0.3175245225429535, 0.158298522233963, 0.1165199875831604, 0.25101885199546814, 0.43688419461250305, 0.3952476978302002, 0.21503469347953796, 0.18668553233146667, 0.3833746910095215, 0.4443782567977905, 0.006405755411833525, 0.37783169746398926, 0.07082387804985046, 0.3985891342163086, 0.23680275678634644], dtype='float32').reshape([16]),
            paddle.to_tensor([0.31168538331985474, 0.024267498403787613, 0.323825865983963, 0.04693753272294998, 0.2051754742860794, 0.3290405571460724, 0.17089946568012238, 0.3460395932197571, 0.14575313031673431, 0.38895535469055176, 0.47682470083236694, 0.15708312392234802, 0.0017294655553996563, 0.4126374125480652, 0.2295849621295929, 0.0003751182812266052], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c30b728fe7604ab84e5eb38cb09aaf91(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 26, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.010450023226439953, 0.12970367074012756, 0.016734831035137177, 0.2738230228424072, 0.05626016855239868, 0.22316424548625946, 0.4999924302101135, 0.19109031558036804, 0.2589021623134613, 0.059512533247470856, 0.04365844652056694, 0.14172638952732086, 0.41073331236839294, 0.35516780614852905, 0.08623959124088287, 0.2593693435192108, 0.04805468022823334, 0.09296432137489319, 0.4621601402759552, 0.41147878766059875, 0.008493459783494473, 0.3926345407962799, 0.4234398901462555, 0.2533910274505615, 0.37687328457832336, 0.4636777639389038], dtype='float32').reshape([26]),
            paddle.to_tensor([0.240709125995636, 0.3215588331222534, 0.29508233070373535, 0.30706456303596497, 0.41631683707237244, 0.03314875066280365, 0.2224365919828415, 0.03238270804286003, 0.4322830140590668, 0.394826740026474, 0.112653948366642, 0.37799638509750366, 0.036569442600011826, 0.25898849964141846, 0.23476237058639526, 0.23460036516189575, 0.1500161588191986, 0.16451555490493774, 0.16732949018478394, 0.24018432199954987, 0.29721325635910034, 0.12217734754085541, 0.11082456260919571, 0.3769759237766266, 0.2247518002986908, 0.2605868875980377], dtype='float32').reshape([26]),
            paddle.to_tensor([0.4214794337749481, 0.3197159469127655, 0.36392661929130554, 0.26022446155548096, 0.23825478553771973, 0.4674978256225586, 0.19238850474357605, 0.47519418597221375, 0.12945446372032166, 0.27650272846221924, 0.35399022698402405, 0.060445576906204224, 0.3061189651489258, 0.07425154745578766, 0.17557181417942047, 0.4946652948856354, 0.4589589834213257, 0.0968768298625946, 0.0873560756444931, 0.02988520823419094, 0.40324366092681885, 0.09486963599920273, 0.09708297252655029, 0.3158412575721741, 0.4115275740623474, 0.2709881365299225], dtype='float32').reshape([26]),
            paddle.to_tensor([0.2889493405818939, 0.364406943321228, 0.006330138072371483, 0.4927132725715637, 0.3247333765029907, 0.28299593925476074, 0.3551029562950134, 0.16186414659023285, 0.32085278630256653, 0.35828250646591187, 0.40592747926712036, 0.23904664814472198, 0.26141905784606934, 0.3120867908000946, 0.47206756472587585, 0.19915203750133514, 0.3946550488471985, 0.013684626668691635, 0.1428288221359253, 0.4472651481628418, 0.0850868672132492, 0.32939231395721436, 0.1491226851940155, 0.2522057592868805, 0.3562416136264801, 0.19745860993862152], dtype='float32').reshape([26]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b45c4e1676ee16c9c46a44d6d9327e9d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 32, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.22342564165592194, 0.05266697704792023, 0.278504878282547, 0.39925965666770935, 0.39775991439819336, 0.4968000054359436, 0.1982782483100891, 0.4787493348121643, 0.29986390471458435, 0.09128008037805557, 0.3281673789024353, 0.13294300436973572, 0.2968483865261078, 0.44678428769111633, 0.4486144781112671, 0.3237954378128052], dtype='float32').reshape([16]),
            paddle.to_tensor([0.38421595096588135, 0.3399111330509186, 0.13780632615089417, 0.2758950889110565, 0.32238373160362244, 0.16202545166015625, 0.4509813189506531, 0.37224751710891724, 0.13215316832065582, 0.4075804352760315, 0.2433677762746811, 0.14233995974063873, 0.41063788533210754, 0.2090403288602829, 0.39706695079803467, 0.02703184261918068], dtype='float32').reshape([16]),
            paddle.to_tensor([0.46530136466026306, 0.3883226811885834, 0.263029009103775, 0.33940836787223816, 0.033643390983343124, 0.44849443435668945, 0.4639875292778015, 0.12753061950206757, 0.307628333568573, 0.35012659430503845, 0.37548789381980896, 0.4146667718887329, 0.07124080508947372, 0.3904839754104614, 0.06549330800771713, 0.0005751678836531937], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3487672805786133, 0.23647655546665192, 0.40277108550071716, 0.16488319635391235, 0.1780153214931488, 0.10892912745475769, 0.3278331756591797, 0.4331061542034149, 0.27328819036483765, 0.05930840224027634, 0.4069588780403137, 0.1778518408536911, 0.1546967774629593, 0.45919978618621826, 0.32979169487953186, 0.49894365668296814], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_92f31a4adb928839bfa85e646d6a25bb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_b4f4da3a8d44d1b5b3a2903a097df186
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_40737990982decc71dbcb25d656827da(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 100, 168], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_696ac6583a0c789762ac239e1e0c7431(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 76, 76], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_36847493cede48c7c7db8306ac6084ff(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 512], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.36172807216644287, 0.10578285902738571, 0.45792272686958313, 0.25832581520080566, 0.04628659039735794, 0.1968088001012802, 0.029220907017588615, 0.41394108533859253, 0.44255250692367554, 0.33370089530944824, 0.3688839077949524, 0.24207143485546112, 0.3830290138721466, 0.2889902591705322, 0.3484288156032562, 0.029585590586066246], dtype='float32').reshape([16]),
            paddle.to_tensor([0.17074963450431824, 0.47501280903816223, 0.378668874502182, 0.26082441210746765, 0.4780508875846863, 0.4945383071899414, 0.4293938875198364, 0.38745445013046265, 0.3325612545013428, 0.3590261936187744, 0.3423474133014679, 0.18743593990802765, 0.1974116414785385, 0.3427179753780365, 0.4757896959781647, 0.28657180070877075], dtype='float32').reshape([16]),
            paddle.to_tensor([0.21729545295238495, 0.29799729585647583, 0.3068884611129761, 0.03971324488520622, 0.1483542025089264, 0.3578229546546936, 0.4777364432811737, 0.42482736706733704, 0.4005645215511322, 0.08430808037519455, 0.04740459844470024, 0.42650336027145386, 0.41843724250793457, 0.1298602968454361, 0.25859904289245605, 0.3245108127593994], dtype='float32').reshape([16]),
            paddle.to_tensor([0.26218488812446594, 0.2647296190261841, 0.27857455611228943, 0.23042231798171997, 0.44430798292160034, 0.4875198006629944, 0.40134209394454956, 0.2921757400035858, 0.14370223879814148, 0.4939243495464325, 0.1896921843290329, 0.4852294623851776, 0.4706990420818329, 0.3220977485179901, 0.08256049454212189, 0.18630650639533997], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ab30194b66b2a7a6148da4d0f19fa0d5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 320, 320], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f52eb43afd4d6ea4fdb013747d13db3a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 164, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([164], dtype='float32', min=0, max=0.5),
            paddle.uniform([164], dtype='float32', min=0, max=0.5),
            paddle.uniform([164], dtype='float32', min=0, max=0.5),
            paddle.uniform([164], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d03e09abd0c9c3fcf4a66ab90e09616e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_995b012684593e484bd318f7409d2597(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 100, 168], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fc8c89bf721f7b9d8aacd2d73580d631(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.48907244205474854, 0.3885045349597931, 0.4757488965988159, 0.20530590415000916, 0.4849684238433838, 0.4534493386745453, 0.43793627619743347, 0.08259618282318115, 0.20851507782936096, 0.2866930365562439, 0.43625935912132263, 0.3932431638240814, 0.34967637062072754, 0.15034575760364532, 0.2680867612361908, 0.24187688529491425, 0.42963355779647827, 0.23968209326267242, 0.33504924178123474, 0.17679351568222046], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4499965310096741, 0.4361189901828766, 0.377338171005249, 0.49990344047546387, 0.3062238097190857, 0.10970775038003922, 0.17206935584545135, 0.08592745661735535, 0.37643393874168396, 0.0037043741904199123, 0.15860891342163086, 0.1630590856075287, 0.3641497492790222, 0.31882113218307495, 0.17431406676769257, 0.07767046988010406, 0.20561163127422333, 0.3072842061519623, 0.48238152265548706, 0.24216988682746887], dtype='float32').reshape([20]),
            paddle.to_tensor([0.24825510382652283, 0.4011625051498413, 0.026280002668499947, 0.304314523935318, 0.44292378425598145, 0.04518086463212967, 0.3837103843688965, 0.21571889519691467, 0.41665416955947876, 0.11645139753818512, 0.10911113768815994, 0.3623950779438019, 0.05822960287332535, 0.4462645947933197, 0.12429074943065643, 0.48040613532066345, 0.386276513338089, 0.06595852971076965, 0.47215133905410767, 0.15468719601631165], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3656531274318695, 0.014883140102028847, 0.3330586850643158, 0.49121537804603577, 0.46189165115356445, 0.2771533131599426, 0.3321570158004761, 0.2748278081417084, 0.1583009511232376, 0.49181362986564636, 0.08131009340286255, 0.4683682322502136, 0.17083604633808136, 0.20946484804153442, 0.36861133575439453, 0.22392886877059937, 0.0013471420388668776, 0.21178528666496277, 0.1032179594039917, 0.288583904504776], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0725043ab09ccf9e1775d3b0afe76c08(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1c89209b4004f62773bea4ea22bea530(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 240, 240], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b248aa287b37492cecf69688a4efa19e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5b7edc0ccd81b717909287901b4505d0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_f5f2213be92f20fda5378beeed72688f
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d785fee8ea8409b14dae99604fef72ae(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0ff408318a8f614c4a83c5b1470205da(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 200, 336], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0ad1079508ab2f9f95bfc71b31dc2a52(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 164, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([164], dtype='float32', min=0, max=0.5),
            paddle.uniform([164], dtype='float32', min=0, max=0.5),
            paddle.uniform([164], dtype='float32', min=0, max=0.5),
            paddle.uniform([164], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6c2cdc23030d9a6b5a85bb29b958fcef(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.to_tensor([[[[79089.1953125]], [[94146.203125]], [[99092.7578125]], [[91481.46875]], [[78636.359375]], [[104488.5390625]], [[79464.734375]], [[93596.5859375]], [[94778.7265625]], [[90327.6484375]], [[87169.828125]], [[86130.890625]], [[100516.4140625]], [[93585.8046875]], [[88864.484375]], [[90534.5078125]], [[72220.8359375]], [[77951.3515625]], [[92485.1953125]], [[103248.9140625]], [[80574.484375]], [[99506.0078125]], [[103246.59375]], [[85274.5703125]], [[87156.5859375]]]], dtype='float32').reshape([1, 25, 1, 1]),
            paddle.to_tensor([0.16263407468795776, 0.42007774114608765, 0.27604496479034424, 0.14375706017017365, 0.36266833543777466, 0.14465031027793884, 0.4900346100330353, 0.3091086745262146, 0.44068992137908936, 0.02605544403195381, 0.29400286078453064, 0.47233298420906067, 0.37702783942222595, 0.3491102457046509, 0.4469517171382904, 0.05360887944698334, 0.20731893181800842, 0.12756657600402832, 0.1251523196697235, 0.2971535921096802, 0.05447068810462952, 0.25232434272766113, 0.30768802762031555, 0.37624219059944153, 0.2691813111305237], dtype='float32').reshape([25]),
            paddle.to_tensor([0.37797385454177856, 0.49487146735191345, 0.13919971883296967, 0.3186049461364746, 0.015673702582716942, 0.4030400514602661, 0.019969888031482697, 0.3030467629432678, 0.22758688032627106, 0.20944388210773468, 0.033636435866355896, 0.3346860110759735, 0.07158069312572479, 0.48215004801750183, 0.44782575964927673, 0.3151743412017822, 0.45808252692222595, 0.3788016438484192, 0.2714121639728546, 0.1615609973669052, 0.16830506920814514, 0.3250376880168915, 0.19569048285484314, 0.467683345079422, 0.05401984229683876], dtype='float32').reshape([25]),
            paddle.to_tensor([0.2597052752971649, 0.20281727612018585, 0.1910466104745865, 0.3901054263114929, 0.017080023884773254, 0.4533204436302185, 0.028316162526607513, 0.03519685938954353, 0.013642221689224243, 0.0786416232585907, 0.3720370829105377, 0.4411281645298004, 0.07662726193666458, 0.06744785606861115, 0.46307533979415894, 0.2097788006067276, 0.4967244565486908, 0.14058099687099457, 0.3962017297744751, 0.43338117003440857, 0.26561203598976135, 0.0483962818980217, 0.40544143319129944, 0.45154231786727905, 0.06071687117218971], dtype='float32').reshape([25]),
            paddle.to_tensor([0.27061590552330017, 0.21318049728870392, 0.05172707885503769, 0.17964647710323334, 0.21638409793376923, 0.3587806522846222, 0.24957036972045898, 0.2191423773765564, 0.4372701942920685, 0.3806587755680084, 0.4332602918148041, 0.42962607741355896, 0.24837353825569153, 0.32859665155410767, 0.43223249912261963, 0.24000747501850128, 0.050719890743494034, 0.1639413982629776, 0.0692625418305397, 0.07414007931947708, 0.053958047181367874, 0.4641614556312561, 0.18217943608760834, 0.39256584644317627, 0.3251664638519287], dtype='float32').reshape([25]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2c45f2763ed182b45258c7ddcb12f9c5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6d9c2229e7e9d41ff87450b7d2f56244(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 16, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a7e0d149c519d3a422d50aa390ec5832(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bd956cc3f36e1f7ce97696d4da969b0e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 56, 100], dtype='float16', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_52d9f9cc37fa88acf973cd7cc91d9439(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1248, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1248], dtype='float32', min=0, max=0.5),
            paddle.uniform([1248], dtype='float32', min=0, max=0.5),
            paddle.uniform([1248], dtype='float32', min=0, max=0.5),
            paddle.uniform([1248], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_62bd5c2de69fc8ee2244f2f769a50bee(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 240, 240], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_58764fb3b6ca3aa152d527df6fb29f4f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([8, 256, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0c222ad3df2714168bbbea5f1ccf5f0b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_b4f4da3a8d44d1b5b3a2903a097df186
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6e5f8666aad414a8f1f1e8ac4b028ca8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 624, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([624], dtype='float32', min=0, max=0.5),
            paddle.uniform([624], dtype='float32', min=0, max=0.5),
            paddle.uniform([624], dtype='float32', min=0, max=0.5),
            paddle.uniform([624], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ccc19c878f54c46dd79cf292711fed17(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 728, 38, 38], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8a0c5342c6a9e56fdfcfcf259d92bcf1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fe497642d62913c54b5e621201897ca9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 8, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_06f5fa28038d4fd23982c16fe85519d2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_62b1c7b03d597e3952ac5a7571654bb1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 208, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a77261b048b93deda3ac773c9310b172(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 4, 240, 240], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.1869821548461914, 0.4468449056148529, 0.27399224042892456, 0.2934857904911041], dtype='float32').reshape([4]),
            paddle.to_tensor([0.17758730053901672, 0.2323203682899475, 0.3742569386959076, 0.24878640472888947], dtype='float32').reshape([4]),
            paddle.to_tensor([0.28727343678474426, 0.0243198461830616, 0.1950269341468811, 0.29284828901290894], dtype='float32').reshape([4]),
            paddle.to_tensor([0.20588593184947968, 0.05431743711233139, 0.3560389280319214, 0.07013023644685745], dtype='float32').reshape([4]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_681e2eedd991f9b469fd3b05e0832ae4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 570, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([570], dtype='float32', min=0, max=0.5),
            paddle.uniform([570], dtype='float32', min=0, max=0.5),
            paddle.uniform([570], dtype='float32', min=0, max=0.5),
            paddle.uniform([570], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_641fa009ab43def83afabee0e551174b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_811436490465e8a0d5d495304ef37393(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 640, 640], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.09446853399276733, 0.05570997670292854, 0.46258556842803955, 0.49542519450187683, 0.4368799924850464, 0.07684466987848282, 0.26280108094215393, 0.3384602963924408, 0.3897107243537903, 0.10497565567493439, 0.27433332800865173, 0.2304472029209137], dtype='float32').reshape([12]),
            paddle.to_tensor([0.44057705998420715, 0.43984559178352356, 0.3922331929206848, 0.017491135746240616, 0.24887976050376892, 0.12717965245246887, 0.23303619027137756, 0.09831079095602036, 0.428014874458313, 0.009928863495588303, 0.41834163665771484, 0.41995736956596375], dtype='float32').reshape([12]),
            paddle.to_tensor([0.13465829193592072, 0.460126131772995, 0.08848533779382706, 0.39933133125305176, 0.13097557425498962, 0.04170502349734306, 0.06782533973455429, 0.1201956644654274, 0.11455992609262466, 0.01686019077897072, 0.3027969300746918, 0.3059382438659668], dtype='float32').reshape([12]),
            paddle.to_tensor([0.17788758873939514, 0.49888965487480164, 0.3979722559452057, 0.08577946573495865, 0.3620993196964264, 0.4587302505970001, 0.10063417255878448, 0.1550135463476181, 0.3244674503803253, 0.39513784646987915, 0.4447031319141388, 0.0873313620686531], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dafc78883b9322df9a066661471942db(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.14795275032520294, 0.18939326703548431, 0.49377915263175964, 0.1263446807861328, 0.10533168166875839, 0.3965648412704468, 0.3136722445487976, 0.29381635785102844, 0.2020782083272934, 0.04881644621491432, 0.048554424196481705, 0.2444552481174469, 0.07908342033624649, 0.25978848338127136, 0.20589585602283478, 0.480004221200943, 0.06291605532169342, 0.46516817808151245, 0.09046216309070587, 0.46912097930908203, 0.3082624077796936, 0.0723714828491211, 0.24859519302845, 0.15354277193546295, 0.4404846131801605, 0.45347756147384644, 0.482880175113678, 0.13353349268436432, 0.2769988477230072, 0.24385233223438263], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4543214440345764, 0.10639803856611252, 0.2825651466846466, 0.3213481903076172, 0.12404174357652664, 0.33881106972694397, 0.25100237131118774, 0.26551613211631775, 0.4875287413597107, 0.02135375700891018, 0.002985692583024502, 0.0110207200050354, 0.19878876209259033, 0.33444762229919434, 0.05811578035354614, 0.36119356751441956, 0.4086143374443054, 0.010926242917776108, 0.06985680013895035, 0.19431714713573456, 0.3764815926551819, 0.3549829423427582, 0.18223904073238373, 0.45759114623069763, 0.49730828404426575, 0.02048908919095993, 0.26094311475753784, 0.2720523476600647, 0.03710053488612175, 0.06266378611326218], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4546225368976593, 0.33141475915908813, 0.4089565575122833, 0.012703053653240204, 0.11503978073596954, 0.11185554414987564, 0.3473447263240814, 0.20580242574214935, 0.08764425665140152, 0.20518270134925842, 0.46478164196014404, 0.29253360629081726, 0.050120577216148376, 0.14467869699001312, 0.24764621257781982, 0.4892847537994385, 0.3038536012172699, 0.04980570077896118, 0.43823009729385376, 0.45159071683883667, 0.46879416704177856, 0.27497199177742004, 0.30285853147506714, 0.30591443181037903, 0.37919485569000244, 0.1263917088508606, 0.2193233221769333, 0.2933942377567291, 0.24400848150253296, 0.43571212887763977], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2877887487411499, 0.026931840926408768, 0.20436608791351318, 0.10552468150854111, 0.12621989846229553, 0.03568967431783676, 0.3955276906490326, 0.22445836663246155, 0.11255820095539093, 0.4789183437824249, 0.13847310841083527, 0.29296278953552246, 0.4955112636089325, 0.4530743360519409, 0.19596222043037415, 0.37217962741851807, 0.36225059628486633, 0.06660964339971542, 0.28232768177986145, 0.45885589718818665, 0.16253304481506348, 0.47859901189804077, 0.4890734851360321, 0.4030141532421112, 0.3451509177684784, 0.471353143453598, 0.10135390609502792, 0.03469838574528694, 0.020711438730359077, 0.0525345541536808], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7e8798c8b53c2455d3b612cfc349aebe(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_51833a9a06b622832271bede3e3d925c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_adbf8d824836ca4fd41ee4e7d0a04ed3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2738224267959595, 0.26334571838378906, 0.26621395349502563, 0.32321038842201233, 0.35471311211586, 0.2966426908969879, 0.45474299788475037, 0.15806692838668823, 0.07425478845834732, 0.22744840383529663, 0.07417323440313339, 0.3724726140499115, 0.13976749777793884, 0.21789908409118652, 0.32613688707351685, 0.46952444314956665, 0.3175663650035858, 0.4394893944263458, 0.3489530682563782, 0.4927729368209839, 0.20505787432193756, 0.3950720727443695, 0.019280575215816498, 0.3033655285835266], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4918537139892578, 0.4754970371723175, 0.39051127433776855, 0.2737066149711609, 0.07085532695055008, 0.17372259497642517, 0.1789943128824234, 0.05158562958240509, 0.09680990874767303, 0.4434121549129486, 0.3405044376850128, 0.12263619899749756, 0.3947315812110901, 0.05057192221283913, 0.07654350250959396, 0.22064068913459778, 0.14126482605934143, 0.12856779992580414, 0.046780675649642944, 0.030689971521496773, 0.48698192834854126, 0.20666681230068207, 0.39185646176338196, 0.3213731050491333], dtype='float32').reshape([24]),
            paddle.to_tensor([0.0792330950498581, 0.10098323225975037, 0.33785519003868103, 0.4658651351928711, 0.056544315069913864, 0.2944098711013794, 0.3794775903224945, 0.04118335247039795, 0.057583216577768326, 0.15008510649204254, 0.3846660554409027, 0.040221646428108215, 0.08368238061666489, 0.23022516071796417, 0.45646926760673523, 0.07211871445178986, 0.08561109006404877, 0.12794066965579987, 0.42693766951560974, 0.16195528209209442, 0.3093941807746887, 0.340831458568573, 0.4699937105178833, 0.09724283218383789], dtype='float32').reshape([24]),
            paddle.to_tensor([0.34233418107032776, 0.3223665952682495, 0.3781014680862427, 0.2756100594997406, 0.01621740683913231, 0.43267324566841125, 0.49038365483283997, 0.0791018158197403, 0.4992257058620453, 0.17568856477737427, 0.47643789649009705, 0.2962152063846588, 0.07973773032426834, 0.09329813718795776, 0.38211312890052795, 0.4334581792354584, 0.2362920194864273, 0.3522438704967499, 0.42770814895629883, 0.42031753063201904, 0.07743823528289795, 0.44236940145492554, 0.3924771249294281, 0.2690148949623108], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_226d966230e3a97386bdbd5eff631d5d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.35119566321372986, 0.04077789932489395, 0.46548810601234436, 0.23948338627815247, 0.31676724553108215, 0.01620413362979889, 0.3484744727611542, 0.43974432349205017, 0.10963902622461319, 0.42839744687080383, 0.11622386425733566, 0.11994631588459015, 0.005570366978645325, 0.08340761065483093, 0.2417035549879074, 0.2904212176799774, 0.10180321335792542, 0.38881754875183105, 0.3860135078430176, 0.38306570053100586], dtype='float32').reshape([20]),
            paddle.to_tensor([0.33569297194480896, 0.40943267941474915, 0.2356373518705368, 0.4892774820327759, 0.44525977969169617, 0.09252933412790298, 0.18896400928497314, 0.3728419542312622, 0.24098242819309235, 0.3990333676338196, 0.22264477610588074, 0.27913928031921387, 0.4445667266845703, 0.30672961473464966, 0.4017661511898041, 0.40025341510772705, 0.4118990898132324, 0.3836119472980499, 0.4316333532333374, 0.2449481338262558], dtype='float32').reshape([20]),
            paddle.to_tensor([0.40315547585487366, 0.18306700885295868, 0.4165183901786804, 0.12469534575939178, 0.18721745908260345, 0.37863287329673767, 0.31454983353614807, 0.3563874065876007, 0.32937535643577576, 0.13830210268497467, 0.24914811551570892, 0.47240665555000305, 0.28195300698280334, 0.36164671182632446, 0.15200990438461304, 0.48898568749427795, 0.40862134099006653, 0.10869961231946945, 0.21346043050289154, 0.3568553924560547], dtype='float32').reshape([20]),
            paddle.to_tensor([0.05444267764687538, 0.20366103947162628, 0.3730103671550751, 0.07603328675031662, 0.30153506994247437, 0.33343103528022766, 0.3627329170703888, 0.23786410689353943, 0.37321221828460693, 0.23843885958194733, 0.031040344387292862, 0.4697454273700714, 0.32292765378952026, 0.2546060085296631, 0.10084207355976105, 0.3897371292114258, 0.22001945972442627, 0.09261675924062729, 0.41812554001808167, 0.494372695684433], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a388e16955560c0ec2e8780947ce2ee5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_b4f4da3a8d44d1b5b3a2903a097df186
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_582b87eb12e5187687dce92893184a24(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 4, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_141c9701537c3bc3af4d8f50784e1d34(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 480, 480], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.011868823319673538, 0.4207583963871002, 0.3736187815666199, 0.1001904234290123, 0.35648590326309204, 0.488384485244751, 0.08088646829128265, 0.46814706921577454], dtype='float32').reshape([8]),
            paddle.to_tensor([0.28044331073760986, 0.07527591288089752, 0.13294164836406708, 0.40404579043388367, 0.486605703830719, 0.28817129135131836, 0.3246995210647583, 0.12665344774723053], dtype='float32').reshape([8]),
            paddle.to_tensor([0.21193036437034607, 0.4873203635215759, 0.3904854357242584, 0.17163169384002686, 0.03031357005238533, 0.08534333854913712, 0.46377313137054443, 0.3529086112976074], dtype='float32').reshape([8]),
            paddle.to_tensor([0.12693791091442108, 0.0757012739777565, 0.4163839817047119, 0.14316251873970032, 0.3461463749408722, 0.11222302913665771, 0.22803014516830444, 0.027232062071561813], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bb093e468721862fffa002cf25c8fadc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 8, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d0def22617b26610c9035e6f67a98867(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2817731201648712, 0.3869737684726715, 0.07608113437891006, 0.17046746611595154, 0.47029969096183777, 0.15063543617725372, 0.4091479182243347, 0.18028828501701355, 0.38306719064712524, 0.01070595346391201, 0.28522583842277527, 0.20477765798568726, 0.18491332232952118, 0.38163599371910095, 0.19638237357139587, 0.22610929608345032, 0.03319729492068291, 0.15094581246376038, 0.419784814119339, 0.49651697278022766, 0.009581281803548336, 0.2583833634853363, 0.0753868818283081, 0.27847394347190857], dtype='float32').reshape([24]),
            paddle.to_tensor([0.010645134374499321, 0.07057363539934158, 0.2768573462963104, 0.07940475642681122, 0.016089782118797302, 0.06425394862890244, 0.3473796248435974, 0.3860818147659302, 0.13844726979732513, 0.33870241045951843, 0.28392529487609863, 0.2970767021179199, 0.4231174886226654, 0.14622697234153748, 0.15189425647258759, 0.2911621034145355, 0.33414819836616516, 0.15974226593971252, 0.1325576901435852, 0.19521024823188782, 0.16545668244361877, 0.3136310577392578, 0.001968276919797063, 0.1531287133693695], dtype='float32').reshape([24]),
            paddle.to_tensor([0.36426419019699097, 0.290890634059906, 0.09634842723608017, 0.2336747944355011, 0.06274879723787308, 0.32325971126556396, 0.025382302701473236, 0.35444507002830505, 0.47547557950019836, 0.17011646926403046, 0.14956219494342804, 0.3662857711315155, 0.2820862829685211, 0.017316825687885284, 0.3954419493675232, 0.44546157121658325, 0.3802172839641571, 0.29425981640815735, 0.011233380995690823, 0.35105830430984497, 0.13730232417583466, 0.4000726640224457, 0.43161913752555847, 0.3054194748401642], dtype='float32').reshape([24]),
            paddle.to_tensor([0.04331204295158386, 0.46931999921798706, 0.06874275207519531, 0.4977656602859497, 0.38769933581352234, 0.03405192494392395, 0.18006232380867004, 0.43865397572517395, 0.44610077142715454, 0.34839561581611633, 0.2587112784385681, 0.45079395174980164, 0.22640137374401093, 0.48563504219055176, 0.03379872813820839, 0.05471210181713104, 0.2330382764339447, 0.05268549919128418, 0.4588310420513153, 0.06249374896287918, 0.24502865970134735, 0.4501938223838806, 0.025321809574961662, 0.23490925133228302], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4a572b30efa3d7cb953c89f8dc61a5fd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3912654221057892, 0.18014487624168396, 0.3560222089290619, 0.44917476177215576, 0.25290074944496155, 0.3947133719921112, 0.3736455738544464, 0.3637371361255646, 0.15839225053787231, 0.12284418940544128, 0.37852007150650024, 0.19497644901275635, 0.12356116622686386, 0.49706706404685974, 0.2219184935092926, 0.23960931599140167, 0.11404892057180405, 0.14041562378406525], dtype='float32').reshape([18]),
            paddle.to_tensor([0.04714786261320114, 0.08298062533140182, 0.23665286600589752, 0.4444437026977539, 0.3389490842819214, 0.04422872141003609, 0.26456841826438904, 0.34667015075683594, 0.01823212392628193, 0.0809728279709816, 0.4819241166114807, 0.24526487290859222, 0.2303476333618164, 0.35437634587287903, 0.17355284094810486, 0.458315908908844, 0.2601082921028137, 0.16047725081443787], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4395618736743927, 0.09506870061159134, 0.4142197072505951, 0.03805672749876976, 0.43230462074279785, 0.34754520654678345, 0.3821098208427429, 0.25455594062805176, 0.45701414346694946, 0.10565920174121857, 0.43642356991767883, 0.20277737081050873, 0.29959598183631897, 0.3124491274356842, 0.4963957369327545, 0.25835296511650085, 0.10258948802947998, 0.20701630413532257], dtype='float32').reshape([18]),
            paddle.to_tensor([0.09080813080072403, 0.1993248462677002, 0.08631729334592819, 0.3154921233654022, 0.30987831950187683, 0.0012649851851165295, 0.008680112659931183, 0.3651377558708191, 0.2240723967552185, 0.30086129903793335, 0.4926973283290863, 0.331791490316391, 0.37140965461730957, 0.4579906761646271, 0.3040880560874939, 0.2933203876018524, 0.1103578507900238, 0.49192649126052856], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cfd4315567621efdcfd80b6d1c2d4f41(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.08547168225049973, 0.2572774291038513, 0.31009867787361145, 0.11188175529241562, 0.1967318058013916, 0.1728755682706833, 0.11298161745071411, 0.023370781913399696, 0.1790613830089569, 0.47644343972206116, 0.35790854692459106, 0.4260265529155731, 0.23932082951068878, 0.3819134533405304, 0.451264351606369, 0.28069743514060974, 0.18146447837352753, 0.09074681252241135, 0.4791644811630249, 0.2710596024990082, 0.32004427909851074, 0.11136254668235779, 0.4578844904899597, 0.48402246832847595, 0.35078898072242737, 0.34979134798049927, 0.3430575132369995, 0.29229190945625305], dtype='float32').reshape([28]),
            paddle.to_tensor([0.4916801154613495, 0.28866448998451233, 0.3784189224243164, 0.36027106642723083, 0.1737278550863266, 0.2762998640537262, 0.06109994277358055, 0.46514734625816345, 0.0168972946703434, 0.3456363379955292, 0.45887333154678345, 0.21224619448184967, 0.21586358547210693, 0.14571377635002136, 0.30883389711380005, 0.23139631748199463, 0.24457938969135284, 0.14926724135875702, 0.11265049129724503, 0.1695711761713028, 0.20765358209609985, 0.048584841191768646, 0.3275417387485504, 0.20341847836971283, 0.49229973554611206, 0.17370590567588806, 0.08675505965948105, 0.32386162877082825], dtype='float32').reshape([28]),
            paddle.to_tensor([0.11363277584314346, 0.2960645258426666, 0.4626273512840271, 0.0010548543650656939, 0.1732083559036255, 0.2034682035446167, 0.3777930736541748, 0.27759966254234314, 0.06118194758892059, 0.28208646178245544, 0.12680363655090332, 0.23829105496406555, 0.26964911818504333, 0.44475993514060974, 0.28119903802871704, 0.15811662375926971, 0.31156983971595764, 0.41728949546813965, 0.05920589715242386, 0.1302289068698883, 0.23314523696899414, 0.041755758225917816, 0.493198961019516, 0.49007001519203186, 0.3739961087703705, 0.49021923542022705, 0.4203105568885803, 0.1494341790676117], dtype='float32').reshape([28]),
            paddle.to_tensor([0.12322742491960526, 0.49176937341690063, 0.21143527328968048, 0.19391612708568573, 0.24277374148368835, 0.20109675824642181, 0.09411759674549103, 0.11784742772579193, 0.46431127190589905, 0.46844974160194397, 0.36598309874534607, 0.43102335929870605, 0.2890615165233612, 0.4210042655467987, 0.07559576630592346, 0.13112938404083252, 0.3740246891975403, 0.4099911153316498, 0.3304744064807892, 0.07513506710529327, 0.1968449503183365, 0.11691633611917496, 0.08764656633138657, 0.42771780490875244, 0.2549740970134735, 0.09445248544216156, 0.09000585228204727, 0.45227381587028503], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d119b6afa91ce6c31ac0b78d8b3ca506(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 120, 120], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c90ebe45c29c4a5753998552095d3f4b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 6, 6], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b628a784a403ca544f738436358df2cf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 75, 75], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5bd5943a8a8e1d0fa3dda4c8b86842a4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9321ce7dc90c358fcefdc9c2f0290569(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 120, 120], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.21874673664569855, 0.46537667512893677, 0.4744665026664734, 0.26721808314323425, 0.4104975759983063, 0.2373509556055069, 0.46934428811073303, 0.35649850964546204, 0.16541852056980133, 0.18338704109191895, 0.4206019639968872, 0.47549498081207275, 0.39953839778900146, 0.3201800584793091, 0.05670354142785072, 0.2767452895641327, 0.039107538759708405, 0.17987224459648132, 0.46868056058883667, 0.08546604216098785, 0.28930073976516724, 0.22485092282295227, 0.15525192022323608, 0.3756760358810425], dtype='float32').reshape([24]),
            paddle.to_tensor([0.004351750947535038, 0.3333803415298462, 0.21283361315727234, 0.10467904061079025, 0.44004321098327637, 0.40581297874450684, 0.28381863236427307, 0.20853260159492493, 0.41175639629364014, 0.42850369215011597, 0.22353121638298035, 0.42619824409484863, 0.43979355692863464, 0.3126925230026245, 0.025308892130851746, 0.15531860291957855, 0.26709696650505066, 0.2556421160697937, 0.20256474614143372, 0.4635307192802429, 0.1293935924768448, 0.06827334314584732, 0.3999128043651581, 0.4271462559700012], dtype='float32').reshape([24]),
            paddle.to_tensor([0.18885959684848785, 0.37454959750175476, 0.4852333664894104, 0.4211213290691376, 0.3549162447452545, 0.22175772488117218, 0.28186580538749695, 0.2947612404823303, 0.3670530915260315, 0.04102438688278198, 0.09384526312351227, 0.4962328374385834, 0.3443012535572052, 0.21026502549648285, 0.42843762040138245, 0.43844693899154663, 0.4016120433807373, 0.15582740306854248, 0.1606561243534088, 0.03282530978322029, 0.3886459767818451, 0.11189926415681839, 0.29732879996299744, 0.19170048832893372], dtype='float32').reshape([24]),
            paddle.to_tensor([0.27870261669158936, 0.4775814414024353, 0.4461638331413269, 0.42501696944236755, 0.06668738275766373, 0.45640677213668823, 0.2303692251443863, 0.24948766827583313, 0.12831829488277435, 0.18354305624961853, 0.4300523102283478, 0.0649832934141159, 0.23687347769737244, 0.0022433288395404816, 0.24644990265369415, 0.06193197891116142, 0.369484543800354, 0.40474244952201843, 0.3336220681667328, 0.05466902256011963, 0.3139968812465668, 0.4511778950691223, 0.20881550014019012, 0.4838610589504242], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a61d7b5fce7832728b9ce5a1a0b0e35d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 240, 240], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.10032902657985687, 0.06530845165252686, 0.07500503957271576, 0.4937628209590912, 0.4254128038883209, 0.2857170104980469, 0.13039328157901764, 0.49185821413993835], dtype='float32').reshape([8]),
            paddle.to_tensor([0.448325514793396, 0.21941876411437988, 0.4221165180206299, 0.19140605628490448, 0.4851585626602173, 0.04236073046922684, 0.332736074924469, 0.19780196249485016], dtype='float32').reshape([8]),
            paddle.to_tensor([0.13326789438724518, 0.13155800104141235, 0.3088719844818115, 0.09083793312311172, 0.24712252616882324, 0.37337544560432434, 0.28262442350387573, 0.48306795954704285], dtype='float32').reshape([8]),
            paddle.to_tensor([0.1305582970380783, 0.48221346735954285, 0.4155867099761963, 0.3313920795917511, 0.40544798970222473, 0.20732460916042328, 0.22989924252033234, 0.40074875950813293], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8b2b25f45efe0c1e0688c7f8dd6eb1a2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.09365193545818329, 0.13752029836177826, 0.3748738765716553, 0.234975203871727, 0.16098198294639587, 0.3220912218093872, 0.19058960676193237, 0.046603474766016006], dtype='float32').reshape([8]),
            paddle.to_tensor([0.2833139896392822, 0.20548312366008759, 0.3505028486251831, 0.4184909462928772, 0.4131574332714081, 0.24602483212947845, 0.34451979398727417, 0.4176301062107086], dtype='float32').reshape([8]),
            paddle.to_tensor([0.01798797771334648, 0.2091192603111267, 0.3631240725517273, 0.45712414383888245, 0.31723150610923767, 0.16696996986865997, 0.4510907530784607, 0.08458493649959564], dtype='float32').reshape([8]),
            paddle.to_tensor([0.006272419355809689, 0.31513988971710205, 0.20549817383289337, 0.15763254463672638, 0.11925584822893143, 0.008168292231857777, 0.22400780022144318, 0.24504625797271729], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a9ea46fef6b7b52b93ee4913f038c396(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 768, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8eae295f5c2e5c3f259f5573438583fa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9677fc4c8dfb93fd81b71474bd5576ea(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_96a16a730cb55bf28e6b0c242f8e2b7e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 4, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aae891d65f73839d8b677464a24d7a8e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 32, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2428fe41e069348e823b33e06ac6ea70(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e8fecbdcda28c829aa8e0167cc20dc5a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 32, 100], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_85b798c303135d19ff37608537e732da(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 144, 31, 31], dtype='float16', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6101fc7ccfc03a3c71645f6f6e3d2cac(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4f51ace0e2ebe67d942c802bcff5f009(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c7039215ee7f1b9252399acee1d153db(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 512, 1024], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.016121186316013336, 0.3973487615585327, 0.08470537513494492, 0.051142141222953796, 0.37607747316360474, 0.02827155776321888, 0.47988396883010864, 0.21205195784568787, 0.05869487673044205, 0.03992234915494919, 0.12751036882400513, 0.11709389835596085, 0.34732523560523987, 0.17201091349124908, 0.42180779576301575, 0.1849433332681656], dtype='float32').reshape([16]),
            paddle.to_tensor([0.15271440148353577, 0.08316449820995331, 0.23920468986034393, 0.38072264194488525, 0.1639639288187027, 0.025327030569314957, 0.23954330384731293, 0.4351273477077484, 0.06249742582440376, 0.3737652897834778, 0.1513255387544632, 0.4119172990322113, 0.21870681643486023, 0.3980177938938141, 0.00993346981704235, 0.04227874055504799], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2712686359882355, 0.28177812695503235, 0.2787134647369385, 0.3592609763145447, 0.4188480079174042, 0.07675214856863022, 0.4628077447414398, 0.010709738358855247, 0.4151894748210907, 0.0938500314950943, 0.1247754842042923, 0.09743431955575943, 0.08130000531673431, 0.37328284978866577, 0.09686999022960663, 0.3255687654018402], dtype='float32').reshape([16]),
            paddle.to_tensor([0.34915581345558167, 0.38661903142929077, 0.14316794276237488, 0.38383516669273376, 0.2910211384296417, 0.40535813570022583, 0.1745910793542862, 0.01192957628518343, 0.29321372509002686, 0.11113675683736801, 0.21066848933696747, 0.2106717824935913, 0.3208651840686798, 0.25250470638275146, 0.008733085356652737, 0.3850122392177582], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1c7cfefbaf278ab78f60978f0ae4e142(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 17, 17], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a724e66c634816c9be0830436463d761(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 32, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.07501243054866791, 0.24088498950004578, 0.41401389241218567, 0.4231632351875305, 0.4185725450515747, 0.4268796741962433, 0.28690212965011597, 0.1760077029466629, 0.15994569659233093, 0.42206600308418274, 0.4227750897407532, 0.15549542009830475, 0.30803409218788147, 0.013916954398155212, 0.46474266052246094, 0.48614656925201416], dtype='float32').reshape([16]),
            paddle.to_tensor([0.052714280784130096, 0.03844200074672699, 0.11610781401395798, 0.3287266492843628, 0.453041136264801, 0.18327893316745758, 0.33834636211395264, 0.3790256083011627, 0.16260357201099396, 0.23733441531658173, 0.11637533456087112, 0.21436969935894012, 0.12480950355529785, 0.360610693693161, 0.052509453147649765, 0.26747244596481323], dtype='float32').reshape([16]),
            paddle.to_tensor([0.09675058722496033, 0.1273084282875061, 0.21315394341945648, 0.06828616559505463, 0.2860627770423889, 0.1852228194475174, 0.1800910383462906, 0.47456198930740356, 0.022602936252951622, 0.39275622367858887, 0.2902042865753174, 0.13695654273033142, 0.4016115367412567, 0.20504875481128693, 0.23430950939655304, 0.21409407258033752], dtype='float32').reshape([16]),
            paddle.to_tensor([0.14835543930530548, 0.05172915756702423, 0.42213907837867737, 0.2888830900192261, 0.19137954711914062, 0.29202064871788025, 0.35484543442726135, 0.07001382857561111, 0.13691028952598572, 0.2191053032875061, 0.2517264783382416, 0.42223525047302246, 0.34248754382133484, 0.02583148330450058, 0.38176971673965454, 0.45443791151046753], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4a59a8f5d9335940b15a1f640b05f3f8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.28926321864128113, 0.3924355208873749, 0.4843003451824188, 0.4772259294986725, 0.030370265245437622, 0.1551830917596817, 0.07288806885480881, 0.17315062880516052, 0.31965890526771545, 0.4306981861591339, 0.33887824416160583, 0.3102771043777466, 0.05813441425561905, 0.3622053563594818, 0.40364280343055725, 0.22636757791042328], dtype='float32').reshape([16]),
            paddle.to_tensor([0.008831452578306198, 0.306727796792984, 0.00988005567342043, 0.06932754814624786, 0.2393176108598709, 0.497270405292511, 0.175725519657135, 0.2650114595890045, 0.040793437510728836, 0.2672196924686432, 0.4520605504512787, 0.44852593541145325, 0.047459717839956284, 0.22635217010974884, 0.3404947817325592, 0.43317320942878723], dtype='float32').reshape([16]),
            paddle.to_tensor([0.13911646604537964, 0.16810554265975952, 0.39387157559394836, 0.20613093674182892, 0.37250444293022156, 0.3876452147960663, 0.12781934440135956, 0.2707463502883911, 0.11547563970088959, 0.16681602597236633, 0.16373473405838013, 0.41920390725135803, 0.1168229803442955, 0.42724359035491943, 0.2340240478515625, 0.21239446103572845], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2708129286766052, 0.4426301121711731, 0.03039010800421238, 0.3438427746295929, 0.23474836349487305, 0.11947961151599884, 0.0998358279466629, 0.1261586844921112, 0.0968126654624939, 0.4065246284008026, 0.019194038584828377, 0.020125459879636765, 0.021484868600964546, 0.03383088856935501, 0.29210367798805237, 0.21505914628505707], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d2f8421f235c440c23bc03430a33cb3a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 400, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([400], dtype='float32', min=0, max=0.5),
            paddle.uniform([400], dtype='float32', min=0, max=0.5),
            paddle.uniform([400], dtype='float32', min=0, max=0.5),
            paddle.uniform([400], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a085e37fa0df592c782fb69c75b0c5c5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_eb526f1051ce7a64efbe7095972e93b3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.36778122186660767, 0.2543427348136902, 0.03004700317978859, 0.3311799168586731, 0.0014156331308186054, 0.42354124784469604, 0.32007691264152527, 0.13397035002708435, 0.008487903513014317, 0.4334776997566223, 0.2640470862388611, 0.4401566684246063, 0.3182477056980133, 0.45755475759506226, 0.30963894724845886, 0.27920541167259216, 0.2208453118801117, 0.35619238018989563, 0.4444119930267334, 0.3952106535434723], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4945557415485382, 0.25795140862464905, 0.02889176644384861, 0.05337733402848244, 0.4727558195590973, 0.2412288784980774, 0.44407665729522705, 0.4087560176849365, 0.33280032873153687, 0.10162852704524994, 0.05595129355788231, 0.4949607253074646, 0.17364691197872162, 0.05857834219932556, 0.29472020268440247, 0.05353251099586487, 0.4903413951396942, 0.05818304046988487, 0.33816736936569214, 0.18586869537830353], dtype='float32').reshape([20]),
            paddle.to_tensor([0.2514496445655823, 0.24929368495941162, 0.4717809855937958, 0.4674318730831146, 0.42778679728507996, 0.2974417209625244, 0.1506505161523819, 0.40895482897758484, 0.29470330476760864, 0.47479134798049927, 0.052720170468091965, 0.13528728485107422, 0.30209288001060486, 0.171421080827713, 0.1255788952112198, 0.20489218831062317, 0.34677305817604065, 0.08916061371564865, 0.21099083125591278, 0.33989211916923523], dtype='float32').reshape([20]),
            paddle.to_tensor([0.31892579793930054, 0.13246385753154755, 0.40055206418037415, 0.30586162209510803, 0.4899570941925049, 0.048471514135599136, 0.28783899545669556, 0.29800915718078613, 0.492760568857193, 0.09659949690103531, 0.03236084431409836, 0.1666504442691803, 0.18620577454566956, 0.39325472712516785, 0.21117126941680908, 0.18883273005485535, 0.13887275755405426, 0.09734299778938293, 0.015004012733697891, 0.20402534306049347], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_82a4f2e72ce452ea172b793f34cf9fcb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1472, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1472], dtype='float32', min=0, max=0.5),
            paddle.uniform([1472], dtype='float32', min=0, max=0.5),
            paddle.uniform([1472], dtype='float32', min=0, max=0.5),
            paddle.uniform([1472], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_58b4b74648ba171b249d7bef639c29fc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 32, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d585aca6cceb2924e36e898cb68c93c2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 4, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_c8eca113b3029d88b25158f95c03e4fe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-06'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f78910a24ab00d45797c31273495c7e0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-06'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aa2f69b8d2bb435db583b9490adcbad5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e1b19a292b33d2adecafe5164671e766(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5441acb8165a81688c6b7dc201fc8c0a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 20, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_62c0c2ffe276d6c810cba03d6d5dadf7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4ff6ea202881aecbdab6f3d61896bf38(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5fc991c75e15990436c661ad4f31bc2c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 76, 76], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2a07ea5061d7ad2a0c11974d825fb43b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 360, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bd62a60ad014fc4d8d3bbdad0dc812e6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 62, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d54dd1a553b1016b2cb9efe5deb82d10(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b9a89d6c5d649626b0ab5c3dc05a8ee8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3b2dcaad09e2aad7cfc7870b395b86b4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 2, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b42e2b0e2baa229ef0a8f97466061c29(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9b35cc9659988100db4c2855c4d8b2ac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 35, 35], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d86166d29866b8690a7ca1699e996024(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_edf331ca5fa8f99449330bf1525d87ae(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a9f563c62a91878b553c08e2cc15b4df(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 20, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cf01bb348290461e0b99bb3ee855ed13(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 49], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f29f53619829634831b832b49653cd59(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_68999480cca03708b0919edb32d1a515(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cf9747313572e8321f152fb443a5b43a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c70905879475ab161d7af4f4c9614458(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 17, 17], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_408d245d25126df77149f7d08aa9340f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d0f61ef67fec7264cd1f3036a26ed6ea(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0a8f2e0b225571cbdd0378af5cd9cb3a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8676f53cab2101556a3aa705d3f615f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 350, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_092cc30aab8f74baa2505d9006fd7209(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bde20d420d7966db0d5ad3bba41cdf3e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f674ecd4caee1f22c0094e96a755c269(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 12, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_306980a35aa819da3afc5c4d630f72be(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8f3d8a328339bec099dcaac963428d56(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1888, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e28a464948bafd11a8012207f50f6fb3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 16, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_23598a8f265065965154dcdcde6d11d2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 2, None], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_37c9be2f2073a048ee4868212a97123c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 175, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2a5d33304f33681fe1edc766fa8ea08d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e0667d9bad3e3f9948ecf0e9ab7356b3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_192bfa443e2ebf2c6bfa87b81cc89e2d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f79b3849c0ff7b562d2f8d37623473b9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 8, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ed693f230fbb7e9cdd184fea7edf650d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d0b1a0fda2fcb56df040381307a1519b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3ddfa948ffba0daaf2bab1fc3fe10349(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1b1de1cb6185e0e58217db08ea6109f6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 32, 128, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9765f38f093f55ef89226fee7f91366d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cd851a3b357e92bf5843a71272052852(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1488, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cdef1e178dcf7458f57f9c3d0f14101c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_85e67324a9e48c7e743d2bd01f5caa73(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_88d2ee044265eec9b2d13f287f9e5975(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ba5d32036b331464bee1a454f001a9f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a77b7cfb6a89a98f582848027cf0d454(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8ff360b784807127d8dbb229d9af68fe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b198c962704a1b49b5b64a507202a445(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 8, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6857ca0c0268dc8b0debff2e586c173e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9b9a6a813dd98d200bcb55dbd392a4aa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_95aba9563c2d8490e87f0ebea1f79ef9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_04553fcf88c43fb8aadb6eb006c585a8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 256, 512], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f830538a49880f7faa7090ed914b6a21(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9bd488df21cd55f3ba5ac049a495705b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 16, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f89151d5524d3ee789d2b4cd1a607c9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_37e9aada9535d6dfb765f3e078755a94(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_49c30032d4e97e0f384c253072b5a30c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eb0b82455812b1e069ad7fe596f1bf3d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 32, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5b297cbdbc20fd574274afcfdf60015f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3cedf31f95c555c6bdb8a2150ea8087b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_90033cb242d6a196769e55d5671fa6a7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2780c15474f5780d697bcab8c75e8283(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 184, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ba50b5cc13932f74cac90d833003cee7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1b68c0dd3defc98dbe90fd15975e97f8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dcc0d2be3e3ead42d6e2ee26a70970cb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_614edde15238b37cab52731fd37f39b7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c5f8394951e89737c107542c164c5fc4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d50b2ed5144cbdd1ca5042fed72047fc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_60ec4ec8f0d673e48eaae3861d000368(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bec13ebeff44409d3eef899327206ea0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b8cad70198a579ef46dd2d7f8f14031f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a571297f364fbba0735c6f470addef2a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_df0a557d314c037d833d2be6f20c411f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 50, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_24286b574aace75bf578db8ff07e0e99(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_31c2fa12be1f4c1fc369656d85ae0b8b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0b4b8879c39d03f646f9a91b3184e36c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_32edf953fce616597ad2b44f8310ea33(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7c8aafac5a643847685bb567642debfc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 32, 128, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2aca463f726fc557a6eb6fffd721807b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cfea30a48e65df8701477c50ffaabbaf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b22a4c2a1aad45f0f951cac899fb522d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b3f81b916e59f28b56d7c33903e8a638(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d20b18026bf4113dfbc298bef48cc387(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_31ca4dbd630951b82d9b5bd88e830717(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7ea9f08a9b8f521acd61667dd33f31da(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 18, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_44420878aef6a461745211eca5545482(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1245b3cb3e107d262d7ecf1acbf2e1e0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1c27edec4a7eb3e614a35b687d2cc820(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1242730d4df1050dfbe00528d15112b9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_78ac0280d335b5d020d80c7278bc659f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 728, 19, 19], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6894ddba73590d8743b835ce9fdde0b8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3ef1128a7f0385fe2e93b55d907ff2c7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad543cd8907ef994e0ba07c7d7732da5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 2, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_22e747980adc24d3e691b98fe7f97177(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8ba8b0345268f0dc8f4ea2e68bcb1214(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9580c5aad3dc54b081d5076a4dec374f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 20, 32, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2c4c3cc386cf13265b1440a9178fc404(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9479e2f3222000ddf2ecb70587464185(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f5b7b0fafa1d70e4588dab9864032d2a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f4cec7d657e259f4c2913c4230392b9f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e84b9c88555246fdc19a330a2dc95ed7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0cab8e769743e9b14df2ada13681fbb1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 12, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_14fc0a24ced80903662f15f415902531(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4e066cb3d7c6f78d55cfbada4f260ec9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_373728b1490bc33e9f944e7dc2e95787(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 16, 512, 1024], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_acd0d714098c95881a5ad397bed31056(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a977517bd9688cb9754372ab263ef171(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b081fb752a993ad40bd7ed7adfc6290f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a512991055d5115589257aad5b14b81d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 2, 2], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0c90e6f36ee62d83e7a99e0bfa0cb061(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 28, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e0c71bc41aeac4c359b0d5c6e94ea65c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_01be06e6f549cb4256fd5fbcd5391ac1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 4, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_04e521ec6933d773533dce64e3d45124(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0f55a3fe994b3b3ea44afa5ed331d5a0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8691d18211a75347bf7b588c139f54f1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2fc9a891a92f336694b4fffc9b756281(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 736, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_476fda35bc4beeae5de4624189dfb962(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 32, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e2466faae82137046237da50a7dbcf3f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_82cb6c9abe3eac1e80505b61c84166b2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_02825ebecc52b0450c7e5430b04e81b6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 16, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9d30283f0e3a737119e83c43c06fb560(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e85b60b290717e59a466822d1e42eb3d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_68d9ca62f121598cf5fce845fe4e3830(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 224, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6fd75af07736e9d01ce333ebaf269a6d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f43c1722e1112a30fd429a884406979(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a175dad4f9465d67843636e9bee397e5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 32, 256, 512], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4834a71b097f80307b114c152580156b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e04a6d914746c47bf4f361eb509d1ae5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_801eff40f6ce09cb8d1af376fd7f30ac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_69d4d8d352c35b330e34d3483128f699(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9e5b5017ef481ffe84d96a7e526c5fdc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9f84fbdce742b74de5544e057cd28838(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a14b87cec90c13cb6254bcbcfcb2843(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0727011728786dc7406d169be3df53e4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 528, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_964f2c00730c0f617a2caab3ff97bb96(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 972, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2109b8194661f3138acf2c8b007d6b84(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_06d57437dde5bc9c97fe8a1ae10cbb06(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_433314088006787be77fb1fc69ad92d2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_04f7d080fabba449b0b04ad4c7ae09a7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2cfd04874ac8e2deccd00b885bbd9725(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f632f6b71e76f85dde9e6a9e74d99131(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ac25a9f438fbee8212f63b2d4fc03f5b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_76ab480359ef928c8720cc2e4d4649b2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 8, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b9857c98fc49adea29001e09fea3d68d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a5af6cbfbe32b6178579798aa12e5117(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b7377c3d7f0a477016a5dc92e1a42451(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9e30fae60f3b8093db147f92de4ca394(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 8, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dcda6b8c7486d0b9bc82aaf2f1b677d1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 26], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_95752e262f48b523693ff1b90222dec0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7ffe1a6ab63c287d150f682527662f65(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2529de372a4867bc4899bd15cd785d5d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_be7b23f378ba717a9d80dcf4b4dd4ee5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5bc89a0588a01a52fe89ef44e204e770(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8e4a97bc06557d54f21f41f06e3995ea(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fd6c62a5b14788f5279b50233d0e159f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0cfcadeda21a27d8481eb1381f9e54f7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 256, 512], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1805ee5f66925e2e2281b2c7beb70074(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f3ef53dd7ffd9dd955963e7797739157(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_41bdd673393c791b8d2e8d2405883fa4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_365d00131505c7d39b3fc12f494811a5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_47a6a1a509697146a8e9e71dd351270d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cf0dfdaf0c4824bd9e3f298c3ab3d32d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_68a3a37a967d00e45a032a75e100cbbe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_73abd023f1b54ec5f4d5e00139e06a6e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 8, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eea460abdf980da44cc6e72122d0288b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ce3c5438c2af00accb1b7081761e25eb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1921d031b73340a7414904caf27a3935(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_db3075fe5be00cbb4c1ed9b2bee0f102(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 61, 61], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5e07108a5fd9f5f4cdc70500da5a1d42(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d7ef8646f6fce33d8636affb4ac762fb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 38, 38], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bce7f856811e02f9600c9a863f63cc58(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d8a247c0ec9ff71630a7b32ce2c9213d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 76, 76], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9e245464a02f8d5c9be937d9791fa951(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 4, 256, 512], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7af1867cf062e8e55bd074c997d4b02c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_90e8782fd11c47935d861c75e6382c54(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f15ba70e4e8d2e88b323c4a8eb5175e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_84ee48c3345df24cca6b87e894987d91(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 19, 19], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_93dffa82bac9460712e64f88ed24475c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0dabc235b01a7023f14e74ac18c8cdbc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 350, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6884bef74e02ae00c3bff219ebcfee46(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 100], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_99be4f5ea99d94116b6830a47a351b9e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5294ebd8899a711fe2314802f5ecba45(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_398861261c8746d2881cedaba6979814(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3c644406c3cf29f49b29643c4b71b219(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6072bbc3aa1bc3812bd0572c3fc3ee0b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e799d33cdc34eab378e0e515e5630486(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1408, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c23f660599def9fbcf07210411b7c578(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_093e50fc34ac9d700f8462a8951e7dd6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3669a9be9c0c3f4eccfeccb2035ed7b6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 8, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4cefbe2f0fcd0a3c7d12d04da6294ddd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 25, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b460480ca04be6d2344634c87163ebc1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_72afea2bd83e26c58d662a0ddac1de3c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_88c91ea1acff8ca87cb913f04c8fb8c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f8b6f0326bb25d20671547dea2ea67b8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7941a58505ea31eaa83ce3690d3d3db9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3b283aa56dc1a8591160c9e27af049ee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b82eb8b23f613db89ec89a2c78416e3c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 48, 48], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1893a07ad07eac7754dc602161ea858e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 76, 76], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e2e08385fc1ffddf21545f9dd55ed8b5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ab836b831e4fd09aff2ca9c06012a5f7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 6, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_88d417846c710f6bcb097ebfa8ce0714(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8991372857f1063deb0639e3e0988529(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 18, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aaf74873a9105f0af43d9af182072d3c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6019f3d38c8be9d7dcead7fde144fc69(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ba12f00683fce2176c748a4d1787d506(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ff9a641fa2df9e7c95e9352624577d70(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2ae2ff6147168e6bdf1ebed988f5fba8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1d4c603928c952a4be367cbf82cfad9d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d0216cd2c002adfdd03e4fb5ed82c99f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dd18401b50f195d1b69ba1a453845f02(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2560, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3cb3ebb4edf0758fe4ae9d28bb2db0fb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2d81571c6c5da8d099f4e8488504251f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 728, 19, 19], dtype='float16'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3039e15c3bff93f37bc753a08dedbe9e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 2, None], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0f299a987c28e8264dec78dad28a831c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 28, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_653a2863d504e0a421142e940bd29d6e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3547c3c3d5ba9688e3cca445783999de(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_93cb196bfa65ee5c3f96ce05d181a31a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f8cdcba81ed75913b22b320109803f41(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 704, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d9d060b5e64903648e88dc3cc8cf97c0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c4c699c445b062d30c0454b975ad4443(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_09560c7f59586714659b8574fbdbfc34(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2df9b1c06f9256f8108b4a1143df35e1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 2, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a84edadf5bb6023f4da6bbef1abdfe8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 16, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_40febeaac423e00552e9dbdc617b7213(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4fe064b7d072a933fc3df6cde2593246(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_801797befffbea1a2ce19d019107bf8f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 8, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b60fcd0b6f26428bc5c7770bad7ffa1d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_95c2a976b83a4958b1be6cfc2b6c3597(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 360, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_04ec187a23fb298c7d6e878a60cd785c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a674dc3c5ebc2981cf0bf4f72a31dacb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_11552d4e360ad473a97ca3997cbdb69b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0657ef22db0d18caaee4c49eefa3e86c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f49279ac3043eda0a82805a7a8ea9779(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_424f752909aaaad1bfdd43dd820623a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8592516a8edd37f0f9798a35009b999b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3c414d695d1891198249a313f1480f9d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eefcbc750dc9b0e65d40ff7907f1312b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, None], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_deaa4bea46982ee0218f6e86772f7fed(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ef279ca824491cb465daea4379cfef4f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_903d1293b7e98fd85bcbb07e234cacbf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_22abb3e6174fe3d2c487d3c21111b5bc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bccf32763057437a36894e2efc788dbe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f0e9367ac584b9476f9ca96ce73f7233(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8de2a97e06d91e33f36e2fe3b70e55ea(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_11a1f24d7a4be003f6b0979b40654476(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4c58552bb28ad7d4dac823baed3c6698(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1856, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_47253853fe7e4c594fd18626e3ff76f7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4313045e50e143d0b04f4beab2d1ffe6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 17, 17], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f7304499082d0cdfd12079a9338d57ba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ce24ad847c2f00a65300d4a5d0606410(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ce8fcec7c43c5e4a66ab86f6974b50ed(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9fb4d2c35d0d8e55697ed2f596b3c41c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1660dc9441cdb2a96059f89ab09eaa94(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_90ba953ddfc60bc98ceb008c571446d0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2c9a65811391b48c16663893efad06f2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d0af63b422c5434e9c5ad57c6b33b9d3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_423b983ffdfc7e65b383ccfeafd45b4c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9c791f891b65821f9042c01087883c7a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1044, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d3a35937d9c5845f9df447ced66c3c0f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8cff36adecaaee29f89c236933757419(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f2b51560d87b84e91350c0e6b6f086d2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bb6c2f0038982adc38c6daa844fa9d46(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f0d93ff64af4d85361149e262f90dc83(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_75ccfb5156a59ebd3b59de71bebace12(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 26], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_70d4159fe200b677a52f2592dd80cc4e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0126aeef335dbb2dd53ff29c62ab0a41(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_96c65253665b360fd1a7b9c2f0fbf56f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7134ccac61be0ea0fd0aefa3dabd9403(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 432, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_35dbfde51f98a7f8586486c5e8c555a0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d39b6179e48b5737892563f03fbd1dd8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_43c10356140810fa3f3a87f08f368419(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d888a268af70f20a6e8eb7f00a9b04c0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a8300eb81472fa39883d842ffa8e5226(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_92c7cf44ca9e699763bcfc4145325e45(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_672dd7aa42f3a021e6d00bf92e9b1d76(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_08c45d993c57945b8989d36d0b06f306(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1504, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bec5c67fc72cf90ddb9c224701278864(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_263b59190fe88b474344cef4ce31e75c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_045547d9fd3b70e80400ddb0704629b5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_399ad121ba8b7abf1eedd38cf9d4f3c3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f3652582614c52d5e020edd850bbf63a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 20, 32, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cd1ad2d50aa07808a6d27b1f1abf8ef2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_99e42cd7f95916211bc300acf6ff6d97(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c411f6f33680b03a4a7622bf3bbfefdd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2d1cf063b27b9372e920c977c556bfdd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_24cb734c54a26d6072a990b24082626f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_552489f69ad39a25d3ae7d7767b1907d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 4, 256, 512], dtype='float16'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8cc3eb9723866708e4398e370127417a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 118, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_66686e954e0e07c03d3f70fb917de5e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 88, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4ecd3e332c82c379847f781922767736(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_31ac0e1776ae9a4a2dc523abd9dcaa95(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d115a16c90d0f6858881e4f05a9f4700(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dd271ad96c296143ba2bfc64860ac86a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_10a96d116bdf16eac747ffae2f89d57e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9f5b52cc907400eaaf11705256ec7039(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_83f47e87307cfd4dd8a195d9d35fd4d1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_65710063276558e901a5a7cc7b2c59de(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_52fb4c7709facf72bb75957b3bc13cea(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 52, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6e3ca2d981870e25e3d4d5e6d1a4bf5e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d005e9235308a277e8637c9104600f9a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 704, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_81915bd6f97fa7cc8ad5815845b097e9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_87d1861b61a5421593d15b19010c587b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_84a078b7f89b849f5e6f8da37b96b34c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 162, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e68a28f7acc5e1fec128bfef6425f8d8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 17, 17], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1b831d030b659453f9f36a2daaa0d61f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_378be52d1342297d3b33c6f3d55b3bee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4213f15def29e76c2417266d316d747c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8908757f1e09639d6bcae97a8afe8842(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3cf26023911c7eda97b16db699ae0ff0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 352, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0f091825d71d4e1ffa027da64f5d4394(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 35, 35], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_66848f63380fe3caf878efaf5f970000(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 56, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c181f9728f90d171e0887bcbb8afe44d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7f27863bb556262ce9ca842c1183b000(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 44, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f6d9f98a7283b104ec057a546d2c2fe1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 19, 19], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_499000feeabf66e84f457239b361acaa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 208, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a0a46aa0a89a0eb975d77352fd88c9ed(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1bd086ea33b1b0f960e73d3baae21f66(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d3b7ddd8e1f1dc602d6a5a567c3eaf5c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 416, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cf58e051a021fc0201a846c5b7961f9a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1248, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_390e91adf11e1069414d8e99c9dae65e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 175, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b37a9d0f63cb1d8171d304dd8f545e82(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0b2e3d95f27029d02d70f310dffa5275(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f04b1f723198053247940073cf961599(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 100], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1469e5829ad6dd174a4d4301d92df512(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 38, 38], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_51bab3307d196b0917f4c943a4dae42e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_89218944f1439edc3cce4e16883d0d01(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_480b42b318e09534977c677347792d04(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_97114e27a4e86e0b0689cef0f1ac820d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0ffdf4b0113987366d210bdd01ca249a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b27edf12bc366327fa179e9bda6aa083(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 26, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ea0b2107e75d7da7dece16d2818e62dd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 49], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c624d63392a8c8c6e4d759243edecf88(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_82424129111531e256cacea5e01fc53a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 300, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bfc693cb64eafdcc84b0e80cbfb15276(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 24, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2d4a06ca563163a1911037f377fd7b54(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3890f5a700857b435490454b960a454b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ab1ad85a256f22d05c0ac5a2c9d383eb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2eb2104ff4f2d8c45a218e5646e56e39(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_14b082ceadee49af49a550bae9fc8c51(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 528, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e4cab3560020f8e75167f6010b432790(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_171e444d3443a35c9e028717a8c7c8bb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 352, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6a86473a8eb8714791b7ea8aceca3b47(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b6eae07d54d9ef5dea95b59f6544ff89(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_937f0b1ae193bd9da7f88fe70cc20312(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_48202d5ecfe341ee2cc8a38321d39991(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3473e493d0e249ac682f4c69cbbe0a25(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 52, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3f89882aa66b7d6e0e6cf03848a21bf5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f20b7d281ea20a4072d0f48c8a19c847(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad0d3f301065e9667c07af51436d8926(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 4, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_25b4477b7535e9e0765139032b8778a7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 175, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2eae31498e57b686bccc36b149fb23bc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d90e79439c86f700f2883ab09af3ca8b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5ac548483b1eb78cd51c68228482ed73(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7f61c38bb31decba43362a7297fa1282(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 6, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_98e11c90b7a6bbcdbacaa094234728d8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_720e8ed98eeae62b50a77b14408c2b10(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9132d1421c6776b558135871c20de887(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c264d3b0c0b1a61a28086f770ee56d74(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 8, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a00bb4e257ab0a36e29ebfeeff0d842b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 200, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7b12fe585d6439f409f2d665b43cdde5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6673197adf120ee098622608c376aa51(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_32f082526f4718640e33f8509d211cda(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_11d9df73c6718586e31773eefe4f9f88(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fafb45dd41fc17f3a0094974515819a2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b2f70fc54e606b9ae002fdec0cfc7df6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_56388e95fe8a66c4dec6933f7243c2be(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_876d3325e90e8844fa413da7c1376370(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_425450a29c03def74064017764b43a41(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3307b0fc511040ca02006ee72fb33cd8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_21cdbaf050444108fe284ba07c7eab75(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 75, 75], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3d0dedb55bd15e3225ad133db883e64e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, None], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_11b14a1ae41b0a31593836f12c423b4d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 3, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_73d715474df6ef0f0d2778246c30dbba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1184, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e790ff09b57e113e0ae53401cbebfed7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_470c513e8f7f2702c7307c0ef6a7242c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_557d967e46ba84e246ef28143474892e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 88, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b87e946dbb1b9643a1a946af8e4fc59e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 32, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4a1101b6ba9ad30c33ffb1728d76a0d3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 232, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_24e7b3aaa2ad76692a7ac50c0a6fad88(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c112c4673f3887b2d0c4814080b7d845(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_504e7b0e17e361e8bbfbcfd1c468bd6d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_877a4855048705141aeb14c3e0cbd641(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1ad5322a25dffead414186b218aab2b4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6ed2f9a7a3b3c0ddc363ed2cf9c5621f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 5, 5], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_78c34eb38582027a3bfe5a17bdc8a49d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d5d4acd8a1de3893d6b69b05d68ff56f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 12, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_91169ace885e4a24d300106550e4c6a1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 152, 152], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1cc6aad071abf8ea64d3fe3ef4684e56(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 4, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a2e2e20ea35ca3f2eb4df3088dc67f39(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5235f72a7f34d2f45209d273b1ca4ce3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1968, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c5344cdfe72e997e5159f4cf3d17f3fa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1344, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e14df009148bd5c0fa59a070b1ebf16e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4011c567561a3bf8cf22c9e03146685a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 52, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_448c0f99cf80e2fd7cb8976990d3f7f2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cea65a08f23b566a04190ff1bc59e5e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7bd3f8b6a0fa274a9e326618eee146d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f546035d8aac286bca7b3ebe029dd18e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 26, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_89131f01c734536b69f798872c41bf81(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1c0d78493af12479591cd66912c25938(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_46f1db9af75718b28443482c27151a5b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6e3f86add704cec9080165103b6b20f7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_867f6b2e4842a55cae6550a09fcf98b1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 232, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_93b7eb5220c982e74801e19028ab6cea(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 4, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_068236b963f20ae3772d95db2836bcc6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_31b49c768d52ebc24a46e57240ade1a2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_80eaa0c7bfdbd2b1e435a1a302663c21(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3b30e6c17eef017676cfb2b3323b0c80(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_86bbe428ad7cdf86b46d39572e0601ab(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ea5dbc47a611c52732e4b437c6ca230c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f18595d2024ae3d46e77191cf6bf85e6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bde2707e5c148515f5b22de093b4d6fd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad27f36a6a00300e6d2db1669fcbc05c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0aa32758ff3449ee07ce9667f89e0a15(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1e9860a676272a81feaf9cf26dc4e4db(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0dbd9fe2e1e04bf2270b23b74c3d9df4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 12, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9a6c60662dbee1ec9a8016799a0067fc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7368ec1ffb57fc339f15151366d9e8e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 26], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_86effc94cc6dd89c0a0bf56d23b38aab(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 44, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_08b46924600e40a736dc15f1f0c34c6d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f8c2b56849a92c7300392b789d801624(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_25903a5acdd264202599b2e994bbd6cc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2b3dcda2a21fd69aef4c26fca2eaccad(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_68d42fd40adb5d3b982284414f88d27f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 128, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4f79fd4b24bb5012a8cdfb4b3210f826(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cba8ac77abe105a6ebc0c0dc696f46ba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_22166f4a1b8e7b18c57ffc83278e54fa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a09f14130ee5b9fa144a8e7689b473f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 832, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8e8f73e3bbd4fb2d7bba6e6d36e22f91(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 2, 4], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_21c434ef269a633bfadd6adaf8a4a98d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bf9ca9a52e7714e3b3f92960025f51ac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5e1344a34fc3a134b70131e7016a7e01(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 75, 75], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a9301b445d7a6ea9af96c2efe8af0364(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e58ed3e90c06992381b31f7b32cf3514(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, None], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7bfe4867b2ee50af53c3d7bdc5f7f58f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 16, 256, 512], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0b78a4c3f6f276413f0a6ddcb7b8c12f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1376, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0967bad01a11b325be2075c12bcf6e8d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 38, 38], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6a135dd1b999265b811e0cba22b585f9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 88, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c3f0b2690804cceb3a998c4afc73fbc7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f490811295f67042d6aa6ee33d9fc946(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cc3eda264da511fb1672e23fe0d95a53(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 96, 96], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_148cd4160d3a517d70ebea43e8198ce1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ff40ca29a1d7f7c404a184f6433a19a0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1440, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4d4fb26491e469e694653a760f51e107(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_37ca527caca66fa6bd8b014b3f6521f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9d82ba9551361ce0e94ae8e857ddafeb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1ef9a37669994e790928e31cdc463b1e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 26], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_926de857b9640246d3a861bb96dc7238(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_025a876c127e7008591dd621431ad86e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 17, 17], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5a7448d0c9ed8b89a0a1ac8fbc10cf8a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_35ff2db50ad7ca1bf33f0c22454b1841(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_419833893c17c3b5d1c376f82c06ab01(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4ef7505da06e846b325d100749737dc6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7b169e5997ed90c117631737166d16c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9437e9a2ff510eb57d949f967612d7d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b54f59a7929886bfe02b395062ae82a0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7e47f00b89a83b58f2b40e9c647ac925(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 16, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aed2ef1f3b09eb7417781b46d6c304a7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 32, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_03d2fb3dfca39e5be82fbabc33a13c2c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_14c847de69b40a056a624183fdd9b27f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0449ac4c00d9aa171a40fc10cc3bfa5a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 100], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3d0a848e20544900d50d701a3eb9b131(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_31f4a4025c50f577a0d40bf10d25fb87(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d66bd28761544b0977a883bd31ab43f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d60be19e367192a55056eb8a81936ab8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4b14494f518840e1d8943d5a28f48e86(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 416, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7a26ecf10bc78b5487d0ed21b0a39662(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0483512e4e012119dfa48bef933ef5bf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f7cfab02d7eee8ea96de3e3e634ec8cf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9f7e315e1209de59d7c3d8832718ef93(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cdb498724826f2d1d9074560e678eafe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 28, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eb989c26d3976af75d654c8ead02f5b3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 4, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8d462e81eee1dce8cb3768f466d00faf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7ac957503cb9abd3088c2dcf6c9489dc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_475036204effe55e8c35258b4f43447a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1968, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4804a0d2e5fe69e735374909976ae0f7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1200, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bdad8bf219e6ee27eb339ed3428412a6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_74aeeefd61bfad3687975db378e50ec8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 42, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_127ed08cbf7d19b8962ef5d19cc14847(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_af0f41bdd5b07e59f95fbca1a3f3f7c7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 70, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bf44cd9e9ae104c61b72ccf5ce9bf251(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ff41a1da4d2282704bf6a4f294a4e176(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_639564a6a54579140c8cf73996e45a72(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_17dc6585acd08b6f30f9c456f2d4d68f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_75e478ce1c9870e0491a349dfe73dce1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 49], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4a21513bcc72ad90df1926e60f25981d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_016ed345c77e700944f86327d3e197d6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6acbf0634458293ad1d55909d3372c67(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 100], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a26556ca24a31378aa5d2b7a3043ef68(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_16a62df0a4b2e940b226096560e7c1c5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1296, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6ae3234e602e64832c0c8da733fef782(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_be7a980d242db80da503d468c7b6bc00(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 8, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dd5a4aad19df3645493d32f6bec951d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 32, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e298979dcd323edbf2ba8f2f311beba3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 2, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f68ee94db56350745cead1204ae65147(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_49740e807080b999383343e1de877743(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c9114ff816b2f3e8d535e52e7d6050e7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 16, 256, 512], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9fc8111224fa03c870923298a0763440(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_88907b574d986afac9f3889d9e2f029f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b8cc8eed43baa042d1303635c404d3fb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_36df1ca41ea7c15a1af97a21087edcdd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8d97cda29ed22e28b86427ac2b826eac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 544, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5fa00bf5118cba4b4368cd3a35cf721a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 2, 4], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4ea47978706cc46255775a9741b672c9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 12, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a89884487612d4804b14f00b0ed71e98(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 800, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_299dae75cca55f0032f06e8de76caac3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8b69adeff2c71ee7b815f794bb461a6a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f7a00d309a2128ebc111858770c69ce9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_658067f7e912b60287bb6ef6f7f62f00(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 432, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_759a950012c9a587d288dd903532d5ef(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_be909b13142b62385fa32c76b9f65ef9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a65cad9da910bfc21515dc93ae466f7c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 8, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d558e69ba7079f1bfbd6bffa22b02a32(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 4, 4], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5195ebe7af6da40d319d41f61bf2ac2c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 3, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2f3de459085cd99a8d7aa22a10da2843(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 504, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_624cb3d959ca392f1eb717c362b9df66(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1696, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c9b3f21cd8e4297e6ac1d8630524b493(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9f150bcd6de150d77ac8d3cac5019fbc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_09f774ffd15045b465da35e642040316(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 62, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f445cb80822c337d55dac39a368586d8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 12, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7b82cf83ff45043ef66c84f20c5b0beb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 26, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dbb0395720298544a11586fc9d5563b5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 416, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c4bbb9b8e17842bbd03aecaca5e7438b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1344, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0003daa770852f0bc16a1add0a2c4e28(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1dc0990adccbbca8c2f6291e6ea35d61(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aa7e0b68c846dc747dd1bd9131491ec0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 73, 73], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_634fbe3fe8596f1608e8d2bffa59c189(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_25a90f7243b0e1836ddf4f434eac60d0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_263cd5f275bc1c540115cfb0253257d0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_76c4233d2f19f89ed4488de2d0bad695(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_156d2b02725383e937bf08c7a755c7a7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aca3f1beaff9d735bbb1a39a91103cc0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_59325918902e52b53ad2792238317f38(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b6c7a69c80a82f51877a679621124dc0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 48, 48], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_32afe5eddedac1a313481c9673ebea20(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_91d3aedc21153cdc01dfdc591767df6d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8de152baa28ab7e9da7e2432f6719840(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 38, 38], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_407935324dee569608d3b6e8e126ddf7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 2, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3336a67ab9f984ea2d3fdb4dcba03618(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 12, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e161b0e414bcce57157ab486158e34a5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 32, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ea0e8d966d9ebe90e2e6cf8e9a0f999a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4c61f7bc0e45296f623737d3903f679f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_216919a91b32dc576df34acf88a4bbd2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 150, 150], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c0cfa8e556c17c27752ea924bef83b51(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 2, 2], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ee7c94107106bd22819111f42d41d7dd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3d76b968cb943f794da8117263870d04(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 1, 2], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3f5da1d25591a17618f32d5f4cdb2abd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 17, 17], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_acaafb69840e887faefad3ca97ae8bba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 12, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a77ec7dc329722caafa62532aed3870(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 304, 304], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c48808d6c4a1774361a66dc6663beaaa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e7f539abfc4c607a5f18a9fdd1e81b06(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a7f9e1287adef234b360f8c5ce7bf39b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 608, 608], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_441e62d2bdc69610d368e2d17dd3cadd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1760, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7281c73301669007031a5922202d9dc6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3dd51b2ec7b03b02c4aa857a43f3dffc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_56d19fb8aa02332835fbe6c0ab8ab7f2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 432, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ff3939c6254965c12dafccc71f346fe4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3046179a193fe152f8a0c8032c94fb97(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8861c9bac0c52fa7c8171476d14c50da(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ddf56b24aa755252d82b107dabd2bb60(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3c866ab54cebbaed9f5d6da4974354d8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 42, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0627566346fd8d16b36302f5776ac6dc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_011bb84273957ef1e8fcf8977296eb19(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a67d6452b2a2ceec8c222c4c76a8aaf1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1584, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bbec8392257533d466fa69c0a1ee8f14(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dc77783f4f885beba79336191518261a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d2856a2bb276ab11c3e63dd5174de92f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3ce24945cf09613f0e8e068528b5b165(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2e6e13cef89b90578915c4c6583801aa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_566aec332f1857bb88d57c37943246f9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1200, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_62213fc8161303858ca4ce0a0316e3dc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_04d13aaf40278fc81288610b41085998(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_19f30ddf3f63d31426e6ec22a02a60f8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1584, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_755455be86c40f73334f229d81cfb610(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d5a30160af175087d67221cd759bd282(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d15a7bc616b152ced00d29a8e4488a03(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0fc88152d11556dead22b51cf8347e8a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 82, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4f14c932d02b6507f9245815661bb29a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ec88a9a1602c9efb410e4e001ac243c8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_acd83c9a1d85a8d1ea6e14e7c158d54e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 88, 61, 61], dtype='float16'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bf9ee670235dff52a659518f1e62b055(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_00b7f414cd5d1873df88c7b75f6f611f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_809a14e7883394d5783c5ecea2723f94(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_441ab70336b8cf138794def7506deaa9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 100], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6a5770448924111a18dd88356c399321(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c5860002cd4776fbf03869697a41cb9a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f0ade45bd152ef82b588f2a11bdaab9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 32, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_432bc6d74660ffe150b7bb18a72891b2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_87fdaf18cdc8c15e26667373853ec63d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1728, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_41d527d62e185f65abb0ee3c1bfbf1f1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 4, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6672a4101d2c1bf5bf196ed91fc6113c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c69868eec40a5ad5dc2d6eb4fedf8513(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0e0a708a01ea26658a7494dbfc156383(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 960, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a35dfd75f4a39aec9d44b367a36edb83(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fc756a9190ee566e71ff2d981c920b58(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_32f7e7e456ac4fa76328f4c94180176e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 82, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b2dcd37d98e3adf79f9f85ea7f694dc4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1d1a1451341812987def2e844afb08fd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 2, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_892560361a7154478f5b2366eb05ea43(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5cb185b3b482fcf1e63a6baf58c67717(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1872, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d0eb36d2a7b10a53ddf6786bc2ba46c9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9a4bbba9f675abeabe36ab9163defc76(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2, 112, 224], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_77f52497a6de9a22ebc3d5a809e41670(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_31eec24c3fafd54628b30060097f3435(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d1eff69a71e3ffa184d13ff92f3df2ab(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 4, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_82dd2e51bf82f64805cdeb9e77f57b2e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ffba008e796ae2ad1f7d7fca90d15a0f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_01c50f499900b120da6225e338d2aa5d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1920, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_506dcb579ce2bd0471091f30183046ce(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_578658953af6c5334a305c1d756ab95f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 88, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_25f103fa3896da099d3cde17bf1dc78e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 224, 224], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a3bd1bafc24393f61078257ba8e253d7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_248b29acd43f66463b4bf3cd2bd304b2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 150, 150], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f2e4983afb6846aad56ac221ef11a43b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8cc70e99c5eb0598dfe49f580581078e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_842b6ae9eb48422206b3058acc979f12(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 152, 152], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_01f1d79d398e2cfc61a4d9da75324daf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_42db160513b08a34ac34c58fd9f4cdfe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9ff1465bf90f692f92fcdbb9f0609310(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c2dc45c893631346faf0c93366b70b07(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_467937c1b73b0a6445e3ed6e1e91f50b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_517c329316f36622f36ec2c73e504025(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 14, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4f5421ea7cc936dcbad01422103e791b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 960, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8cabf75de230bd48aadcb8135ae89fee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fb22ed800caf18e500816b81cb5a7da6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 320, 320], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9e6e8c696fb140f453fb8604690d225f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_895bf33b9928199543d8314581d69c80(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a6a02bcf79acd87c4a2d312c69458e8d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 88, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_20e3e436c0a2a41e09dca057c88978d3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_396e1479eb15e2e6034ce5d21e596c0c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_459f781711e3a8ec07a566bed8fc9870(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 38, 38], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_70e9da8c86903b09f59bb5b27e4c9293(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bcbd2bd27060e5444ea5c4872a8dff65(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e039cbf6ffd8b4b9597ef7e012ea5b2c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a36275c4e1ca7ee9b59d383a698ba621(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_be4e7ef61b88705310d92d4ae3c098fe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 640, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_963b499957681d183a90ee6fbcf07b4e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e0ef5e6f2b6f232144c21a5064e862d1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bdfecce40297b641425a981e6aa610fb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1792, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d746ac404df2cb95d7525b307d9e10ec(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d2edb31d7ed5044dc395a2ce8ae41db4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_152dac02533dcdd72854cac3edf3c57d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 12, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3f81715057e631b27e8769c1ecebc423(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 82, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_73a66f5a4072a54571f8e6b41c24a31b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 102, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_df260ae3a5b7b61ca1667b85607c9a67(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 12, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6547322fd2cf406a2d8f4d79f0d12efa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a9d9a15467cfdfd89cbb3d3fc8c28aaa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1440, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a15e488b34fc5df2fd08f2ab54403dfb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 12, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9d4af7c9a02c07253705daca1609ea05(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9ff4635bbd75d4d9fac9dcc46fc4bae0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ec29c855527613d9aa5cb61230da837f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 44, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0531163f843401361762cdccecb950eb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_adfbf8b3643a0b20cce605267bdb05fc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 4, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c3c5069945d6e6e01fc969e96a2de2bb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eb0cf3322ed5ca54d3ec6aba3fa1ed38(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 8, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_da28452afd87fa7ec6700e3a52e92bcc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_26add3720a47ff50de0328dcec26b585(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e1a2bde34d7432d425068fe9e4eca25b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1976f70b33b0bd4b60b0ccc3db188442(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 832, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7dc14e15b0cf7a31536b29ecaebbc806(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_40574a8c62f9d3b897f69456cf690e39(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 35, 35], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_87f7d7b8718088814721bc24351b1ad7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 138, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f3c8740f8eb41d61ae8d759be2f100d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 19, 19], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cd3b91bc227b5b65664dfc2688a3b672(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ceb994a879f8829a5efe1033d526d81f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 350, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7e32ab27fd7a674e8bdf0c4fc681e7e1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f0e3c2be8a5f1a675a0f7eb63f584142(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4998577f2715fb1654cdd7b9ad85a960(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c892c60e1007241eb02b17ef2d1c1ece(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f0eda39cb0cd71973c376984841f0139(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 128, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bd66dcd297483cb71457afafd752e6f1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 10, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dea9f847be393beab56b023670ca37aa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 128, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7ca719eb7eda5d71abaf08e1fe892374(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1044, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b2696b10e83c1d3c2a92b52e01f50ed6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 128, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad432a6b8fec317a29146d7760c1bc85(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2112, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_04c277ea268a9056176872ab5d1a5d4a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 19, 19], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_af48b53061b6c74277cd331b33b43783(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f1ffb74d8fc89de617e0bc961f4e6da(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 232, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_22236779444c94f02ab1a29f69c66e9a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e910b95a8213a95415372c2921f6d8cb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1792, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_80e042b16153a6a13000fc69bfd64db1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 64, 48], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_75f5ed0c9a39ff313160be27327a8985(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 224, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_625ea443ece00e0ece297543991b2b7f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_53dd2771c402a2715696773cf64d4723(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f1c461cb5f6f22728e44806e063a738c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_012624d96a179bea34276af76c94f5da(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0acfc461b9347a995634a111ab651e07(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2560, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a7c051871ccaee599d41bd3baf7a0dde(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 4, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_54af904919118db57b8a0801eea6f455(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1642fa8b2e044a5d3f3bf262d50d5508(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9819e83c80fad20383c50c93b943ba45(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2906dd3f3e0640042e9e677d7525ed36(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f636fa30a0565d79092b2b22dd6d384f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cc105e3dcf83ddef145c899763feb1e9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3163f9128c785d514dbf57aba22c95fd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e2bf442264d3c3537af1dd050f480205(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1920, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1908cae05be05cf11f8e07dc0f2507d0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d892ba2b1df01c09159a3b7835f549af(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_238f535c78e73773dc86e6d27b960b9c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9ea90a6b5fb2e8b263179eabec9990ff(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2c4b0279f4467672e4b0396db0c2bcdc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_474d841a3d7fd81c6525c72f0f53949e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 76, 76], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f8f872561986ee45e1ad7aed84c9510f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 320, 320], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2166fbdf0dc76caa9eaeb4e7f2a18035(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 164, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_acd1fa80420f1a6d3d273063bc1956d7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ce70511b552491bd0868bf96927138c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bd9785b1c71ce71addc5a0faecd777f4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a57c9796087bacba8e8c4e883f980882(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 164, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_47ca2c4ca14cb788357420b8389cdcbd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 25, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c7756dc33bcfe086f44d82256691cc50(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_59c4e0bc731681aacc4f2d01dffa1b58(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8c241c5d842a81b164e8cd441263071c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1248, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9b2c2d5669307fc6f4b29b51d01f95fd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_11b605be9747462b91e5f789e3a7c2ab(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 624, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0651a63728a76e450adae5c8c420ebc3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 728, 38, 38], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_70478f5bc84b794c41b5b155e2f71fa1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6f601b8e703e0031cc4699e666abcfeb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, None], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6dfff219623c2d00fe823701d0bb2d6b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4607809436989505002a1958a7354e68(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 208, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f76191763e5b0e9c94d9a20705e3d44(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 570, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_782b1acaa87910c071667b87c8ca249c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cf3bea7e81eba2f8803fd04ad3ecbdef(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7f6dbe06aa383090672c873099fd6dfa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7ebeea026c6e9dc157fb83b21e461b0b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b34827b700b57c2f272776970c550fa0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_73b2de52b3a25edbe06de17c3b6ab65a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, None], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0b1ae707c8b4ab10db8aec9c3823350f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_efc5e703d69a415622d3df8531a072f6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eba84ae045edc580a5322d6110ee9b6c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 75, 75], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6b4849f1d1bd3486ed197dbbd329b78d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4443cb96dab2b94b7b32f581252245ca(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f8f826ad0801bba2f1aa15a16f0af589(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1624ac6e972a6b37a9132e27deaa15ee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7757654bfe9b9d46a4cd48c8a31d3306(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4e1aa685230bf4542d74141d26c0a6c8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c3c82948e8ceddbaefb14f1f6e0adfe2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6ce9c0d4736e8154caf21aad5b0a853e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dd488a3ab73b95d9e1bdd5c6be0fe6a5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 100], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_86de788e6bb94bd4959b9b0ba3ac5b8d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_50ffa5edbcfdf476d9ac4dce58622b7b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d0ce1bc02a3fa4134e621d76919466ca(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6aa048fb1689e5cd251f916ed4cda099(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 17, 17], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7f76141c7f28531e80d93ce1df1964fc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 400, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_13c08fa84333383f04cb6725995513f2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6431429537b11f1f7cafb96123d13ff8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1472, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_86243190853738b96a63e9b23bce77ba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_346cb3eb7a1be8bbb2058808f9fe9edf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 4, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_814f4928eb7842b35504dbba4522c3f2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cc775f221a7a29440383ed934d6f629d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7e483b644adfab6a13faad290fd32bc8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1392, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_903c56f67d56a4bf30536590ac5a0891(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f924a2e0c96ef51562722729350c13a3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 8, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_78c4f2c3f90cca266e143cbe6eb5de63(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 840, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_20c4c08b1310eb3f64a0bced48ef615d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d611060c19d2b5cb27db87cc3b170ddf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_831f90d1946f5d88b73a11bf9e88126f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_659275afe5a1f942293ad5b944b482d6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b755acb08dfb84146f7673f238a48da2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e82b94524c25643933cc36c58a30e558(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_20ed86f8dadd07a4674aa0e32b32eae3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fd4d8fd92633f0918a445794c425b03f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 448, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_79a5d13472e01c086f5eba705a3482a5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_645d76d5c9cb991a81f41522b47a9679(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_26198720caabf032d5962371af520b5a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 4, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9289ce5059ee500d56412e66c3fb5acc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_680b0435e1e42580f10b70824ad2c173(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_68d960ab8f7b2fae2e15eee62fbd6493(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1376, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5881edd31c99b5b3ac47d160a72576b5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 448, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_14436615a0f1d12a9ec45300b59a5a48(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_322b006503f083c81d7cbf2702b856ce(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, None], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_df0a64ffe0bd41c2fc472b41150c17d5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b33da5cfb82e732a25322a60aa9f34e6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 244, 244], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_defe38e4814345693cd22be6063cb31f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 147, 147], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f010ac89e95313785804b533a6f28f66(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 56, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_02d2de9f3d7b29bc3390442d83ae46a2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1776, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_61a8af54b96b78bb67ab9a5783933e1e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_07b63dcebc8a727bf977076bbf0b3bc0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 200, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c316a32894e7c28555759959a17c9869(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b38f647774f7456158edb7bbe2614217(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 175, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c347cb6185eb6b1a8768caffbd4c20b9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 436, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b9c38b4e2b00f747c31761d68272255f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 32, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2f3626e73caf1cf25b8b075ab4aeac6d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 244, 244], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e8b98632cfbae41426fc220f445e6486(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1e650a105736cfce972de59b79c9de2e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 162, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad4300dd0dd7c1655f8157362f7a3fce(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 1, 26], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6a26da77a4e4fb14b1a41ff1868ad0c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f350b4c2e9ee3864e1625b6b59db562b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a43b16ef5c6d1f041d67efa1df4ef913(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f9d88b6dda9e692e5c8155ab0c881f8c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cfd61e99ed9054a30920fda1d94f7475(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_067694d2f6a1153403fd29036112d116(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 35, 35], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0988a78b8dc830e8cc8cd52428e02408(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_829e10c433363b93c63afda25e46f6be(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3caf0ad33a9071ab1f2803f58fa730cc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 149, 149], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_98d6280b617eb479b02b2ca845c2b604(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 300, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_13672e74ed748a972307260bf7e23292(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 152, 152], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_58210f89e6aa313e3e1f81e847ac0802(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 28, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_57546f95726f7922fd067f54dd9b651f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 2, 27], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f6b9111cef8062043c1cb3d4a1b73a7f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_67829caec9fb9189d7dae66df909adaa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 38, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_795d9cc3f8e663f9f242086e329c6e31(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a3e21fe52e3c2c2a118074fc71e41be9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cc5107c08852c64c8bad5e05415fdd87(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 2, 4], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bfb83320a839d9c940a6fea84319cafc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 636, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_758efcb799911c6f0c6d536cf461001a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_534bd8605a1bc9db4c21abd12de320b4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 4, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_16a6edfa5b6409b415d09ad05808ddc5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fa5e018a7eef8e787a8176b3d388e876(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_07c8ef96c5099df647626e5400c6e7b9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1901bf125bb1a77e4198b2c97ca6df6b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e09aa6a49354526ead685eacadee9c30(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_75fe4f0c8a04a0ae02b307b58d9938d7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 50, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_899d17da3b944425f83c54ff28b71924(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 906, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_968feae99ca59e88c788c7ab6dddb91d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f89ec4b18ab354fc7c6cf31b335ec0d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 35, 35], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a7f7776619c52925ca8dffec3fc3eba0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2518c43ed17c92ba61515166afdb2e58(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a66c9ea1116f694a0225c128cd0ce6a3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 320, 320], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_480a729c402cc1c9b50f2fa16dc802a3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_970c9da2f86e92b99b203dec83780b08(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_16c5998ca4ababc65fb3747fe0c754f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c7d2d72869661ab929394d1ec970ae31(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ba9b51392051e1265ee4a8d918f6e698(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1056, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dbd626ea8f6cd8bc2426556814b8ba10(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0a83b1b40261c8df6f7d4e11860b3fb7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_856ad6cbb55e3d24b41245c4b9560a41(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0d4ca7005d2aaa8cc3db49df521f1e39(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1968, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9b1661a5cd671a8b70b1304b03ebf241(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e5fdb4037aea50a357085c04275bec90(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 17, 17], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e9100befd923b9387a58d311c9f1d4af(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 2, 2], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c35518f3334794ee130ef13ab20e2243(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 960, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_626f3382699b88b9e7045ebd7392ddf0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 832, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b606cf513c8f64479a9b2e2cbb83813c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e0f6ac9b4958357415fc8b72ec5ef4ad(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 1, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_59a74a1aa3538b67792395a662b46c96(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a5751d1bddadb2e498a869c2225f2f15(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0384eb41ea893946136c8462fa41d819(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1ace33d8bf5c0141d2ececd56c48ff1f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5ebf70de1703320a6f0fdaa614413bda(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_57a31e312c2e0e2f69d658b889ce8c6e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 61, 61], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_056c5654ab2a0e7ab0fd6aaeeb9464ae(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 185, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_43c0695f75d5141303c5c0768d82a893(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d4400d0065e88fba0edb22f813447e7e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e60b25b6ed143be25e842d03da8550fd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 304, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ca44f8eeb684e8da550a314b2e180d9f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_44202e3624021085343f21e059d11524(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 92, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[92], dtype='float32'),
            paddle.static.InputSpec(shape=[92], dtype='float32'),
            paddle.static.InputSpec(shape=[92], dtype='float32'),
            paddle.static.InputSpec(shape=[92], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_714efb99a8ff6570f582a86d8c671fbc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 122, 122], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c5edb2f18c8e47a8bf1b6d78f784956f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 150, 150], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e8d0c135d4805cf056ec128baf6021b6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 504, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_454d1a24aa37124b5dc12d58d14943d6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f01e32f14cd76fc4e89468d4c13603c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d7596fea3bbd1ad27288e9ee160c14c4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1728, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bdd537d775a796d7f54474406ed58350(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1008, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8c13024078fbf85f169eb5a282e12cc1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_540325bcccbcf7dc27dcd00ff9f304dc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 35, 35], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_086aa9ecd36198eb2f410fb51ec5d3c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 38, 38], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_45d43e72ade43b488209dfb3f3e161b1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 2, 27], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bec0e815d3128b0780b7221166cacd9c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 20, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_609ec7bf2a07fd26a1254d491f9e4e1e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 262, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f6f835efd8494d188b7d808e1bf6ed16(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 106, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_206d8378e199fbf2586f83ad79a8b719(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 82, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8947a0ba9763d29f6df3181faa00e811(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a37223a4fd84d051cac78ea33cd6f506(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_def4008dc88fc8e348ea94877e806f89(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f323dba5e9c868990c2fd1d31f924e8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 32, 256, 512], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f966f37784637dd7f934a44ee272a76d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1776, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0fc98f3322a6295ddc05c1843662f7d0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 1, 26], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fcd0ef96fde17bfe36d316834ad81acf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a294fe275f6cd8d1f3c173f6f1207c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_06188808edfa48caa40aa1fad819536e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f79d05a5e1260dd3828d3c4e9cb9c6e2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 24, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_db9f06a06e5ec263aa27a6578d0add80(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 200, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_54a5f985cd6257bfcb5e076f9e76e1e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 88, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_92831198afc76b390f9884e9aaa79e85(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1920, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0e7108aa4d5e6ec1d2977be007d1d6e5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bb759b99bdfcb9011ade436d7f22edac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 432, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_00c9d6d672498715a61d2ae2a6f98c7d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 88, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a21a55bb7927674d19430ed6366df76(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_42b2574f34c443a4523c2ee045cbc4df(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0924953f854a529bce7b5983b306f26a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_09d7974ad6e02321106838df9e145c82(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b88bdd4822487797778819bf35905a09(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_72b552375f023e383d17aa69b279f370(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_504102c0c322ef1243f795b3deabb28e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ec15b4f365883c06a3a1e295ec9f5f69(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f87c021f4db14462734097720e6b742(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 208, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c3b4a6d65f831f3e96d102564a9c4de6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f8079b1546954a0de68d082173b56fe1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 640, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ab7a00ca20552836652977003ef332fe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ca3414dfe7bb7b85fd0ac3ef4a8fd707(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0243fce825de868f4efe321aa58a736b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_22078a374c6d5d4d8c3827de97839e41(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 88, 61, 61], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_972b3a4697e46b3ca787702f45ce6c14(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 61, 61], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_07277bba8546a096d2aa0bb67f8b406d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 132, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_89fc545d308c9959a47926c571a761f1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4fd88af2ada1b12474fa31f895c5a800(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_60fafc0aedd1d0ef03e639ab59eed917(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ba70d8ff83ade7d49521c28915f714e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a001f631eb3482fb4092b22194a111fd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ee746a5623fda017a4125de9688f8805(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9d0fff58868f807bae16be1c5473db29(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4cec5627b665aa86e36d6bb0743d780c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_03076558df529649942a6626db697bc8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1200, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6f8d2811b8281ef9cc6d7e408dbcf600(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_39d2d19b7d303851549f0d5fdbc4f59a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1680, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e54c9b446b4ef105808a049283955adf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7de4059ee7757ed73afab8a6a75785f0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1632, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_189ca6ed72ffb5645192b7ab85ce4e09(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_83391cfece5cd7542dbfff9afea65200(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 864, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_723b30814165a2832e63237aa12019f6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 320, 320], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6e736b36f0f4b87a9f4b5897e00b971d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cd2bb7faa7685a1a88ad514922fbdfbf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 176, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2c8dbe695d486f315b97329be5e529ba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1632, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f1e8941418a2f8bb9b0465e9d839d2ef(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 366, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_842b58828ce8b0b3fea279b71a56f3f1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_41bee5f0c71d054ae209ee5ad94286ec(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 106, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cb4fd531d3a3e93a949a1ba0d55bd5d1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_71b303d09dce0592b239a8d2f459e33c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d610cbf6adb1ac86386c4dbe83923177(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 624, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9d25370aca2ae679810e076223d2c19d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 4, 512, 1024], dtype='float16'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_99af99761d0b8f2f67a9a4bce2b0398d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dfc6beecf20948b2d428320a8ff22420(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 10, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ba222da548968190858df4cf68bc622e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f1a082db6ea56cc6f834213b72feb187(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a4d9198f3e3167489c2e0fb8793391b3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_de5634a75203069b0dbf3fbeeff796df(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_efcdcd8cb72d0add1f8e32f942773658(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 140, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a0110df3ba1d7944208519d64614db38(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c4d19e8d910b39d7d8899b665bb1f6a7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 702, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a06f1475253de64e3094a1f5e5fc06e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 10, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e58f60cd2c596281532515892b76457b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3f59838f0a9cd17f718200550a2bdf3a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 102, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a65a037fa38df36bc9206503c5bc49ed(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 256, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b4b14c7ab1450b3a7d537c3eba9ce1d5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e61071eff7204eee14630444e919506c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_58335340d600825583357bb7285fdaf2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 5, 5], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b0a4b7c90da363fc3fd7250aa8d5641d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 256, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d685f2531a4e8e1ab9346744dc305143(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c3d447776905d934c8118eef09940f2f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 100, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[100], dtype='float32'),
            paddle.static.InputSpec(shape=[100], dtype='float32'),
            paddle.static.InputSpec(shape=[100], dtype='float32'),
            paddle.static.InputSpec(shape=[100], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3b3f6ba1cde7cd5208a94f870259742a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 53, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_43cbbb0e3509424a4b9cc93860dc6aaf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cc82439e4f1db73811bcc5c3210c2e04(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 200, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c0e228c07c5e1b9d3db71607a0a58d57(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 350, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9864e859e39ce967be5cf9baf8b4d3e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 366, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c22e309c5201e4b0852ec0fa242bab74(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c47b8a59da9ba8b283ceaee01f4f4e4f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 7, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4139c4405ffe0867954eac812c78a849(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_af156719f364b1bbcc99a25272c7d11e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6c5ec2ea31615061027a4c86396cf6be(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 960, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3ff41849c1f594b322c9c2499c6faf7a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_467e0b54e8b3ce1fe94e1c2291ee10b1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_67484094503db28bd97a3dc60c1ca3e2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 608, 608], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_541c5e8fce76898fc2f665d66f8d8d58(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2016, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ccfefb6a7a202d41a9529e931fb5938e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1408, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8e8f0133986ffb4135513dfd7b141efb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 44, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_038d2626f2f1364e422ac477d1de94c8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 350, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dea80a9adc2725dd7126b65cfeff8f4a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_950c1cc2a634fed8a379cd4dfa33615a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_07db224a34ba25292369d1e9d0d61db2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, None], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fe2cbcb53d2ab5f6bbc138deb132c192(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_644964eb153e3db1922193903e2afb1f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3b17e66f7026f48e8e8b7b11d641c130(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c2219a45e63d1019663f2e68962d68dc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_40bdf43a585f465081df6be985c57b1c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 152, 152], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f3a752b0548f4aad8400ccaaa7d67719(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1632, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_25dec4d33c0318100e2218ef5aa8a01b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 1, 2], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e30289a77f655e48c492be98ebf63d03(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ae5b13e0e939198ab229111bdbeea01b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 304, 304], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6e7ff31e0b879dcba9b6ebba828ef520(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 972, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6b99617946dbcbd795bd7875ad9ea3de(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 122, 122], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad8c8da3857e6bf3898e178beaea5391(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 228, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b9f67ea0ac53699cff422db5b35f4e7f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6275e405292e7be448c4e45a93e79d9b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 224, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_73ad91d23619060c1394e490a731523b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 26], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_736bd2913629b3dcf106e7ca0229d249(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 4, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3254ec6234d29c6b79ccc83ef1b64d71(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_33f3e60285f76e0d85b8df1938b63567(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 175, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c1cd91d7c1f51f7aaf5ae5a61587df98(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b2209e036a8d188c0ae50a19c154061e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7900aa0afd93247c2471f0ee5a44e050(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1104, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_29cce43550466adc735962c12bec1544(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4662f7dbf1bfba37b6e568739b9af629(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 208, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0e05ff9dec9e57dda66ca079eab7ed32(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_45037d8ea16b2d2e3487e5b2fae77fa2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 4, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e73dbcb3579ad48eae5a945696942094(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_636654b5ec89cb97d90c57cd6bdfefee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_37e481209dfa75f478bb7af2b6546dc0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 608, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bb603ef7e789ce3a3c734fe1a71e5586(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ab8d8cc91b147b04c1301f363ffac4e9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a8a7335ee76d483408684aa7a0d340cd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 16, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_976b91526a5dc2f3a9e97c4a16e0b14a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d25e7427dd6d57504ddd98f54354eff3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 8, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9578368f12d97c0d445fae3606a50612(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_84035fb2f3979b4ba0f62b4fa8c68bf3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1344, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bc261f501c79d13332c58d7ddd9be9c6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-06'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4e0c57e49b7bb71ad7507c34d30b87c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 236, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a235d6f6c02c5b2da51e817edea4be49(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 624, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a22ef7c70c72a5961e472030518977ff(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a39b08a6d0b8c8bb33c0aae736ac2e0f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_386d24c874bfe130a74dc32f974e8743(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eb3355eb5b3a1d649b681e628ff1b3bc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a9f5f909b34ff4c21dedcc1fa91a151a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 150, 150], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_96a1b2b0a9e248c70d83dee2efbe4820(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 4, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_79abae8720848df294fc716c787f83ee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_15ee5177d1b96df05331b81cd2a8b8d0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_463237fde79324bae936444e62ab79a1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 70, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_debd32127a4e41e44c2010993e6a022e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 4], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5e3f12584c8ba623b5a6686df679b43a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 95, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7e952e52418ffc8a014ade38876a7a85(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 352, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4e126e17b02f24fa0aeed65cc485aa65(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_98f6af822451e6d8c0c9ff8cce770453(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f67d1a33739513a1ccf71884ffd49a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 28, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1442575d9d6d8d21262f054611450fa7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 256, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5e737435984eade283e467402a97dbae(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_148a3483c0f7584668318beaeeb76022(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 270, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_488b2e1de9d2ce6934a4a2197e547553(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 52, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_163c4646273b5e8bce82af6b7496fc22(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280], dtype='float16'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6f5027e141674adef965d45913bf3e82(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 96, 96], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9ddf65cc5820d79f0cf44cf133b6ab42(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 528, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_95f3a13b0cc6136f39a3708c17c63e9c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3d69c22bbe12a85cc0b2687fa479e150(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 75, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3b849076f67f776bce6a44e2f283a361(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 16, 512, 1024], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_87b6cbb642368169a0a6a927e0de77b2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1600, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_08a77721120b46e1693801de3678d379(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 117, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_affec5a07fd82d74e9335cf58367bd07(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 70, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d39e7200e9bf7d28cc6a48a96099dc48(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 832, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_672672e9aaa178cbcfa2285d1b93598e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6d9296fab21040021a8756f2db62936e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 48, 48], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7be5f0cb427996b075b93f0c99ba8f58(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad8305bcdf7029aba50842d7fed98e6e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b79a03b9889171b06565fd33ddcec551(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_361174189e20d55ed399f9d9a385b3e5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 117, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e6bbce1dac03ffdcfed09c99e5d56cb5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1312, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fff5f20a5fd67f2fd593178094d7ab30(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 2, 27], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c1d8828d7848d83703110219cca6a1f7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8014a0a060c0e4e6a65b292adf06a740(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2f0e1860449b1b7ebd00d0c0044deec2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c1bd354b6b2279703032a8552648a54f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2f6636f0103abee5641ac26ee4dd8bcc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 544, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_12c379cc113a403f53a4caf467d6d562(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d092b5f6fce32e2a8e87b36f36408ab1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5ed73f86ee2dc70f7d877855c00607d3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 4, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f6df65240ac8925a89299b808655ce54(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, None], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e1010f71f59d34d315de368d1204f260(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 2, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_43f623d01e33e4dd3ada01998e069b88(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 17, 17], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c96ef0ab05eec9f0e73cc37e470d0097(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6ef1b1d00a9af60b2d7ce60ffa060a36(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1584, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b96bb580d591d5c3b46ed9cd4fc63c4d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 24, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1e09db09cad87ecfbedcedd6cddcd4d3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 75, 75], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b4c0fe2fca612284865477be8e7df52a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e14c44bb7fb8421d2f6ff9de06993154(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d47d6ea8c05fac573da3b91bfaffb8ca(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 704, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_75b1be54a495016f7a339c8c158ba5c3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0bd5599424b42dedca8b043b614148ef(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2112, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c25a8b8f243c9cfd1cd6059f193a96d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 35, 35], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dadc711cf95ce5c6825bd88d9a162838(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1120, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7c7e6f5bbda6509b41494959f924cae4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5044464335106b9f394e8b42a34c2418(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c085bbf49f874f3d448e2e9fa624208b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_92329144f48a3e23c44fb1c13743ae94(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 228, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f3218f271032194f87b942bbcdc87929(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_edbe30540ec177cce554ce48aac27083(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0a6fd276aa8102f6d3d02f53851ec6b3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 28, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ce43855413328140ee22b283997d87ec(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 232, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b3a9c28ba2de97e4bf731fe5fcce0b52(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1104, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c7b2e018e3bc863a28446ac27d7cdf15(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 416, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_307ff8dfcbd3f3dc711b97745f106617(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8bb03deac1cb8964dbb9828bfc08fab8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1200, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_05ccffff69f6576dc66e7c98f18af5b9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 26], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cdeac6b9a7d9d6d44b3055add64d33e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 304, 304], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_90d324a1f7a05e6c93e4c5157ae1ba91(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2763dff4561e1c891db3c3a2068e1058(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_19bbd1b0980169ab4c479175a10906d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1200, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_455292ca3cef8e477c828481e3a2100d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4847d91611badf5406733cc49e5f50e5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_acfdc8595c1523b3ad35de36da9e8119(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0f48c60a5de0a31df82507672ae23681(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4f7322eae16189652b7ed5899e4a5aa0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2064, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e78aae5a583b14bbbcf1ce11fd4a7365(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4e1519cd585b63180b56b6e0fc574dc2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8db2c8ea7e3b8c1d864304caad894b96(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 1, 26], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5ea9bcc6c5efaff0e969963c1fbd5d7f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_195a3ff1f6a14a915f756aa330dc479e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 740, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2b078752569198765df0efc82d9b357e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 84, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_203d377c673ee4774c67da3b7df226d8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 702, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6fa389afc2ca633de225c3e7427fda16(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 208, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6872978f377bda77c1b49c5e0bf899f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dc971ad76fd1055194fa611ddca13446(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 64, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d99a1335c90ba60b4aafbcd7298b901a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 82, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_34ebfe9e21ba9c66a500d45fd7f22e41(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b260b97a7dd4463e7f763f97d23bbb0c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9a1a7d5ba2e42c33e716ce604cfade5b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 96, 96], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cb59aa2c490dbef566260bca50a12cd0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_70aba72961631a21282cfb34425d9745(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d529bf5776f65ea06d894b0b3e3899d7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1664, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_02e3dc0424af03931aaf0480e7113fca(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4ae532ac2d899558c963968af9e9c8cd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 304, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a5e52bc3285cb8166af50a2ee45803c4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 81, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_32eaba6084a13cca4928f97bea910403(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 24, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bc61aa3e884868c6d7ea2b51aefc111f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 47, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_df6f636f40813b72b55d61f3fe6a52b4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1776, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c03fa0a42d9b996e79b755dcd7c02fac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1632, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c85ba9321ecc7a325e149f98792a3fe7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 174, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2413fec6a4581a7335100a8ec156542a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 132, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_20e7dca2d21b620ebcea6442292033d5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8264cd0ed09d33c4efb6b54b04181ed2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 58, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_264a1ceeb5ec3adbfe3fddce6195166a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4461f75cf5c1fb8a609859991616732f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 19, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bb7bdcfc57af9f15c0c2fa0f429ffa87(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ac9c506333fd23e58f317f5b39f87cf8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_358243a6c0028056dc5741d7b08c7e4c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 184, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f159a255d63dc15c3c3578e376ed7fba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b94beeb62600b02926793987f4e3ec6c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_458ff5cc93c2853052d9f3baad6cac72(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1664, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7eb422b6e701c3e8483c93d320397fb9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3eba81a3bc8b921c66b1715a61894576(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e418ce2e68184e8ffa573e948b40ac86(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 864, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9ea508b07f7b60324874cbe201e660d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 185, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6cf4c086dbf6fa4fa6d2360431a57ca3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 432, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_29630784f95f784bcc017bebd7e94415(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 608, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aa301bbd5a1e416e0b921d7e268a8e4a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bb18f953b4e05538bdade2c7611d012d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dfaf72fe729072533518bfce8d3ee0f6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 49], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6b466436acd8abbe30b6123b8d9f3da3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fa2b8ac4c3930469a8943e54c818190c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_35aac71abde1014925ad154485383613(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f7667cbaf3d2a84b08da0bcbc1ff90d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d4c980e711c4e8864589d6bba9225820(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7a41d4c7f9022be3baa88964b959dfb3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 224, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9bab6ffa8f76d6e5d8dd3c9144454c5c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5e860e3a3156f27bbc563cb769888402(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1824, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c879500ef43c9583e591cf88058373fe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b4aace2371eeae83ccd23970301645e6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 122, 122], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e2c7a5f2b9b5f2b02274d479cab4c5e4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 70, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bafe13e1cc66d01a31631a32bf01915c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5b62c1eb1eea18fc53875f8c7b26385c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_adbe90a6483c6d7a731fd2e1e9398487(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1488, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3bc7ae257a778d0de8722c35efd20e3a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_61880af5b4bf23d7514c17811f77f734(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a4da64c4daa8327aeab4500c67f4a858(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 48, 48], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_84021c0cf0b138b0d1fa628478787faa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5c4c45fe10373a4887651be55e63693a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1184, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a90ad7972227a94e7fe7981f2ebabe70(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_76d92055e5e8b62b69ae9755a09f3f70(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1296, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5144b12e6f28571ed44049cf5b11694a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b242da51ea0bf8a59aa10b12507e499e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0b8f44bc97a356e5c42c3042b7bd1505(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2423ebd1101a36394ab1b611cdd61dc9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_09c329535e4dfc090393750161f37a90(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1824, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c0bdbfe9de0855b84834472a3b76ddfc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1000], dtype='float16'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_815a31a17cf07eefa6f7f921cee037bd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 175, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0552be49233cc4ef96bce93ad608f02d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 47, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b192f3bed431296b739a7e68dd3cf5b3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 8, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9d8cb553c35dbdf8a43d27661cc9fc0c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_82af7e822b695e09d38e6ea79a3bf084(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_461425de9bf2a33cced22dafb4c617e0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1b872f8572aae335a7c38cb1cc0c210f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_963e64b1c5f9da6455d21443ca2e14b1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a411859b7a06d507d70073643073f19(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 19, 19], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ecc600a71fc16850cf7a760f447c83f3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_16ee940437a12012167b722195d2b167(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 740, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_43a85f8db69f9254d3acc022ca1f59a2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 32, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7bac455786c9c12769ec77051cc472e1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_80088b0ee55d4d921d63acab21fdb431(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1952, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_954e05d384686e7f64edaf73adcd531e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f8f7211854d3fd37264b6b4fa71ea8e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 71, 71], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2dd377f23ba6d8724508ee0503e7e7c0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1392, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_22d7cb24c63d8d8d9eb4cb0b9a2aa78d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 816, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_21a16052f5089a16cac936325c38f956(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_19eafe3a5d3f9df80de4387e50945935(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2112, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8c5167bc8aa6796e64bc2519c4f3a09c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bc4581fc47f74a192165f23667354af1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dbba9c6fdc045c77d11e5ce051888a27(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_46d4a4697665df3cdd855c4e6229b962(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 8, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_344329caff1bca5104933b5553a8f640(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 38, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ed4c9c36390ae7eb199866066fe8c806(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1216, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_376eb4e64c16e7dc177259bc5f5ae981(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9aaffcd1723b899daa7ac8d9d877163f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 24, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_10a6e0f122747cdb1636ff08d1dba959(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_92c0ba69f1d3da47805e4633b3a97d29(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a69be4e44c5f356df59147f61e8e82e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a78c095564d577f66b6b6211bede521c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 16, 128, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4cc0a0f67bde72d35892dd309ed5aa8a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a64af683a2d6deaf7a7b974a51c6ca37(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_54fcde5cf795d819b395aaf19da10d8f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1872, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_935880d6a6dcada2471b24b5bbf2820c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 10, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b7d925347799e3b4b1f0dcf60a0ba371(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_224a201907eed15b1de5c8ae3bed5fc2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_45e50a0de819dadc779430c6014fe33b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fc855164fbc439186f3dc281de66f87c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a5874bd6cd476b15d682e91765085065(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 96, 96], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_138aa3c869fc81b18cd947421698b73a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 320, 320], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5548a634096bdea06c688a80376ff53e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 35, 35], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_70f635c2e3f70bc2b9b030d020760420(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 58, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bafe67d17663c70b06eb0ebcf0421bd9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1968, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ecff041ed4f2865598c13f6893c95147(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 320, 320], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a0459a49b41db6a39edb87eaa20aa89c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 176, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4192dcd65a629e7e519714b3d3047b86(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 118, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_140fff394b9bab4c3eb8c208a8bd719b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 840, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2c016dd7817b554333a16cd34b1a9c4d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 88, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9ef2f6d566b5486e9623c562160de48a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1296, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2bb8c3500ffce4b5f421ae1a79c4e03f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 138, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_20ebcae51176c55d4c98ad38f10d90a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f9f6ec79ccff23b64aeae5231be51af2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 304, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8eb5dbf52e9523fb2b2f2fcfe9efaf89(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a918669557d320cf0b2c025803eb766b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 100], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4411473bb05eeca76341b499dfcf4c73(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_76819a13296acb267a71536c7d3a5599(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bd8359a9fa2df57fc823821810233f7b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1984, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8272a9122063f5942bfd66068ee30970(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 704, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_621e043c1b9c17d3e7dba8409765afa5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 912, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b182fcdc516e36875fcde1c5e04c6870(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 4, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d817aba74df7b35ce1e36288368ef6b4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1440, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_69ec786644c8aff47383c886f382f69b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 1, 2], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_de25d736449bbdfaf418d14f121fd0ec(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4a3780e143c6593be6a429686d661bb2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 4, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f19d743a92f26dc8cbc59531e4a073fd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_282105b8a4b9005639370dd9fb1097c9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, None], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_696637a7fa2dd68d58407021599fbc16(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 24, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_67a3ffd7441e4f21b97d12c4ee68c34a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2016, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_18499de8d9e0603581052a3660719c44(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_11949321c9995b8a52b792790afc1e4b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 236, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cac9f33a1d94791f207118c5166b9344(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1696, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_26858e4bcc071dbb21f5b46f3b0410fd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 24, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_367efcfceb41a4eac561294e555d7a26(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 208, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3fb28a18176d8047772168f049a5f096(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_17467ab67ef840c508175cd1d2686d33(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 624, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f6e78da54ca3a40268fa01c537e98ffc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b5f3fe04440aef0974d3c167829a2095(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 816, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9e5a434e12da2696a9f9d868b9f45477(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 53, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b24b502d328b553d9ef8b792d72b4b61(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cf4191e91099a34af466b305863c956e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_45cb315e383525e33a9df82784065670(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 26, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d7b7f624e1be1fb0d2bd47d575068cac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7896920255674cc01e20a46599623ade(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_049f5fe343bd1b155b9420ba95b6c506(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_16fd19de33b48b46334b998f8ea7abd2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b22610661c3aba4c91d66f9c02532a06(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6ec6c331c3423d959a4fcd62b6abf104(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1200, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7aa782216c8bde1b95745d01bda60367(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 27, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b2a27438a910061f2374bc6ff571b104(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1952, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_92b620aef5acafad4c0d157ef777b660(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 8, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b0beaf0879818191f4dbac2d4c9de54e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 832, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dfd191ac4d5b9dbeaa0c4a7641e92bf5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1008, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7e323d9cc4d140b84a43a2e2bc7c76b5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 175, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_024d3110644a58f47c54fe0c5e6f8dff(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_92d6fd2dc40fd765a0c08202e2f8e538(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 728, 38, 38], dtype='float16'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5b3c8fc32ea41d5175a58db71a369fc3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3066684247f0145c25f963b02c805430(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1716243e54b0b8be76f839f145a0844c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e1fa640922dffe9ebdf7932ff113bf51(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_117f8489baa6e501d70eb4bff85cf1d9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 4, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_589998bbd81988b3e1d94e3b20397c1b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d45ea446e0bee54a250317f48b43b7e5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 262, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e878490e40f031cbdc6c269e2f5bef62(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e528e1003f083ad06760d210da39d461(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_484b9a7cf89e7e836bef6dfb32fc77f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2af563fefd23155cf2ef7a456335678a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bf10236c687704c2c9a551d17669e0f6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ba5d7d0fced3c68f4e0a7a428e5b7ba9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e0186a6004e5dbd78c27b1d461e918a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 87, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0866e64952cf5cd57c2696e7aa5e52bf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 19, 19], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_16756718fec4c09e92352396baa2db01(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1920, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cfb0a2c14de50deca6e43948761b253b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_beab501839800f2f2cd590f0c425f144(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ec58cf47a015169c640dba668a5ac0d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 4, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ca1c2938c7fadc96b0dc8dfa9209c1c9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 82, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ee6a11d23b5e90b971cddd7859cb3f18(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 350, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b09ad1b67d807cc3d7f03b035f0d8c90(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1728, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7ceff3051ce8ffe6a3af6962933f1156(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 24, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d1427fd56e8a37dcd84075fe241cdac1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 350, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_21ef9c97a89964f4ef1bc3e4fd9758f8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_019561d773e1092e6a64ee236ce8da12(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 304, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_354d921a3759c2556cc892a56f9f4f21(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 224, 224], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7ff7fed0f17c0351313ecfc46dc48cb3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 48, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_96750a0f5d1bba3bce0b9b30195dd6a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b5932dbf13639be2f7d7c2f7e01d89fe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b1da824bbe8be5568b51258a56eb234e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1056, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1c44d044048a874d2e962980a95e10bb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1856, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6fdd1bc8e0e47d357c9681e3afc3aa73(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2016, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4d152c44fc2b0626740a6b54eee6520a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 234, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aadc1f2d06d73f2d1a2e0c0c53deebd9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 300, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f9d1da8241b988564b20c5f52acfb12(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5cf4bac0db298058a7de96a85d35606e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 168, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d7f6d683fd92f78f719a0cf8725cd79b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 200, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a4cf30a4d5b9ad84515e68b21657b155(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2efda438036f5a8927b64a011fdf01af(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_40567c7d44448ac41e165254a7ef9497(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 96, 96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0d6acac54a26f9a7b8028cc698d0c0d7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 147, 147], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aaee242e34764c026caea234b9e37583(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 48, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad7841ec75393e1bda8df1d78b1bb062(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_275d357f63fd1ce0ac4dca3ad5d569df(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 410, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5b03cf60275a890056ef00c1c753d978(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fedc7d2ba8af28a5f2c30f5b54772f7f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 61, 61], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1be5aad69366d175e7b219fd7a176437(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_745afe881745275c03bbd782f2897adb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 608, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_faca234a3b0e486e94372fd901fbd331(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c892f8255afcce5721399d8970d0c97d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_51f2ac2cd50c5d1639b2f39112e37803(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4805c3dc30ed25876b62bc7c6d061d59(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 832, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_72b5909584deb756eebecd96ec2248ba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7870053f6ad183e47186933ccabb9856(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1728, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_45f7eefe39f0cdc7e697e422c7996796(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 366, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_462e07d96819e4c7122493866f197baa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c0eb1e58665b6937952e19232a64ffa1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_69864754ba1223a990da32a7ad8aec62(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f01a7f175598123e3bf7903757624d6d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 4, 512, 1024], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_75c37502a040ce81a897458d73328af1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_26c1b854eb872ba56ebedf9793de3768(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7185d170951224540264a15c38f820bd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 162, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bf4172b5e8d85da0334345b3af72fd78(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 48, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_294c3e3754c446ce8ef00acc72e1fe52(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bf527dc1fcf1a6fd8a5f4eaf7405ac3e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 122, 122], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4eeff78ab9f44a29f6819ea991ca46eb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 350, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a92fd16fd9e3b064b244d6b28a1a5996(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 1, 26], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_073057315402ed17b5b67313b7b57236(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0c2f69248d988e152e721152baaefc6f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c7ec793f5fa22ce0ec1b09f6e26f303d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 320, 320], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e10ff6dc7d9b5018dce6730c897ea458(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 174, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a61a6fdaddd8ab64e00f90b107930627(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5544828a7f181373d095ef45852af1a0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2112, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f96500f1380943bc67b1f56886e95fb3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dbd9633630d82736e0e78f1238847b14(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1488, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3ff10796688edddd66d59da0c3e0da2e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 228, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fb39e8e283e78cd77af02a9e175f5e0f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 64, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_43c844305a33618763210ee186f27102(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-06'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f0310f299a03f53c6ac5e812c44939f7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1440, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8756cd79d20184e2edd37a3b21596ae0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 61, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6ba1fd2d16db2400e955308f53f3c0e0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1344, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d4c51ec0a78417bef59b68da57c0db62(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_011b6c68b397738302a4af50e26ee9a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dd1b6274f2c1be924839d5d12c6206af(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2432, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7fc0ea8eca85f35aafbd5d37c26a8733(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 432, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_864143a2abfd0f3d4c024275d30b97e4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1056, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3e2f62cc2e8f268471a9d1db21d2dad5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cb83104a20b06dc4b6ef79a0d2e04cc1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1504, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_be55e6b7bc3740624a173e767643bfa0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 17, 17], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8289b332e10e28c5a84932d651624d7d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 118, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f70d253a8c85f173170351faade870e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 8, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eb54a6198d0d098b5df559ad16cf3d3e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1872, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2ade1c8c123212d0b4da7fdef1129c38(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 912, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9199062195c29090c0af0cba91381c55(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_78b820bce8ae8b83dfbfd35bf290faf9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 38, 38], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f987c0d4fff88db50576a915fbddc61(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2064, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b6a497dbe1ffae0c9659fed8feba17d2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 960, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_001883c96c6f4644672acdea4d262848(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 100], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_379943e81a6a4c6bf2afc9dd9ce81850(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f8dc552edc4be255e46b47d76908b345(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3f42d5a9dbdc022e1c99000b5a383ec2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 436, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8f9a9f3e7b40fc374b85b405c3aff53a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 71, 71], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e24cbc95fb090ef9681d5e1937fd754f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 800, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f4a3dfadd1b3aaddc906e16f8f00c76d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 228, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ef23543cccc6d8990591be5a7ae61303(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 19, 19], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dcbac1292344de20ab0353af24353e0d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9e98ccad392217803a8c7c0fa7e146a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2208, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_54e8bf0baf56a0e079ef4d3c0a37f1bf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e85aaae890c2775c3c0da038a97fc088(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1104, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_458b69cc397b881816c00af070304cd6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1824, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a70b972739753d83ff305a95acc22b14(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 2, 2], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f38ab147038e4d8fcfdba47ca449d86c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_42c7fa3df3c9ea87887339f90ff4d1e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1056, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f53ebda9bb5560de5f9349dfa1388398(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 17, 17], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d2f89acc798a3450b177fda556d9c640(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 64, 48], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_966af676a02e7234598162cc353f5a0c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c7e92a9b76af9d3c0da8e14097506bd7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9ab99cbf2437e4d5fd27a05ac0ad88a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 636, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_677631133c87b517e069a44dc621d676(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2064, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_318c00cd40030242db3f705a93b2891a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1584, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c2e753fdbe3567442fc1e03f5e7ae07e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1984, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1b08cf49c3e0c08ba7759192916c262b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 906, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b163aa986bc09bfc647f09b83b07825e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 608, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4e20ab64f4cf542c00878428292e57de(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 8, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b9e6dde078f05face473d58a826284c7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 32, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eb7c00b666aecb66afa5d87f4384b4de(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 48, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_84568216994c42dc9a78a7638acfc9f2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_28fc9786c8eb51637dc327186b33a0c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 61, 61], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_754f3b2eceb3c969cafb4bcbd526a9be(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4410ed18d17cf56c7edb48d8d9487131(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 4, 4], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8662976d923fc316f42c68b348aac645(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2160, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_46b849d24e80432a68cb996293a9a1cf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4295435d2d0a847d799e7a323e990fc1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f5147e6dcf0a2f6457540422bd0a3b0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1824, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e688e2824ed9df8542cecce3c58a2d73(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_caa476a83ca48a09742ad8c422eaae6b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 2, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_237e94dd03dbb3d597731d100450d5a0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_32887b43f5b58adac22b860128eae73c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ea3d781e827ac74579346115b20b8002(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d03669759404c17eea5fbe0fd1605b35(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1104, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8176ec319d7b31f697db792d91833999(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 352, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3e2e5292c99d809a2b77c71c9a22725d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9e0e872a60af4fbfabce465b69e0efeb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 61, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f268700c4da013d6a83d4a923b88f716(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-06'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4d0b99242f00e944a18b9bafd47ea522(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dea0f47511d5ea514ec4b33b6dcf178e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ee81696f9229deba291d9f8d23cd674d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 2, 2], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_00e024924948f9530c4c5682daa6652b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c99838ebd7cb34868e5934a9df014868(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1cf9fed88898a52836a6b9e7175f3073(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 48, 48], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8bd61de5b447285a1a98100e41b57a01(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 168, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1864570f21287a00d91d55c94bd03f33(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 3, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8bbf3293f35b054878bddb7e19320873(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 448, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c9c78c6e93a10811a79f7a80805b6d47(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 162, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5d9f2c3bc38fb6027110b24fa6671d9f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 175, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8caf97a8480c55f50ffb5e8a321f1a14(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 304, 304], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_99613e5121b570118b8f970abbba7f46(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e69a017b00574952acf834813ed3760c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1600, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_76d55cfaa6d0232d25592b0177013e3a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9f78f5cb4cf7428f7add9adec1b269a8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_26aa9a3de6019caaec46f3a13bf74266(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 256, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7efaa2655c87330018e1ae7b3862dde2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 2, 27], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dcb2f2f8150f2b965cf23f091bee3fcc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 151, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d84f6f9a89ad1fa3a5427852edccd2f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 320, 320], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_214efea8c0e6c920f8a4b8780c5b7a4b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5c24ba61461480d4c6d3eb014c3fc86d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 8, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_445ddf41551001cd9d5b69548dc48794(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 100], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cfb69fd406c5282be428fef1cb855749(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1680, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_695ade52943d861cbb70a061e1e9a339(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 2, 2], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5b8fa27111695d91eadafafb1a520f56(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7f9778ea4534f3fc63e1557084e6046b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1120, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0aeca0fb119a33c1a5a9a8bebf74ed03(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, None], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2e9afbd276ea8658a969c5b6afad74cc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6da9a98efad0ff7ae24f0aae4d7d96b4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1392, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2db559d88ad496e3ddf832b3c8e53914(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a0aff5902af7fa4414fbcb1026214c4c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 2, 4], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_825e97677a5e98b9abcd3a54e281a971(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 140, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a31fbafeb6158af7b3bc553784331220(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_512d64956c2da18527392987c6edf207(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_75ee84f7236948cee9ff459d8bebe011(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1680, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_635d8c6eee3d97d4ebe54fb73bcada55(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 147, 147], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_11304b7dd07aac514dcf1769ed48cdf2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 448, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c198ebb05e54a668041e8d96c745ecc4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1248, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9b2b4704b181faf4a2a3c3b95962d9fc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 704, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aff0a1d31bbbdf620cbc6dfa93b5124e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b9bb75184127fca2f1c57ca008a66b75(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 38, 38], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b8f3f640971c1628767991bd97777cc4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fef8fe758bca37ef2d38ea6431493539(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 118, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3af3a6b6d2c5c0ae73a92daa06d7104c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a40598a92049528676957c4113ab8b71(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1718dfd92c51788ed97747a2f0e216bb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c4bfbc0e06310a32dd8ec11ccf8397dd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_02d589f4fd9bc2be1304315eb1756767(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2, 112, 224], dtype='float16'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fcc6dde260acc7188025c4c6de7f0c74(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 61, 61], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7141b8d9126bf91af1d01faeff73cc6e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fac92df0b62c9b0932a8ac0aa1ab203e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 149, 149], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_742a892484ff8f554a435063c08680e4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 7, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aed7e060052c4499918ab45fb112be96(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2160, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c1772c848ed84edbc3c4009987e7f26a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1680, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_27a114a1ea98a13a7bb957d7d400fa15(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9bca70962b358c3795b4a1775aa1749e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_98ee279dc273eba3588a15388601fa2c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1888, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8359ec07e6727424210642ef7753057d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 84, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_99b9674fb106caee579db82db9d6fed3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 150, 150], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_29b7631c13ce6f45783542634b6f9809(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 400, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_316a5c1bf1092b7bd8bd6dd93656af77(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 270, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_777ac0525f84696e6440d3ec6ec78be2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1006a06063d0d7187b8ed43c448efc37(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6868ec7b78224230b1c6b5ca3874bf9b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 1, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1fc9939ee612c9963c71c7c09e5c4ef8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dd120fd62325ef88316b34cd1cfc1f64(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 50, 350], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8a82e1bc607f551a13054d8cd7b874b5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 28, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e4be4db5ae01ae41df990d1e11794aa8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 19, 19], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_897e67fbd5d9de8a56c074c35802522f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 570, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f9f28d79bb8fc25e95db546ff0ad50b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 200, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_579400ce34160557edeed1dea40b82a1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1472, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e68404a4033630df4d02cbe3c9aa4a92(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c401d1176ab51469a816b7bc7ba19212(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 48, 48], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_74f7178073080c772e3857dd3fff3ea6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_23134b2b217e82c7d766d0ca62787db4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 150, 150], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8b71e647fe0089f035928ef765b0f2b4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 147, 147], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b5f7017c2ff4851109f25ef4646d6508(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 81, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_08e61d2bb0735c24358435bd74a7ffb5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 300, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5be7e6bac60987a818142ffaeaa572e7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_85ae8fa92315e18b68b8dc16979bcd5d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2dee71c3d502d6667acafa1bf8810326(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1488, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cbf87a3922b26ccb48d13f6f2f36adbb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d5c09a718bde4f4f9b2728061f047e4a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f235e37fe90c6c3445d82bf4fa701baa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3ba232b3e2066a7ca07d9e9f577e151a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 528, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a688e009b68e15cd63b76c86ddc74bb7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 19, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8a2a6ead45870594f2d0f3f9c27bc82c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2064, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e751baaa77fd63c21bb116e6d6b3bb51(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 70, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e2e914dd9ce790e1723068f6c824bad3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1568, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6ee54eaec46cead9dfed8ffc36bfc004(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e3d577d4507d8cf60b0d71c3eade4704(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 73, 73], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4c20f1ab47ef6ecb4fdaaf7c4394d1e6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f13d91383492b6e76763077e02aa2a30(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 14, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_18a785bcdfcfcb78b66199742907e263(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ec3b3a2a6bb81c72145d5e0bbcee4810(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1312, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fd9bb3eff78417dce39f1c4c17582da6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 151, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d740db72a5878b910e548c0335883331(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 1, 2], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bb16a312934e2197793f27fc62de7457(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 96, 96], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c12e0a77c8c7a291cb84244dc2e185b1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1776, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7d292400455f3928d3db699b9b9c3a07(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 410, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_31a7804c24cf00b89a64598a2aa3f789(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_25ecfef120be3bf167579a0043641a4e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4f90e3704bca0bff852ea5a203b4b63b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_827369307d64dcfbca3d462bf795cce0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1872, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f93a4f741072d1bd0bddce02bc3e8f18(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2688, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_72066883c5c8cf4c909ee2905df02c64(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 8, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1b6c71d21f96d7faf2a2db0f5a70d973(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_81171a73171edbafffb791570671a2bc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 4], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f6cb4dd98a786871b8712340ce32611(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2688, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e1c59463e9917d9924e85605f4931ef3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1216, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_faab88388cbfec387fbee6d3da415e38(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2208, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0b1de604e126b1fa09ae4b5eb369b696(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9a851f4c322cbd327bf4b5a9c7d23882(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 16, 128, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_12e434c0aba4bdedd142af1d54405487(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cafb0986540bfd1a3adbb5480dabffdf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 70, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a245c093e969413a000f5b4cdefe6057(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 100], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dc07229b50b6f866a2bf90a74fd7609d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 27, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_754aecdb21f90647937ecba1c265a35c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f6596b2204ca91d19f0a77cd570c1cf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1296, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f881908c233cff0d518f7bbddbe95ff2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 234, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_90cd01123450907824b1892589e818bb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 75, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c681fab815e93f1cbf1a2b6f35ed0f5b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2432, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c6baed37cb404a4869b3e13de7f6e733(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 736, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b7c85a042e09774cd876c6c793c9f2f9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1392, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9d9a59cc1149d5482d170ab28e98dede(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-06'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d655591997fa9244a08e06784e6c676f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 3, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_487c1fe21a345440b06740e79a052423(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_df574d33db59afb378e0feda4f1d4f51(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d49a26835e4478a1258aea1911ea27d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d9207fb6e42df320687e1abddc8cfd8e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 138, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e6281efc4237d516bf62349b005066d3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e8261252208644d1e6de9fe1fff3934f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1568, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6cd78aa28625ecef49649dd900f1df0a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ca645b29629be37054ef146b655740b8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 87, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2e4bcdc5bafdc514906583519befe93c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 704, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_127ee73f1be2c7092762a34c121d6b79(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 12, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_629b5c94a3254f00139132c6cfefec6c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b16fefd9c298c968f9c51950d5c477df(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1248, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6d6ef9509cacb40b8f843af35ad1060a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e12387775509b6aa385a0c26a8955671(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_db7396cc0f24295e5ab6004b01dde58a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_752486afaea0903136dae08f821d2b70(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_07eeb3f0cd7e65e6c274d576a8cf26a8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2016, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a6de8498df15d65b6c1e88fdee42bb3c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d8efe7286b43a5f44d9b4957f89df325(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1a961731f39b2fa32ac9da1944219062(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 95, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5cf4d4b147717d00b0b75a61caac84ce(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_08670a12885360769ac09c15fed4cd71(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_24cad3ff89579e26f4b24bc3de0dc351(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 366, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3cc17f3071d80ab3eb49c5addce341bf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 50, 350], dtype='float16'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_601deb1983c5cb746610281b55124c97(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c793602ede73f9274317587bec0f6eb4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d733ad76e6def93a6431f82f0276bd67(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 138, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cab09c7c4fa56e2488ea07ea43375fe3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1760, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_30ab71bcbd86487bbd826b41f391c7ee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 28, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None




if __name__ == '__main__':
    unittest.main()