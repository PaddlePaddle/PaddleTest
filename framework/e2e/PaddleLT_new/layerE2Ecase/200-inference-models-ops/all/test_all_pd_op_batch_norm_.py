import os
os.environ['FLAGS_cinn_new_group_scheduler'] = '1'
os.environ['FLAGS_group_schedule_tiling_first'] = '1'
os.environ['FLAGS_enable_pir_api'] = '1'
os.environ['FLAGS_cinn_bucket_compile'] = '1'
import sys
import unittest
import numpy as np
from dataclasses import dataclass
import typing as t
import itertools

@dataclass
class Stage:
    name: str
    env_vars: t.Dict[str, str]

cinn_stages = [
    Stage(
        name="dynamic_to_static",
        env_vars=dict(
            PADDLE_DEBUG_ENABLE_CINN=False,
            FLAGS_prim_all=False,
            FLAGS_prim_enable_dynamic=False,
        ),
    ),
    Stage(
        name="prim",
        env_vars=dict(
            PADDLE_DEBUG_ENABLE_CINN=False,
            FLAGS_prim_all=True,
            FLAGS_prim_enable_dynamic=True,
        ),
    ),
    Stage(
        name="infer_symbolic",
        env_vars=dict(
            PADDLE_DEBUG_ENABLE_CINN=False,
            FLAGS_prim_all=True,
            FLAGS_prim_enable_dynamic=True,
            FLAGS_use_cinn=False,
            FLAGS_check_infer_symbolic=True,
        ),
    ),
	Stage(
        name="frontend",
        env_vars=dict(
            PADDLE_DEBUG_ENABLE_CINN=True,
            FLAGS_prim_all=True,
            FLAGS_prim_enable_dynamic=True,
            FLAGS_use_cinn=True,
            FLAGS_check_infer_symbolic=False,
            FLAGS_enable_fusion_fallback=True,
        ), 
    ),
    Stage(
        name="backend",
        env_vars=dict(
            PADDLE_DEBUG_ENABLE_CINN=True,
            FLAGS_prim_all=True,
            FLAGS_prim_enable_dynamic=True,
            FLAGS_use_cinn=True,
            FLAGS_check_infer_symbolic=False,
            FLAGS_enable_fusion_fallback=False,
        ), 
    ),
]

def GetCinnStageByName(name):
    for stage in cinn_stages:
        if stage.name == name:
            return stage
    return None

def GetCurrentCinnStage():
    name = os.getenv('PADDLE_DEBUG_CINN_STAGE_NAME')
    if name is None:
        return None
    stage_names = [stage.name for stage in cinn_stages]
    assert name in stage_names, (
        f"PADDLE_DEBUG_CINN_STAGE_NAME should be in {stage_names}"
    )
    return GetCinnStageByName(name)

def GetPrevCinnStage(stage):
    for i in range(1, len(cinn_stages)):
        if stage is cinn_stages[i]:
            return cinn_stages[i - 1]
    return None

def IsCinnStageEnableDiff():
    value = os.getenv('PADDLE_DEBUG_CINN_STAGE_ENABLE_DIFF')
    enabled = value in {
        '1',
        'true',
        'True',
    }
    if enabled:
        assert GetCurrentCinnStage() is not None
    return enabled

def GetExitCodeAndStdErr(cmd, env):
    env = {
        k:v
        for k, v in env.items()
        if v is not None
    }
    import subprocess
    result = subprocess.run(
        cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
        env=env,
    )
    return result.returncode, result.stderr

def GetStageExitCodeAndStdErr(stage):
    return GetExitCodeAndStdErr(
        [sys.executable, __file__],
        env=dict(
            PADDLE_DEBUG_CINN_STAGE_NAME=stage.name,
            PADDLE_DEBUG_CINN_STAGE_ENABLE_DIFF='0',
            PYTHONPATH=os.getenv('PYTHONPATH'),
            ATHENA_ENABLE_TRY_RUN="False",
        ),
    )

def AthenaTryRunEnabled():
    return os.getenv('ATHENA_ENABLE_TRY_RUN') not in {
        "0",
        "False",
        "false",
        "OFF"
    }

def GetNeedSkipAndSkipMessage():
    current_stage = GetCurrentCinnStage()
    assert current_stage is not None
    if not IsCinnStageEnableDiff():
        return False, ""
    last_stage = GetPrevCinnStage(current_stage)
    if last_stage is None:
        return False, ""
    exitcode, stderr = GetStageExitCodeAndStdErr(last_stage)
    if exitcode != 0:
        return True, "last stage failed."
    return False, ""

def GetCurrentStageTryRunExitCodeAndStdErr():
    if not AthenaTryRunEnabled():
        return False, ""
    current_stage = GetCurrentCinnStage()
    assert current_stage is not None
    return GetStageExitCodeAndStdErr(current_stage)

def SetDefaultEnv(**env_var2value):
    for env_var, value in env_var2value.items():
        if os.getenv(env_var) is None:
            os.environ[env_var] = str(value)

SetDefaultEnv(
    PADDLE_DEBUG_CINN_STAGE_NAME="backend",
    PADDLE_DEBUG_CINN_STAGE_ENABLE_DIFF=False,
    PADDLE_DEBUG_ENABLE_CINN=True,
    FLAGS_enable_pir_api=True,
    FLAGS_prim_all=True,
    FLAGS_prim_enable_dynamic=True,
    FLAGS_use_cinn=False,
    FLAGS_check_infer_symbolic=False,
    FLAGS_enable_fusion_fallback=False,
)

import paddle

def SetEnvVar(env_var2value):
    for env_var, value in env_var2value.items():
        os.environ[env_var] = str(value)
    paddle.set_flags({
        env_var:value
        for env_var, value in env_var2value.items()
        if env_var.startswith('FLAGS_')
    })

if GetCurrentCinnStage() is not None:
    SetEnvVar(GetCurrentCinnStage().env_vars)

def GetEnvVarEnableJit():
    enable_jit = os.getenv('PADDLE_DEBUG_ENABLE_JIT')
    return enable_jit not in {
        "0",
        "False",
        "false",
        "OFF",
    }

def GetEnvVarEnableCinn():
    enable_cinn = os.getenv('PADDLE_DEBUG_ENABLE_CINN')
    if enable_cinn is None:
        return True
    return enable_cinn not in {
        "0",
        "False",
        "false",
        "OFF",
    }


def GetTolerance(dtype):
    if dtype == np.float16:
        return GetFloat16Tolerance()
    if dtype == np.float32:
        return GetFloat32Tolerance()
    return 1e-6

def GetFloat16Tolerance():
    try:
        return float(os.getenv('PADDLE_DEBUG_FLOAT16_TOL'))
    except:
        return 1e-3

def GetFloat32Tolerance():
    try:
        return float(os.getenv('PADDLE_DEBUG_FLOAT32_TOL'))
    except:
        return 1e-6

def IsInteger(dtype):
    return np.dtype(dtype).char in np.typecodes['AllInteger']

def ApplyToStatic(net, use_cinn):
    build_strategy = paddle.static.BuildStrategy()
    build_strategy.build_cinn_pass = use_cinn
    return paddle.jit.to_static(
        net,
        input_spec=net.get_input_spec(),
        build_strategy=build_strategy,
        full_graph=True,
    )

class InstanceTrait:

    @classmethod
    def instance(cls):
        if cls.instance_ is None:
            cls.instance_ = cls()
        return cls.instance_

    @classmethod
    def static_instance_with_cinn(cls):
        if cls.static_instance_with_cinn_ is None:
            cls.static_instance_with_cinn_ = ApplyToStatic(
                cls.instance(),
                use_cinn=True
            )
        return cls.static_instance_with_cinn_

    @classmethod
    def static_instance_without_cinn(cls):
        if cls.static_instance_without_cinn_ is None:
            cls.static_instance_without_cinn_ = ApplyToStatic(
                cls.instance(),
                use_cinn=False
            )
        return cls.static_instance_without_cinn_


class CinnTestBase:

    def setUp(self):
        paddle.seed(2024)
        self.prepare_data()

    def _test_entry(self):
        dy_outs = self.train(use_cinn=False)
        cinn_outs = self.train(use_cinn=GetEnvVarEnableCinn())

        for cinn_out, dy_out in zip(cinn_outs, dy_outs):
          if type(cinn_out) is list and type(dy_out) is list:
            for x, y in zip(cinn_out, dy_out):
              self.assert_all_close(x, y)
          else:
            self.assert_all_close(cinn_out, dy_out)

    def train(self, use_cinn):
        if GetEnvVarEnableJit():
            net = self.prepare_static_net(use_cinn)
        else:
            net = self.prepare_net()
        paddle.seed(2024)
        out = net(*self.inputs)
        return out
    
    def prepare_data(self):
        self.inputs = self.get_inputs()
        for input in self.inputs:
            input.stop_gradient = True

    def prepare_net(self):
        return self.get_test_class().instance()

    def prepare_static_net(self, use_cinn):
        if use_cinn:
            return self.get_test_class().static_instance_with_cinn()
        else:
            return self.get_test_class().static_instance_without_cinn()

    def assert_all_close(self, x, y):
        if (hasattr(x, "numpy") and hasattr(y, "numpy")):
            x_numpy = x.numpy()
            y_numpy = y.numpy()
            assert x_numpy.dtype == y_numpy.dtype
            if IsInteger(x_numpy.dtype):
                np.testing.assert_equal(x_numpy, y_numpy)
            else:
                tol = GetTolerance(x_numpy.dtype)
                np.testing.assert_allclose(x_numpy, y_numpy, atol=tol, rtol=tol)
        else:
            assert x == y





need_skip, skip_message = GetNeedSkipAndSkipMessage()
try_run_exit_code, try_run_stderr = GetCurrentStageTryRunExitCodeAndStdErr()
class TestTryRun(unittest.TestCase):
    def test_panic(self):
        if not AthenaTryRunEnabled():
            return
        if try_run_exit_code == 0:
            # All unittest cases passed.
            return
        if try_run_exit_code > 0:
            # program failed but not panic.
            return
        # program panicked.
        kOutputLimit = 65536
        message = try_run_stderr[-kOutputLimit:]
        raise RuntimeError(f"panicked. last {kOutputLimit} characters of stderr: \n{message}")
class PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_385d768ff95cf75f94de6693ba2a2e51(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 4, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_4497fcdfb9639144ea558daa18214543(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ceb41475781de0bb08558b98bc2c1d5e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_41dde5624aaa94a6e01c001574e93b08(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 240, 240], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_59076b23a8a27d9d5e37f4a25a3ff217(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 8, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2ed4c31fcd8c6db6a7350e137957e302(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 640, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f2d9bdf1d2964eadc3bf99e0b1aa186a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cd9aae4fc180c83520e004d951968db9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_356efd400ed99c0d9098594ffccac793(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 256, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_1a2b114b88ba56b95370f75ce469d256(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1d1109c16c316b9a965a4a94622f0c0c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_100ce3bd2382abe40998da31ecbff016(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6528d6ca198f03b1721b87ebdc2cc27c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 4, 12], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_41c4069d4ffdc77ecbca165e7912fd61(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0366cd303febed6bbb667de44d70348e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d5248ea0432185b0509d3a6cd0a4da00(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 28, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e0f15764a58e7ecbce9119f3eafcd630(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 4, 3], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_95441c62bad39c6e4e1dab03940bc577(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3662342429161072, 0.3428390324115753, 0.3868209421634674, 0.009858453646302223, 0.061945006251335144, 0.17148469388484955, 0.4968242347240448, 0.17593659460544586, 0.14125674962997437, 0.17043396830558777, 0.07286263257265091, 0.2278633415699005, 0.17953690886497498, 0.2840469777584076, 0.31774821877479553, 0.41650956869125366, 0.42939749360084534, 0.3827384412288666, 0.05737534910440445, 0.13147127628326416, 0.015083159320056438, 0.2259611040353775, 0.3749145269393921, 0.4329436719417572, 0.21486105024814606, 0.3182086646556854, 0.18884369730949402, 0.11104921996593475, 0.31734904646873474, 0.3088545799255371], dtype='float32').reshape([30]),
            paddle.to_tensor([0.30969443917274475, 0.46373799443244934, 0.4371044337749481, 0.03455515578389168, 0.09794057905673981, 0.25512343645095825, 0.47640466690063477, 0.2333521693944931, 0.4599730968475342, 0.06539709120988846, 0.03680749237537384, 0.24058926105499268, 0.19728754460811615, 0.0692165344953537, 0.03586745634675026, 0.17319737374782562, 0.42173370718955994, 0.39987894892692566, 0.18259090185165405, 0.28973624110221863, 0.3810057044029236, 0.23503872752189636, 0.38959264755249023, 0.35962963104248047, 0.17084579169750214, 0.48764875531196594, 0.47187528014183044, 0.14985349774360657, 0.17468801140785217, 0.31513047218322754], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3188827335834503, 0.1288881003856659, 0.18590328097343445, 0.47897869348526, 0.046194180846214294, 0.28171297907829285, 0.3721570670604706, 0.32276618480682373, 0.05693402886390686, 0.48145851492881775, 0.2814633548259735, 0.011611017398536205, 0.40405064821243286, 0.12909843027591705, 0.37267571687698364, 0.16349700093269348, 0.25666308403015137, 0.08231811225414276, 0.24599730968475342, 0.28160426020622253, 0.05625735595822334, 0.24903930723667145, 0.1676904708147049, 0.37434160709381104, 0.23770864307880402, 0.14765463769435883, 0.41830989718437195, 0.17075873911380768, 0.4954676628112793, 0.26127853989601135], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3963524103164673, 0.17731106281280518, 0.2084573656320572, 0.25163790583610535, 0.1156802773475647, 0.03383117914199829, 0.10903742164373398, 0.4460245370864868, 0.0966639295220375, 0.09571550786495209, 0.11343978345394135, 0.25790077447891235, 0.2579532563686371, 0.2178141474723816, 0.42496609687805176, 0.05193878710269928, 0.2667320966720581, 0.4994419515132904, 0.11658084392547607, 0.20300984382629395, 0.316439151763916, 0.2998521327972412, 0.22316309809684753, 0.34801462292671204, 0.448249489068985, 0.1565706580877304, 0.19177542626857758, 0.2547970414161682, 0.2285171002149582, 0.16274645924568176], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b082f346cd2b4ebc03ecb8cc0c7a5419(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b11522da1514d39c31ec9e60418e62d8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 60, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f1c904092ea8394ce9f25737cf8479ed(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f755362f87af70121435839dfe30a02e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 31, 31], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6604ca43e007a966a0b18784d6699c6e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 10, 10], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_da4d85bc213f70819a72e68f21214cdb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a189650ad2cf49138ddade1316ee5cb6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 4, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4233af795a92b4c97303357fd3bb10ea(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 240, 240], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4f561135c28322f531f70d17eb84fb3a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 320, 320], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.34932634234428406, 0.14266933500766754, 0.4141688048839569, 0.11165226995944977, 0.30618682503700256, 0.10354115068912506, 0.2107677459716797, 0.19438564777374268], dtype='float32').reshape([8]),
            paddle.to_tensor([0.34006649255752563, 0.015924641862511635, 0.1484174281358719, 0.4371776878833771, 0.1295556277036667, 0.35544586181640625, 0.04795840382575989, 0.21727579832077026], dtype='float32').reshape([8]),
            paddle.to_tensor([0.2782144546508789, 0.4006979465484619, 0.3507254719734192, 0.2197539359331131, 0.4467852711677551, 0.396190881729126, 0.2709618806838989, 0.2912371754646301], dtype='float32').reshape([8]),
            paddle.to_tensor([0.37367257475852966, 0.39299583435058594, 0.11430060863494873, 0.06692321598529816, 0.018933525308966637, 0.4010910391807556, 0.04112526774406433, 0.019530100747942924], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e2e55b2214eeff4b7df8046b485348f7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_08ecfbca71b5233f6357b289e88ae3f0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_41b6de854fd474036a513ea9a448d340(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 61, 61], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_be0e8241d3ddbbbcafb19b5ce765b079(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0ec78bf32924ecc0a97b9b65d030ffdb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1920, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1920], dtype='float32', min=0, max=0.5),
            paddle.uniform([1920], dtype='float32', min=0, max=0.5),
            paddle.uniform([1920], dtype='float32', min=0, max=0.5),
            paddle.uniform([1920], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4a81b38ecb5cf95d75bb8e36ec109d70(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 64, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_58c3adacf0d2f4b7df3d8db3096515c4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_25904b2044077939c1bc60599fd88d85(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4991402328014374, 0.4575498402118683, 0.3546712398529053, 0.21502457559108734, 0.2007199227809906, 0.2912932336330414, 0.27397024631500244, 0.3264232873916626, 0.3410760760307312, 0.44021034240722656, 0.1501998007297516, 0.15183167159557343, 0.30893605947494507, 0.0393204465508461, 0.1674453318119049, 0.49593713879585266], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2717740833759308, 0.4683583080768585, 0.0012006883043795824, 0.21603511273860931, 0.055884990841150284, 0.49914389848709106, 0.13998152315616608, 0.28898802399635315, 0.12404004484415054, 0.16421626508235931, 0.1344718039035797, 0.18481308221817017, 0.4756976366043091, 0.35267242789268494, 0.19450661540031433, 0.43217939138412476], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2529461085796356, 0.2748369872570038, 0.4644949436187744, 0.26930728554725647, 0.2153252214193344, 0.009749476797878742, 0.32609790563583374, 0.004018689505755901, 0.008113453164696693, 0.4838692843914032, 0.471780925989151, 0.47413724660873413, 0.3994632959365845, 0.3657079041004181, 0.11870238929986954, 0.11600153148174286], dtype='float32').reshape([16]),
            paddle.to_tensor([0.032887231558561325, 0.41516903042793274, 0.07227057963609695, 0.33862999081611633, 0.27215489745140076, 0.3520416021347046, 0.28860756754875183, 0.10793733596801758, 0.34143221378326416, 0.2616090178489685, 0.3032914102077484, 0.3070058822631836, 0.3947279155254364, 0.3308369815349579, 0.4093777537345886, 0.0828588604927063], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_d589af777f5a8ae699611515c41499fb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_376239c61115cf538f847365e3d10d93(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_d589af777f5a8ae699611515c41499fb
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 320, 320], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dce1f0346e5725e2d062320335a8a5f0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 120, 120], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3918633759021759, 0.24334363639354706, 0.10869754105806351, 0.18950296938419342, 0.36955130100250244, 0.4372948110103607, 0.4219338297843933, 0.008915182203054428, 0.3006196916103363, 0.3868383765220642, 0.2618686854839325, 0.3042088449001312, 0.39390820264816284, 0.38728734850883484, 0.3075386881828308, 0.1651657223701477, 0.40331128239631653, 0.04801614582538605, 0.11032853275537491, 0.44399917125701904, 0.025821378454566002, 0.40360724925994873, 0.0956754982471466, 0.16688719391822815], dtype='float32').reshape([24]),
            paddle.to_tensor([0.440682590007782, 0.25034019351005554, 0.2660289406776428, 0.2747321128845215, 0.4343474507331848, 0.09204253554344177, 0.2790403664112091, 0.49259695410728455, 0.4302559792995453, 0.24097049236297607, 0.05783146247267723, 0.2955625653266907, 0.4345995783805847, 0.30892807245254517, 0.4813806116580963, 0.4236448407173157, 0.24182865023612976, 0.2951752543449402, 0.14157021045684814, 0.2810265123844147, 0.2599746584892273, 0.41784149408340454, 0.49833106994628906, 0.49221688508987427], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4051341414451599, 0.4424993395805359, 0.43841108679771423, 0.09669297188520432, 0.4489176273345947, 0.08135300129652023, 0.42586472630500793, 0.289463609457016, 0.45654797554016113, 0.34931111335754395, 0.04776453599333763, 0.2366151213645935, 0.37760335206985474, 0.0799272358417511, 0.441709965467453, 0.4845034182071686, 0.4853958189487457, 0.04172098636627197, 0.300394743680954, 0.03452003747224808, 0.13334408402442932, 0.4834400415420532, 0.1338258981704712, 0.04119601473212242], dtype='float32').reshape([24]),
            paddle.to_tensor([0.1023271381855011, 0.24549821019172668, 0.2478359341621399, 0.30580511689186096, 0.172546848654747, 0.19146373867988586, 0.1747608482837677, 0.3008786141872406, 0.34293031692504883, 0.0024417515378445387, 0.3095424771308899, 0.14818927645683289, 0.06433609873056412, 0.031550128012895584, 0.43361836671829224, 0.49065807461738586, 0.1738535761833191, 0.4501165449619293, 0.058409880846738815, 0.03082597814500332, 0.48682427406311035, 0.17629283666610718, 0.17425134778022766, 0.10602882504463196], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3c560d72834438ed3c0feda28291a02d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 4, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3bb059612ece0d7fb3eafe9c4788596f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c750cbc8bf5dc7e47fa2bd9214ec567a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fe1797688acc54ad18730fa2adcedda9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cd7eb1b0c25429421d48c29b50497aa9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f0324d285fcedacdc327f1dc97e21d4a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 840, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([840], dtype='float32', min=0, max=0.5),
            paddle.uniform([840], dtype='float32', min=0, max=0.5),
            paddle.uniform([840], dtype='float32', min=0, max=0.5),
            paddle.uniform([840], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7ca4fc6550c821a632e8ebdf08e9f7f7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.41297975182533264, 0.10422833263874054, 0.10926562547683716, 0.10331948101520538, 0.039517633616924286, 0.024338092654943466, 0.18819406628608704, 0.40417909622192383, 0.09349023550748825, 0.011218542233109474, 0.3580896556377411, 0.28769078850746155, 0.25778067111968994, 0.33946797251701355, 0.3325963020324707, 0.4762631058692932, 0.12563790380954742, 0.011929592117667198, 0.43463656306266785, 0.04622865840792656], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4635208249092102, 0.45259976387023926, 0.3140714168548584, 0.2062014490365982, 0.23077906668186188, 0.23306703567504883, 0.15729790925979614, 0.2108946591615677, 0.010380527935922146, 0.10427139699459076, 0.31009969115257263, 0.39626699686050415, 0.019746679812669754, 0.40118229389190674, 0.46387428045272827, 0.05876738578081131, 0.4626252353191376, 0.43784621357917786, 0.40012750029563904, 0.09008625894784927], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3689497709274292, 0.32896190881729126, 0.3430512547492981, 0.029831675812602043, 0.26283755898475647, 0.49980658292770386, 0.015347755514085293, 0.055874623358249664, 0.20904850959777832, 0.425612211227417, 0.44424283504486084, 0.2474474459886551, 0.47222888469696045, 0.07900170236825943, 0.41342607140541077, 0.22420300543308258, 0.3980717360973358, 0.1482454538345337, 0.32549262046813965, 0.21112410724163055], dtype='float32').reshape([20]),
            paddle.to_tensor([0.04128643125295639, 0.06237809732556343, 0.16620855033397675, 0.3978187143802643, 0.1889398694038391, 0.1590544879436493, 0.13317666947841644, 0.07454722374677658, 0.20287717878818512, 0.16642631590366364, 0.4320419728755951, 0.29320454597473145, 0.11328349262475967, 0.3032485246658325, 0.18437054753303528, 0.4516693651676178, 0.19226224720478058, 0.27005016803741455, 0.3154579997062683, 0.4733662009239197], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_955e1571a68c2039cd04a0e3f11eb212(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5f414939bf280b87e1fa443c0c619365(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_423671de0a9b5d6da8a152e90233796f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 320, 320], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fb6e93a0efbe3845737153319ead4ca2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 75, 75], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9225e615465c1c888698e73a2fe2d888(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c99fde2121ca9becd4ba587e9aaf98c4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_16b3f84f42f4a8263accbda1d3cf5793(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e117494db308a5339041ca22fb156a22(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8121ff122a866ad7f46e7f3d482e3e91(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_01c289ac9d05072efc778c1a98e677cd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_23eb6c11ae7ffebf5efbb1b8545f47ef(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c8eafa4e3ac0d4f5a7fefc5a23eedbf6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 300, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([300], dtype='float32', min=0, max=0.5),
            paddle.uniform([300], dtype='float32', min=0, max=0.5),
            paddle.uniform([300], dtype='float32', min=0, max=0.5),
            paddle.uniform([300], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d29bfab3b53fa029396af889665ece6d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 16, 32], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.16100750863552094, 0.40780046582221985, 0.44149094820022583, 0.20871490240097046, 0.4839986264705658, 0.057000987231731415, 0.40962594747543335, 0.43322378396987915, 0.23046840727329254, 0.03886944800615311, 0.11149238795042038, 0.004483301192522049, 0.2677153944969177, 0.48751071095466614, 0.42094871401786804, 0.35006943345069885, 0.12916694581508636, 0.21998755633831024], dtype='float32').reshape([18]),
            paddle.to_tensor([0.1787053644657135, 0.18442173302173615, 0.3899925947189331, 0.23023487627506256, 0.05017301067709923, 0.15873607993125916, 0.16973243653774261, 0.241957888007164, 0.23170867562294006, 0.2528038024902344, 0.04397229850292206, 0.19356215000152588, 0.14518392086029053, 0.039618197828531265, 0.24146628379821777, 0.45592397451400757, 0.1431976556777954, 0.0504922941327095], dtype='float32').reshape([18]),
            paddle.to_tensor([0.1128273457288742, 0.374362051486969, 0.34107428789138794, 0.44240155816078186, 0.1699880212545395, 0.12461491674184799, 0.3263606131076813, 0.1271408647298813, 0.18177205324172974, 0.0036623687483370304, 0.1065979078412056, 0.44033384323120117, 0.18323422968387604, 0.4244285523891449, 0.1975652426481247, 0.37148746848106384, 0.029991447925567627, 0.3869052529335022], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3011948764324188, 0.2983614206314087, 0.19148528575897217, 0.17192402482032776, 0.03788604587316513, 0.0369366854429245, 0.09548186510801315, 0.48778656125068665, 0.18679441511631012, 0.005071560852229595, 0.4231996536254883, 0.2126006931066513, 0.13088631629943848, 0.25703251361846924, 0.4230417013168335, 0.24038976430892944, 0.2134314924478531, 0.28602135181427], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4bdb90595ddf49643317659b0909588a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 52, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_1a8301b810bccba10e4dfdee0df73334(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6213deb209e311651f3c95d19b0c3642(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_92d9736106932f944a1c40c945857c97(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 512], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.05399005860090256, 0.29042041301727295, 0.03339209035038948, 0.18658681213855743, 0.3025275468826294, 0.26054835319519043, 0.14919328689575195, 0.4336749315261841, 0.16952349245548248, 0.13078051805496216, 0.3986895680427551, 0.30782273411750793, 0.18632318079471588, 0.05197548866271973, 0.4544391632080078, 0.10237224400043488], dtype='float32').reshape([16]),
            paddle.to_tensor([0.42025256156921387, 0.33672696352005005, 0.014828703366219997, 0.04938898980617523, 0.30528736114501953, 0.33708810806274414, 0.21506793797016144, 0.12860994040966034, 0.1750594526529312, 0.2188432365655899, 0.0859743058681488, 0.17110559344291687, 0.48715415596961975, 0.08473827689886093, 0.4127727746963501, 0.31149718165397644], dtype='float32').reshape([16]),
            paddle.to_tensor([0.05231725424528122, 0.2702236771583557, 0.40354129672050476, 0.475195050239563, 0.15819649398326874, 0.49524760246276855, 0.02232707291841507, 0.23660768568515778, 0.28693318367004395, 0.29141589999198914, 0.3621406853199005, 0.2964422106742859, 0.16844652593135834, 0.13128980994224548, 0.10852104425430298, 0.3098984658718109], dtype='float32').reshape([16]),
            paddle.to_tensor([0.13635960221290588, 0.391119122505188, 0.35121265053749084, 0.08881095051765442, 0.3080230951309204, 0.3279971778392792, 0.3904482126235962, 0.32089152932167053, 0.10716883838176727, 0.3320906162261963, 0.17142353951931, 0.16506922245025635, 0.08299127221107483, 0.48739930987358093, 0.3282962143421173, 0.009005933068692684], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b9f2630dedb605cb6cc4d1e88a6fabd0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7cf0e49397bc65bd9b722f9b03a00623(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 64, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6d9c2229e7e9d41ff87450b7d2f56244(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 16, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fc659a2ea6091af53e425e7806f98003(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8027db681a8337bb8b3ec8897f8621c1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.366787850856781, 0.29879215359687805, 0.49569422006607056, 0.167120099067688, 0.27429965138435364, 0.1336941123008728, 0.1862003207206726, 0.2454545795917511, 0.07455014437437057, 0.10445112735033035, 0.08061225712299347, 0.2003643810749054, 0.47920897603034973, 0.13130901753902435, 0.01607501693069935, 0.31053265929222107, 0.30213096737861633, 0.46125033497810364], dtype='float32').reshape([18]),
            paddle.to_tensor([0.32456323504447937, 0.40791943669319153, 0.36203864216804504, 0.45875784754753113, 0.16174504160881042, 0.49550488591194153, 0.39160650968551636, 0.4897944927215576, 0.43281829357147217, 0.05628073215484619, 0.42187222838401794, 0.3677671551704407, 0.2916039228439331, 0.08496099710464478, 0.19183386862277985, 0.4021017253398895, 0.04833642393350601, 0.27702274918556213], dtype='float32').reshape([18]),
            paddle.to_tensor([0.38654598593711853, 0.22730541229248047, 0.16500449180603027, 0.16638444364070892, 0.25680428743362427, 0.32545003294944763, 0.39837273955345154, 0.48549792170524597, 0.11690875142812729, 0.3495975136756897, 0.20925204455852509, 0.0333072766661644, 0.24152496457099915, 0.2244323045015335, 0.3623139262199402, 0.4151003658771515, 0.02890026569366455, 0.06879303604364395], dtype='float32').reshape([18]),
            paddle.to_tensor([0.37490415573120117, 0.05104070529341698, 0.3665412664413452, 0.3575294315814972, 0.18448098003864288, 0.24854524433612823, 0.14791731536388397, 0.4409043490886688, 0.3388499915599823, 0.09179458767175674, 0.3683336675167084, 0.26524877548217773, 0.23752519488334656, 0.0999743789434433, 0.11100269109010696, 0.468703955411911, 0.1509980708360672, 0.307088702917099], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_590b938bdacfcee33d0a4259d23a37a6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e4dada82336f848d47d879b66705e648(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 52, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_0edd578f48a07aa5d1d79f9f678b472e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e02bd9b223075dd2023e26edbe2e5a92(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_0edd578f48a07aa5d1d79f9f678b472e
    def get_inputs(self):
        return [
            paddle.uniform([16, 384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a62766a8b419dd50ff93483076c4c8ea(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 60, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1ccb5464a2502be094df9404274e1449(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fe357bd2132e30fdfeb88040c3888d08(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c0eeefefc0b8b59802798f7eda3591e5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 8, 6], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aa92acc5daa3be34542850d1371b9f27(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_be54e93bdd5284ccf261683f4fa893c4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_18ed171a2c05abc406dd682b4dd89a0a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 61, 61], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_465c0017c1a52c557472087970776538(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aace3e61c17184d9d634af90b0fd2dcb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 16, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_585f319f0a336dc373218775aa1e9029(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 14, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d8d6f16cdbf1f1096df1810058621e0f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_980ad54a16c9c222510883e1d0f05a74(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4223272502422333, 0.40585169196128845, 0.17468060553073883, 0.3314419090747833, 0.430693656206131, 0.41243770718574524, 0.19956490397453308, 0.11396129429340363, 0.31246131658554077, 0.05306077003479004, 0.26127302646636963, 0.39197930693626404, 0.020706234499812126, 0.37680280208587646, 0.4663253426551819, 0.2726951539516449, 0.4180245101451874, 0.4391563832759857, 0.36273542046546936, 0.3460596799850464], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4010567367076874, 0.14275063574314117, 0.31761184334754944, 0.38550347089767456, 0.23255333304405212, 0.2794598937034607, 0.29367244243621826, 0.40090981125831604, 0.12426140159368515, 0.03969258815050125, 0.4904401898384094, 0.01215755008161068, 0.17013347148895264, 0.18479786813259125, 0.29513707756996155, 0.11868008226156235, 0.3532356023788452, 0.4811234474182129, 0.14048020541667938, 0.27341005206108093], dtype='float32').reshape([20]),
            paddle.to_tensor([0.049077343195676804, 0.23658069968223572, 0.11357345432043076, 0.2796972095966339, 0.44294822216033936, 0.14245058596134186, 0.16178454458713531, 0.17087706923484802, 0.1584233194589615, 0.22455543279647827, 0.4413720667362213, 0.296642005443573, 0.08224085718393326, 0.1906069815158844, 0.0690651386976242, 0.08198250085115433, 0.1551581472158432, 0.22113037109375, 0.4700053930282593, 0.3843155801296234], dtype='float32').reshape([20]),
            paddle.to_tensor([0.37141233682632446, 0.40865248441696167, 0.462030827999115, 0.08522781729698181, 0.19458279013633728, 0.0068578519858419895, 0.17858853936195374, 0.052830398082733154, 0.008326149545609951, 0.11622792482376099, 0.47745126485824585, 0.4120725989341736, 0.016467442736029625, 0.187120258808136, 0.3716451823711395, 0.029521889984607697, 0.34724095463752747, 0.041286200284957886, 0.03750721365213394, 0.12292718142271042], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b62888419ace5e97051364cca9bf9d50(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9cde48c19eaae5df03ffe522c53c9c38(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 5, 5], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c3f352fca7c4a9234b3a30630a15e146(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_876d3d31e6507da55f65d8f3e0226555(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 16, 12], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2f5fc2b98d6821f038f60e03fd3443d4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2bc99dcc5500f7a29a5584225e16aa1c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bc70696477abe5858d665b60f3258817(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a33430326591022362acec9607a09fce(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 8, 6], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_95b5016f483150de6dd04d96375a0416(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_54f5a721ce9366e2ba070e62b0a00190(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_d589af777f5a8ae699611515c41499fb
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6390db9e941d4b2588b0f93d072cdc44(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7f87325796c8b773ed8597e60ca11d5f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_b4f4da3a8d44d1b5b3a2903a097df186(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0c222ad3df2714168bbbea5f1ccf5f0b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_b4f4da3a8d44d1b5b3a2903a097df186
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4c1c77ca16ed7240c83da96cc4253e06(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0c845e0918402437383bcfa5d465828e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([8, 64, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_62b1c7b03d597e3952ac5a7571654bb1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 208, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6145d805b9d2025bec8ae85e89b783ed(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 16, 12], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d28ed095c9e7bfa24842b50df21517c3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 240, 240], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7ea6a4c390ee132dbb7b325909e348b3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5bd5943a8a8e1d0fa3dda4c8b86842a4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2afed2188fc5658e23815e9de49cc7fd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_41775a0ad6fce6db60f47eec578e42e9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 432, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_db302fc68674ced0050fd6a551693744(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fd2797ab8d2bc520bf017fc3a14c3865(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 10, 10], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1bd65f8b66a9a27a5a6e440cb030323f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 16, 12], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_01d361c32a0ccc44897e6a1c3387465a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_24c13f53efc7297015e9ef3d08efa560(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 1, 2], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_58c6364c1618ba6c389ace35e56bccb1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 304, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([304], dtype='float32', min=0, max=0.5),
            paddle.uniform([304], dtype='float32', min=0, max=0.5),
            paddle.uniform([304], dtype='float32', min=0, max=0.5),
            paddle.uniform([304], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_35a30005a5e8d89204aef54df339b008(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.44868674874305725, 0.37080758810043335, 0.021064424887299538, 0.4923309087753296, 0.3836457133293152, 0.22504834830760956, 0.10607390850782394, 0.48547300696372986, 0.08523371070623398, 0.4995478093624115, 0.4252505600452423, 0.3426767587661743, 0.3812923729419708, 0.483674556016922, 0.33830446004867554, 0.09502021968364716, 0.14171825349330902, 0.2216005027294159], dtype='float32').reshape([18]),
            paddle.to_tensor([0.14087004959583282, 0.3404660224914551, 0.19208543002605438, 0.10312508791685104, 0.3827088177204132, 0.4980098605155945, 0.28477364778518677, 0.033949170261621475, 0.24159643054008484, 0.31803902983665466, 0.05781117454171181, 0.29861322045326233, 0.20616410672664642, 0.2957629859447479, 0.26649749279022217, 0.4471859037876129, 0.4082181453704834, 0.05657300353050232], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4447194039821625, 0.29629895091056824, 0.14389462769031525, 0.46454092860221863, 0.050637055188417435, 0.31005311012268066, 0.368836373090744, 0.48512667417526245, 0.059926971793174744, 0.2138962298631668, 0.4271906316280365, 0.34222254157066345, 0.4704902172088623, 0.44078564643859863, 0.36928895115852356, 0.3599543273448944, 0.2399340122938156, 0.15686607360839844], dtype='float32').reshape([18]),
            paddle.to_tensor([0.42672207951545715, 0.35141029953956604, 0.4686536490917206, 0.33102408051490784, 0.28507474064826965, 0.30322134494781494, 0.3640284538269043, 0.20898482203483582, 0.00716151250526309, 0.1410530060529709, 0.36214450001716614, 0.20579640567302704, 0.3417733311653137, 0.4028688967227936, 0.11741627752780914, 0.49181944131851196, 0.2897947132587433, 0.1150709018111229], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b1f8c57a96ac87455883a1c03cde7874(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_021252efb52ede89c8b44ec6099311e5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c6bb5ff6fbadffe1bc7cbbdb76745b5a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 88, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_03058d14e912fd6847449ad535bc356d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 88, 25], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_548e5b257d3e6edd65480a65972b2be6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bb182dec18071514978b6fbe32bd98fa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d5b45eb739c58d192ae1f23164f64445(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([8, 512, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5cf8893654aa6ed0d25983cfd73a1818(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([8, 512, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c1f33cf53455cb0ef3e1a8824fe888d0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_89ff2cd406d34d720d4ca2b66914247f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 8, 6], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f7ec1b8b617f931c748bda2efd7d2d11(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 128, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c6c59d5f619a2ab9a0109c016626db6d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0ff408318a8f614c4a83c5b1470205da(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 200, 336], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_759135d1a53afcf66f97008c893151f3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 576, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e15276b31c10e67b61121ff3648d8c8e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d3bb317e2490c6a31555be1ecc45448d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 32, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4842e4544bbbf6b4b528c0eab5716a5b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 4, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e05b80f7b3a899460748f64ebe53c56d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_76fab50cdf5245bdbced2ef6c5022d95(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 8, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_73bc17bd75d5f77e970b0b6758de3277(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 350, 25], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8b60bbfee1fd574c6ea0d4d654a72ce3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2e213d3cdbcd171194337bc1fa9bfc64(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 480, 480], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b9701a2fcddbf1a292d4499882b279be(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 38, 38], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8b9b4a3cc08e30f84f507863d51d9db3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 60, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_abe07b9c8f267539da34390f8ef009f1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 512, 512], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_94335ef4e5897047138df29683c959f1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cffbd8b3caf0d757da888be4dc1e5c4b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 52, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fe2d6412a4949ed7695fbb7d210274d3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ccba862fa9c2ecd5256194d5f8ff2b8d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 2, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4489a16ce90ff3ce51b9f0c0abfa013b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8368f0a01542e4c9534fd05080b8bb71(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 4, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_43b7fef28f60b921a302b4d5bd0c9f06(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_183fecd1ba0631fd4f29b967c2b6012d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 350, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f14a1f7fccd4a55476e2f3701b907f2a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ad0d2524e0aedb228de6741698de9592(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 17, 17], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d3ce43103b40de3c17b5a7d9b395ab83(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 120, 120], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.20301756262779236, 0.3687237799167633, 0.13654589653015137, 0.2680690884590149, 0.07990109175443649, 0.03884657844901085, 0.4360043406486511, 0.4163578748703003, 0.3150733709335327, 0.4043005406856537, 0.15894979238510132, 0.09684209525585175, 0.4278638958930969, 0.09224449843168259, 0.11673909425735474, 0.4501127004623413, 0.113283671438694, 0.058872394263744354, 0.3989321291446686, 0.245635986328125, 0.49346616864204407, 0.4391816556453705, 0.016005804762244225, 0.1634674072265625], dtype='float32').reshape([24]),
            paddle.to_tensor([0.0013811325188726187, 0.0025673536583781242, 0.39464229345321655, 0.21414777636528015, 0.2454015016555786, 0.23094448447227478, 0.3306539058685303, 0.1168573871254921, 0.12862072885036469, 0.30945900082588196, 0.0879298523068428, 0.14867673814296722, 0.015912247821688652, 0.12089118361473083, 0.4710114300251007, 0.1800086498260498, 0.4230279326438904, 0.447340726852417, 0.4803910255432129, 0.4606209099292755, 0.01579522341489792, 0.3918918967247009, 0.3157716989517212, 0.035360198467969894], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2211984246969223, 0.387323796749115, 0.18900465965270996, 0.3816790282726288, 0.03464394435286522, 0.40347784757614136, 0.3409203290939331, 0.33180275559425354, 0.469715416431427, 0.1753242164850235, 0.08720284700393677, 0.45698481798171997, 0.26904717087745667, 0.2432243525981903, 0.03243089094758034, 0.27306869626045227, 0.1573164165019989, 0.4054796099662781, 0.17774902284145355, 0.33468207716941833, 0.11806807667016983, 0.46422091126441956, 0.007868815213441849, 0.10871849954128265], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2755662500858307, 0.2504606544971466, 0.2939646244049072, 0.22535152733325958, 0.3838563859462738, 0.08475399762392044, 0.024598702788352966, 0.06131270155310631, 0.1569291353225708, 0.35336384177207947, 0.4402414560317993, 0.07155290246009827, 0.21702374517917633, 0.18575213849544525, 0.3693697154521942, 0.21475806832313538, 0.424307644367218, 0.48767903447151184, 0.45961010456085205, 0.2162783294916153, 0.46669960021972656, 0.4899812638759613, 0.15544338524341583, 0.37870535254478455], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b46042fd4384181bc348ef81c010d449(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 8, 6], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9169a6687477bfda78bb7973d02820e1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 6, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_99bf095c8f96213ee9a134c4fa7351ec(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4143522083759308, 0.09023312479257584, 0.05366210639476776, 0.22341036796569824, 0.4206608533859253, 0.0025261607952415943, 0.008699947968125343, 0.3305962383747101, 0.18837782740592957, 0.03756259009242058, 0.19152961671352386, 0.12541866302490234, 0.11540615558624268, 0.20320439338684082, 0.11763492226600647, 0.1069035455584526, 0.05343417450785637, 0.2359260618686676], dtype='float32').reshape([18]),
            paddle.to_tensor([0.03965636342763901, 0.4825538098812103, 0.03353164345026016, 0.37753984332084656, 0.2340703159570694, 0.0384044423699379, 0.3571694493293762, 0.06227966398000717, 0.38386839628219604, 0.32750144600868225, 0.034065160900354385, 0.13923537731170654, 0.35856133699417114, 0.11235411465167999, 0.2484164535999298, 0.23988203704357147, 0.26266592741012573, 0.44404831528663635], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2617731988430023, 0.13929156959056854, 0.41602909564971924, 0.07270349562168121, 0.2948741912841797, 0.18544843792915344, 0.21537354588508606, 0.14881764352321625, 0.07936079055070877, 0.32787278294563293, 0.22668787837028503, 0.07068541646003723, 0.4856133759021759, 0.45905646681785583, 0.14359840750694275, 0.10029336810112, 0.004364182241261005, 0.3004334568977356], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3022661507129669, 0.44535550475120544, 0.3844548165798187, 0.2908625304698944, 0.44551607966423035, 0.057494424283504486, 0.1991785317659378, 0.31530797481536865, 0.3195398449897766, 0.4130769968032837, 0.06584504246711731, 0.2582188844680786, 0.04691513255238533, 0.48989182710647583, 0.42491382360458374, 0.045918744057416916, 0.36282169818878174, 0.34278079867362976], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_48604b98b1458fb0e9218ec4a4763146(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1b1c7e4c8d991282aa0d0aa711fd3448(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 175, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d3d422084cbeacc4eea2d26a10e34d3f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.19033308327198029, 0.061783790588378906, 0.36539140343666077, 0.08385557681322098, 0.3539256751537323, 0.05386589467525482, 0.08563333004713058, 0.21607567369937897, 0.24408437311649323, 0.44392168521881104, 0.05027594044804573, 0.2216297835111618, 0.20301109552383423, 0.4735620617866516, 0.4231274724006653, 0.13892051577568054, 0.09386701136827469, 0.20591795444488525, 0.13879913091659546, 0.1299702525138855, 0.17276139557361603, 0.023812176659703255, 0.3213571608066559, 0.35490456223487854], dtype='float32').reshape([24]),
            paddle.to_tensor([0.31997328996658325, 0.265092134475708, 0.024718526750802994, 0.18356911838054657, 0.1720031052827835, 0.21711142361164093, 0.08993672579526901, 0.11537481844425201, 0.3627929091453552, 0.0031288289465010166, 0.1255275309085846, 0.4278813600540161, 0.2946752607822418, 0.17532765865325928, 0.3395592272281647, 0.4865560829639435, 0.42881911993026733, 0.38357868790626526, 0.19142261147499084, 0.399109810590744, 0.3681059777736664, 0.3552858829498291, 0.3381434679031372, 0.18719007074832916], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3938981890678406, 0.37771764397621155, 0.009254291653633118, 0.08564285933971405, 0.0489339679479599, 0.4543789327144623, 0.33963000774383545, 0.4044124186038971, 0.004271059297025204, 0.18075668811798096, 0.2688674330711365, 0.07323214411735535, 0.3081596791744232, 0.09433820843696594, 0.14479272067546844, 0.38478440046310425, 0.09605458378791809, 0.2795339524745941, 0.3117823302745819, 0.22183391451835632, 0.43745604157447815, 0.07568086683750153, 0.3078455328941345, 0.063601054251194], dtype='float32').reshape([24]),
            paddle.to_tensor([0.027441231533885002, 0.18154126405715942, 0.0255235955119133, 0.17837080359458923, 0.07276881486177444, 0.26073771715164185, 0.28580084443092346, 0.09935777634382248, 0.35906150937080383, 0.048252496868371964, 0.0068687135353684425, 0.4496943950653076, 0.3636619448661804, 0.38001877069473267, 0.491830438375473, 0.3312269151210785, 0.25157058238983154, 0.11947403848171234, 0.05202818661928177, 0.24783271551132202, 0.2415868639945984, 0.10534494370222092, 0.47635921835899353, 0.11476992070674896], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_db1890183f90469b6470f2eb30d82595(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 16, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_392f345d39880094263f542fc25200c0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b111c331e9bab02e6ac6eb5ca7dea2e4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_37328a85719eba45c513032bd175e19f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4296801686286926, 0.16850146651268005, 0.22747036814689636, 0.2948917746543884, 0.16486582159996033, 0.27970603108406067, 0.1574421525001526, 0.270935982465744, 0.17642134428024292, 0.13352639973163605, 0.4486847221851349, 0.09489593654870987, 0.20116227865219116, 0.06676997244358063, 0.11663904041051865, 0.4888508915901184, 0.14745084941387177, 0.4276158809661865, 0.19873791933059692, 0.08318653702735901, 0.09802006185054779, 0.2010367065668106, 0.17602171003818512, 0.05841708183288574, 0.32992517948150635, 0.32953259348869324, 0.4326615333557129, 0.41757649183273315, 0.06388859450817108, 0.16224347054958344], dtype='float32').reshape([30]),
            paddle.to_tensor([0.31487399339675903, 0.33124908804893494, 0.023550674319267273, 0.05911065265536308, 0.14550046622753143, 0.3063298165798187, 0.21326597034931183, 0.3151106536388397, 0.0018347851000726223, 0.4728044271469116, 0.20186923444271088, 0.13343311846256256, 0.46781355142593384, 0.2997799217700958, 0.10029749572277069, 0.046253129839897156, 0.00495222769677639, 0.38284555077552795, 0.16161447763442993, 0.42173460125923157, 0.28138694167137146, 0.340035617351532, 0.30753058195114136, 0.057923514395952225, 0.43467238545417786, 0.13116472959518433, 0.4550938010215759, 0.30147379636764526, 0.37138864398002625, 0.4186210334300995], dtype='float32').reshape([30]),
            paddle.to_tensor([0.28496551513671875, 0.07069287449121475, 0.26662567257881165, 0.2661984860897064, 0.11572450399398804, 0.3070482015609741, 0.18855167925357819, 0.333076149225235, 0.06898763030767441, 0.49379828572273254, 0.20828160643577576, 0.4940209984779358, 0.27574455738067627, 0.19674737751483917, 0.19044438004493713, 0.24304138123989105, 0.06811054795980453, 0.31803250312805176, 0.06678946316242218, 0.05451150983572006, 0.13115864992141724, 0.4864386320114136, 0.40947356820106506, 0.19089144468307495, 0.06856346130371094, 0.4834993779659271, 0.19701433181762695, 0.266978919506073, 0.25345438718795776, 0.2751521170139313], dtype='float32').reshape([30]),
            paddle.to_tensor([0.034434862434864044, 0.1461450308561325, 0.2759248912334442, 0.10630976408720016, 0.0769379734992981, 0.14843222498893738, 0.21995601058006287, 0.21400070190429688, 0.385270357131958, 0.2186964750289917, 0.4031415581703186, 0.25394681096076965, 0.35012707114219666, 0.1376063972711563, 0.4165757894515991, 0.24857927858829498, 0.09459474682807922, 0.08785949647426605, 0.15248629450798035, 0.4673437476158142, 0.2802220284938812, 0.2426050752401352, 0.053648434579372406, 0.16964176297187805, 0.3465198576450348, 0.12144570797681808, 0.40958210825920105, 0.10415555536746979, 0.0554073341190815, 0.019343765452504158], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9307efc97f2663214fb982288c5157c7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3521438241004944, 0.2634998559951782, 0.1924242377281189, 0.3636018931865692, 0.23043113946914673, 0.2576616108417511, 0.44425755739212036, 0.15249264240264893, 0.15007665753364563, 0.47118765115737915, 0.4953923523426056, 0.04923746734857559, 0.32309746742248535, 0.3125278353691101, 0.26138409972190857, 0.35624468326568604, 0.2798615097999573, 0.15493476390838623, 0.4252338111400604, 0.4513663053512573, 0.28440940380096436, 0.15945787727832794, 0.17264071106910706, 0.06571465730667114, 0.39349037408828735, 0.1005168929696083, 0.2604500651359558, 0.16394059360027313, 0.4754275381565094, 0.3745800852775574], dtype='float32').reshape([30]),
            paddle.to_tensor([0.31869977712631226, 0.010225740261375904, 0.005299024283885956, 0.32825276255607605, 0.42653170228004456, 0.1891709417104721, 0.10940933227539062, 0.4294331967830658, 0.36279723048210144, 0.33709609508514404, 0.0005484062130562961, 0.2664937973022461, 0.39132240414619446, 0.44062739610671997, 0.29327982664108276, 0.036951348185539246, 0.42434439063072205, 0.4242273271083832, 0.22410281002521515, 0.3888196051120758, 0.12962788343429565, 0.058984123170375824, 0.36611616611480713, 0.06991202384233475, 0.4567476212978363, 0.33382970094680786, 0.42532142996788025, 0.38233256340026855, 0.057707078754901886, 0.24380411207675934], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3106192350387573, 0.10903161764144897, 0.40178602933883667, 0.16680975258350372, 0.1524222195148468, 0.02699151635169983, 0.23004740476608276, 0.1230955421924591, 0.2757742404937744, 0.06710647791624069, 0.21404555439949036, 0.04648934677243233, 0.09864332526922226, 0.23175276815891266, 0.22821985185146332, 0.1793598085641861, 0.09776917099952698, 0.01061209011822939, 0.07585778832435608, 0.4319927394390106, 0.07379166781902313, 0.33332735300064087, 0.05070679634809494, 0.12354937940835953, 0.053952448070049286, 0.13170279562473297, 0.3317045569419861, 0.24618157744407654, 0.11239496618509293, 0.32003045082092285], dtype='float32').reshape([30]),
            paddle.to_tensor([0.312791645526886, 0.04157835245132446, 0.11468248814344406, 0.39411693811416626, 0.2993585765361786, 0.4646606743335724, 0.05306987091898918, 0.28072425723075867, 0.06897580623626709, 0.40668341517448425, 0.06355637311935425, 0.49386483430862427, 0.49774792790412903, 0.2925930917263031, 0.2882748544216156, 0.33896416425704956, 0.14696335792541504, 0.17839236557483673, 0.4948367476463318, 0.35712072253227234, 0.10199632495641708, 0.21365244686603546, 0.24110999703407288, 0.4876318573951721, 0.4172588884830475, 0.24042542278766632, 0.010716895572841167, 0.08449088782072067, 0.3565228581428528, 0.45190054178237915], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fbdb8ac77e97c0db51c073d4b42eb789(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4f81eb475f9ceb8fd6358ef73a947efb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8515fbb702653dd24718abeacc5dd679(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 256, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_151f76afa21cd8b8780c5d4d4a39fce9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 4, 50], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3372122347354889, 0.37127581238746643, 0.07512413710355759, 0.20737478137016296, 0.4123198986053467, 0.371592253446579, 0.04582241550087929, 0.3186837434768677, 0.4812367558479309, 0.39028072357177734, 0.12997527420520782, 0.2166699469089508, 0.3429040312767029, 0.003529770765453577, 0.2646673917770386, 0.06473272293806076, 0.16879867017269135, 0.13519012928009033, 0.1345781534910202, 0.0037901371251791716, 0.31910964846611023, 0.21747659146785736, 0.15963584184646606, 0.2905808687210083], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4955352544784546, 0.10499446839094162, 0.40464892983436584, 0.287630170583725, 0.36035314202308655, 0.35052263736724854, 0.19107931852340698, 0.21736089885234833, 0.2432757318019867, 0.18448606133460999, 0.0657658725976944, 0.08612699806690216, 0.35294070839881897, 0.30537474155426025, 0.07912910729646683, 0.366039901971817, 0.19140461087226868, 0.32194340229034424, 0.24225671589374542, 0.36856961250305176, 0.08804825693368912, 0.02010406367480755, 0.44656723737716675, 0.05698450282216072], dtype='float32').reshape([24]),
            paddle.to_tensor([0.20785029232501984, 0.11194425821304321, 0.21380096673965454, 0.4999781847000122, 0.0653536394238472, 0.11412793397903442, 0.2842485010623932, 0.09927261620759964, 0.26583412289619446, 0.4488154649734497, 0.37306705117225647, 0.07448063790798187, 0.013548902235925198, 0.25822022557258606, 0.14662526547908783, 0.35573938488960266, 0.1746036559343338, 0.23860809206962585, 0.005167955998331308, 0.18288975954055786, 0.4606817662715912, 0.42005646228790283, 0.2907536029815674, 0.018651144579052925], dtype='float32').reshape([24]),
            paddle.to_tensor([0.09350237995386124, 0.22129015624523163, 0.49763065576553345, 0.478801429271698, 0.06173161417245865, 0.40491679310798645, 0.10613454133272171, 0.49061986804008484, 0.434901624917984, 0.3991260230541229, 0.22236444056034088, 0.046440958976745605, 0.010106771253049374, 0.2997726500034332, 0.12817205488681793, 0.28413864970207214, 0.10684194415807724, 0.22036544978618622, 0.4593755900859833, 0.2337389588356018, 0.29085320234298706, 0.2667517066001892, 0.3671726882457733, 0.08145729452371597], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a4325fa88056847cad207cd9ee22f2a4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fa5b028763acf8883398433006dfcc90(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 200, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_eaa680a4ae5cdb85f42a36941b55ea3b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4f683497f258f869d3291d480f4d70ac(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 256, 512], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_faece81c88b8cb5cfee46d23390eb268(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c36a42869d60f34992b5389045efa76d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_98460042b9a2c0be929396e26143b5ac(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 240, 240], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4707430601119995, 0.3083232343196869, 0.30550849437713623, 0.0444360189139843, 0.0516342855989933, 0.44553160667419434, 0.330541729927063, 0.36207497119903564, 0.3083600699901581, 0.2009354680776596, 0.282088041305542, 0.19898702204227448, 0.06445202976465225, 0.37144654989242554, 0.24159102141857147, 0.09290210902690887, 0.040179699659347534, 0.056951750069856644, 0.4802343547344208, 0.017383795231580734, 0.37512075901031494, 0.33558809757232666, 0.22967883944511414, 0.20690800249576569], dtype='float32').reshape([24]),
            paddle.to_tensor([0.49917617440223694, 0.32553938031196594, 0.13454076647758484, 0.2586868405342102, 0.4293118119239807, 0.4079533815383911, 0.08943761885166168, 0.131760835647583, 0.4528030455112457, 0.464952290058136, 0.4766688048839569, 0.052425093948841095, 0.09186239540576935, 0.45004087686538696, 0.16044072806835175, 0.44987380504608154, 0.05072324350476265, 0.00411656778305769, 0.1113174632191658, 0.03371727466583252, 0.4385364055633545, 0.4819355607032776, 0.17675283551216125, 0.20928892493247986], dtype='float32').reshape([24]),
            paddle.to_tensor([0.058842308819293976, 0.37599608302116394, 0.11508715152740479, 0.17421498894691467, 0.10607001185417175, 0.10973756015300751, 0.13276265561580658, 0.49782779812812805, 0.09838378429412842, 0.4041840434074402, 0.14880411326885223, 0.1383381336927414, 0.4529649615287781, 0.020659642294049263, 0.10401812940835953, 0.020679907873272896, 0.09359073638916016, 0.31119459867477417, 0.2870163023471832, 0.4323306679725647, 0.14641951024532318, 0.4369726777076721, 0.2948836088180542, 0.16700010001659393], dtype='float32').reshape([24]),
            paddle.to_tensor([0.22130782902240753, 0.4732738137245178, 0.4678575396537781, 0.12475159019231796, 0.2963199019432068, 0.0867055281996727, 0.44632160663604736, 0.4481649100780487, 0.07113587111234665, 0.07588282972574234, 0.09766393154859543, 0.4168572425842285, 0.12851904332637787, 0.2118275761604309, 0.3379063308238983, 0.33356723189353943, 0.42712169885635376, 0.2391490340232849, 0.03331371769309044, 0.23254910111427307, 0.0465923435986042, 0.02446850948035717, 0.1286310851573944, 0.0023184397723525763], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d03e09abd0c9c3fcf4a66ab90e09616e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e019fb13c2774a1ba9c3252289915b90(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 256, 512], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_5e7cb23119f65b33bdbad71952f6dd12(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a4c2cd58bd8d455823bed9cb9544eee5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_5e7cb23119f65b33bdbad71952f6dd12
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c9e24031fa87c6968e4c41876914940e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 8, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_66979e97ef842260e7113e8657a75073(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_995b012684593e484bd318f7409d2597(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 100, 168], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_958a3ffded16a4004df47e4d68e64ad1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.24030008912086487, 0.33531197905540466, 0.30193156003952026, 0.37141644954681396, 0.010993698611855507, 0.21980910003185272, 0.38925471901893616, 0.18070977926254272, 0.20030374825000763, 0.34578976035118103, 0.36821261048316956, 0.46962228417396545, 0.4193284213542938, 0.051328688859939575, 0.3506799340248108, 0.24663448333740234, 0.3673935532569885, 0.20353929698467255, 0.3015676736831665, 0.2601739168167114, 0.151288703083992, 0.4620800018310547, 0.2053666114807129, 0.21110594272613525, 0.01329637411981821, 0.43858063220977783, 0.12394515424966812, 0.04022609815001488, 0.2678276598453522, 0.3992633521556854], dtype='float32').reshape([30]),
            paddle.to_tensor([0.37119361758232117, 0.33255836367607117, 0.18841660022735596, 0.07329722493886948, 0.304696649312973, 0.4069530665874481, 0.20825019478797913, 0.49849238991737366, 0.09716407209634781, 0.05861898511648178, 0.48435863852500916, 0.24084235727787018, 0.41852298378944397, 0.10544007271528244, 0.3617364764213562, 0.23245759308338165, 0.12194972485303879, 0.24642953276634216, 0.48364314436912537, 0.019660821184515953, 0.4918068051338196, 0.0523901991546154, 0.2626214921474457, 0.47265875339508057, 0.10220509767532349, 0.1738629788160324, 0.22329533100128174, 0.3223665952682495, 0.01852347142994404, 0.1233428567647934], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2159346342086792, 0.10125447064638138, 0.36376646161079407, 0.4207061231136322, 0.07834887504577637, 0.23134547472000122, 0.2160920947790146, 0.40997275710105896, 0.10793467611074448, 0.39598146080970764, 0.36581191420555115, 0.29705432057380676, 0.20749248564243317, 0.3091965615749359, 0.04583176597952843, 0.3695182800292969, 0.4766247272491455, 0.37721386551856995, 0.07580286264419556, 0.04258250817656517, 0.4048961102962494, 0.21050530672073364, 0.36325353384017944, 0.29342496395111084, 0.1400483399629593, 0.49730175733566284, 0.4012809991836548, 0.32119446992874146, 0.37737810611724854, 0.2787451148033142], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3100055754184723, 0.49935096502304077, 0.4809127449989319, 0.29628992080688477, 0.306522399187088, 0.16939474642276764, 0.42160624265670776, 0.1857052743434906, 0.25511038303375244, 0.08721796423196793, 0.2443964034318924, 0.3927842378616333, 0.4686567187309265, 0.33031606674194336, 0.3199315071105957, 0.28004029393196106, 0.15584298968315125, 0.05082456395030022, 0.20045028626918793, 0.41030558943748474, 0.45262038707733154, 0.432882159948349, 0.24287563562393188, 0.17646272480487823, 0.42431414127349854, 0.13708996772766113, 0.4720247983932495, 0.08667583018541336, 0.11503366380929947, 0.48217371106147766], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e955537f4e8f61f85f209244cfb2b955(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 8, 6], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8392266e22057d57d07b65bb0b93c6f8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4dbe1df24e24eacf815abd30731be171(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7c768321aa2dd0365749c3c67257aa46(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6f1bc158519854c96ce4c18119f95ff1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c2a8c60227dafa23e6c917c5f8a96ca5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 27, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3599008321762085, 0.4715549945831299, 0.4273189604282379, 0.19913628697395325, 0.13941621780395508, 0.22187647223472595, 0.02922574058175087, 0.1444474458694458, 0.10935895144939423, 0.24096065759658813, 0.3668748736381531, 0.3629041910171509, 0.21932664513587952, 0.4924296438694, 0.10742182284593582, 0.35538598895072937, 0.15626850724220276, 0.286246657371521, 0.13800889253616333, 0.09929581731557846, 0.4781992435455322, 0.14541158080101013, 0.18243655562400818, 0.30311086773872375, 0.3040241599082947, 0.3279459774494171, 0.3457222878932953], dtype='float32').reshape([27]),
            paddle.to_tensor([0.19115516543388367, 0.22543607652187347, 0.39814770221710205, 0.10938823223114014, 0.0008160569122992456, 0.2577814757823944, 0.0465485155582428, 0.47213974595069885, 0.034183092415332794, 0.40479180216789246, 0.4575212001800537, 0.11374108493328094, 0.051390476524829865, 0.37391963601112366, 0.08899467438459396, 0.2197105884552002, 0.045680932700634, 0.2131010740995407, 0.19437289237976074, 0.4685121476650238, 0.3259269595146179, 0.23793691396713257, 0.11954493075609207, 0.026546992361545563, 0.22881799936294556, 0.11443645507097244, 0.10215847194194794], dtype='float32').reshape([27]),
            paddle.to_tensor([0.3166285753250122, 0.21743564307689667, 0.0032481872476637363, 0.25692006945610046, 0.18997201323509216, 0.15557067096233368, 0.10102638602256775, 0.4338840842247009, 0.28017309308052063, 0.3151151239871979, 0.4784134030342102, 0.39234068989753723, 0.4741268754005432, 0.014899917878210545, 0.41482675075531006, 0.13152216374874115, 0.44898319244384766, 0.1003061905503273, 0.41345691680908203, 0.2821395695209503, 0.47759848833084106, 0.20695999264717102, 0.42671313881874084, 0.4966282248497009, 0.058193378150463104, 0.01137510035187006, 0.22117352485656738], dtype='float32').reshape([27]),
            paddle.to_tensor([0.11945287138223648, 0.19581294059753418, 0.4522266983985901, 0.44064146280288696, 0.3003448247909546, 0.3656059801578522, 0.18498043715953827, 0.486600399017334, 0.0415923073887825, 0.4946322739124298, 0.23139867186546326, 0.1422242373228073, 0.48129215836524963, 0.3287302255630493, 0.29536008834838867, 0.4032769203186035, 0.20083852112293243, 0.11990733444690704, 0.10685861110687256, 0.29727911949157715, 0.023647762835025787, 0.33363229036331177, 0.27978411316871643, 0.17065532505512238, 0.07143263518810272, 0.4585862457752228, 0.3584432005882263], dtype='float32').reshape([27]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e6697f78d40b8b5e7989f1b7405bd365(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3393304944038391, 0.3505360782146454, 0.1902753859758377, 0.12676185369491577, 0.3882170617580414, 0.14431503415107727, 0.30344265699386597, 0.4399348795413971, 0.11771593987941742, 0.3109327554702759, 0.03245004266500473, 0.2814350128173828, 0.49326378107070923, 0.25888359546661377, 0.1357273906469345, 0.1463402658700943, 0.30752408504486084, 0.08773299306631088, 0.06105037033557892, 0.08733902871608734], dtype='float32').reshape([20]),
            paddle.to_tensor([0.11323399841785431, 0.3168996572494507, 0.0034323418512940407, 0.2449289858341217, 0.2606663405895233, 0.1840064823627472, 0.4989255368709564, 0.29229336977005005, 0.02536662295460701, 0.4391978085041046, 0.010189450345933437, 0.3409424424171448, 0.3603039085865021, 0.06152138113975525, 0.06681063026189804, 0.495072603225708, 0.29306384921073914, 0.4416396915912628, 0.3280399441719055, 0.1933683156967163], dtype='float32').reshape([20]),
            paddle.to_tensor([0.17877903580665588, 0.2897096276283264, 0.17293718457221985, 0.03822876140475273, 0.02117180824279785, 0.3438563942909241, 0.2817404568195343, 0.4933847486972809, 0.037092745304107666, 0.11077114194631577, 0.4625266492366791, 0.41729798913002014, 0.08103036880493164, 0.20575711131095886, 0.23715300858020782, 0.4605301320552826, 0.20300811529159546, 0.19136092066764832, 0.31517133116722107, 0.23481117188930511], dtype='float32').reshape([20]),
            paddle.to_tensor([0.47740036249160767, 0.037460315972566605, 0.18204955756664276, 0.29747337102890015, 0.4902089834213257, 0.03741626814007759, 0.140269935131073, 0.34748566150665283, 0.3656749129295349, 0.10535885393619537, 0.07229162007570267, 0.03378019854426384, 0.21192999184131622, 0.34872323274612427, 0.07487164437770844, 0.1671876609325409, 0.11105236411094666, 0.25545141100883484, 0.47650307416915894, 0.23970083892345428], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c6fa826e1c802aad5b43f4e261623b79(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 61, 61], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7a744866386aab0c7cba4745ce3875c8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9e60825b03f828fb83877bb718213267(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_94507acd165b07b6baa4d8d0246e1900(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 26], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4fa6e0f95a1fb8606ad00df10b41b4a4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 12], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_53b04dff33e61d0d4776e6a2bad8a478(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1888, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1888], dtype='float32', min=0, max=0.5),
            paddle.uniform([1888], dtype='float32', min=0, max=0.5),
            paddle.uniform([1888], dtype='float32', min=0, max=0.5),
            paddle.uniform([1888], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_79754386fb192cfe16683b2ec62349dc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 31, 31], dtype='float16', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a817dafa4c5656d8c94d0e366593db27(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.0515180304646492, 0.11167518049478531, 0.09470667690038681, 0.40228143334388733, 0.2246038168668747, 0.41482123732566833, 0.3145681619644165, 0.10859698057174683, 0.3568870723247528, 0.327295184135437, 0.4860621690750122, 0.2192930281162262, 0.3654092252254486, 0.06172437593340874, 0.3572864234447479, 0.34082967042922974, 0.362062007188797, 0.3535630404949188, 0.2541467249393463, 0.04560444876551628], dtype='float32').reshape([20]),
            paddle.to_tensor([0.2864489257335663, 0.3412013351917267, 0.1391499787569046, 0.34484896063804626, 0.3698221445083618, 0.08573456853628159, 0.41859427094459534, 0.4902452826499939, 0.2594705820083618, 0.09525980800390244, 0.2309325784444809, 0.4866226017475128, 0.20902247726917267, 0.4356311559677124, 0.28580352663993835, 0.29708534479141235, 0.19041161239147186, 0.06160440668463707, 0.203993558883667, 0.4945465922355652], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4775758385658264, 0.42437222599983215, 0.3419039845466614, 0.37712883949279785, 0.4284151494503021, 0.18984942138195038, 0.04037822037935257, 0.32849109172821045, 0.32509899139404297, 0.28370052576065063, 0.19173629581928253, 0.475521445274353, 0.3551660478115082, 0.25395599007606506, 0.22385282814502716, 0.04829249903559685, 0.09374908357858658, 0.290189653635025, 0.27354344725608826, 0.12289248406887054], dtype='float32').reshape([20]),
            paddle.to_tensor([0.001385584007948637, 0.46952366828918457, 0.43071725964546204, 0.14091737568378448, 0.22216252982616425, 0.24271957576274872, 0.410348117351532, 0.12310497462749481, 0.3190038800239563, 0.4662030041217804, 0.010458331555128098, 0.2287975549697876, 0.32188111543655396, 0.12489690631628036, 0.41988858580589294, 0.13115525245666504, 0.08556794375181198, 0.18456265330314636, 0.056171953678131104, 0.17082250118255615], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3118afd092327a85086757fa6dfde0ab(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 32, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.36247891187667847, 0.4202049672603607, 0.38717347383499146, 0.19715845584869385, 0.42698878049850464, 0.4504951238632202, 0.3385770916938782, 0.22733476758003235, 0.15431837737560272, 0.14864814281463623, 0.40293267369270325, 0.17292040586471558, 0.0006904160254634917, 0.11907940357923508, 0.3829686641693115, 0.4144047498703003], dtype='float32').reshape([16]),
            paddle.to_tensor([0.25910016894340515, 0.16142161190509796, 0.3213244676589966, 0.08875632286071777, 0.2727956771850586, 0.46840620040893555, 0.4465813636779785, 0.2750799059867859, 0.26519402861595154, 0.46448904275894165, 0.49934831261634827, 0.36409953236579895, 0.34052565693855286, 0.07678312808275223, 0.3641171455383301, 0.23907166719436646], dtype='float32').reshape([16]),
            paddle.to_tensor([0.25852152705192566, 0.2590276300907135, 0.009824695065617561, 0.3631193935871124, 0.455710232257843, 0.46486175060272217, 0.08922995626926422, 0.3376123309135437, 0.21545517444610596, 0.46228981018066406, 0.10650762170553207, 0.31707432866096497, 0.31783220171928406, 0.2786126434803009, 0.2779282033443451, 0.14077994227409363], dtype='float32').reshape([16]),
            paddle.to_tensor([0.044380005449056625, 0.07059857994318008, 0.3585996627807617, 0.09160087257623672, 0.12484366446733475, 0.12611284852027893, 0.40427786111831665, 0.3353532552719116, 0.23720325529575348, 0.3499491810798645, 0.39214497804641724, 0.031216580420732498, 0.3683663308620453, 0.17100520431995392, 0.4086519181728363, 0.33254697918891907], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_299272a36ec614b22a58ec5bbdc3a4a9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.29283443093299866, 0.09264598786830902, 0.3212057054042816, 0.012855164706707, 0.13733939826488495, 0.19944752752780914, 0.32550862431526184, 0.24407486617565155, 0.26969602704048157, 0.2967386245727539, 0.4049423038959503, 0.23604369163513184, 0.05647695064544678, 0.06956937909126282, 0.0574280247092247, 0.34678369760513306, 0.4353073537349701, 0.46517470479011536, 0.053200386464595795, 0.20935289561748505, 0.4715809226036072, 0.36390385031700134, 0.07667671889066696, 0.2753547728061676, 0.39848628640174866, 0.3099711537361145, 0.21179762482643127, 0.2530459761619568], dtype='float32').reshape([28]),
            paddle.to_tensor([0.45329180359840393, 0.27172860503196716, 0.1538723260164261, 0.023552360013127327, 0.3663256764411926, 0.04999624565243721, 0.16657549142837524, 0.05301456153392792, 0.1930655837059021, 0.32332533597946167, 0.2741474211215973, 0.290209025144577, 0.35595405101776123, 0.0018633415456861258, 0.3428519368171692, 0.24935868382453918, 0.015429023653268814, 0.47889646887779236, 0.335284948348999, 0.042814843356609344, 0.31749168038368225, 0.3257886469364166, 0.057398125529289246, 0.3637043237686157, 0.14951343834400177, 0.4137517809867859, 0.3309738337993622, 0.12272844463586807], dtype='float32').reshape([28]),
            paddle.to_tensor([0.13403619825839996, 0.022781934589147568, 0.04141956940293312, 0.23999203741550446, 0.2942163944244385, 0.3792167007923126, 0.32527562975883484, 0.18797895312309265, 0.39579248428344727, 0.2223854809999466, 0.46685633063316345, 0.03253614529967308, 0.44579610228538513, 0.4314827024936676, 0.39746594429016113, 0.10125001519918442, 0.24167628586292267, 0.30712512135505676, 0.27830466628074646, 0.38944846391677856, 0.1974324882030487, 0.21320132911205292, 0.4874829947948456, 0.1130654439330101, 0.39445462822914124, 0.057348232716321945, 0.3319174349308014, 0.3901619017124176], dtype='float32').reshape([28]),
            paddle.to_tensor([0.3314806818962097, 0.016604583710432053, 0.22216446697711945, 0.24936115741729736, 0.007215563673526049, 0.41999292373657227, 0.2146531045436859, 0.46907109022140503, 0.4481370151042938, 0.3501875698566437, 0.2592909336090088, 0.3023819625377655, 0.19501031935214996, 0.4842931032180786, 0.048146821558475494, 0.004824239760637283, 0.4007270336151123, 0.1659783571958542, 0.0014524904545396566, 0.04970421642065048, 0.3353857100009918, 0.23064975440502167, 0.35677069425582886, 0.29607293009757996, 0.4272577464580536, 0.2620064914226532, 0.3738826513290405, 0.137170672416687], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1c7cfefbaf278ab78f60978f0ae4e142(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 17, 17], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ab2a28a2c38c47c606f4dba5154041a8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 10, 10], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e54a6a35d70cac51a3d14aa2063b9f61(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 50, 84], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b6bd8b01b7b47644eafd81894307bb66(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1632, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1632], dtype='float32', min=0, max=0.5),
            paddle.uniform([1632], dtype='float32', min=0, max=0.5),
            paddle.uniform([1632], dtype='float32', min=0, max=0.5),
            paddle.uniform([1632], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_851cb4eebe53841cc73526feded6db07(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 60, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e4c7e25f1301f94472cb3086fc804438(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 480, 480], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.41833004355430603, 0.2539440393447876, 0.10420938581228256, 0.4012240469455719, 0.3207617402076721, 0.2237565517425537, 0.0013434516731649637, 0.10668681561946869], dtype='float32').reshape([8]),
            paddle.to_tensor([0.38233888149261475, 0.1394396275281906, 0.49462637305259705, 0.3865812420845032, 0.3414652645587921, 0.3889056444168091, 0.015528307296335697, 0.2695756256580353], dtype='float32').reshape([8]),
            paddle.to_tensor([0.19193702936172485, 0.1885913759469986, 0.2798801064491272, 0.30370810627937317, 0.11074444651603699, 0.3026326298713684, 0.06420537084341049, 0.19971995055675507], dtype='float32').reshape([8]),
            paddle.to_tensor([0.055587317794561386, 0.3810986876487732, 0.2440190464258194, 0.35267502069473267, 0.19083474576473236, 0.1866830438375473, 0.24079270660877228, 0.040956079959869385], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1a1f2849bb8b420b4f1f901c960beb59(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 14, 25], dtype='float16', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7e19e021a85ead28cdbbf7ec51804e44(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 128, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_abdaaab8c8060b378c49e64ad433ae82(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 8, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_582b87eb12e5187687dce92893184a24(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 4, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8c6dccf1c702ebe105911afd50e22556(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 4, 3], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_013ddcec6017c89b64f9987936c1a751(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 256, 512], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4de5d81e8af6b44d3ec66d3cb146fdf1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 76, 76], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8ab29aa41f40d83dd1f017e1d79c2e74(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 25, 42], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8834139b6e81cbef21eb6c154545d04e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f27b73d4f01bbe03786b1a2c1cd9ca5b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_16baf02b4527b73c17598177911ff1b4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_01b3f2d579f6874d604b63749660b257(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 208, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_917cc8a4b10b86d5c2c65cf264ee37c2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4b460388ed3007bc0d10f9dcf9dd3a93(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 320, 320], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3072814345359802, 0.4155638813972473, 0.18911702930927277, 0.15975065529346466, 0.3195071816444397, 0.38342469930648804, 0.40491950511932373, 0.18897272646427155], dtype='float32').reshape([8]),
            paddle.to_tensor([0.2688722014427185, 0.4234241545200348, 0.12074004113674164, 0.41327446699142456, 0.4320865273475647, 0.4847188889980316, 0.4615023136138916, 0.33715298771858215], dtype='float32').reshape([8]),
            paddle.to_tensor([0.3598969876766205, 0.15177136659622192, 0.40267109870910645, 0.22164379060268402, 0.2168308049440384, 0.2471771240234375, 0.2410879135131836, 0.26273876428604126], dtype='float32').reshape([8]),
            paddle.to_tensor([0.2201746106147766, 0.19571474194526672, 0.16985256969928741, 0.18727344274520874, 0.2780952751636505, 0.3556196391582489, 0.011361634358763695, 0.4676913022994995], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3d707893348799ff82cab38f282844e6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1056, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1056], dtype='float32', min=0, max=0.5),
            paddle.uniform([1056], dtype='float32', min=0, max=0.5),
            paddle.uniform([1056], dtype='float32', min=0, max=0.5),
            paddle.uniform([1056], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9fbf45bc52a5e9effbf1909fdc573523(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 8, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8475c464a7ea42b8ab132bd49c791dea(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0ddbc0b70247b0a58783404413824345(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 512], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.47537270188331604, 0.46721431612968445, 0.030640296638011932, 0.32138609886169434, 0.03506624326109886, 0.15896578133106232, 0.40749016404151917, 0.28598785400390625, 0.07085679471492767, 0.3182154893875122, 0.21904554963111877, 0.16934604942798615, 0.473979651927948, 0.3338947296142578, 0.26565903425216675, 0.21784673631191254], dtype='float32').reshape([16]),
            paddle.to_tensor([0.19256602227687836, 0.010849693790078163, 0.1448400765657425, 0.04058509320020676, 0.2837921380996704, 0.29185280203819275, 0.014661408960819244, 0.002361083636060357, 0.11041314154863358, 0.21088582277297974, 0.3528505265712738, 0.3582024872303009, 0.4471915364265442, 0.4861012399196625, 0.10387611389160156, 0.21033835411071777], dtype='float32').reshape([16]),
            paddle.to_tensor([0.16844980418682098, 0.39174947142601013, 0.14703667163848877, 0.18308675289154053, 0.30361834168434143, 0.24537532031536102, 0.14910480380058289, 0.13972368836402893, 0.33525219559669495, 0.22012324631214142, 0.4270714819431305, 0.13362954556941986, 0.4415742754936218, 0.07783839106559753, 0.43898820877075195, 0.3433351516723633], dtype='float32').reshape([16]),
            paddle.to_tensor([0.35905012488365173, 0.16054494678974152, 0.4393172562122345, 0.2788681387901306, 0.12248639017343521, 0.27746546268463135, 0.4052385687828064, 0.40324047207832336, 0.15952716767787933, 0.34625959396362305, 0.3418326675891876, 0.09174857288599014, 0.25304490327835083, 0.41966304183006287, 0.17648668587207794, 0.16695688664913177], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9ec3947b599c84b5e222e8fdaf1f1e07(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ee89ee362ec528719d879b4e46f494b0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.024394117295742035, 0.16608397662639618, 0.39127349853515625, 0.2579449713230133, 0.3778083622455597, 0.26918449997901917, 0.22096747159957886, 0.27972647547721863, 0.42989861965179443, 0.482263445854187, 0.38257402181625366, 0.3023761808872223, 0.12431960552930832, 0.18716305494308472, 0.372970312833786, 0.4616803526878357, 0.2953384220600128, 0.27042144536972046, 0.10838546603918076, 0.37453168630599976], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4060116112232208, 0.32293546199798584, 0.3494107127189636, 0.4774286150932312, 0.06525621563196182, 0.029075976461172104, 0.2989582419395447, 0.4706970751285553, 0.11066258698701859, 0.35591045022010803, 0.4479406177997589, 0.40076127648353577, 0.4730687439441681, 0.034630876034498215, 0.46762993931770325, 0.23165668547153473, 0.20898263156414032, 0.21122850477695465, 0.0947323739528656, 0.22304897010326385], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3255082666873932, 0.04662975296378136, 0.3510530889034271, 0.48704278469085693, 0.17594626545906067, 0.38835543394088745, 0.10864648222923279, 0.425007700920105, 0.27215760946273804, 0.34581974148750305, 0.3691293001174927, 0.38876113295555115, 0.21800978481769562, 0.030869806185364723, 0.46365129947662354, 0.3731813430786133, 0.3073389530181885, 0.22167643904685974, 0.06356840580701828, 0.3095495104789734], dtype='float32').reshape([20]),
            paddle.to_tensor([0.024469411000609398, 0.37108901143074036, 0.2650968134403229, 0.05128438025712967, 0.1455305963754654, 0.35242313146591187, 0.02546103298664093, 0.03832660987973213, 0.477691113948822, 0.28286346793174744, 0.34200260043144226, 0.3976231515407562, 0.2559843063354492, 0.335410475730896, 0.14163172245025635, 0.30638906359672546, 0.24182015657424927, 0.2000674158334732, 0.18805518746376038, 0.11600079387426376], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b59f47c4d53477462af5d042f93103b4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 76, 76], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_08db05122c1b633a0530508462126b52(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_39efe9e3488ffdc7984afef5584797fa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1440, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1440], dtype='float32', min=0, max=0.5),
            paddle.uniform([1440], dtype='float32', min=0, max=0.5),
            paddle.uniform([1440], dtype='float32', min=0, max=0.5),
            paddle.uniform([1440], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_efa044b8c49bc663d99ae2b96ab11e8d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_d589af777f5a8ae699611515c41499fb
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_24f544f1c7e0357a7fe180cbaa670e7b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.20963041484355927, 0.30135124921798706, 0.33708903193473816, 0.41912126541137695, 0.02708006463944912, 0.16823379695415497, 0.20517338812351227, 0.07442141324281693, 0.4138312041759491, 0.30604949593544006, 0.3943370580673218, 0.44149020314216614, 0.4321403503417969, 0.31504306197166443, 0.4379296898841858, 0.005568269640207291, 0.3519279956817627, 0.32481059432029724, 0.1273709237575531, 0.13322369754314423], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3149268925189972, 0.08992619067430496, 0.3999568819999695, 0.28510645031929016, 0.07357127964496613, 0.26122573018074036, 0.3329636752605438, 0.0004234088701196015, 0.005985698662698269, 0.02639433555305004, 0.08171986043453217, 0.20647840201854706, 0.3773266673088074, 0.2389547973871231, 0.21602238714694977, 0.2298736423254013, 0.48685622215270996, 0.31449413299560547, 0.16641004383563995, 0.047260530292987823], dtype='float32').reshape([20]),
            paddle.to_tensor([0.16128984093666077, 0.14163736999034882, 0.3337455093860626, 0.1034112200140953, 0.48277363181114197, 0.020141486078500748, 0.49687743186950684, 0.3669193387031555, 0.1171555295586586, 0.3647761940956116, 0.025501856580376625, 0.10431760549545288, 0.344316691160202, 0.20511320233345032, 0.3746279180049896, 0.14879544079303741, 0.3743923604488373, 0.2848590910434723, 0.4495290219783783, 0.47343558073043823], dtype='float32').reshape([20]),
            paddle.to_tensor([0.10303404182195663, 0.32412204146385193, 0.4414026439189911, 0.49245893955230713, 0.013462134636938572, 0.2946668267250061, 0.17088130116462708, 0.01553448848426342, 0.29572761058807373, 0.3355156183242798, 0.008128207176923752, 0.41370463371276855, 0.20156116783618927, 0.04292457923293114, 0.34648483991622925, 0.028604010120034218, 0.38709425926208496, 0.07740012556314468, 0.0908312052488327, 0.04901192709803581], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b0ffc51c83cb735fb1213f7531d5d896(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.24517583847045898, 0.09859170019626617, 0.470348984003067, 0.49619847536087036, 0.41214028000831604, 0.4793819189071655, 0.042651399970054626, 0.1552116870880127, 0.3284558653831482, 0.2056744396686554, 0.1321984976530075, 0.003812078619375825, 0.08104123175144196, 0.1716149002313614, 0.09542135894298553, 0.45001089572906494], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3059656620025635, 0.11681680381298065, 0.4259011745452881, 0.22037380933761597, 0.07020875811576843, 0.25637656450271606, 0.31325823068618774, 0.11218125373125076, 0.3136160373687744, 0.2849470376968384, 0.29929566383361816, 0.42133307456970215, 0.3196330964565277, 0.15717847645282745, 0.38709190487861633, 0.4809654951095581], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4945526123046875, 0.22993454337120056, 0.09774745255708694, 0.06525417417287827, 0.47777748107910156, 0.08411025255918503, 0.2029532492160797, 0.2330281138420105, 0.47268611192703247, 0.1320037841796875, 0.2462359219789505, 2.7680478524416685e-05, 0.13407333195209503, 0.40817251801490784, 0.42418643832206726, 0.3078768253326416], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4413852095603943, 0.22919084131717682, 0.41450169682502747, 0.0014364488888531923, 0.31324928998947144, 0.040461838245391846, 0.04175868630409241, 0.3906300961971283, 0.4198525846004486, 0.38853639364242554, 0.32822656631469727, 0.18017449975013733, 0.115049347281456, 0.10969792306423187, 0.27654334902763367, 0.33452194929122925], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e17ccab8683234328cdfc2eb162c541c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 162, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([162], dtype='float32', min=0, max=0.5),
            paddle.uniform([162], dtype='float32', min=0, max=0.5),
            paddle.uniform([162], dtype='float32', min=0, max=0.5),
            paddle.uniform([162], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1dff30227db4c56cfe175d9ad8d54667(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 960, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6159b4ea3940844fcc323f50c946d485(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 728, 19, 19], dtype='float16', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3a661a55a7fad362bb165f61941e5011(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 144, 31, 31], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_641fa009ab43def83afabee0e551174b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d76dc775e3dff6308121c6b6063db1f8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_75b1040599ed84d9de2a5eeda5646b48(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 576, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a085e37fa0df592c782fb69c75b0c5c5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7e58733432019e545cb4a40927925308(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b36209feb2b87f521aff2a79b25e7a36(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.33847731351852417, 0.3124159574508667, 0.3910957872867584, 0.23744429647922516, 0.03857569396495819, 0.4830593168735504, 0.1323859542608261, 0.4767334759235382, 0.42340293526649475, 0.41830047965049744, 0.06840912252664566, 0.3339732587337494, 0.14432081580162048, 0.17872367799282074, 0.35750865936279297, 0.15244872868061066, 0.44342923164367676, 0.16313792765140533], dtype='float32').reshape([18]),
            paddle.to_tensor([0.1429460644721985, 0.37270277738571167, 0.4194437563419342, 0.2576755881309509, 0.28185343742370605, 0.37198466062545776, 0.15224669873714447, 0.3740210235118866, 0.32123762369155884, 0.26440146565437317, 0.33323657512664795, 0.3205483853816986, 0.2651202380657196, 0.47502410411834717, 0.17364954948425293, 0.334696501493454, 0.3610243797302246, 0.04386550188064575], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3618355393409729, 0.16337601840496063, 0.29919251799583435, 0.2643045485019684, 0.3948245942592621, 0.21442773938179016, 0.0329890213906765, 0.032626714557409286, 0.13402561843395233, 0.05273488909006119, 0.44111061096191406, 0.4383426606655121, 0.3864944577217102, 0.37913456559181213, 0.44113945960998535, 0.4246220588684082, 0.35757505893707275, 0.35615548491477966], dtype='float32').reshape([18]),
            paddle.to_tensor([0.09100713580846786, 0.13393783569335938, 0.46411702036857605, 0.3636414408683777, 0.0476260781288147, 0.10884369164705276, 0.34723660349845886, 0.03790036588907242, 0.3911115229129791, 0.009111236780881882, 0.08062925189733505, 0.2175544649362564, 0.39325523376464844, 0.005802461877465248, 0.057142406702041626, 0.34172457456588745, 0.15833832323551178, 0.12920209765434265], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1d9b5ffef488c01337cae32e44553350(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.029809987172484398, 0.48248541355133057, 0.20824921131134033, 0.23680810630321503, 0.005193858407437801, 0.16343259811401367, 0.21807695925235748, 0.23211035132408142, 0.17211584746837616, 0.3170715570449829, 0.3271678686141968, 0.37048372626304626, 0.12215957045555115, 0.4256030023097992, 0.23199865221977234, 0.35118046402931213, 0.32609257102012634, 0.16485080122947693, 0.14608298242092133, 0.06964943557977676, 0.16236263513565063, 0.43635183572769165, 0.47489383816719055, 0.35164526104927063, 0.2809337079524994, 0.2824704945087433, 0.21043621003627777, 0.2096114456653595, 0.3060641586780548, 0.313840389251709], dtype='float32').reshape([30]),
            paddle.to_tensor([0.13533219695091248, 0.4166257679462433, 0.07355310022830963, 0.4713052809238434, 0.17794378101825714, 0.10307981073856354, 0.39452528953552246, 0.3358686566352844, 0.03716476261615753, 0.2722981572151184, 0.31503263115882874, 0.34871581196784973, 0.1417395919561386, 0.15366816520690918, 0.03136509284377098, 0.056975431740283966, 0.09944257885217667, 0.000656387594062835, 0.13942667841911316, 0.3272082805633545, 0.07775333523750305, 0.036040298640728, 0.0594702772796154, 0.2552551031112671, 0.11186566203832626, 0.02920062281191349, 0.3145790696144104, 0.18233780562877655, 0.3771715462207794, 0.08404635637998581], dtype='float32').reshape([30]),
            paddle.to_tensor([0.14884847402572632, 0.11579373478889465, 0.3374885320663452, 0.4897761046886444, 0.23048259317874908, 0.31729891896247864, 0.4165734350681305, 0.023783601820468903, 0.442448228597641, 0.11658728867769241, 0.4280387759208679, 0.28862059116363525, 0.3561858534812927, 0.10989981144666672, 0.21752561628818512, 0.22643277049064636, 0.06252162903547287, 0.013954788446426392, 0.23497441411018372, 0.4699402153491974, 0.2724408209323883, 0.4592883288860321, 0.34975409507751465, 0.08308345824480057, 0.183619424700737, 0.06264133006334305, 0.2515404224395752, 0.04180658236145973, 0.3571755290031433, 0.05104319751262665], dtype='float32').reshape([30]),
            paddle.to_tensor([0.1569233387708664, 0.4869340658187866, 0.45207780599594116, 0.19048519432544708, 0.05362469702959061, 0.4735521376132965, 0.14584560692310333, 0.23359765112400055, 0.32600224018096924, 0.3178476095199585, 0.2560195326805115, 0.47955092787742615, 0.3415490984916687, 0.0721176490187645, 0.2457156479358673, 0.06313887983560562, 0.4417594373226166, 0.10457021743059158, 0.4766246974468231, 0.09790708869695663, 0.4392637312412262, 0.1912400722503662, 0.26569685339927673, 0.17129527032375336, 0.2533070743083954, 0.39472365379333496, 0.367836058139801, 0.10159685462713242, 0.061401959508657455, 0.019012736156582832], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b6579daf6a7ea8852537f1ebafd127f1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 17, 17], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fc69e580e1f5bf769ce9df1dbdadaf51(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 32, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_02fc409db1efd356d649c7a1c95ceca0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_14001a54693369c25d83a9d825817dfb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 48, 48], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.08274570852518082, 0.22527837753295898, 0.08412066102027893, 0.13781698048114777, 0.34610575437545776, 0.37865149974823, 0.026341723278164864, 0.4689643681049347, 0.11090592294931412, 0.3626517355442047, 0.10361934453248978, 0.22242827713489532, 0.4379810094833374, 0.23286741971969604, 0.23653526604175568, 0.19939570128917694, 0.15509790182113647, 0.1467747986316681, 0.3364872932434082, 0.36537420749664307, 0.0711466446518898, 0.14743542671203613, 0.3676055371761322, 0.21302929520606995], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4052691161632538, 0.3927687704563141, 0.14142364263534546, 0.41117772459983826, 0.493992418050766, 0.004092145711183548, 0.41945841908454895, 0.11907800287008286, 0.2520372271537781, 0.3274073004722595, 0.12093483656644821, 0.25954338908195496, 0.39420369267463684, 0.4880412817001343, 0.1962292194366455, 0.2686851918697357, 0.4982832074165344, 0.3322891294956207, 0.2127729058265686, 0.22980406880378723, 0.09283041208982468, 0.3561941683292389, 0.17708978056907654, 0.23209607601165771], dtype='float32').reshape([24]),
            paddle.to_tensor([0.13267293572425842, 0.42446085810661316, 0.2941845953464508, 0.22116898000240326, 0.2711072862148285, 0.22737012803554535, 0.3776032626628876, 0.44021737575531006, 0.21586550772190094, 0.49463143944740295, 0.004663141909986734, 0.11301582306623459, 0.3200382590293884, 0.295857697725296, 0.24393071234226227, 0.47826385498046875, 0.19178363680839539, 0.4448016881942749, 0.3590034246444702, 0.2977183759212494, 0.33572423458099365, 0.31149768829345703, 0.4477490782737732, 0.03247086703777313], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2850115895271301, 0.3635723888874054, 0.4407515227794647, 0.26548174023628235, 0.48624977469444275, 0.2742544412612915, 0.2810647785663605, 0.43051865696907043, 0.25692281126976013, 0.053795669227838516, 0.35242360830307007, 0.14149110019207, 0.1750240921974182, 0.39901745319366455, 0.27087923884391785, 0.3836256265640259, 0.20007216930389404, 0.3603062331676483, 0.13078640401363373, 0.39739111065864563, 0.1960020810365677, 0.3632916212081909, 0.08900683373212814, 0.332838773727417], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fb67f3ad68844a718d35b5d825e1c137(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 432, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b340f80c367196e43f70603799bfa159(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 30, 30], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_518c63d08a0a618b675e0240f4a2b37a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 240, 240], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_61915cea78ad895c7199a5fb51c36321(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 236, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([236], dtype='float32', min=0, max=0.5),
            paddle.uniform([236], dtype='float32', min=0, max=0.5),
            paddle.uniform([236], dtype='float32', min=0, max=0.5),
            paddle.uniform([236], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ffe7764d7dd8ba02e3d7794753ad6a44(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 120, 120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fb440921a2081dfef2c49c166e5ec6c8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 12, 12], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a9c1d02ca50853324e19c23a57391970(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 320, 320], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4364447295665741, 0.47170713543891907, 0.2559099495410919, 0.012170618399977684, 0.42604392766952515, 0.4344456195831299, 0.09575474262237549, 0.44834262132644653], dtype='float32').reshape([8]),
            paddle.to_tensor([0.04167900234460831, 0.48515841364860535, 0.3778352439403534, 0.19000130891799927, 0.28105098009109497, 0.2534582316875458, 0.3162919878959656, 0.21559298038482666], dtype='float32').reshape([8]),
            paddle.to_tensor([0.3854382336139679, 0.17396827042102814, 0.4742254614830017, 0.4928750991821289, 0.14701980352401733, 0.42810508608818054, 0.4448983669281006, 0.11131999641656876], dtype='float32').reshape([8]),
            paddle.to_tensor([0.035615865141153336, 0.03287143260240555, 0.37116631865501404, 0.4310572147369385, 0.26065516471862793, 0.2793191373348236, 0.08599124103784561, 0.36274272203445435], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_752a1e247095788c510518931c242ac8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 1, 1], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2f7708197cbf2a878490f5319b673eaa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 144, 16, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_11e0bdaed2841d89b397581dce666cf7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_413e9bf57a4f044069c86c92c72e60b6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fe687a83c7395703e953e11f1301849f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cc9135e64bbbe71235b834ac72f6c4b4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.23315446078777313, 0.4636867344379425, 0.46511802077293396, 0.07970800250768661, 0.2558555006980896, 0.41675955057144165, 0.2153673768043518, 0.07212620973587036, 0.20221242308616638, 0.44491928815841675, 0.2353954315185547, 0.15095758438110352], dtype='float32').reshape([12]),
            paddle.to_tensor([0.24318784475326538, 0.05230306088924408, 0.0913606733083725, 0.47072833776474, 0.219106063246727, 0.08777112513780594, 0.19051994383335114, 0.3302369713783264, 0.4548604488372803, 0.15151658654212952, 0.4859029948711395, 0.20834386348724365], dtype='float32').reshape([12]),
            paddle.to_tensor([0.07449798285961151, 0.19756585359573364, 0.07472719997167587, 0.052884791046381, 0.4317648112773895, 0.47903963923454285, 0.4806690514087677, 0.0259410310536623, 0.12908296287059784, 0.22180894017219543, 0.0583978109061718, 0.40134045481681824], dtype='float32').reshape([12]),
            paddle.to_tensor([0.4555370807647705, 0.14233966171741486, 0.02939923107624054, 0.20278394222259521, 0.11052921414375305, 0.3054825961589813, 0.007380036637187004, 0.3229342997074127, 0.21680128574371338, 0.16772858798503876, 0.49776285886764526, 0.15918143093585968], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7e746e3013ad885c814ef8db594b1a65(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 16, 12], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b2286ecc58aae8c27c5dc5ab2b12f31a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 28, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b9f6a474f5cb148d14fbd14dbc7d9622(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 4, 3], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e5515205ff6279da181e30520651049c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2f2ec5544e17c5110d7a56a24e7f5d9f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 640, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b2df2ce156ba2a71e0dc28492905ca9d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_be2634e3e0da946dca4cd0e651ae6105(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6122b1a0441921508d39c58afd6eb10e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ec70c7c394f42a08d00d717446e5c53f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1d7d0aa1be55253ec5e9c76e0a035bbe(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.14459273219108582, 0.024095507338643074, 0.3476986885070801, 0.20615103840827942, 0.26460254192352295, 0.23991897702217102, 0.022046927362680435, 0.22142167389392853, 0.01248415932059288, 0.31154191493988037, 0.03940447419881821, 0.18356017768383026, 0.26745501160621643, 0.286708265542984, 0.008384933695197105, 0.4642043113708496, 0.46411117911338806, 0.3762763440608978, 0.19936881959438324, 0.13112983107566833], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3699999153614044, 0.21047371625900269, 0.12253688275814056, 0.45898115634918213, 0.46422433853149414, 0.33960190415382385, 0.28758126497268677, 0.3778947591781616, 0.22645577788352966, 0.48514485359191895, 0.4476509690284729, 0.07449857145547867, 0.08194658160209656, 0.14459586143493652, 0.05679960176348686, 0.10644818097352982, 0.2505178451538086, 0.0018533668480813503, 0.23418273031711578, 0.13980403542518616], dtype='float32').reshape([20]),
            paddle.to_tensor([0.02726040966808796, 0.27924904227256775, 0.040859777480363846, 0.044623009860515594, 0.23886768519878387, 0.16345368325710297, 0.2922157347202301, 0.0618223175406456, 0.001072237268090248, 0.45980343222618103, 0.20621217787265778, 0.10264287143945694, 0.1368662565946579, 0.47087180614471436, 0.08342915773391724, 0.4227316677570343, 0.0717761218547821, 0.41636452078819275, 0.307820588350296, 0.23049581050872803], dtype='float32').reshape([20]),
            paddle.to_tensor([0.11535176634788513, 0.4463125765323639, 0.42211365699768066, 0.25976455211639404, 0.10107231885194778, 0.4703599810600281, 0.14288420975208282, 0.005165786482393742, 0.37490564584732056, 0.13600970804691315, 0.4392315447330475, 0.46707114577293396, 0.12501341104507446, 0.45849835872650146, 0.4773644506931305, 0.2813575267791748, 0.17532046139240265, 0.023347148671746254, 0.39999550580978394, 0.05236988887190819], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_934ad465a492909700d4b4e94475bb82(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 320, 320], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c36ba18e77462c0a827bc6c454f266dc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 8, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_fc6b6917e04b7b7cf9fdb39dde97ec40(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_06921f41e7bb15c71174748cc6494574(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_fc6b6917e04b7b7cf9fdb39dde97ec40
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9f45d081c0b6f9e483d080e1b3c5115c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_546d57d58adbde1715af6dce37e0207a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 2, 6], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_eb777c0c87b0930ad8ac08a1fcc8297f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_932bdc22f3c5cf405ec0d4df0befe3f3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([8, 1024, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_506f37d223f243152f90bc806a124c3a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_327cf002253a6c4a03642cdf43912302(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1536, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cafeb6f7b4cac2346d4cba08981d90bf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 64, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_252abe33809bd5a1b2d1b323abde7880(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.15870264172554016, 0.08179840445518494, 0.2555045783519745, 0.13559268414974213, 0.3272929787635803, 0.0021847370080649853, 0.31089675426483154, 0.3783971667289734, 0.19196085631847382, 0.2469853162765503, 0.0853034183382988, 0.38556018471717834, 0.046899255365133286, 0.0983014851808548, 0.1812293976545334, 0.41646748781204224, 0.10487067699432373, 0.4314686357975006, 0.034498125314712524, 0.0836564302444458, 0.2418028563261032, 0.23749910295009613, 0.3415778577327728, 0.08205791562795639], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4689770042896271, 0.24506059288978577, 0.3124406933784485, 0.3895983397960663, 0.2881889045238495, 0.43058472871780396, 0.21405521035194397, 0.08352069556713104, 0.10765278339385986, 0.2327437698841095, 0.22273661196231842, 0.15946072340011597, 0.21872578561306, 0.35086789727211, 0.21431277692317963, 0.47474661469459534, 0.0628049224615097, 0.29901373386383057, 0.17326873540878296, 0.48790666460990906, 0.21820347011089325, 0.2879149317741394, 0.4173589050769806, 0.06750992685556412], dtype='float32').reshape([24]),
            paddle.to_tensor([0.42343375086784363, 0.21021714806556702, 0.187782883644104, 0.43633195757865906, 0.09608349949121475, 0.053037308156490326, 0.12935923039913177, 0.05508900061249733, 0.271167516708374, 0.0585177056491375, 0.29714319109916687, 0.1276717334985733, 0.03868412226438522, 0.17937111854553223, 0.3114568293094635, 0.06259685009717941, 0.2222684770822525, 0.2084183543920517, 0.21514424681663513, 0.027896903455257416, 0.26333627104759216, 0.4058845043182373, 0.13764135539531708, 0.4379864037036896], dtype='float32').reshape([24]),
            paddle.to_tensor([0.13382306694984436, 0.17624709010124207, 0.29420971870422363, 0.28130438923835754, 0.3084712028503418, 0.04667123034596443, 0.4868357181549072, 0.47382932901382446, 0.4502076506614685, 0.4978216886520386, 0.4449895918369293, 0.4898793399333954, 0.2410140186548233, 0.4304933249950409, 0.0332149975001812, 0.22893087565898895, 0.04545527324080467, 0.13994312286376953, 0.37130609154701233, 0.285138338804245, 0.28108513355255127, 0.14250074326992035, 0.4013824760913849, 0.31042689085006714], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_908f29e2942977ed1a09d74ec5005e39(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 62, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([62], dtype='float32', min=0, max=0.5),
            paddle.uniform([62], dtype='float32', min=0, max=0.5),
            paddle.uniform([62], dtype='float32', min=0, max=0.5),
            paddle.uniform([62], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e3ea92bb071d55144e266a6c8f999e16(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f2b1aa3b9465437313283e091ce2d7ec(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c2928656c51c1ba6ab13381a19867213(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8cffce9648203988a5f986e3ac4c4f15(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 320, 320], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c0be56c9e55ad7c7068d1c27b836fdaa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 19, 19], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e59e5e3b4e3af3a1769b356c33d1f663(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_97b47dc24d5154d982ef8cc701e9017d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_78ee2e26ae0f78d9914b70443d5e057e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 200, 336], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1ba0fb5cc0ff05a24f4f30802d06b3bb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 400, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([400], dtype='float32', min=0, max=0.5),
            paddle.uniform([400], dtype='float32', min=0, max=0.5),
            paddle.uniform([400], dtype='float32', min=0, max=0.5),
            paddle.uniform([400], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_acf72844352797819772688d4a94f29b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2241479456424713, 0.16149412095546722, 0.39225664734840393, 0.09738703072071075, 0.19720089435577393, 0.3526401221752167, 0.45978668332099915, 0.4565722346305847, 0.19141773879528046, 0.28006112575531006, 0.10117442905902863, 0.3571628928184509, 0.1808970421552658, 0.4551233649253845, 0.4246382713317871, 0.20781166851520538], dtype='float32').reshape([16]),
            paddle.to_tensor([0.11075103282928467, 0.15832476317882538, 0.009188735857605934, 0.10865060240030289, 0.43982070684432983, 0.47262871265411377, 0.0027290189173072577, 0.2891731262207031, 0.464931845664978, 0.38145482540130615, 0.24041356146335602, 0.12038613855838776, 0.09536751359701157, 0.4162313938140869, 0.1934739202260971, 0.4613366723060608], dtype='float32').reshape([16]),
            paddle.to_tensor([0.20169250667095184, 0.2522692084312439, 0.34927672147750854, 0.1488318294286728, 0.006493082270026207, 0.310269296169281, 0.3043769598007202, 0.025560323148965836, 0.2904142141342163, 0.26990780234336853, 0.1963782012462616, 0.2250208556652069, 0.4769775867462158, 0.16757777333259583, 0.41750261187553406, 0.30604326725006104], dtype='float32').reshape([16]),
            paddle.to_tensor([0.43489259481430054, 0.02195930667221546, 0.4006229639053345, 0.04247095808386803, 0.20968514680862427, 0.47694534063339233, 0.23382866382598877, 0.29104083776474, 0.3411550223827362, 0.19451455771923065, 0.30477258563041687, 0.11321452260017395, 0.3556589186191559, 0.3575263023376465, 0.09503794461488724, 0.25042209029197693], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_026a1368cd4f20a985083c719213675a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b114937a1009f2830939d865db725b1d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d3659f024956290ab910c58f1526246e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fde2b4dd08efa7bcbdf3d088fe09e9e3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 144, 16, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_97c4070257cbb4c2a6ef119fbc133e23(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 416, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9896baa85c577d8626e158f27caf5d23(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ac505b07b23666ee81d73f2c39804a2c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 512], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3394086956977844, 0.05886742100119591, 0.35338881611824036, 0.00922586303204298, 0.01528816670179367, 0.4167952537536621, 0.1655600666999817, 0.4488588869571686, 0.009904604405164719, 0.38911759853363037, 0.20420962572097778, 0.3413611352443695, 0.03842098265886307, 0.2836001515388489, 0.4916467070579529, 0.035178571939468384], dtype='float32').reshape([16]),
            paddle.to_tensor([0.30225926637649536, 0.12453030794858932, 0.06315727531909943, 0.08074341714382172, 0.0022760494612157345, 0.23570165038108826, 0.07115232199430466, 0.08874677121639252, 0.4532238841056824, 0.29749372601509094, 0.3718124032020569, 0.14718572795391083, 0.24221280217170715, 0.08526422083377838, 0.15514954924583435, 0.13696439564228058], dtype='float32').reshape([16]),
            paddle.to_tensor([0.36155450344085693, 0.33636242151260376, 0.22965599596500397, 0.19508297741413116, 0.03470722958445549, 0.20674555003643036, 0.11109324544668198, 0.14212332665920258, 0.4145423173904419, 0.29054248332977295, 0.3526359498500824, 0.29688194394111633, 0.024595677852630615, 0.38541242480278015, 0.3359188437461853, 0.15922923386096954], dtype='float32').reshape([16]),
            paddle.to_tensor([0.36200717091560364, 0.0912167951464653, 0.41640549898147583, 0.038477420806884766, 0.44727253913879395, 0.13161878287792206, 0.45604372024536133, 0.41551658511161804, 0.47296515107154846, 0.23814937472343445, 0.1020084023475647, 0.03957798704504967, 0.24019445478916168, 0.16884814202785492, 0.3032993674278259, 0.1203862801194191], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6a9b6334f75cb517fd3b5aff823dace1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e7b8406cf69c549f72734dacd726eb79(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 608, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([608], dtype='float32', min=0, max=0.5),
            paddle.uniform([608], dtype='float32', min=0, max=0.5),
            paddle.uniform([608], dtype='float32', min=0, max=0.5),
            paddle.uniform([608], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8469fd0e4207c6d70f56ab35ff869c0a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.40009304881095886, 0.3349747657775879, 0.48736003041267395, 0.1317724585533142, 0.33662205934524536, 0.13114069402217865, 0.058423541486263275, 0.3337283134460449, 0.3572932779788971, 0.15179073810577393, 0.24488236010074615, 0.46459317207336426, 0.27522873878479004, 0.1102449968457222, 0.007056697271764278, 0.19926626980304718, 0.3422830402851105, 0.32985061407089233, 0.44765958189964294, 0.2982674837112427, 0.39730945229530334, 0.20878013968467712, 0.2868348956108093, 0.10942593216896057], dtype='float32').reshape([24]),
            paddle.to_tensor([0.007285154424607754, 0.30456435680389404, 0.273568332195282, 0.4112740755081177, 0.46367311477661133, 0.42524343729019165, 0.31695348024368286, 0.29734981060028076, 0.23305630683898926, 0.08717239648103714, 0.42134615778923035, 0.1566598266363144, 0.05777401477098465, 0.23046493530273438, 0.0829382911324501, 0.3390514552593231, 0.3647174835205078, 0.30926308035850525, 0.2662542462348938, 0.23669905960559845, 0.34031596779823303, 0.17671869695186615, 0.040532954037189484, 0.112544946372509], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4993886649608612, 0.23577290773391724, 0.18508127331733704, 0.45369797945022583, 0.06415704637765884, 0.10665977746248245, 0.3487963080406189, 0.37580084800720215, 0.4885682761669159, 0.3200163245201111, 0.26154589653015137, 0.45290613174438477, 0.0040749236941337585, 0.08882022649049759, 0.2826091945171356, 0.16897757351398468, 0.10390986502170563, 0.49403101205825806, 0.24348925054073334, 0.32943370938301086, 0.3961455821990967, 0.30295515060424805, 0.06292536109685898, 0.21371592581272125], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2783222496509552, 0.21749964356422424, 0.25988250970840454, 0.2851879596710205, 0.25267115235328674, 0.027947433292865753, 0.12162886559963226, 0.03211422264575958, 0.4037295877933502, 0.2188722938299179, 0.346263587474823, 0.3518937826156616, 0.2864886224269867, 0.41009730100631714, 0.2759087085723877, 0.24625249207019806, 0.08398021012544632, 0.3782162368297577, 0.14427582919597626, 0.2440941035747528, 0.3066064119338989, 0.09002844244241714, 0.05349859595298767, 0.36052051186561584], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a145b94d20563d9cc7e605270e2b1385(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 88, 61, 61], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f536b4656537b8c9fafa34f5be9cc133(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3e231f747c0aeadd056215b75e37d26a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 64, 128], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.32604265213012695, 0.05176621675491333, 0.3820158839225769, 0.3709590435028076, 0.3957884907722473, 0.30472782254219055, 0.0989818423986435, 0.24290964007377625, 0.3017330765724182, 0.3612930476665497, 0.03182608634233475, 0.13399915397167206, 0.0021262443624436855, 0.32295238971710205, 0.29925569891929626, 0.05116040259599686, 0.26534610986709595, 0.32584235072135925], dtype='float32').reshape([18]),
            paddle.to_tensor([0.41562244296073914, 0.12590952217578888, 0.09263752400875092, 0.07405546307563782, 0.25515323877334595, 0.3574783205986023, 0.012357721105217934, 0.34974464774131775, 0.43218088150024414, 0.37717655301094055, 0.07727911323308945, 0.3927627205848694, 0.009560457430779934, 0.18518899381160736, 0.3997080326080322, 0.3529604375362396, 0.1126907542347908, 0.40181517601013184], dtype='float32').reshape([18]),
            paddle.to_tensor([0.12588371336460114, 0.264504611492157, 0.39096516370773315, 0.12121831625699997, 0.12683133780956268, 0.4858394265174866, 0.4128013551235199, 0.08358172327280045, 0.12675057351589203, 0.03889911621809006, 0.17997269332408905, 0.32459670305252075, 0.40975162386894226, 0.42521873116493225, 0.2857133448123932, 0.05215287208557129, 0.16072586178779602, 0.46440115571022034], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3421706259250641, 0.2024533450603485, 0.12501268088817596, 0.4664056599140167, 0.47789373993873596, 0.2914150655269623, 0.19492347538471222, 0.4381455183029175, 0.2006426602602005, 0.2951396703720093, 0.38333168625831604, 0.24967063963413239, 0.13142631947994232, 0.4119114577770233, 0.439765065908432, 0.07402483373880386, 0.001763882813975215, 0.3849310874938965], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6698206ccd58209134b1df29b48d85d0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_fc6b6917e04b7b7cf9fdb39dde97ec40
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0e8e13213e1803188b870ccd5e98c352(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 704, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([704], dtype='float32', min=0, max=0.5),
            paddle.uniform([704], dtype='float32', min=0, max=0.5),
            paddle.uniform([704], dtype='float32', min=0, max=0.5),
            paddle.uniform([704], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_56e2e055f345f5b1781847323966694c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1044, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1044], dtype='float32', min=0, max=0.5),
            paddle.uniform([1044], dtype='float32', min=0, max=0.5),
            paddle.uniform([1044], dtype='float32', min=0, max=0.5),
            paddle.uniform([1044], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_95d0a3c4a737721d0c1f89f19bcb49c0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.31006866693496704, 0.20104128122329712, 0.07687697559595108, 0.3324059844017029, 0.2932848334312439, 0.18651555478572845, 0.14402736723423004, 0.006922771222889423, 0.36746105551719666, 0.1451701670885086, 0.12866108119487762, 0.04583314061164856, 0.3358868360519409, 0.3892361521720886, 0.17676028609275818, 0.023225679993629456, 0.4057839810848236, 0.30886971950531006, 0.006094175856560469, 0.08092835545539856, 0.48305419087409973, 0.1031113937497139, 0.41309064626693726, 0.27025556564331055], dtype='float32').reshape([24]),
            paddle.to_tensor([0.39092904329299927, 0.1308845430612564, 0.0281541645526886, 0.32621464133262634, 0.050770387053489685, 0.2913188636302948, 0.051732901483774185, 0.4198657274246216, 0.36422377824783325, 0.17733493447303772, 0.14887969195842743, 0.18682293593883514, 0.3597981631755829, 0.20338498055934906, 0.3493158221244812, 0.00946212187409401, 0.27936503291130066, 0.14709453284740448, 0.45285725593566895, 0.461360901594162, 0.041604407131671906, 0.37118998169898987, 0.4480957090854645, 0.29281625151634216], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4265877306461334, 0.20576193928718567, 0.3784376084804535, 0.117625892162323, 0.1468696892261505, 0.4192963242530823, 0.21832065284252167, 0.26613569259643555, 0.281019389629364, 0.4486064612865448, 0.2945074737071991, 0.43971389532089233, 0.40758243203163147, 0.4822447597980499, 0.2841428816318512, 0.41789937019348145, 0.42178940773010254, 0.22893060743808746, 0.333281546831131, 0.4457021951675415, 0.4692763090133667, 0.25267478823661804, 0.2726149260997772, 0.3728589415550232], dtype='float32').reshape([24]),
            paddle.to_tensor([0.12124038487672806, 0.16826796531677246, 0.44998905062675476, 0.2563076913356781, 0.11270714551210403, 0.1091746985912323, 0.11676405370235443, 0.08737187087535858, 0.004091736860573292, 0.22654470801353455, 0.17129935324192047, 0.16592083871364594, 0.39309489727020264, 0.129721000790596, 0.21421445906162262, 0.28685837984085083, 0.12470228970050812, 0.039206378161907196, 0.03554407134652138, 0.000989800551906228, 0.18394158780574799, 0.023059669882059097, 0.08835505694150925, 0.49777206778526306], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5bd330aec97027f5f0759d455d0ab73f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 240, 240], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4f51ace0e2ebe67d942c802bcff5f009(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a4c6e30cb749b023689623a89faa08a9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.20289917290210724, 0.41931113600730896, 0.4446706473827362, 0.10151306539773941, 0.2856050729751587, 0.025048526003956795, 0.10349266976118088, 0.38964319229125977, 0.31319302320480347, 0.05477367714047432, 0.3810518682003021, 0.06635492295026779, 0.3689437508583069, 0.22456039488315582, 0.040400829166173935, 0.27693653106689453, 0.11080341786146164, 0.3765484690666199, 0.2720630168914795, 0.3269958794116974], dtype='float32').reshape([20]),
            paddle.to_tensor([0.17752762138843536, 0.42924389243125916, 0.42621782422065735, 0.1619190275669098, 0.19283708930015564, 0.03130519762635231, 0.26220372319221497, 0.47507962584495544, 0.16645410656929016, 0.4223785996437073, 0.29569411277770996, 0.09361831098794937, 0.4953266382217407, 0.3391281068325043, 0.05222262069582939, 0.3897625803947449, 0.4832853078842163, 0.21715222299098969, 0.009555731900036335, 0.4473879337310791], dtype='float32').reshape([20]),
            paddle.to_tensor([0.16031666100025177, 0.2549089789390564, 0.33519744873046875, 0.47351884841918945, 0.2929611802101135, 0.044110678136348724, 0.020965775474905968, 0.39124229550361633, 0.47002509236335754, 0.0016957817133516073, 0.4481492042541504, 0.010179830715060234, 0.33396339416503906, 0.2011951059103012, 0.30086246132850647, 0.04644758626818657, 0.1906030923128128, 0.2348155826330185, 0.45500776171684265, 0.22465354204177856], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4713950455188751, 0.2103370875120163, 0.25823843479156494, 0.09858907759189606, 0.23977670073509216, 0.31057238578796387, 0.20364397764205933, 0.1335088014602661, 0.29891151189804077, 0.38863053917884827, 0.45624837279319763, 0.4169326424598694, 0.08299661427736282, 0.3751789331436157, 0.30401915311813354, 0.2279655784368515, 0.4755952060222626, 0.3220054507255554, 0.1575629562139511, 0.3746696412563324], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ac50710aa695d22d836aafd358efef5b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2383243590593338, 0.21551422774791718, 0.4102381467819214, 0.08976100385189056, 0.3338403105735779, 0.3756836950778961, 0.1551467478275299, 0.20318017899990082, 0.12028640508651733, 0.0445249043405056, 0.005108886864036322, 0.15372703969478607, 0.3576674461364746, 0.05784505605697632, 0.18975049257278442, 0.48511016368865967, 0.38401341438293457, 0.18244020640850067, 0.40336811542510986, 0.10710885375738144, 0.3256513178348541, 0.04635808989405632, 0.43869534134864807, 0.19640713930130005, 0.1516435593366623, 0.31464672088623047, 0.07872030138969421, 0.4893677532672882, 0.1744055151939392, 0.08307171612977982], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4042902886867523, 0.3096008002758026, 0.26407235860824585, 0.41511204838752747, 0.4350566864013672, 0.2388591170310974, 0.17392002046108246, 0.4058035612106323, 0.03810067102313042, 0.11971953511238098, 0.4438354969024658, 0.037075988948345184, 0.1288325935602188, 0.4366191625595093, 0.10949278622865677, 0.46109628677368164, 0.2393002212047577, 0.06773537397384644, 0.1369413137435913, 0.46254414319992065, 0.3867463767528534, 0.10905594378709793, 0.21417708694934845, 0.0965351089835167, 0.2745437026023865, 0.03702978417277336, 0.2944711744785309, 0.14755181968212128, 0.44871315360069275, 0.494851291179657], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3835173547267914, 0.1795719861984253, 0.2188231348991394, 0.2791731059551239, 0.033152855932712555, 0.40704333782196045, 0.11151057481765747, 0.40651533007621765, 0.3356356918811798, 0.4629265367984772, 0.0801527202129364, 0.2479049563407898, 0.05393964797258377, 0.11111978441476822, 0.15752659738063812, 0.39228516817092896, 0.24516607820987701, 0.23047704994678497, 0.2548021972179413, 0.07939847558736801, 0.2572724521160126, 0.2708364427089691, 0.03041025623679161, 0.33251258730888367, 0.33933791518211365, 0.34997981786727905, 0.4071348011493683, 0.2853190004825592, 0.418538361787796, 0.31047698855400085], dtype='float32').reshape([30]),
            paddle.to_tensor([0.12715354561805725, 0.01812676712870598, 0.1833089143037796, 0.3078896999359131, 0.20163662731647491, 0.42839881777763367, 0.1874561458826065, 0.10704897344112396, 0.32639992237091064, 0.3674710690975189, 0.20422209799289703, 0.4015246331691742, 0.444312185049057, 0.376785010099411, 0.3330431580543518, 0.31244024634361267, 0.23332315683364868, 0.01302859466522932, 0.48346105217933655, 0.02267812192440033, 0.4497637152671814, 0.492214173078537, 0.16085751354694366, 0.10046105086803436, 0.08043918758630753, 0.4532094895839691, 0.2696821391582489, 0.41664448380470276, 0.09632106870412827, 0.41970065236091614], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8a6a060c8f346231c0eb898a1e937ab3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 120, 120], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8f59aa7663625a1bfb2ea9232682d630(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6311937a32c65a39e7ecdb865d1effb3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4d18c1cfcc58c5093d1779f277da386e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9b4ddf112b4c78e73e1831e99073c066(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f2cda932dcb43c99d1b2ddf4d8595bf5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_07abc2fa4dee0778c0d7c2d77d374a24(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 208, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_98334398df21fef8658cbfd6ace00ab8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.002336402889341116, 0.44564440846443176, 0.3859848976135254, 0.4326305389404297, 0.3429679274559021, 0.07126295566558838, 0.1996988207101822, 0.06553026288747787, 0.1027672290802002, 0.4734850823879242, 0.26194092631340027, 0.288652241230011, 0.2936058044433594, 0.15005335211753845, 0.3145369291305542, 0.4265761971473694, 0.20244723558425903, 0.0567011684179306, 0.028948212042450905, 0.466887503862381, 0.25089231133461, 0.432081013917923, 0.1654052436351776, 0.038698818534612656, 0.45026737451553345, 0.17016811668872833, 0.21382512152194977, 0.31960898637771606, 0.4479752480983734, 0.41040292382240295], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2802204191684723, 0.12533703446388245, 0.04893531650304794, 0.4570886492729187, 0.4669747054576874, 0.25419020652770996, 0.4450252056121826, 0.25803354382514954, 0.08932540565729141, 0.3964511454105377, 0.3029288053512573, 0.4639095962047577, 0.014905606396496296, 0.1678670048713684, 0.4562871754169464, 0.1207091212272644, 0.08331947028636932, 0.1372871994972229, 0.1925271898508072, 0.3096475899219513, 0.38846221566200256, 0.3132610023021698, 0.3429754674434662, 0.37241876125335693, 0.46166250109672546, 0.25285565853118896, 0.18367786705493927, 0.12306434661149979, 0.1650647073984146, 0.4099213182926178], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3355773985385895, 0.2930689752101898, 0.3651348948478699, 0.3803901970386505, 0.4300001263618469, 0.4429491460323334, 0.42781564593315125, 0.13245263695716858, 0.25777509808540344, 0.32565367221832275, 0.32762405276298523, 0.33291807770729065, 0.47305676341056824, 0.0946403443813324, 0.48111557960510254, 0.4297392964363098, 0.12636056542396545, 0.34297358989715576, 0.2377946674823761, 0.3458094596862793, 0.10098080337047577, 0.4343855082988739, 0.0630006492137909, 0.24171330034732819, 0.1444050669670105, 0.17534933984279633, 0.11751718074083328, 0.3695174753665924, 0.36290696263313293, 0.029246142134070396], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4556911289691925, 0.38131436705589294, 0.250993937253952, 0.3980681896209717, 0.23760175704956055, 0.29813048243522644, 0.1167299672961235, 0.18988265097141266, 0.06835924088954926, 0.10246291756629944, 0.31175127625465393, 0.17643466591835022, 0.1922314316034317, 0.4879738390445709, 0.4678378701210022, 0.4970352053642273, 0.3621622622013092, 0.35824453830718994, 0.18161198496818542, 0.3527626097202301, 0.1463862657546997, 0.018190938979387283, 0.031405702233314514, 0.14193996787071228, 0.2163723111152649, 0.26444023847579956, 0.46021202206611633, 0.4539867639541626, 0.19978725910186768, 0.2521665394306183], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e08a467cbf68d452f9404daccfaecf04(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 15, 15], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9a5c8fd16e5e495fe7f909f999eb01a9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 4, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c3c5faf99de1ca42aa0607f79f4c1ba3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 38, 38], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d40a0ce9c72056e3d65edf44a2fce08f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 200, 336], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cc1faf6f796f1bb90da08d6fb8b18275(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c3b3fc5f26abcf991fa140db5aff1bb5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.09560687839984894, 0.3302118480205536, 0.3539700210094452, 0.4827580153942108, 0.19638976454734802, 0.08572851121425629, 0.4317251741886139, 0.4975491464138031, 0.31715497374534607, 0.08809894323348999, 0.1864108145236969, 0.20778757333755493, 0.23007257282733917, 0.3073134422302246, 0.13155652582645416, 0.00687577947974205, 0.48942136764526367, 0.3915342092514038, 0.3116041123867035, 0.028539124876260757, 0.3449195623397827, 0.33130472898483276, 0.08222680538892746, 0.22440288960933685, 0.000674920214805752, 0.031843673437833786, 0.11326268315315247, 0.30171436071395874, 0.4858938157558441, 0.3090038001537323], dtype='float32').reshape([30]),
            paddle.to_tensor([0.15487022697925568, 0.04000256210565567, 0.3600822687149048, 0.30676907300949097, 0.23507839441299438, 0.1907573640346527, 0.12585817277431488, 0.061220910400152206, 0.10708305984735489, 0.3782014846801758, 0.03733990713953972, 0.10674113035202026, 0.2737589180469513, 0.431535005569458, 0.2001653015613556, 0.15049709379673004, 0.1747920960187912, 0.20104444026947021, 0.29798322916030884, 0.0649527758359909, 0.09966836869716644, 0.2634970247745514, 0.3355505168437958, 0.3360685706138611, 0.3720645308494568, 0.42427703738212585, 0.07842613011598587, 0.11293823271989822, 0.45594358444213867, 0.36419352889060974], dtype='float32').reshape([30]),
            paddle.to_tensor([0.15606725215911865, 0.13399863243103027, 0.08001237362623215, 0.04772048816084862, 0.15477728843688965, 0.32056301832199097, 0.40268391370773315, 0.3466870188713074, 0.2294011116027832, 0.14723269641399384, 0.0006787522579543293, 0.0022473447024822235, 0.15061992406845093, 0.1268245428800583, 0.4765360653400421, 0.29583117365837097, 0.10272420197725296, 0.22844789922237396, 0.40568915009498596, 0.4685220718383789, 0.4469188153743744, 0.3224388360977173, 0.3387529253959656, 0.49178698658943176, 0.2109324336051941, 0.29422104358673096, 0.45199188590049744, 0.20137141644954681, 0.12865526974201202, 0.10125403106212616], dtype='float32').reshape([30]),
            paddle.to_tensor([0.039517179131507874, 0.3718312680721283, 0.290416955947876, 0.21918384730815887, 0.4696072041988373, 0.3282955288887024, 0.43682917952537537, 0.332626610994339, 0.4121006429195404, 0.16570140421390533, 0.24383676052093506, 0.3851095139980316, 0.13014890253543854, 0.0347818098962307, 0.02349872514605522, 0.13167281448841095, 0.3804126977920532, 0.41279199719429016, 0.008315726183354855, 0.34884729981422424, 0.2666173279285431, 0.09334746748209, 0.12675143778324127, 0.13290897011756897, 0.2688843905925751, 0.11188651621341705, 0.4745289087295532, 0.13074596226215363, 0.09255920350551605, 0.2775094211101532], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d023fc383f4c2160e6d1bd7a3050ed22(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cfbcd3f6d0cd9fc41c6fa7817eddc552(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 8, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_04ee250d3f7b909faa7e4fb9b9bb33cc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_236d86d5f0638c40a3f1874a57050b35(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_79ce24998f3e759d86ab3b960574ab50(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7141c770764a87dcb1346b4d8004e1d1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 512], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.03441232070326805, 0.47062039375305176, 0.4030188322067261, 0.3905690312385559, 0.35807859897613525, 0.23405598104000092, 0.34953629970550537, 0.20588605105876923, 0.37454837560653687, 0.3402574062347412, 0.23559826612472534, 0.3722689151763916, 0.017579223960638046, 0.379357248544693, 0.4130583107471466, 0.3164435923099518], dtype='float32').reshape([16]),
            paddle.to_tensor([0.22911839187145233, 0.14363881945610046, 0.1348298043012619, 0.036166347563266754, 0.06680549681186676, 0.4835948944091797, 0.4437221586704254, 0.1062360554933548, 0.026848962530493736, 0.13959965109825134, 0.14172355830669403, 0.20804496109485626, 0.43337371945381165, 0.08648577332496643, 0.4026079773902893, 0.4977194368839264], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4216596484184265, 0.3140668570995331, 0.3083404004573822, 0.41283556818962097, 0.4760388433933258, 0.40332505106925964, 0.023308178409934044, 0.31425026059150696, 0.05087043717503548, 0.030269509181380272, 0.34281182289123535, 0.3554861545562744, 0.47668778896331787, 0.38446980714797974, 0.2867615520954132, 0.43984782695770264], dtype='float32').reshape([16]),
            paddle.to_tensor([0.11635126918554306, 0.356946736574173, 0.4847904145717621, 0.03607535734772682, 0.010216879658401012, 0.09197629988193512, 0.2505456805229187, 0.27185437083244324, 0.07948756217956543, 0.11681206524372101, 0.49370571970939636, 0.3913532793521881, 0.30059534311294556, 0.23721900582313538, 0.040434401482343674, 0.3999439477920532], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7334d82914828d2e07ceb816ab037200(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_33c70920af34986458894bbd38487c47(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 16, 32], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.1480657309293747, 0.3003461956977844, 0.43717557191848755, 0.4666649103164673, 0.14447712898254395, 0.4147184193134308, 0.04869455844163895, 0.06245984137058258, 0.006621882319450378, 0.09781456738710403, 0.4174865484237671, 0.23873692750930786, 0.31156647205352783, 0.42686885595321655, 0.07124195992946625, 0.027529612183570862, 0.025813044980168343, 0.09645006060600281], dtype='float32').reshape([18]),
            paddle.to_tensor([0.19147783517837524, 0.47851431369781494, 0.44325971603393555, 0.1053769513964653, 0.0030501887667924166, 0.19819891452789307, 0.2595590353012085, 0.15911392867565155, 0.43178221583366394, 0.08648314327001572, 0.4432601034641266, 0.48591378331184387, 0.377926230430603, 0.08052629232406616, 0.21232819557189941, 0.49486544728279114, 0.396616131067276, 0.06379857659339905], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2895251214504242, 0.3510499894618988, 0.21186299622058868, 0.2880382537841797, 0.45703381299972534, 0.23586368560791016, 0.4640379846096039, 0.3509095013141632, 0.39673328399658203, 0.08053740859031677, 0.31722259521484375, 0.19993770122528076, 0.013621119782328606, 0.4119488000869751, 0.338462233543396, 0.1250407099723816, 0.10620828717947006, 0.09181782603263855], dtype='float32').reshape([18]),
            paddle.to_tensor([0.29964354634284973, 0.20632949471473694, 0.3709201514720917, 0.03843787685036659, 0.3209397494792938, 0.498049795627594, 0.12760190665721893, 0.3257625699043274, 0.4524790644645691, 0.1712704747915268, 0.2530481815338135, 0.3912024199962616, 0.18156760931015015, 0.3035679757595062, 0.22192659974098206, 0.20749828219413757, 0.2418023645877838, 0.3083897531032562], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_650cb3d5dac21c3a05ed2dba9346ea24(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 8, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3326b2235a23a219b6aa8471c04c280e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 35, 35], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8a0c5342c6a9e56fdfcfcf259d92bcf1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d80ad80c6536ec5615ec97e69d0daee3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f0d004a9f801f540989da8f415a851ee(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 3, 3], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a6ed405cf659c4787d83ff95f1485357(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f43e355aa01dc3784db9500e3911cd50(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.to_tensor([[[[88592.8984375]], [[106968.7734375]], [[94144.453125]], [[96680.140625]], [[99676.703125]], [[105608.484375]], [[99952.296875]], [[103403.1640625]], [[104053.8359375]], [[105514.3125]], [[100274.8203125]], [[107971.125]], [[89384.6640625]], [[100848.296875]], [[105065.6484375]], [[96743.015625]], [[98812.9375]], [[94960.6015625]], [[103534.0546875]], [[95268.6015625]], [[97233.0078125]], [[102297.0390625]], [[106664.984375]], [[98176.8359375]], [[103419.2734375]]]], dtype='float32').reshape([1, 25, 1, 1]),
            paddle.to_tensor([0.06522124260663986, 0.19632172584533691, 0.20577485859394073, 0.39686280488967896, 0.1844586730003357, 0.014327886514365673, 0.45444196462631226, 0.019776135683059692, 0.4715750515460968, 0.07283101975917816, 0.3882042467594147, 0.013818126171827316, 0.4319390058517456, 0.042269930243492126, 0.3586297035217285, 0.45473283529281616, 0.042424242943525314, 0.41963791847229004, 0.3056403696537018, 0.4862538278102875, 0.3777759373188019, 0.2785787582397461, 0.2530137002468109, 0.21899285912513733, 0.02975073643028736], dtype='float32').reshape([25]),
            paddle.to_tensor([0.33073627948760986, 0.0282816793769598, 0.34281906485557556, 0.44802650809288025, 0.40853607654571533, 0.03782935440540314, 0.17385555803775787, 0.3958992660045624, 0.07158462703227997, 0.061196859925985336, 0.18715961277484894, 0.19811929762363434, 0.4746493399143219, 0.06613739579916, 0.38990476727485657, 0.4965602159500122, 0.14767418801784515, 0.11426955461502075, 0.49484118819236755, 0.25570935010910034, 0.1009705513715744, 0.4521228075027466, 0.07639645785093307, 0.2267242968082428, 0.37541306018829346], dtype='float32').reshape([25]),
            paddle.to_tensor([0.4307991564273834, 0.11224666237831116, 0.41050565242767334, 0.09982198476791382, 0.29122498631477356, 0.3073887228965759, 0.25820213556289673, 0.3295654356479645, 0.2774659991264343, 0.1696920245885849, 0.09245000034570694, 0.2094087153673172, 0.4780764877796173, 0.46665263175964355, 0.4263499081134796, 0.1299929916858673, 0.03526800870895386, 0.3277457356452942, 0.07759414613246918, 0.29412609338760376, 0.46545159816741943, 0.40433430671691895, 0.446492999792099, 0.48425278067588806, 0.1856486201286316], dtype='float32').reshape([25]),
            paddle.to_tensor([0.03713636472821236, 0.28756535053253174, 0.3097003996372223, 0.13753880560398102, 0.07104410231113434, 0.1720055490732193, 0.20051966607570648, 0.49949583411216736, 0.10173550993204117, 0.4517276883125305, 0.15274865925312042, 0.2429545521736145, 0.13546374440193176, 0.12472468614578247, 0.49170973896980286, 0.25411659479141235, 0.2305825650691986, 0.0027900265995413065, 0.06984882056713104, 0.2734479606151581, 0.10071096569299698, 0.1593286544084549, 0.4632056653499603, 0.4438631534576416, 0.46013882756233215], dtype='float32').reshape([25]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_189560c36d66c4e23ef6668aed2a6292(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 304, 304], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f03a2c54d78746e32418082eb7ba541e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 17, 17], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a9efed97d2578016508328964aec588c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3a2f02479652f72aa6fd2920bc5b11df(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 17, 17], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f69a2e73735b68ca9a3ae9e3efe0d08a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1a053da07c1a1887a7d1207207dbc5b2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 16, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_78526eb339c022cc1f78a69eec0682f9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 2, 2], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bf87ae2f597cb157cf84607637714ea8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2f177e87032dc6aac13ed79960655b81(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.14729957282543182, 0.4326944947242737, 0.34085750579833984, 0.14895938336849213, 0.21504415571689606, 0.06372583657503128, 0.26471146941185, 0.41179415583610535, 0.3630445897579193, 0.20861971378326416, 0.2455553114414215, 0.42398005723953247, 0.15261539816856384, 0.35581347346305847, 0.13713189959526062, 0.167733296751976, 0.13156171143054962, 0.19562308490276337, 0.17805258929729462, 0.11281681805849075], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3159504234790802, 0.3157009482383728, 0.10106855630874634, 0.47934165596961975, 0.0446309857070446, 0.3913510739803314, 0.3857087790966034, 0.09221877157688141, 0.36785414814949036, 0.3765651285648346, 0.048451896756887436, 0.07873846590518951, 0.3885383903980255, 0.34020453691482544, 0.25918081402778625, 0.15660375356674194, 0.40509751439094543, 0.382575660943985, 0.4172857999801636, 0.4568762481212616], dtype='float32').reshape([20]),
            paddle.to_tensor([0.22272668778896332, 0.15851014852523804, 0.3315029442310333, 0.1700887531042099, 0.14312703907489777, 0.07648564875125885, 0.1338702291250229, 0.4487559199333191, 0.03427189961075783, 0.2756454050540924, 0.10204516351222992, 0.21249717473983765, 0.06836798042058945, 0.23044463992118835, 0.06879006326198578, 0.25635597109794617, 0.10306436568498611, 0.47065991163253784, 0.03291109949350357, 0.10680891573429108], dtype='float32').reshape([20]),
            paddle.to_tensor([0.019535785540938377, 0.23424910008907318, 0.02608511596918106, 0.25416862964630127, 0.39251866936683655, 0.17200641334056854, 0.21659162640571594, 0.43651682138442993, 0.1416839063167572, 0.31710782647132874, 0.25779834389686584, 0.3164396286010742, 0.2367333471775055, 0.3183930814266205, 0.06853923201560974, 0.35701707005500793, 0.22898097336292267, 0.1634053885936737, 0.11123426258563995, 0.15652766823768616], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3650ad2b4f4d06890e2b8beb13f20640(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1a3056bc756454240ab28c2607c3c916(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 87, 1, 1], dtype='float32', min=0, max=0.5),
            paddle.uniform([87], dtype='float32', min=0, max=0.5),
            paddle.uniform([87], dtype='float32', min=0, max=0.5),
            paddle.uniform([87], dtype='float32', min=0, max=0.5),
            paddle.uniform([87], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a20578f63cd6d70174569ed4772d0682(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 1536, 10, 10], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_c8eca113b3029d88b25158f95c03e4fe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-06'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6075657ddf3635d7d429d63e40580396(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_c8eca113b3029d88b25158f95c03e4fe
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d29aa8aec2fcf454e2fa0a4beebfa4e4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 112, 199], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_47cafdf047cd3893c16699258564dcd2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fec517ed208f583ea3226c71c6c94591(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.18309026956558228, 0.2745409309864044, 0.33817145228385925, 0.15414994955062866, 0.3956370949745178, 0.06252943724393845, 0.2691993713378906, 0.2904084026813507, 0.2826510965824127, 0.4741570055484772, 0.4091007709503174, 0.4497797191143036], dtype='float32').reshape([12]),
            paddle.to_tensor([0.3116677403450012, 0.33181339502334595, 0.2761838138103485, 0.0625210776925087, 0.16430334746837616, 0.4429515302181244, 0.33611881732940674, 0.1720304936170578, 0.17517408728599548, 0.41429802775382996, 0.18256740272045135, 0.30585941672325134], dtype='float32').reshape([12]),
            paddle.to_tensor([0.2557280659675598, 0.41729286313056946, 0.06761645525693893, 0.3422262966632843, 0.36321818828582764, 0.388416588306427, 0.01482812874019146, 0.2515397369861603, 0.28778791427612305, 0.4323153495788574, 0.0994551032781601, 0.09825752675533295], dtype='float32').reshape([12]),
            paddle.to_tensor([0.0008796249167062342, 0.040445998311042786, 0.4905500113964081, 0.13807904720306396, 0.3099971115589142, 0.07993569225072861, 0.20468337833881378, 0.3816213011741638, 0.2990413308143616, 0.48352280259132385, 0.25057098269462585, 0.39576584100723267], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b8c116177d81b1497e3455e666532c16(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3771449327468872, 0.26692700386047363, 0.22318503260612488, 0.03982514515519142, 0.20886074006557465, 0.2237495332956314, 0.34684866666793823, 0.420518159866333, 0.03786890208721161, 0.26889732480049133, 0.3571201264858246, 0.05809802934527397, 0.2078942209482193, 0.04620610922574997, 0.08973734080791473, 0.22029809653759003, 0.36757931113243103, 0.49491044878959656, 0.02056417241692543, 0.4342869222164154, 0.3430102467536926, 0.08963251113891602, 0.05390031635761261, 0.26076409220695496], dtype='float32').reshape([24]),
            paddle.to_tensor([0.13050878047943115, 0.11119627207517624, 0.16784490644931793, 0.416109561920166, 0.08912181109189987, 0.2841273248195648, 0.005993780214339495, 0.3830091059207916, 0.4239034950733185, 0.324930876493454, 0.386331707239151, 0.430923730134964, 0.3407270610332489, 0.23858240246772766, 0.2590598464012146, 0.2156621664762497, 0.2583111524581909, 0.06741856038570404, 0.02830957993865013, 0.3442516624927521, 0.07565300911664963, 0.24681350588798523, 0.17904670536518097, 0.49896591901779175], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3803342282772064, 0.2386467456817627, 0.40985482931137085, 0.28943532705307007, 0.33748602867126465, 0.17527489364147186, 0.194459468126297, 0.17902255058288574, 0.30435001850128174, 0.48842868208885193, 0.3608551025390625, 0.49156391620635986, 0.10718677937984467, 0.47874757647514343, 0.14829641580581665, 0.3029561936855316, 0.4980614185333252, 0.45131874084472656, 0.45900148153305054, 0.10681391507387161, 0.40314245223999023, 0.19015322625637054, 0.02953970618546009, 0.0007551005692221224], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3974052667617798, 0.1617380678653717, 0.4835253357887268, 0.49917811155319214, 0.38536179065704346, 0.21746566891670227, 0.08580394834280014, 0.470054566860199, 0.24398326873779297, 0.48480960726737976, 0.2697835862636566, 0.23267820477485657, 0.09201746433973312, 0.026875728741288185, 0.3751470148563385, 0.09553085267543793, 0.296658992767334, 0.2647489905357361, 0.021424947306513786, 0.30978092551231384, 0.004498469643294811, 0.2750367820262909, 0.15966367721557617, 0.34258022904396057], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8989d276558eb847b09619e14bf9e68e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4963969588279724, 0.34850895404815674, 0.01145290955901146, 0.272655725479126, 0.05554717779159546, 0.44502848386764526, 0.19011621177196503, 0.09267724305391312, 0.16667264699935913, 0.3128146231174469, 0.31548118591308594, 0.3967120051383972, 0.18518248200416565, 0.06534690409898758, 0.2246037721633911, 0.12278604507446289, 0.03671173006296158, 0.1829715222120285, 0.33874547481536865, 0.4829246997833252, 0.051553837954998016, 0.42366689443588257, 0.2834519147872925, 0.13860775530338287, 0.12316252291202545, 0.07999613881111145, 0.0738425999879837, 0.027708524838089943, 0.4540194571018219, 0.32244065403938293], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3608614504337311, 0.3455660045146942, 0.4570882022380829, 0.06522902846336365, 0.4198213219642639, 0.06507818400859833, 0.39440539479255676, 0.086982361972332, 0.2056998908519745, 0.11514429748058319, 0.07357721030712128, 0.15536855161190033, 0.07165031880140305, 0.40614140033721924, 0.3246469497680664, 0.07303912192583084, 0.36259353160858154, 0.015710124745965004, 0.15615755319595337, 0.41480758786201477, 0.493662565946579, 0.47925418615341187, 0.1447160691022873, 0.26304078102111816, 0.17279060184955597, 0.15337784588336945, 0.33467763662338257, 0.07962771505117416, 0.21681183576583862, 0.46507397294044495], dtype='float32').reshape([30]),
            paddle.to_tensor([0.013352014124393463, 0.047124575823545456, 0.276449054479599, 0.3644510805606842, 0.08287028223276138, 0.16606876254081726, 0.09583618491888046, 0.05440520495176315, 0.1535370796918869, 0.41823363304138184, 0.24070046842098236, 0.2731897234916687, 0.12421174347400665, 0.16757667064666748, 0.09005263447761536, 0.13709524273872375, 0.2653944492340088, 0.20801569521427155, 0.4707663357257843, 0.4111436605453491, 0.07036615163087845, 0.00482288608327508, 0.17768946290016174, 0.13969522714614868, 0.3369581401348114, 0.4670811593532562, 0.2530093193054199, 0.2536063492298126, 0.08669498562812805, 0.4924700856208801], dtype='float32').reshape([30]),
            paddle.to_tensor([0.46184292435646057, 0.061697814613580704, 0.22716428339481354, 0.446185827255249, 0.08551205694675446, 0.22929085791110992, 0.12334208190441132, 0.14931493997573853, 0.0017542839050292969, 0.2844706177711487, 0.04079371690750122, 0.144470676779747, 0.4926738739013672, 0.04721340164542198, 0.492327481508255, 0.1295473426580429, 0.009975451044738293, 0.003604467026889324, 0.06529667973518372, 0.017428219318389893, 0.005230285227298737, 0.3670068383216858, 0.01150947343558073, 0.29817456007003784, 0.4701755940914154, 0.1391054391860962, 0.27572712302207947, 0.09862872958183289, 0.3117731213569641, 0.38614556193351746], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1520116557362c5f17951722a1649b58(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 512, 1024], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ad1f3ec9740f5898d60480d344f97741(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 8, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_105a7c0fea25c37573bff1fdb4267b34(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 768, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9653a8c9658758f169e155e1ff65ac71(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d32d5087fd068eafe1179401dcef04f0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_31d0c137872e62333693de7cab0e3ead(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d785fee8ea8409b14dae99604fef72ae(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1c89209b4004f62773bea4ea22bea530(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 240, 240], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_77c96a32af44d736ea42aaa65d0ef19d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_32d97dc4625dc16e30d6c81590385748(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 8, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f528870d4ae728bb6a1f656ed393b5e4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_baa2acc5ac4f28070257c4799a8e0fbc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_92307ecbed030511174339039665bb95(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4fe2f7e1f9e5f7542105523a4dc622be(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a98cfbdd1c102a970baa1a385ad54470(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 366, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([366], dtype='float32', min=0, max=0.5),
            paddle.uniform([366], dtype='float32', min=0, max=0.5),
            paddle.uniform([366], dtype='float32', min=0, max=0.5),
            paddle.uniform([366], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e4db3b44d461b698d6974537b2851ffe(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.1800008863210678, 0.32972049713134766, 0.29147905111312866, 0.36564967036247253, 0.23952601850032806, 0.1914215385913849, 0.24998262524604797, 0.0017064216081053019, 0.005439820233732462, 0.007421860937029123, 0.1775386780500412, 0.25819888710975647, 0.460515558719635, 0.3621457815170288, 0.21251198649406433, 0.34948575496673584, 0.20172972977161407, 0.11896663904190063, 0.2536928653717041, 0.39183539152145386, 0.30955126881599426, 0.21865341067314148, 0.16531428694725037, 0.1290009617805481, 0.0003933261032216251, 0.09992694109678268, 0.301270455121994, 0.39710599184036255], dtype='float32').reshape([28]),
            paddle.to_tensor([0.24507464468479156, 0.0022422911133617163, 0.3914247155189514, 0.26874756813049316, 0.15498879551887512, 0.26966139674186707, 0.4777032732963562, 0.0985431969165802, 0.045572102069854736, 0.1178063303232193, 0.25802281498908997, 0.3768504858016968, 0.07258044183254242, 0.07646441459655762, 0.4472789764404297, 0.46558746695518494, 0.3986915051937103, 0.027663448825478554, 0.2829076647758484, 0.42310047149658203, 0.38638943433761597, 0.4540347754955292, 0.28972163796424866, 0.385823518037796, 0.47834205627441406, 0.46887755393981934, 0.08880327641963959, 0.28864240646362305], dtype='float32').reshape([28]),
            paddle.to_tensor([0.003655160777270794, 0.07971957325935364, 0.49099966883659363, 0.23505167663097382, 0.40526825189590454, 0.06499208509922028, 0.011411759071052074, 0.3282585144042969, 0.2605906128883362, 0.22993004322052002, 0.41517454385757446, 0.3041957914829254, 0.4681321978569031, 0.3135652244091034, 0.3627285361289978, 0.28056278824806213, 0.34804752469062805, 0.18301339447498322, 0.3176265358924866, 0.08997107297182083, 0.44437047839164734, 0.17131970822811127, 0.3601229190826416, 0.12014602869749069, 0.33972322940826416, 0.10149514675140381, 0.25967708230018616, 0.12004696577787399], dtype='float32').reshape([28]),
            paddle.to_tensor([0.4147123098373413, 0.16707807779312134, 0.4325660765171051, 0.09882040321826935, 0.21580341458320618, 0.4455450177192688, 0.4048272967338562, 0.23178908228874207, 0.43688705563545227, 0.16480132937431335, 0.13021321594715118, 0.08831235766410828, 0.3570763170719147, 0.0626957044005394, 0.11966060101985931, 0.427065908908844, 0.3109983801841736, 0.1954992562532425, 0.3126377463340759, 0.11926613003015518, 0.06592030078172684, 0.109811931848526, 0.33834031224250793, 0.34092041850090027, 0.07460163533687592, 0.41330769658088684, 0.11981857568025589, 0.2055162638425827], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7a51618b2bb8f8ad94ac0a39323d25d5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 8, 6], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_59550b940c85dd549d5da0830a0f50b8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a7e0d149c519d3a422d50aa390ec5832(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5cd3ff384bbbf84383f00198f4a1d399(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d97e1437c3d4251789e3959e264d122b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9be6db102a8dd919b7f6d098dcc8bcde(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7f25b00d164630fbca3052d4037bcc0f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.26791098713874817, 0.03391135483980179, 0.28582140803337097, 0.4582003057003021, 0.42900052666664124, 0.30516600608825684, 0.33937475085258484, 0.15940263867378235, 0.13048139214515686, 0.22952722012996674, 0.41845643520355225, 0.004535066895186901, 0.11770040541887283, 0.1832870990037918, 0.37389075756073, 0.37803953886032104, 0.19577401876449585, 0.4479803442955017], dtype='float32').reshape([18]),
            paddle.to_tensor([0.0458265095949173, 0.2941138744354248, 0.019685203209519386, 0.2429429292678833, 0.23597504198551178, 0.06660858541727066, 0.26811161637306213, 0.4523322284221649, 0.41766873002052307, 0.46345755457878113, 0.349444180727005, 0.09355343133211136, 0.34893813729286194, 0.045257072895765305, 0.04742548614740372, 0.27578192949295044, 0.2646639049053192, 0.3016076982021332], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3346330523490906, 0.07433078438043594, 0.2783198058605194, 0.14404833316802979, 0.06756094843149185, 0.10014031082391739, 0.3888557553291321, 0.4336489140987396, 0.48414894938468933, 0.12701329588890076, 0.2608267068862915, 0.16755534708499908, 0.461528480052948, 0.2164374440908432, 0.3629164695739746, 0.15945148468017578, 0.17546895146369934, 0.13935811817646027], dtype='float32').reshape([18]),
            paddle.to_tensor([0.25240257382392883, 0.20054396986961365, 0.07987061142921448, 0.47065556049346924, 0.13952654600143433, 0.1404295265674591, 0.38112494349479675, 0.09841275215148926, 0.41670167446136475, 0.009083358570933342, 0.04256089776754379, 0.289893239736557, 0.13009947538375854, 0.16383549571037292, 0.13034500181674957, 0.2449127733707428, 0.4070459008216858, 0.08561982959508896], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_145ad20e7a3b3c8aa31f6b40d4fe3d45(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ff56fc19f43e0ef7553dc71c00465a61(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_145ad20e7a3b3c8aa31f6b40d4fe3d45
    def get_inputs(self):
        return [
            paddle.uniform([16, 384], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_370df69b9dcc85dc0804c17dc8de431e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 82, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1557ec0894a2f8cfb08dfd5c77714928(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 26], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_68c885aa9fd50f14fa1e73c99efaa7b6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 768, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_176ec3a270238334358328d07a041566(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 240, 240], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8e29e5274dd02c9c2b872fef41b776a8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_88f594b94ed79c90a3762907063b8ca6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 50, 84], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2bcefde44639e3e32ddeb3fb6c2dff1b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f596c8f52f288ef7558069536cca8c40(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 32, 64], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.47236454486846924, 0.3562193214893341, 0.332170695066452, 0.2019476741552353, 0.05878720432519913, 0.28439703583717346, 0.25252625346183777, 0.15519878268241882, 0.4827818274497986, 0.05627451837062836, 0.47574397921562195, 0.014332512393593788, 0.014262690208852291, 0.40498557686805725, 0.23956118524074554, 0.17833931744098663, 0.11088165640830994, 0.2883338928222656], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3166030943393707, 0.34003162384033203, 0.4747643768787384, 0.3957034647464752, 0.10102315992116928, 0.29129818081855774, 0.1451083868741989, 0.15000668168067932, 0.24033214151859283, 0.2964235842227936, 0.27532336115837097, 0.1065814346075058, 0.3415086567401886, 0.2654916048049927, 0.1845419704914093, 0.11177569627761841, 0.4682392179965973, 0.008277056738734245], dtype='float32').reshape([18]),
            paddle.to_tensor([0.34180745482444763, 0.443056583404541, 0.3617316484451294, 0.19521819055080414, 0.41703420877456665, 0.2817552387714386, 0.4550563097000122, 0.485860675573349, 0.05444842576980591, 0.0069701895117759705, 0.09799105674028397, 0.4284231662750244, 0.3358571529388428, 0.4338124990463257, 0.09023284167051315, 0.24041660130023956, 0.06787347793579102, 0.20067857205867767], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3586195111274719, 0.21239003539085388, 0.3268939256668091, 0.12791438400745392, 0.4744952619075775, 0.32540497183799744, 0.02955484762787819, 0.18003900349140167, 0.16744540631771088, 0.34862351417541504, 0.24868834018707275, 0.1963789165019989, 0.267802894115448, 0.2942486107349396, 0.20399661362171173, 0.4526006877422333, 0.21050257980823517, 0.16548900306224823], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9d1872f6c24dd670c0b05d7d06f2e933(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1440, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1440], dtype='float32', min=0, max=0.5),
            paddle.uniform([1440], dtype='float32', min=0, max=0.5),
            paddle.uniform([1440], dtype='float32', min=0, max=0.5),
            paddle.uniform([1440], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_af91295b59c5fca17f890a802deb001f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a5a3048c2f779d9725753401037bcde8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 32, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_38ee47d84c502f0ea2de34186f9e0654(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f5bfe4a12fd44708dd1b35f07e6cec93(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 48, 48], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3495120704174042, 0.3072827458381653, 0.2241392880678177, 0.003764153691008687, 0.2806089520454407, 0.335761159658432, 0.4210965037345886, 0.053516339510679245, 0.25649380683898926, 0.3312316834926605, 0.4787755608558655, 0.05969711020588875, 0.4623159170150757, 0.07641205191612244, 0.32504573464393616, 0.3524969518184662, 0.2621842324733734, 0.14139027893543243, 0.2408267706632614, 0.3406996726989746, 0.18493008613586426, 0.39121660590171814, 0.4228876233100891, 0.3630239963531494], dtype='float32').reshape([24]),
            paddle.to_tensor([0.43241381645202637, 0.3247089385986328, 0.014896277338266373, 0.4721575379371643, 0.15776769816875458, 0.36627131700515747, 0.061493873596191406, 0.010156883858144283, 0.48980602622032166, 0.23761819303035736, 0.18439173698425293, 0.18535511195659637, 0.3315291106700897, 0.3340052366256714, 0.3712013065814972, 0.191938579082489, 0.09186047315597534, 0.4987426698207855, 0.24798215925693512, 0.01247198786586523, 0.4237035810947418, 0.02784002013504505, 0.29056286811828613, 0.014790224842727184], dtype='float32').reshape([24]),
            paddle.to_tensor([0.0671660527586937, 0.17379915714263916, 0.037779923528432846, 0.01864171214401722, 0.10487769544124603, 0.11007770150899887, 0.4192756414413452, 0.054412633180618286, 0.05877314507961273, 0.019409075379371643, 0.04936148226261139, 0.34483349323272705, 0.17241531610488892, 0.3139004409313202, 0.3230988383293152, 0.05394166335463524, 0.16013160347938538, 0.21568405628204346, 0.24562902748584747, 0.04056635871529579, 0.000713513873051852, 0.37672877311706543, 0.00572401424869895, 0.4145351052284241], dtype='float32').reshape([24]),
            paddle.to_tensor([0.14009301364421844, 0.3101831376552582, 0.21994569897651672, 0.0406230129301548, 0.2842768430709839, 0.38311079144477844, 0.32751837372779846, 0.4870714247226715, 0.3809022307395935, 0.43066269159317017, 0.16135172545909882, 0.009772640652954578, 0.43405941128730774, 0.12507979571819305, 0.22748224437236786, 0.18942393362522125, 0.39537209272384644, 0.23325617611408234, 0.2961494028568268, 0.03910588473081589, 0.22667929530143738, 0.4982016086578369, 0.02182650752365589, 0.27365463972091675], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ec82e5e2ce86a9a9f4d3d425b500abdc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 88, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_03eca6fb312f8659782bb2f9472c638c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3c7de9a2e7d41a4f6ae81e6c61f3459c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e54d5841d053102fe6c9b571c065550d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 26], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e7357934254a9e29cb425bd427ca6319(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.24002842605113983, 0.06124601513147354, 0.003389274002984166, 0.43280723690986633, 0.19999931752681732, 0.05757423862814903, 0.09491413086652756, 0.4288530647754669, 0.3707442879676819, 0.44060373306274414, 0.43725690245628357, 0.38715746998786926, 0.004933924414217472, 0.3942476809024811, 0.1355980634689331, 0.2232154756784439, 0.19718612730503082, 0.038787055760622025, 0.41761067509651184, 0.1228911504149437, 0.2557556629180908, 0.48397499322891235, 0.43872153759002686, 0.05463752523064613, 0.18915413320064545, 0.2493649423122406, 0.11279112100601196, 0.32626521587371826, 0.033825986087322235, 0.35151830315589905], dtype='float32').reshape([30]),
            paddle.to_tensor([0.03351639583706856, 0.4619373679161072, 0.12969531118869781, 0.007635689340531826, 0.48294103145599365, 0.2712317705154419, 0.35318100452423096, 0.12818090617656708, 0.020024126395583153, 0.4418039917945862, 0.16592445969581604, 0.3189719319343567, 0.23138807713985443, 0.12164613604545593, 0.27754226326942444, 0.3482798933982849, 0.31195658445358276, 0.36779022216796875, 0.2259513884782791, 0.40969836711883545, 0.12817861139774323, 0.46679481863975525, 0.2947995364665985, 0.17302601039409637, 0.4125591516494751, 0.09067565947771072, 0.014805497601628304, 0.12107471376657486, 0.1967228204011917, 0.11785197257995605], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4541269838809967, 0.38142335414886475, 0.4758489429950714, 0.049149882048368454, 0.25855204463005066, 0.4297047555446625, 0.36952924728393555, 0.0077616022899746895, 0.0906568318605423, 0.11350046843290329, 0.0687292218208313, 0.2601318955421448, 0.3583533763885498, 0.012753154151141644, 0.30417338013648987, 0.4492884576320648, 0.15088829398155212, 0.11267326772212982, 0.39040929079055786, 0.29283761978149414, 0.4679419994354248, 0.3316800594329834, 0.17194780707359314, 0.22709906101226807, 0.020348021760582924, 0.0501994751393795, 0.15648630261421204, 0.24177615344524384, 0.027518894523382187, 0.16314247250556946], dtype='float32').reshape([30]),
            paddle.to_tensor([0.24588853120803833, 0.30330392718315125, 0.4518887996673584, 0.38804754614830017, 0.4860885739326477, 0.28977811336517334, 0.13459499180316925, 0.21246089041233063, 0.46033281087875366, 0.31862592697143555, 0.4587581753730774, 0.4428047239780426, 0.42996302247047424, 0.4017429053783417, 0.3743176758289337, 0.2098434865474701, 0.20181673765182495, 0.44300577044487, 0.46252477169036865, 0.27119266986846924, 0.3617720901966095, 0.3973527252674103, 0.10882208496332169, 0.1479373574256897, 0.37851381301879883, 0.29265010356903076, 0.05952581763267517, 0.06452716886997223, 0.05433867871761322, 0.4668932259082794], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b511345fd87386116d6360c7c281789f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1776, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1776], dtype='float32', min=0, max=0.5),
            paddle.uniform([1776], dtype='float32', min=0, max=0.5),
            paddle.uniform([1776], dtype='float32', min=0, max=0.5),
            paddle.uniform([1776], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3876249f6b63dac34902e628ca392b86(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_b4f4da3a8d44d1b5b3a2903a097df186
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 256, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_79559bdba06194a92cd85944cd8b7845(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_0edd578f48a07aa5d1d79f9f678b472e
    def get_inputs(self):
        return [
            paddle.uniform([49, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ff95c53ca3ad4a71139a89f43eeb9f33(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 288, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([288], dtype='float32', min=0, max=0.5),
            paddle.uniform([288], dtype='float32', min=0, max=0.5),
            paddle.uniform([288], dtype='float32', min=0, max=0.5),
            paddle.uniform([288], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c4bee5819c3d00a59574ced23ae964ae(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 12, 12], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ea919c9ee6ea2b45f4a2fb2a55647678(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f4ff3f0add67c83e386aa08f43a06e0b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([8, 64, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2337d3e6f0690d181df064dd0ebbf595(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 100], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_69f6c4c1c483a7ba9ca4e4741b56c450(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c8018c476c49557c79391b85ee8a14f4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 64, 48], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_74c22a140695b884ad1da8bc137ce3db(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3570745289325714, 0.1701633185148239, 0.0024914126843214035, 0.2706168293952942, 0.23824870586395264, 0.09803855419158936, 0.17832206189632416, 0.3854033648967743, 0.3083181083202362, 0.3841463327407837, 0.08859580755233765, 0.11816772818565369, 0.06729137897491455, 0.05722261220216751, 0.40865984559059143, 0.49152636528015137, 0.4905083179473877, 0.4754638075828552, 0.16135551035404205, 0.15260441601276398, 0.39965367317199707, 0.49684762954711914, 0.3346013128757477, 0.2536149322986603, 0.06644084304571152, 0.05647014081478119, 0.12393177300691605, 0.3753648102283478, 0.20160050690174103, 0.10896926373243332], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4604077637195587, 0.17547808587551117, 0.43087974190711975, 0.01010160893201828, 0.3894003927707672, 0.08325877785682678, 0.1676516979932785, 0.1505969762802124, 0.05159599334001541, 0.07510384917259216, 0.20776121318340302, 0.39957642555236816, 0.48759979009628296, 0.3654811978340149, 0.15811759233474731, 0.05831671506166458, 0.4799247682094574, 0.11799874901771545, 0.22702574729919434, 0.47819754481315613, 0.4555216133594513, 0.2671176493167877, 0.46748438477516174, 0.3664865791797638, 0.4287145137786865, 0.3931887745857239, 0.429749071598053, 0.12535786628723145, 0.06777670234441757, 0.4439639449119568], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2510797381401062, 0.23431836068630219, 0.08237288892269135, 0.4278075098991394, 0.2331564575433731, 0.4886317551136017, 0.22194749116897583, 0.17632797360420227, 0.26324212551116943, 0.19878321886062622, 0.2622065246105194, 0.10418052971363068, 0.2222636640071869, 0.2896265983581543, 0.48888981342315674, 0.0961688980460167, 0.2840067744255066, 0.2520814538002014, 0.4519948661327362, 0.07106611877679825, 0.4180741608142853, 0.41474759578704834, 0.1524098813533783, 0.1166095957159996, 0.36088165640830994, 0.4867405295372009, 0.4766472578048706, 0.41140803694725037, 0.2685439884662628, 0.4903641939163208], dtype='float32').reshape([30]),
            paddle.to_tensor([0.45586246252059937, 0.027928482741117477, 0.39320340752601624, 0.2689051330089569, 0.08764414489269257, 0.031238464638590813, 0.05876661092042923, 0.39215826988220215, 0.15710042417049408, 0.22097787261009216, 0.18009811639785767, 0.45250290632247925, 0.09179089963436127, 0.12131232023239136, 0.050632771104574203, 0.47620508074760437, 0.06449753791093826, 0.4302707016468048, 0.014656396582722664, 0.1712164580821991, 0.05969120189547539, 0.3334009051322937, 0.3801022469997406, 0.01090285088866949, 0.25434789061546326, 0.255580872297287, 0.3683626651763916, 0.14220646023750305, 0.19546709954738617, 0.006551841739565134], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bde1c16c29a25806cede94c737d59361(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b6f51999d78b80bd5604ad310baf38f9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 10, 10], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3bf9eb50bcf80348c36eaf66a599a7da(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 112, 199], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.18987169861793518, 0.04280548915266991, 0.03077341988682747, 0.2327089011669159, 0.3572622835636139, 0.18199463188648224, 0.005417016334831715, 0.4015255570411682, 0.35148805379867554, 0.27280378341674805, 0.17086909711360931, 0.15707293152809143, 0.3644576072692871, 0.058094389736652374, 0.44932299852371216, 0.19258390367031097, 0.44162634015083313, 0.22918164730072021], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3501951992511749, 0.4290916621685028, 0.4804467558860779, 0.059279099106788635, 0.13399042189121246, 0.17107389867305756, 0.48882389068603516, 0.1310749053955078, 0.31059229373931885, 0.4507772624492645, 0.4544800817966461, 0.03711851313710213, 0.21870549023151398, 0.05937676131725311, 0.2891528904438019, 0.3354986608028412, 0.21951867640018463, 0.15145401656627655], dtype='float32').reshape([18]),
            paddle.to_tensor([0.35827019810676575, 0.34115517139434814, 0.4197085499763489, 0.0041802567429840565, 0.03807462379336357, 0.04423137009143829, 0.47744375467300415, 0.3654128313064575, 0.2041827142238617, 0.14335855841636658, 0.394633024930954, 0.12972131371498108, 0.454659640789032, 0.06952030211687088, 0.27619093656539917, 0.34604790806770325, 0.35183846950531006, 0.408549427986145], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4321596324443817, 0.29207855463027954, 0.09079298377037048, 0.37682029604911804, 0.38376960158348083, 0.35878726840019226, 0.335195392370224, 0.08248685300350189, 0.2774956524372101, 0.09903160482645035, 0.15952245891094208, 0.24333049356937408, 0.22300952672958374, 0.49230846762657166, 0.04214983806014061, 0.4772965610027313, 0.21296396851539612, 0.13729450106620789], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1012792595c8e203b63c6e4e0f975c52(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 728, 19, 19], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_137b01375ae252e974bfec490afb7e30(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.to_tensor([[[[103542.7890625]], [[77056.484375]], [[64832.0625]], [[89553.7109375]], [[46696.953125]], [[63599.2265625]], [[90564.9921875]], [[102032.84375]], [[77258.5390625]], [[85762.6171875]], [[93328.765625]], [[109809.3828125]], [[75305.640625]], [[100345.46875]], [[81938.953125]], [[72362.1328125]], [[61392.1875]], [[70184.46875]], [[104527.84375]]]], dtype='float32').reshape([1, 19, 1, 1]),
            paddle.to_tensor([0.40883320569992065, 0.1251739263534546, 0.05939672887325287, 0.15948335826396942, 0.2759006917476654, 0.12612071633338928, 0.05104623734951019, 0.14276106655597687, 0.21390238404273987, 0.2593328654766083, 0.1610676348209381, 0.27594706416130066, 0.4317592680454254, 0.15065959095954895, 0.09223790466785431, 0.16688385605812073, 0.4195784628391266, 0.1171375960111618, 0.1359582394361496], dtype='float32').reshape([19]),
            paddle.to_tensor([0.18609370291233063, 0.1736023873090744, 0.20258007943630219, 0.10255555063486099, 0.470889151096344, 0.06428663432598114, 0.08947185426950455, 0.3889037072658539, 0.4565967321395874, 0.28557857871055603, 0.10268893092870712, 0.427277535200119, 0.05108407512307167, 0.09306056797504425, 0.4014468193054199, 0.1850735992193222, 0.09210296720266342, 0.4680950939655304, 0.19351953268051147], dtype='float32').reshape([19]),
            paddle.to_tensor([0.0026854961179196835, 0.14721649885177612, 0.41570037603378296, 0.016290333122015, 0.2997165620326996, 0.33326297998428345, 0.25316882133483887, 0.19603271782398224, 0.38681459426879883, 0.42512714862823486, 0.015148988924920559, 0.41054025292396545, 0.19111821055412292, 0.0664416253566742, 0.19405536353588104, 0.31773778796195984, 0.2408737987279892, 0.3603350520133972, 0.052782170474529266], dtype='float32').reshape([19]),
            paddle.to_tensor([0.33616283535957336, 0.3261694014072418, 0.38116610050201416, 0.3546447455883026, 0.4483109414577484, 0.19469760358333588, 0.45794376730918884, 0.18416112661361694, 0.05551515892148018, 0.33087536692619324, 0.14367537200450897, 0.20835073292255402, 0.25381597876548767, 0.12534330785274506, 0.4715385138988495, 0.42817047238349915, 0.27525073289871216, 0.14543263614177704, 0.26335060596466064], dtype='float32').reshape([19]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_66dd600bf74cd6e470df63df0bd5aa83(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 432, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c51de251f67846a4b0c315d71864350c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.05651998147368431, 0.05868951231241226, 0.16689810156822205, 0.06758374720811844, 0.3969528377056122, 0.05846203491091728, 0.3643132746219635, 0.40934592485427856, 0.10587722063064575, 0.49765545129776, 0.10969968885183334, 0.2640579044818878, 0.07658055424690247, 0.30578285455703735, 0.12921462953090668, 0.47560107707977295, 0.14789175987243652, 0.047472789883613586, 0.29687851667404175, 0.015193806029856205, 0.3181845247745514, 0.46787044405937195, 0.3214912414550781, 0.3029862642288208], dtype='float32').reshape([24]),
            paddle.to_tensor([0.09780286997556686, 0.4584968388080597, 0.13816353678703308, 0.23607011139392853, 0.39626631140708923, 0.00360137689858675, 0.4495687186717987, 0.29842716455459595, 0.2296653538942337, 0.1919613629579544, 0.23587271571159363, 0.27547144889831543, 0.1902690976858139, 0.3555668592453003, 0.4973192811012268, 0.14632916450500488, 0.1318160593509674, 0.23316705226898193, 0.30245059728622437, 0.14166483283042908, 0.0827573835849762, 0.12816385924816132, 0.49213799834251404, 0.24557875096797943], dtype='float32').reshape([24]),
            paddle.to_tensor([0.11839878559112549, 0.4142623841762543, 0.08441271632909775, 0.27228325605392456, 0.1569664627313614, 0.07532011717557907, 0.4682284891605377, 0.12830238044261932, 0.36487314105033875, 0.031415898352861404, 0.29080647230148315, 0.12424953281879425, 0.16451142728328705, 0.4752134680747986, 0.28161659836769104, 0.15342307090759277, 0.33863261342048645, 0.38674843311309814, 0.3553052544593811, 0.39412111043930054, 0.4466889202594757, 0.19282285869121552, 0.05610766261816025, 0.06582421809434891], dtype='float32').reshape([24]),
            paddle.to_tensor([0.0039446186274290085, 0.25526419281959534, 0.26793932914733887, 0.2915465235710144, 0.43109390139579773, 0.18859811127185822, 0.4626384377479553, 0.09694576263427734, 0.16900713741779327, 0.0811469703912735, 0.1571294367313385, 0.3078490197658539, 0.46649476885795593, 0.36748024821281433, 0.2601378858089447, 0.19254572689533234, 0.023087702691555023, 0.06081891432404518, 0.41226619482040405, 0.32779762148857117, 0.02943553403019905, 0.11105158179998398, 0.378386914730072, 0.18177011609077454], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_413cd0c7f6a7232225e7c39f7af0f02d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 232, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c54e8c6866fd5d864369c499cce8d239(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3421228229999542, 0.08287464082241058, 0.2890787422657013, 0.37943583726882935, 0.017803439870476723, 0.46715858578681946, 0.4297356605529785, 0.2382158637046814, 0.4915515184402466, 0.3343009948730469, 0.38954880833625793, 0.2572907507419586, 0.44885095953941345, 0.14040261507034302, 0.030177943408489227, 0.10149580240249634, 0.38742366433143616, 0.1428695172071457, 0.4517700672149658, 0.10395760834217072, 0.21394559741020203, 0.3957345187664032, 0.1991809457540512, 0.3802184462547302, 0.02309328317642212, 0.1326247900724411, 0.37936490774154663, 0.0806964710354805, 0.16638639569282532, 0.10360345244407654], dtype='float32').reshape([30]),
            paddle.to_tensor([0.15415848791599274, 0.16366665065288544, 0.11541759222745895, 0.3539850115776062, 0.022511892020702362, 0.05002893880009651, 0.20373813807964325, 0.32364508509635925, 0.41043272614479065, 0.33449503779411316, 0.07207642495632172, 0.050194043666124344, 0.16580966114997864, 0.34206873178482056, 0.4264186918735504, 0.4533770978450775, 0.05965946242213249, 0.1214243546128273, 0.4495735466480255, 0.34957385063171387, 0.437202513217926, 0.08014509081840515, 0.4263339340686798, 0.2102471888065338, 0.27387693524360657, 0.3046835660934448, 0.27357274293899536, 0.2146977335214615, 0.11161975562572479, 0.09309488534927368], dtype='float32').reshape([30]),
            paddle.to_tensor([0.02324621193110943, 0.355333149433136, 0.26498377323150635, 0.49207741022109985, 0.43745630979537964, 0.11101026087999344, 0.47950947284698486, 0.14176033437252045, 0.051325201988220215, 0.4510648846626282, 0.46970903873443604, 0.2788330912590027, 0.11423603445291519, 0.27829456329345703, 0.15387870371341705, 0.04210352525115013, 0.4139021039009094, 0.41586539149284363, 0.08010272681713104, 0.050163622945547104, 0.2731451094150543, 0.24952785670757294, 0.08990742266178131, 0.23896335065364838, 0.16374571621418, 0.015642421320080757, 0.4411112070083618, 0.027875512838363647, 0.3759879767894745, 0.26242679357528687], dtype='float32').reshape([30]),
            paddle.to_tensor([0.41738855838775635, 0.47276321053504944, 0.13672050833702087, 0.1760856807231903, 0.2431795448064804, 0.11779805272817612, 0.3254675269126892, 0.23488600552082062, 0.28792402148246765, 0.4732767343521118, 0.49834194779396057, 0.27236929535865784, 0.21269622445106506, 0.179524227976799, 0.30101364850997925, 0.004509362392127514, 0.1118270754814148, 0.023045413196086884, 0.23173603415489197, 0.16848954558372498, 0.18386273086071014, 0.15156345069408417, 0.3475682735443115, 0.08373290300369263, 0.2669338881969452, 0.4832180142402649, 0.08627941459417343, 0.026440205052495003, 0.020269159227609634, 0.01667548157274723], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7bb78e9a9e5ddedea30529a47b8e0c50(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.22320377826690674, 0.18530568480491638, 0.07205401360988617, 0.17830654978752136, 0.24532578885555267, 0.05778346210718155, 0.07480698078870773, 0.08681266009807587, 0.12634138762950897, 0.4691005051136017, 0.38165974617004395, 0.28866177797317505, 0.4981462061405182, 0.14296388626098633, 0.23285190761089325, 0.11222513765096664, 0.291677862405777, 0.126430943608284, 0.16262297332286835, 0.432599276304245, 0.35231921076774597, 0.4047831594944, 0.1884397268295288, 0.4696033000946045], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4529635012149811, 0.46752023696899414, 0.2774583101272583, 0.18109168112277985, 0.48722484707832336, 0.15984083712100983, 0.28057926893234253, 0.03666447103023529, 0.2772778570652008, 0.22363680601119995, 0.036901455372571945, 0.2781096398830414, 0.3089299499988556, 0.36397090554237366, 0.2276577353477478, 0.3269152045249939, 0.10288231819868088, 0.2981387674808502, 0.2034505009651184, 0.3124980330467224, 0.2446594089269638, 0.4723867177963257, 0.37921732664108276, 0.3345210552215576], dtype='float32').reshape([24]),
            paddle.to_tensor([0.33908724784851074, 0.07663402706384659, 0.3101634681224823, 0.45693570375442505, 0.3870813548564911, 0.3472519516944885, 0.4515472650527954, 0.11127597838640213, 0.174624502658844, 0.09798465669155121, 0.021048998460173607, 0.18590770661830902, 0.16965700685977936, 0.34731703996658325, 0.2337794452905655, 0.37797316908836365, 0.18951763212680817, 0.32525208592414856, 0.1644061803817749, 0.3467649519443512, 0.3957158923149109, 0.2840326130390167, 0.4980510473251343, 0.3419516980648041], dtype='float32').reshape([24]),
            paddle.to_tensor([0.19007818400859833, 0.4866526126861572, 0.41574564576148987, 0.43400707840919495, 0.4188314378261566, 0.30314815044403076, 0.4821915924549103, 0.12069059163331985, 0.2787635922431946, 0.09769641607999802, 0.08029906451702118, 0.003646058728918433, 0.12293827533721924, 0.4173518717288971, 0.39957040548324585, 0.34742075204849243, 0.2987258732318878, 0.42776551842689514, 0.33043500781059265, 0.48687833547592163, 0.2702518105506897, 0.07333635538816452, 0.4918067455291748, 0.4170234203338623], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e2d9295c57dc1db9f82548c07f649ae5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.005164654925465584, 0.06091051176190376, 0.13039155304431915, 0.26700374484062195, 0.3135150372982025, 0.45353081822395325, 0.15991811454296112, 0.034707825630903244, 0.1166171133518219, 0.19352351129055023, 0.413316547870636, 0.21609605848789215, 0.25730037689208984, 0.35984596610069275, 0.027668245136737823, 0.45439958572387695, 0.16417792439460754, 0.1918114870786667], dtype='float32').reshape([18]),
            paddle.to_tensor([0.0463641993701458, 0.34224045276641846, 0.1241377666592598, 0.36704230308532715, 0.3842645287513733, 0.27981728315353394, 0.1554761528968811, 0.16614730656147003, 0.28104710578918457, 0.2505885362625122, 0.23841524124145508, 0.13944922387599945, 0.21720115840435028, 0.03075920231640339, 0.2658994495868683, 0.4503673315048218, 0.1411486566066742, 0.42764410376548767], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3883223533630371, 0.26243823766708374, 0.3957044184207916, 0.1457170993089676, 0.06548526138067245, 0.10894732922315598, 0.35707932710647583, 0.17542041838169098, 0.016133468598127365, 0.14099882543087006, 0.3922278583049774, 0.45094338059425354, 0.38160285353660583, 0.4133847653865814, 0.21850425004959106, 0.36668530106544495, 0.03562112897634506, 0.3358299434185028], dtype='float32').reshape([18]),
            paddle.to_tensor([0.47630512714385986, 0.044248685240745544, 0.07412085682153702, 0.23892758786678314, 0.03351845592260361, 0.10160551220178604, 0.3799527883529663, 0.37409284710884094, 0.27401864528656006, 0.1781369298696518, 0.4290545880794525, 0.07101542502641678, 0.0150980269536376, 0.35471969842910767, 0.22145888209342957, 0.47921186685562134, 0.20831550657749176, 0.23199929296970367], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ff3848c7b926280030a7cfc53fd53895(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e12d01ef08dd50737252057d9f58ddd3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3888896405696869, 0.40822723507881165, 0.22239241003990173, 0.4077136218547821, 0.348234623670578, 0.25975626707077026, 0.25825437903404236, 0.4566904604434967, 0.31999754905700684, 0.14836455881595612, 0.11632627993822098, 0.12013232707977295, 0.060866568237543106, 0.4669840931892395, 0.3173343539237976, 0.4392434060573578, 0.07547652721405029, 0.2894951403141022, 0.2044670581817627, 0.29867634177207947, 0.20145876705646515, 0.1300555318593979, 0.1864020675420761, 0.34439730644226074], dtype='float32').reshape([24]),
            paddle.to_tensor([0.09897604584693909, 0.21619164943695068, 0.45482051372528076, 0.003599204123020172, 0.32743898034095764, 0.2660551965236664, 0.23543299734592438, 0.2920859456062317, 0.2498590052127838, 0.31731101870536804, 0.04397192597389221, 0.4075377285480499, 0.42792072892189026, 0.3501063585281372, 0.15087300539016724, 0.09090468287467957, 0.24297522008419037, 0.3963956832885742, 0.49399539828300476, 0.3525001108646393, 0.13438880443572998, 0.09354832023382187, 0.2053287774324417, 0.15761283040046692], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2698984742164612, 0.20738837122917175, 0.09873957931995392, 0.4437771141529083, 0.26464641094207764, 0.04408673942089081, 0.18030092120170593, 0.23622609674930573, 0.3973572552204132, 0.096610888838768, 0.22457455098628998, 0.32511451840400696, 0.49453166127204895, 0.49975869059562683, 0.2781893014907837, 0.2802608907222748, 0.35759520530700684, 0.2587856948375702, 0.18819136917591095, 0.1681264042854309, 0.42258429527282715, 0.4319712519645691, 0.39958760142326355, 0.26129162311553955], dtype='float32').reshape([24]),
            paddle.to_tensor([0.35328248143196106, 0.14182636141777039, 0.23175732791423798, 0.0813763216137886, 0.4604831039905548, 0.4720405638217926, 0.3616524636745453, 0.15935510396957397, 0.07012256979942322, 0.47234615683555603, 0.09337925910949707, 0.0910501554608345, 0.34427884221076965, 0.2947276830673218, 0.2853851914405823, 0.06938770413398743, 0.03376857936382294, 0.12261084467172623, 0.025476161390542984, 0.18794852495193481, 0.4712781012058258, 0.24644266068935394, 0.20079733431339264, 0.4926457107067108], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a3216f17f3ebae06e93b357d6cf0c915(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_975b258f7fdc5512a8c22ad1eebf2c8a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2aed4fdf51323f531ec7773bdc527819(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 360, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([360], dtype='float32', min=0, max=0.5),
            paddle.uniform([360], dtype='float32', min=0, max=0.5),
            paddle.uniform([360], dtype='float32', min=0, max=0.5),
            paddle.uniform([360], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_df250feca2d1420b00e1bbd3a374d953(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d3e2359ca4e3523a60d293f7adbf1408(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ead4e9297cfea0c20f4b600cf2ae66d7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0a001b2820be98ff44a03464682375ed(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aae891d65f73839d8b677464a24d7a8e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 32, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8f24ec13192d9569554bfd7d1f1f6c5b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_602eee06ccf2da9d31916d7b8695d14c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 504, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([504], dtype='float32', min=0, max=0.5),
            paddle.uniform([504], dtype='float32', min=0, max=0.5),
            paddle.uniform([504], dtype='float32', min=0, max=0.5),
            paddle.uniform([504], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_22e16a2388ae9fc6ae9cde5950dab568(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 17, 17], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_516912a9a3c4d9429ae562b758632c72(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([8, 128, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d3eeb39e5f446c318b345b5858d89ea0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 8, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ebc840a3cba0fd337cbd0d13466ca214(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 16, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b7e67a9ae86fbbe874ea4b5585b0a952(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 10, 10], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e3998578c3662eed154471ae3513e67f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_fc6b6917e04b7b7cf9fdb39dde97ec40
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_21c4c8ba0812b24354c8f9758bd6d659(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 122, 122], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.007702804170548916, 0.05566556751728058, 0.005989536643028259, 0.23941217362880707, 0.11905220150947571, 0.09210456907749176, 0.1707300841808319, 0.12415215373039246, 0.4971542954444885, 0.14609378576278687, 0.30288708209991455, 0.16353362798690796, 0.4418414235115051, 0.31111571192741394, 0.18380960822105408, 0.07993331551551819], dtype='float32').reshape([16]),
            paddle.to_tensor([0.013741615228354931, 0.11241596937179565, 0.20253834128379822, 0.321676641702652, 0.18848322331905365, 0.28179600834846497, 0.45747455954551697, 0.007776549085974693, 0.3993692994117737, 0.16510255634784698, 0.3468034565448761, 0.0984884575009346, 0.22786399722099304, 0.25222885608673096, 0.07958297431468964, 0.19231800734996796], dtype='float32').reshape([16]),
            paddle.to_tensor([0.35851606726646423, 0.27339401841163635, 0.0792936459183693, 0.29009178280830383, 0.15718425810337067, 0.18713639676570892, 0.2784629762172699, 0.009774534031748772, 0.07842876762151718, 0.04061385244131088, 0.12870736420154572, 0.09556316584348679, 0.22411121428012848, 0.4430943429470062, 0.3027554154396057, 0.10112954676151276], dtype='float32').reshape([16]),
            paddle.to_tensor([0.15972000360488892, 0.4409666061401367, 0.48509863018989563, 0.2155148833990097, 0.3551717698574066, 0.26157674193382263, 0.49312856793403625, 0.30671313405036926, 0.44721001386642456, 0.45043525099754333, 0.048895254731178284, 0.11808121204376221, 0.2135065495967865, 0.443787157535553, 0.4100521504878998, 0.04791259765625], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d585aca6cceb2924e36e898cb68c93c2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 4, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_adf109be716c0264393e068ace72de88(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 1280, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_01a63ca6997edcb13c2d46ab5e49decf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 61, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([61], dtype='float32', min=0, max=0.5),
            paddle.uniform([61], dtype='float32', min=0, max=0.5),
            paddle.uniform([61], dtype='float32', min=0, max=0.5),
            paddle.uniform([61], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9aa3ca6dde741e0b5f464e8f126bd461(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 480, 480], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.18610741198062897, 0.44357815384864807, 0.4368191361427307, 0.4660521149635315, 0.03355967253446579, 0.26503613591194153, 0.1921553611755371, 0.1305394172668457], dtype='float32').reshape([8]),
            paddle.to_tensor([0.285518616437912, 0.1516759842634201, 0.28005197644233704, 0.4452085793018341, 0.06724386662244797, 0.1827041059732437, 0.39148810505867004, 0.15386304259300232], dtype='float32').reshape([8]),
            paddle.to_tensor([0.43783822655677795, 0.07022639364004135, 0.315006285905838, 0.4073074758052826, 0.15377581119537354, 0.4631079435348511, 0.2839255928993225, 0.027516553178429604], dtype='float32').reshape([8]),
            paddle.to_tensor([0.14456884562969208, 0.3306775689125061, 0.003480873303487897, 0.40634679794311523, 0.18577508628368378, 0.22657234966754913, 0.2323029637336731, 0.19479891657829285], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_09203dcc29a15f88b9ae20bea9b1cfa6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_44b0dbe1a754b0924f474ed2954af360(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 4, 12], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.148145392537117, 0.317069411277771, 0.09286181628704071, 0.25630414485931396, 0.37669289112091064, 0.030020922422409058, 0.34066805243492126, 0.1546233743429184, 0.2214064598083496, 0.2794409394264221, 0.45179906487464905, 0.05344656854867935, 0.3763136565685272, 0.23284462094306946, 0.028395231813192368, 0.4321848154067993], dtype='float32').reshape([16]),
            paddle.to_tensor([0.11561790108680725, 0.17306692898273468, 0.4573722779750824, 0.1739303469657898, 0.11360347270965576, 0.14911697804927826, 0.06131082773208618, 0.3542613685131073, 0.4916272461414337, 0.17027725279331207, 0.29467692971229553, 0.22428300976753235, 0.3054194152355194, 0.15039482712745667, 0.17443999648094177, 0.32500433921813965], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4129980504512787, 0.2838588356971741, 0.01724010892212391, 0.3910847306251526, 0.14874042570590973, 0.20790837705135345, 0.3730979561805725, 0.2524648606777191, 0.24895349144935608, 0.30973559617996216, 0.39157354831695557, 0.08795781433582306, 0.024294767528772354, 0.22606883943080902, 0.36708471179008484, 0.22102724015712738], dtype='float32').reshape([16]),
            paddle.to_tensor([0.40587693452835083, 0.4816281199455261, 0.041455529630184174, 0.4893016517162323, 0.34731414914131165, 0.4845515191555023, 0.4122432470321655, 0.3010112941265106, 0.06892874836921692, 0.4800446331501007, 0.178789883852005, 0.4094739854335785, 0.43248093128204346, 0.3501107394695282, 0.34081265330314636, 0.06701114028692245], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7dc78d09927be49b41da80b128bdaa49(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 232, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4b5894e9bfbc62753f54e3f60ccddba0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 30, 30], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1ba8624e8fbbe98d00d75a328c6dd928(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.21651364862918854, 0.22771140933036804, 0.48903244733810425, 0.27078455686569214, 0.37317219376564026, 0.41235312819480896, 0.12090886384248734, 0.47080615162849426, 0.12252488732337952, 0.45373207330703735, 0.3878978490829468, 0.01740368828177452, 0.3080127239227295, 0.21494179964065552, 0.4070020914077759, 0.1559242159128189, 0.2753988206386566, 0.06710048019886017, 0.2939498722553253, 0.11534169316291809, 0.2397080659866333, 0.10248488932847977, 0.00817245151847601, 0.05790393054485321], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2701285183429718, 0.30848467350006104, 0.34121575951576233, 0.36888450384140015, 0.17378553748130798, 0.4078133702278137, 0.3269542157649994, 0.16420336067676544, 0.33154794573783875, 0.05281338468194008, 0.2866137623786926, 0.3471730947494507, 0.36605146527290344, 0.011022566817700863, 0.3753182291984558, 0.12520352005958557, 0.18668989837169647, 0.008048556745052338, 0.24431011080741882, 0.21483246982097626, 0.14135585725307465, 0.387406587600708, 0.10426483303308487, 0.30589359998703003], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4329398572444916, 0.4693681001663208, 0.16608010232448578, 0.1534348726272583, 0.24355921149253845, 0.4310440123081207, 0.2967466115951538, 0.33415693044662476, 0.4225926399230957, 0.3782852590084076, 0.14572469890117645, 0.025195205584168434, 0.07681393623352051, 0.04424913972616196, 0.21249668300151825, 0.19369608163833618, 0.4441983699798584, 0.3559931218624115, 0.14393025636672974, 0.16027413308620453, 0.3656237721443176, 0.36972668766975403, 0.007405901327729225, 0.4964822828769684], dtype='float32').reshape([24]),
            paddle.to_tensor([0.006060558836907148, 0.3826780617237091, 0.4590839445590973, 0.2655264735221863, 0.19646798074245453, 0.2226855754852295, 0.12493407726287842, 0.1789880245923996, 0.1359262764453888, 0.07287517189979553, 0.1345950961112976, 0.2889293134212494, 0.18514889478683472, 0.2711896300315857, 0.4644118845462799, 0.35220152139663696, 0.18320512771606445, 0.06967473030090332, 0.09494081139564514, 0.19432194530963898, 0.04264206066727638, 0.4130673110485077, 0.1782076209783554, 0.18159954249858856], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_159cda53ce16dd800043598f8869bc83(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4069184958934784, 0.33850809931755066, 0.3226633369922638, 0.18487043678760529, 0.27521389722824097, 0.4797247648239136, 0.3747493326663971, 0.12763111293315887, 0.14394745230674744, 0.15151455998420715, 0.18876467645168304, 0.14638906717300415, 0.00682563241571188, 0.43626144528388977, 0.35836583375930786, 0.22158178687095642, 0.23442724347114563, 0.3559005558490753, 0.2995375990867615, 0.45107337832450867, 0.16472509503364563, 0.03997458890080452, 0.2852746546268463, 0.21540158987045288], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4803275167942047, 0.25420117378234863, 0.024179987609386444, 0.45706403255462646, 0.25835809111595154, 0.10804300010204315, 0.06876712292432785, 0.22062279284000397, 0.2740997076034546, 0.2190706580877304, 0.11554180830717087, 0.1803469955921173, 0.3171166181564331, 0.04042341560125351, 0.2371218055486679, 0.4817131459712982, 0.3904992341995239, 0.35168901085853577, 0.4433394968509674, 0.26945510506629944, 0.29896193742752075, 0.36135923862457275, 0.39117375016212463, 0.4292337894439697], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3839966058731079, 0.11124823242425919, 0.2047203779220581, 0.11700993031263351, 0.28286463022232056, 0.1088179349899292, 0.1756420135498047, 0.41439417004585266, 0.28847163915634155, 0.42197492718696594, 0.40668174624443054, 0.024626106023788452, 0.1495630443096161, 0.07625257223844528, 0.4447035491466522, 0.06907308101654053, 0.26115158200263977, 0.24236498773097992, 0.2692182660102844, 0.19727931916713715, 0.2740492820739746, 0.255266934633255, 0.44117316603660583, 0.3572176992893219], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4103263020515442, 0.34243056178092957, 0.3458283841609955, 0.30907854437828064, 0.0949704647064209, 0.19002151489257812, 0.2905144691467285, 0.49552273750305176, 0.06773215532302856, 0.1131303533911705, 0.17412304878234863, 0.011679515242576599, 0.13852940499782562, 0.26572972536087036, 0.11241360008716583, 0.47637739777565, 0.08395048975944519, 0.0911657065153122, 0.4143050014972687, 0.21070174872875214, 0.2982352674007416, 0.09213972836732864, 0.08564937859773636, 0.4109683632850647], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c9f9cfb572e73322e574f55b885eb7e4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 2, 4], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1c1809567be7789c657be7168a9fe5fc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 30, 30], dtype='float16', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_599f7dede4db3ac9aa8d53814938cf3e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.12693917751312256, 0.17501655220985413, 0.2716301679611206, 0.2893211543560028, 0.24651537835597992, 0.10625652223825455, 0.49213090538978577, 0.48333603143692017], dtype='float32').reshape([8]),
            paddle.to_tensor([0.28669309616088867, 0.20211131870746613, 0.029224971309304237, 0.12131773680448532, 0.06986933946609497, 0.11236998438835144, 0.26273244619369507, 0.4153714179992676], dtype='float32').reshape([8]),
            paddle.to_tensor([0.04892924055457115, 0.4334873557090759, 0.24352103471755981, 0.3911910057067871, 0.05812271311879158, 0.26759621500968933, 0.4839524030685425, 0.10454278439283371], dtype='float32').reshape([8]),
            paddle.to_tensor([0.42143234610557556, 0.41735416650772095, 0.36069563031196594, 0.2543555200099945, 0.12550495564937592, 0.035375405102968216, 0.25154250860214233, 0.12033343315124512], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_07e65756e32cdacabbf7fdc73c01ecee(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6def8d71811d41cce14b3f141ee33ca0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4522048830986023, 0.469695508480072, 0.07482369989156723, 0.3436243534088135, 0.416366308927536, 0.34300467371940613, 0.029215872287750244, 0.09450183063745499, 0.11912032216787338, 0.028239676728844643, 0.00222211005166173, 0.3780079782009125], dtype='float32').reshape([12]),
            paddle.to_tensor([0.3214466869831085, 0.4872300326824188, 0.46815428137779236, 0.2982487380504608, 0.021077236160635948, 0.023351147770881653, 0.2961339056491852, 0.024353478103876114, 0.4980821907520294, 0.4856754243373871, 0.12652277946472168, 0.30883553624153137], dtype='float32').reshape([12]),
            paddle.to_tensor([0.4092027544975281, 0.04907219856977463, 0.2304876148700714, 0.14471206068992615, 0.3207291066646576, 0.36606156826019287, 0.2293078899383545, 0.3198988139629364, 0.11485964059829712, 0.03517540171742439, 0.35809051990509033, 0.1444399654865265], dtype='float32').reshape([12]),
            paddle.to_tensor([0.025784971192479134, 0.30293169617652893, 0.2999202609062195, 0.012332516722381115, 0.22535602748394012, 0.4895625114440918, 0.1793977916240692, 0.08748035877943039, 0.1856399029493332, 0.060170870274305344, 0.2884329557418823, 0.42120298743247986], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_48fcd093ede28cf5738f1f8b07c045ea(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.20776531100273132, 0.22250722348690033, 0.007903831079602242, 0.4341515302658081, 0.45032453536987305, 0.30326342582702637, 0.1869157999753952, 0.4001274108886719, 0.00026284827617928386, 0.006976260803639889, 0.35326382517814636, 0.28626301884651184, 0.07650679349899292, 0.22641967236995697, 0.3576664328575134, 0.04461880400776863, 0.0061181881465017796, 0.3418805003166199, 0.27866509556770325, 0.3599967062473297, 0.0703243687748909, 0.45412591099739075, 0.005762272514402866, 0.05457941070199013, 0.39233899116516113, 0.1490892469882965, 0.1516421139240265, 0.15333271026611328, 0.050876885652542114, 0.09810229390859604], dtype='float32').reshape([30]),
            paddle.to_tensor([0.32322490215301514, 0.11063278466463089, 0.41278690099716187, 0.32992517948150635, 0.41729453206062317, 0.3175114095211029, 0.44015127420425415, 0.29968059062957764, 0.016403652727603912, 0.14127685129642487, 0.1872280389070511, 0.4912704527378082, 0.40833088755607605, 0.12762001156806946, 0.40557971596717834, 0.29959550499916077, 0.223539799451828, 0.009693574160337448, 0.34809044003486633, 0.1778649538755417, 0.160697340965271, 0.20162849128246307, 0.4442417025566101, 0.046720415353775024, 0.4966692328453064, 0.1017940565943718, 0.17368939518928528, 0.45915231108665466, 0.18010659515857697, 0.2292483150959015], dtype='float32').reshape([30]),
            paddle.to_tensor([0.16826029121875763, 0.13482342660427094, 0.34572717547416687, 0.3287867307662964, 0.305612176656723, 0.26753631234169006, 0.49041271209716797, 0.46759840846061707, 0.34825950860977173, 0.09061045199632645, 0.029059045016765594, 0.15219856798648834, 0.16104860603809357, 0.1820371001958847, 0.49743854999542236, 0.05872027203440666, 0.2185756266117096, 0.04994407668709755, 0.3509061634540558, 0.07236862182617188, 0.49397093057632446, 0.3720490038394928, 0.06215055659413338, 0.21328957378864288, 0.36595121026039124, 0.36879318952560425, 0.15916621685028076, 0.15689274668693542, 0.487018883228302, 0.29923322796821594], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3195606768131256, 0.2350500077009201, 0.176253542304039, 0.027708088979125023, 0.16539797186851501, 0.201156347990036, 0.27130165696144104, 0.33779796957969666, 0.00486482260748744, 0.31602969765663147, 0.14647378027439117, 0.4363458454608917, 0.13490469753742218, 0.013072422705590725, 0.39996835589408875, 0.32470184564590454, 0.4704357981681824, 0.4767501652240753, 0.37178224325180054, 0.3169553577899933, 0.3031314015388489, 0.3629089295864105, 0.37951892614364624, 0.04734417051076889, 0.0018743269611150026, 0.4419390857219696, 0.36858582496643066, 0.09007859230041504, 0.4102928638458252, 0.14101369678974152], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_782744257736b8b93fa9600a28562034(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 61, 61], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3878176808357239, 0.4664297103881836, 0.26880574226379395, 0.1361805498600006, 0.13551220297813416, 0.3273218870162964, 0.35372194647789, 0.015930989757180214, 0.32810893654823303, 0.2183884233236313, 0.17891332507133484, 0.4723941385746002, 0.400959849357605, 0.021557724103331566, 0.09594690799713135, 0.33941930532455444, 0.4095807671546936, 0.45728346705436707, 0.14498943090438843, 0.2638639211654663, 0.4148324728012085, 0.3195722699165344, 0.3278944492340088, 0.20668944716453552], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4121733605861664, 0.057180799543857574, 0.341537207365036, 0.4248475134372711, 0.10856399685144424, 0.25704506039619446, 0.06319597363471985, 0.14496196806430817, 0.47609439492225647, 0.46820056438446045, 0.00224972958676517, 0.10723557323217392, 0.3234788477420807, 0.3502463400363922, 0.16812695562839508, 0.03871479630470276, 0.35696497559547424, 0.46003538370132446, 0.46075454354286194, 0.08822198957204819, 0.18073487281799316, 0.07421465963125229, 0.0011927280575037003, 0.32612258195877075], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2031009942293167, 0.26423749327659607, 0.09487536549568176, 0.23728051781654358, 0.393155574798584, 0.15558327734470367, 0.16802763938903809, 0.17853401601314545, 0.2850232422351837, 0.39965131878852844, 0.08745298534631729, 0.07214176654815674, 0.1168544813990593, 0.18038076162338257, 0.1455412656068802, 0.49774739146232605, 0.1822817623615265, 0.35768911242485046, 0.3039904236793518, 0.045760106295347214, 0.39365896582603455, 0.37061113119125366, 0.4906015396118164, 0.03262105956673622], dtype='float32').reshape([24]),
            paddle.to_tensor([0.23749922215938568, 0.2834756076335907, 0.13999569416046143, 0.3907943069934845, 0.25632303953170776, 0.4966870844364166, 0.23614805936813354, 0.25675860047340393, 0.19984206557273865, 0.14533919095993042, 0.31248125433921814, 0.322090208530426, 0.029035326093435287, 0.15778076648712158, 0.17257355153560638, 0.0300725307315588, 0.2324654757976532, 0.391776978969574, 0.385475218296051, 0.012479165568947792, 0.4732164740562439, 0.24579046666622162, 0.160726860165596, 0.20918716490268707], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ddb8c90b149730f9a03a490d2cbf0697(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 480, 480], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_749a91677cd19a2d9b3ac11b4197c020(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_b4f4da3a8d44d1b5b3a2903a097df186
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d7db8fa62e827097949ffe0ba488ad7c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.35262390971183777, 0.15899474918842316, 0.31959661841392517, 0.4477352797985077, 0.2697579562664032, 0.18706358969211578, 0.2397613376379013, 0.4353390336036682, 0.3124307096004486, 0.1751686930656433, 0.08124846965074539, 0.31694215536117554, 0.4597920775413513, 0.041318878531455994, 0.14303576946258545, 0.42826253175735474], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4834946393966675, 0.23418080806732178, 0.3560127317905426, 0.21840865910053253, 0.17723320424556732, 0.47614631056785583, 0.01753969117999077, 0.38734883069992065, 0.1532006412744522, 0.3644253611564636, 0.21560385823249817, 0.48296353220939636, 0.42093515396118164, 0.1551833301782608, 0.28110530972480774, 0.11859279125928879], dtype='float32').reshape([16]),
            paddle.to_tensor([0.0012849480845034122, 0.4138078987598419, 0.39959901571273804, 0.12996384501457214, 0.16336211562156677, 0.1676281988620758, 0.12949317693710327, 0.44005993008613586, 0.18852411210536957, 0.19188454747200012, 0.2290457785129547, 0.15221016108989716, 0.1309308260679245, 0.09159477800130844, 0.3198724389076233, 0.09155800938606262], dtype='float32').reshape([16]),
            paddle.to_tensor([0.49701347947120667, 0.46082428097724915, 0.14921091496944427, 0.32947927713394165, 0.4060758948326111, 0.4800907373428345, 0.23607800900936127, 0.020662061870098114, 0.20414139330387115, 0.20496509969234467, 0.4507219195365906, 0.10122712701559067, 0.2129117250442505, 0.010933511890470982, 0.06379751861095428, 0.1107543334364891], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d1900f2e229a8c61388c6b41417f6e76(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 416, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1867ebb65dfd50c4ed6f9d100364bcd2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 122, 122], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_215a15ba5e0f00c7bb364a979349e9d4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 256, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c3a01b52fc46ee276640f536fea5cf89(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 640, 640], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e47bd296a1aa7690394e4a17e574a3b8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 38, 38], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2469dcfc60b3518ae2a87e0fe4854d6b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.03626256063580513, 0.33591902256011963, 0.37013348937034607, 0.49084997177124023, 0.10209441930055618, 0.28560104966163635, 0.47536060214042664, 0.23270194232463837, 0.01708773896098137, 0.07450689375400543, 0.18377423286437988, 0.07732056081295013, 0.0014949264004826546, 0.4505140781402588, 0.345855712890625, 0.33962687849998474], dtype='float32').reshape([16]),
            paddle.to_tensor([0.06121449172496796, 0.32312241196632385, 0.37118491530418396, 0.4703954756259918, 0.4088991582393646, 0.4764529764652252, 0.19240529835224152, 0.11718843877315521, 0.36851200461387634, 0.32296621799468994, 0.342745304107666, 0.25943630933761597, 0.48349830508232117, 0.2802088260650635, 0.061957139521837234, 0.26070794463157654], dtype='float32').reshape([16]),
            paddle.to_tensor([0.40815648436546326, 0.23328544199466705, 0.32695695757865906, 0.04789133369922638, 0.15386547148227692, 0.029880095273256302, 0.19075970351696014, 0.4620376229286194, 0.446943461894989, 0.22985956072807312, 0.26674962043762207, 0.12274643033742905, 0.1546640694141388, 0.4204045832157135, 0.13701006770133972, 0.3542588949203491], dtype='float32').reshape([16]),
            paddle.to_tensor([0.48076313734054565, 0.1377035528421402, 0.4325450360774994, 0.1472126841545105, 0.06157200038433075, 0.12882252037525177, 0.13423767685890198, 0.3044687807559967, 0.3306300938129425, 0.040664076805114746, 0.14354781806468964, 0.05957265943288803, 0.4877299666404724, 0.3937709629535675, 0.29501938819885254, 0.13238508999347687], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e3d52e433f906b9cf39213b8fdf6b70d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 320, 320], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2677682042121887, 0.30424630641937256, 0.38339170813560486, 0.21399782598018646, 0.024940717965364456, 0.14069829881191254, 0.24651102721691132, 0.0431428886950016], dtype='float32').reshape([8]),
            paddle.to_tensor([0.055285967886447906, 0.34656381607055664, 0.31907230615615845, 0.09774403274059296, 0.39449289441108704, 0.3609594404697418, 0.4199989140033722, 0.20177607238292694], dtype='float32').reshape([8]),
            paddle.to_tensor([0.3370036482810974, 0.06356628239154816, 0.1749407798051834, 0.2089190036058426, 0.24899962544441223, 0.33770570158958435, 0.3302338421344757, 0.3090175688266754], dtype='float32').reshape([8]),
            paddle.to_tensor([0.31731709837913513, 0.12181614339351654, 0.44375407695770264, 0.2441541850566864, 0.27006638050079346, 0.09051141142845154, 0.3463521897792816, 0.17147226631641388], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_22f9b9bea7c1506874f7de7c6dc4a213(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_942b8df679ebb3cfd16faf7acebaa474(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_0edd578f48a07aa5d1d79f9f678b472e
    def get_inputs(self):
        return [
            paddle.uniform([16, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_304d885984149afbda27f1e5e49a3b9d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_def578a427cb3c6a5d6ba0b59092d5c9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 32, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2ef705ce4bf224b032cc8afb9a837c75(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_01f5869b89047f12303150a50f76bcc8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 4, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3690447509288788, 0.20477335155010223, 0.46061015129089355, 0.4430459141731262, 0.4064328968524933, 0.42101436853408813, 0.21801160275936127, 0.15105734765529633, 0.43984630703926086, 0.369928240776062, 0.396718293428421, 0.385152131319046, 0.05697496980428696, 0.35559967160224915, 0.22710689902305603, 0.48893845081329346], dtype='float32').reshape([16]),
            paddle.to_tensor([0.016621991991996765, 0.05598670616745949, 0.05637515336275101, 0.20317012071609497, 0.015135661698877811, 0.42474842071533203, 0.24178571999073029, 0.34149491786956787, 0.2817179262638092, 0.40707066655158997, 0.4886632263660431, 0.45592933893203735, 0.4527881145477295, 0.2598488926887512, 0.14674493670463562, 0.272255003452301], dtype='float32').reshape([16]),
            paddle.to_tensor([0.15707236528396606, 0.21039330959320068, 0.2645803689956665, 0.12624967098236084, 0.20264984667301178, 0.09728209674358368, 0.09432301670312881, 0.17100889980793, 0.3692569136619568, 0.3605195879936218, 0.1186615601181984, 0.30484655499458313, 0.40071094036102295, 0.04298298805952072, 0.02078404650092125, 0.3334302306175232], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3526284396648407, 0.4558965563774109, 0.14297069609165192, 0.07039222121238708, 0.12196774035692215, 0.005890495143830776, 0.1825588047504425, 0.1788167506456375, 0.2854729890823364, 0.3168613612651825, 0.27132508158683777, 0.46786895394325256, 0.20147249102592468, 0.34911125898361206, 0.07068166881799698, 0.4250890910625458], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_760ed7d32ac47ad32d6055eaae31c64b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e4bfe680fe98b4715b413f0a652d4cc3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_76cd5ff0647e3fd5ccd09aaf81577d11(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 10, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.22996242344379425, 0.2669719457626343, 0.3255423605442047, 0.008373875170946121, 0.4816770851612091, 0.12945498526096344, 0.4552031457424164, 0.3600690960884094, 0.32379865646362305, 0.4356262981891632], dtype='float32').reshape([10]),
            paddle.to_tensor([0.07646624743938446, 0.2626945376396179, 0.3546682298183441, 0.447866290807724, 0.3438585698604584, 0.4346880316734314, 0.15355904400348663, 0.32471951842308044, 0.14035896956920624, 0.07514278590679169], dtype='float32').reshape([10]),
            paddle.to_tensor([0.2907503843307495, 0.41488584876060486, 0.24510134756565094, 0.40455108880996704, 0.4319900870323181, 0.1178060993552208, 0.36371368169784546, 0.40208670496940613, 0.34093254804611206, 0.2974443733692169], dtype='float32').reshape([10]),
            paddle.to_tensor([0.11059551686048508, 0.4248383343219757, 0.01556946150958538, 0.46057450771331787, 0.17460232973098755, 0.36711379885673523, 0.25565671920776367, 0.3023482859134674, 0.2532777786254883, 0.4949263632297516], dtype='float32').reshape([10]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f5c2b63aa04256c2e96bbc7aefcc7de3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9003b67630e82d1dc77b743fb0633a86(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 32, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.39787691831588745, 0.10489661991596222, 0.03948083519935608, 0.31969431042671204, 0.49275681376457214, 0.3362557888031006, 0.08141747117042542, 0.14943872392177582], dtype='float32').reshape([8]),
            paddle.to_tensor([0.049929965287446976, 0.4883275032043457, 0.43092989921569824, 0.08319962024688721, 0.41394099593162537, 0.022443274036049843, 0.4330393075942993, 0.436799556016922], dtype='float32').reshape([8]),
            paddle.to_tensor([0.01170958112925291, 0.1478908210992813, 0.36227235198020935, 0.09749811887741089, 0.3478262424468994, 0.34304484724998474, 0.1019485592842102, 0.08342218399047852], dtype='float32').reshape([8]),
            paddle.to_tensor([0.33712032437324524, 0.05200028046965599, 0.002826199634000659, 0.04291774705052376, 0.09255591034889221, 0.19036023318767548, 0.261184424161911, 0.13090763986110687], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e0eeaf2e8d772fd056a048239e47d22c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 2, 4], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2caede957bd379e819dc7c6d42a6d52a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 480, 480], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2517026662826538, 0.34594035148620605, 0.48850253224372864, 0.06917721033096313, 0.4041844606399536, 0.2305733859539032, 0.04510440677404404, 0.2711080312728882, 0.05895332247018814, 0.0791168361902237, 0.47581034898757935, 0.10202927887439728, 0.18306252360343933, 0.48180967569351196, 0.3725648820400238, 0.04017069190740585, 0.26556655764579773, 0.3601175844669342, 0.3127593994140625, 0.18832692503929138, 0.4325576424598694, 0.30516695976257324, 0.12668800354003906, 0.41487348079681396], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2616769075393677, 0.45874732732772827, 0.08434148132801056, 0.2757340371608734, 0.33380597829818726, 0.2046712189912796, 0.14467556774616241, 0.19224581122398376, 0.013080278411507607, 0.40660804510116577, 0.4037795066833496, 0.30871179699897766, 0.36795374751091003, 0.45194798707962036, 0.4063417911529541, 0.2536923587322235, 0.049497995525598526, 0.38151460886001587, 0.16087861359119415, 0.19706524908542633, 0.32725197076797485, 0.21805644035339355, 0.11949454993009567, 0.10552394390106201], dtype='float32').reshape([24]),
            paddle.to_tensor([0.12727509438991547, 0.08766007423400879, 0.49015405774116516, 0.1784955859184265, 0.37023404240608215, 0.21559269726276398, 0.26294049620628357, 0.3887527585029602, 0.018924906849861145, 0.32021716237068176, 0.3842288553714752, 0.015943361446261406, 0.11327173560857773, 0.003078341018408537, 0.315823495388031, 0.25690698623657227, 0.4892391860485077, 0.4768439829349518, 0.17308275401592255, 0.4334370195865631, 0.4160712659358978, 0.34738337993621826, 0.09523537755012512, 0.18297939002513885], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2511388063430786, 0.18091939389705658, 0.32144248485565186, 0.12760542333126068, 0.37724214792251587, 0.36963775753974915, 0.27143529057502747, 0.4634641408920288, 0.38264259696006775, 0.17300143837928772, 0.14830374717712402, 0.46849510073661804, 0.48418793082237244, 0.41530707478523254, 0.41083234548568726, 0.1380152553319931, 0.1764194518327713, 0.39738893508911133, 0.29112812876701355, 0.1903524249792099, 0.1481366753578186, 0.31277552247047424, 0.36501309275627136, 0.49360930919647217], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aa47231b4bb80ea3e6aea444b79a2cbc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 152, 152], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ba999752cd0dcc288c7e6e6412ef683f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 960, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4d5f512b653c9d59da7d5cb13ad06562(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 140, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([140], dtype='float32', min=0, max=0.5),
            paddle.uniform([140], dtype='float32', min=0, max=0.5),
            paddle.uniform([140], dtype='float32', min=0, max=0.5),
            paddle.uniform([140], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_de12a6be1265860c792439aef6cf9f78(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 38, 38], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_00526e8d39b4e12afc9f95f0f6665000(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9e64b7549f6345cacaa662fb03e4c544(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 224, 224], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3967197835445404, 0.09674674272537231, 0.14520356059074402, 0.47909313440322876, 0.18196046352386475, 0.059483930468559265, 0.26141753792762756, 0.0743551105260849, 0.23091883957386017, 0.3090537190437317, 0.18250755965709686, 0.20687340199947357, 0.13524378836154938, 0.059255797415971756, 0.0684991255402565, 0.14058639109134674], dtype='float32').reshape([16]),
            paddle.to_tensor([0.19787751138210297, 0.4031424820423126, 0.4380452036857605, 0.4871234893798828, 0.17514872550964355, 0.2574152946472168, 0.46068984270095825, 0.4194164276123047, 0.17625553905963898, 0.49476489424705505, 0.009621155448257923, 0.32129043340682983, 0.4629727900028229, 0.12044615298509598, 0.4860607981681824, 0.4753345251083374], dtype='float32').reshape([16]),
            paddle.to_tensor([0.14619368314743042, 0.3570820689201355, 0.10487964004278183, 0.1635013222694397, 0.16054730117321014, 0.38013866543769836, 0.3638690412044525, 0.20215778052806854, 0.15190641582012177, 0.4811864197254181, 0.08863270282745361, 0.325741708278656, 0.14892354607582092, 0.34320759773254395, 0.48667672276496887, 0.09277064353227615], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3364894390106201, 0.4391620457172394, 0.4354899823665619, 0.08016540110111237, 0.13819633424282074, 0.4712573289871216, 0.4610360562801361, 0.060594409704208374, 0.026843218132853508, 0.08577214926481247, 0.045198485255241394, 0.18394306302070618, 0.3118980824947357, 0.024711204692721367, 0.07141100615262985, 0.38198208808898926], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3b3851018604ccb275c28a8a2b15424a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b7f4ac1739e33ec250212f0e7ff03838(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 38, 38], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_12a49777084c3fd57a3b0bd2c3e35dfb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 4, 3], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_52779c2f2573eb47fd5eea29c5fb5202(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.45949333906173706, 0.40285739302635193, 0.47303009033203125, 0.2784024477005005, 0.06905462592840195, 0.26608532667160034, 0.351506769657135, 0.0423908457159996, 0.3682446777820587, 0.2684265971183777, 0.39938268065452576, 0.3022399842739105, 0.426135778427124, 0.11104271560907364, 0.11878923326730728, 0.47297799587249756, 0.24177512526512146, 0.3313692808151245, 0.136971116065979, 0.05810048431158066, 0.3549003303050995, 0.44828155636787415, 0.11670796573162079, 0.1507827490568161, 0.4036519527435303, 0.40060484409332275, 0.333030104637146, 0.3329172432422638, 0.3247124254703522, 0.47262588143348694], dtype='float32').reshape([30]),
            paddle.to_tensor([0.41382139921188354, 0.023177145048975945, 0.24203625321388245, 0.3319503962993622, 0.32071760296821594, 0.45052486658096313, 0.3448020815849304, 0.2271447330713272, 0.06694604456424713, 0.3811057209968567, 0.29648128151893616, 0.028832796961069107, 0.43168237805366516, 0.37451431155204773, 0.23049339652061462, 0.14801782369613647, 0.1387123018503189, 0.0502433255314827, 0.40409067273139954, 0.39915502071380615, 0.03907475247979164, 0.1373315155506134, 0.19220975041389465, 0.04779158532619476, 0.49459102749824524, 0.19675213098526, 0.3679754137992859, 0.2941778302192688, 0.39901524782180786, 0.3447036147117615], dtype='float32').reshape([30]),
            paddle.to_tensor([0.22152918577194214, 0.08187274634838104, 0.1968277245759964, 0.3188294768333435, 0.18558059632778168, 0.3341236710548401, 0.1677517145872116, 0.08323819190263748, 0.3799344599246979, 0.4125187397003174, 0.40745195746421814, 0.06425848603248596, 0.2633402347564697, 0.23731420934200287, 0.2769621014595032, 0.48887723684310913, 0.1611132025718689, 0.15166541934013367, 0.43476444482803345, 0.02796792797744274, 0.0649314895272255, 0.4665607511997223, 0.11284056305885315, 0.11409121006727219, 0.392040878534317, 0.46184900403022766, 0.2911391258239746, 0.02756151743233204, 0.10423152148723602, 0.4975184202194214], dtype='float32').reshape([30]),
            paddle.to_tensor([0.06687339395284653, 0.47234585881233215, 0.21676398813724518, 0.15444858372211456, 0.2799853980541229, 0.01401246152818203, 0.16648221015930176, 0.28590908646583557, 0.13548080623149872, 0.45139631628990173, 0.19401395320892334, 0.3443315625190735, 0.47360268235206604, 0.39494574069976807, 0.45406368374824524, 0.3519158661365509, 0.07079647481441498, 0.09352237731218338, 0.11314874142408371, 0.3421865701675415, 0.33946576714515686, 0.372281938791275, 0.3040498197078705, 0.49290943145751953, 0.2088811993598938, 0.30683761835098267, 0.3137848675251007, 0.01611039973795414, 0.4686993956565857, 0.24332170188426971], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_62e5a96212b0fcc6919dad2ef21cf47e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 4, 3], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_42fc67e405a3a1aee052ba4751d3651a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 10, 10], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c90ebe45c29c4a5753998552095d3f4b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 6, 6], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c0596a640f48d665e073b1a73609e6f3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.29084011912345886, 0.3568565845489502, 0.4296388328075409, 0.08353596180677414, 0.09418266266584396, 0.41068020462989807, 0.26945123076438904, 0.4285847842693329, 0.2733698785305023, 0.011790955439209938, 0.3067980110645294, 0.19390636682510376, 0.48230844736099243, 0.21537354588508606, 0.3186572194099426, 0.2993863821029663, 0.4323759078979492, 0.4886119067668915, 0.4441392719745636, 0.02181122824549675, 0.37152621150016785, 0.1840670257806778, 0.08493704348802567, 0.31113073229789734], dtype='float32').reshape([24]),
            paddle.to_tensor([0.11268505454063416, 0.4965590238571167, 0.196775421500206, 0.11360904574394226, 0.17561422288417816, 0.39859068393707275, 0.463090717792511, 0.027779623866081238, 0.36321908235549927, 0.38826337456703186, 0.1155594065785408, 0.18365515768527985, 0.18765173852443695, 0.20652532577514648, 0.4948332905769348, 0.33185672760009766, 0.18903113901615143, 0.4651884436607361, 0.10077451914548874, 0.2755669057369232, 0.4855681359767914, 0.3399803638458252, 0.4979493021965027, 0.06560458987951279], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2020350992679596, 0.24080929160118103, 0.4552687406539917, 0.13385631144046783, 0.3454025685787201, 0.359341025352478, 0.19949433207511902, 0.19324855506420135, 0.0278015173971653, 0.2784128785133362, 0.10880469530820847, 0.4821987450122833, 0.43213292956352234, 0.4136035144329071, 0.4286639392375946, 0.45243847370147705, 0.398994117975235, 0.05734647065401077, 0.0821986049413681, 0.04051638022065163, 0.23590251803398132, 0.31477099657058716, 0.24827063083648682, 0.07276001572608948], dtype='float32').reshape([24]),
            paddle.to_tensor([0.43783706426620483, 0.18943136930465698, 0.27873390913009644, 0.2892552316188812, 0.2941649258136749, 0.4749012887477875, 0.29349327087402344, 0.0666518285870552, 0.0958065465092659, 0.20309288799762726, 0.1120438277721405, 0.04687614366412163, 0.14642272889614105, 0.005906485952436924, 0.2721216082572937, 0.019323719665408134, 0.2647537589073181, 0.004347020760178566, 0.4608970582485199, 0.11217204481363297, 0.4876030385494232, 0.32018187642097473, 0.3538002371788025, 0.10902523994445801], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_96da484e12f2b84112b15e2a1a38e031(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1728, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1728], dtype='float32', min=0, max=0.5),
            paddle.uniform([1728], dtype='float32', min=0, max=0.5),
            paddle.uniform([1728], dtype='float32', min=0, max=0.5),
            paddle.uniform([1728], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_82ebb67fc269d919aff376c5235e0b50(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.39656054973602295, 0.05728951096534729, 0.02130347304046154, 0.08046165853738785, 0.010320335626602173, 0.479194313287735, 0.3306146264076233, 0.43063992261886597, 0.1393778771162033, 0.33200547099113464, 0.008876025676727295, 0.21224425733089447, 0.32552868127822876, 0.03314550220966339, 0.020602647215127945, 0.22952769696712494, 0.266872376203537, 0.030177278444170952, 0.12962718307971954, 0.3611423671245575, 0.4892416298389435, 0.13636177778244019, 0.24151483178138733, 0.31550806760787964], dtype='float32').reshape([24]),
            paddle.to_tensor([0.025108577683568, 0.1172739788889885, 0.3979434370994568, 0.14217641949653625, 0.10681711137294769, 0.20244525372982025, 0.19439835846424103, 0.00841116439551115, 0.11314798146486282, 0.41706064343452454, 0.0026306193321943283, 0.44299858808517456, 0.34129640460014343, 0.045553889125585556, 0.06878434121608734, 0.10528896003961563, 0.32122623920440674, 0.3057602047920227, 0.49416953325271606, 0.3558212220668793, 0.19332945346832275, 0.47825634479522705, 0.4129408299922943, 0.27059879899024963], dtype='float32').reshape([24]),
            paddle.to_tensor([0.28962650895118713, 0.23227915167808533, 0.4010904133319855, 0.4243854880332947, 0.3329593241214752, 0.008755199611186981, 0.46978560090065, 0.46640896797180176, 0.21122750639915466, 0.23501062393188477, 0.24031159281730652, 0.4670569598674774, 0.3152543306350708, 0.10346395522356033, 0.2503882050514221, 0.49369919300079346, 0.4760228991508484, 0.243844673037529, 0.1513361930847168, 0.10175901651382446, 0.253492146730423, 0.26542022824287415, 0.474092036485672, 0.03763120248913765], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3321743607521057, 0.3375796377658844, 0.21670928597450256, 0.22610270977020264, 0.10961787402629852, 0.19087469577789307, 0.4048381447792053, 0.3771742582321167, 0.11390984058380127, 0.0015485992189496756, 0.23075610399246216, 0.1593073010444641, 0.13273315131664276, 0.011592238210141659, 0.2681761085987091, 0.21778619289398193, 0.1702578067779541, 0.48193714022636414, 0.17352449893951416, 0.3325093388557434, 0.39387863874435425, 0.24873851239681244, 0.11356360465288162, 0.3285607099533081], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5421538e781890ee61cf6601fa2192e9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7f2d91bea1dc3d2a79875d4a25395722(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_696bd4e4e5cb5a173b3b1653904e9662(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 768, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3a8f5211399a84f10a158bea8c6903b2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 576, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ec9c5efc4345e6d97fad0ed47fe1f58b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 16, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2b0ea1323cdd00efc1f1b8507c8bfa56(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_66f8844ce42f4539d8be4137d503518d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 512], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.07247870415449142, 0.3634563088417053, 0.2968091368675232, 0.21730245649814606, 0.1840796023607254, 0.3701072633266449, 0.09556996077299118, 0.49661344289779663, 0.4467810392379761, 0.3861449956893921, 0.36304205656051636, 0.27592965960502625, 0.4684712588787079, 0.38945651054382324, 0.1547238677740097, 0.14483106136322021], dtype='float32').reshape([16]),
            paddle.to_tensor([0.351265549659729, 0.4881031811237335, 0.18542660772800446, 0.34872937202453613, 0.3585297465324402, 0.21208849549293518, 0.3036191761493683, 0.12786202132701874, 0.12222976237535477, 0.2582383155822754, 0.4501151442527771, 0.4859684705734253, 0.3122667670249939, 0.08521173149347305, 0.3072732985019684, 0.4004722535610199], dtype='float32').reshape([16]),
            paddle.to_tensor([0.16140100359916687, 0.09103547781705856, 0.428206205368042, 0.09029724448919296, 0.3037392497062683, 0.08986295759677887, 0.1737673580646515, 0.47017914056777954, 0.43385574221611023, 0.15667423605918884, 0.34905749559402466, 0.48468318581581116, 0.12782730162143707, 0.12288543581962585, 0.04512935131788254, 0.24008536338806152], dtype='float32').reshape([16]),
            paddle.to_tensor([0.11356589943170547, 0.068149633705616, 0.4048442244529724, 0.2667490541934967, 0.22469821572303772, 0.04103640466928482, 0.44182145595550537, 0.07618171721696854, 0.25972700119018555, 0.4100090563297272, 0.17941349744796753, 0.3121570944786072, 0.029170138761401176, 0.40725329518318176, 0.33483797311782837, 0.2386588603258133], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_63af24eae4570bdfbddb352e1ca7ef2a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 120, 120], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_76ed51e867f204ebf6abd1881221a09c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 2, 27], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c3aef9ef73ee779cd72700e121b013b8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 512], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.385672390460968, 0.18663765490055084, 0.47908815741539, 0.3033144176006317, 0.25882333517074585, 0.4039228558540344, 0.053661562502384186, 0.20598822832107544, 0.1574973315000534, 0.04065366089344025, 0.3913247585296631, 0.4973205626010895, 0.4834337532520294, 0.2225978672504425, 0.4475042521953583, 0.2821930944919586], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2779053747653961, 0.06449936330318451, 0.3471491038799286, 0.2709199786186218, 0.08897847682237625, 0.20835326611995697, 0.01507448498159647, 0.08050167560577393, 0.2020784169435501, 0.39926677942276, 0.1941891312599182, 0.49526235461235046, 0.49933308362960815, 0.1321261078119278, 0.2550126314163208, 0.2472190260887146], dtype='float32').reshape([16]),
            paddle.to_tensor([0.25685399770736694, 0.1377783864736557, 0.05000659078359604, 0.36619824171066284, 0.11593672633171082, 0.17607706785202026, 0.056350983679294586, 0.40264442563056946, 0.06067735329270363, 0.10276778042316437, 0.12907327711582184, 0.11538152396678925, 0.367114782333374, 0.4986429512500763, 0.23483937978744507, 0.007222298067063093], dtype='float32').reshape([16]),
            paddle.to_tensor([0.297771155834198, 0.2324564903974533, 0.36674708127975464, 0.22190634906291962, 0.32234665751457214, 0.2739808261394501, 0.43523189425468445, 0.3560970723628998, 0.03984316810965538, 0.407863050699234, 0.06122355908155441, 0.2514804005622864, 0.4191465973854065, 0.007320450618863106, 0.4145083427429199, 0.0845908597111702], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c50c1c92563e15641752ced1e35ed47b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_72b3ca841922e574f771cd7265014232(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_0edd578f48a07aa5d1d79f9f678b472e
    def get_inputs(self):
        return [
            paddle.uniform([49, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a4636f80649e28a2f733de92da21b956(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 32, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0293525ad84f3cbc92442f582349c7f5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 1, 1], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_56f9ca120930133a4ae24e1ba469ca43(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 256, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8ac116f40c7435c34e1ad933817dc0a5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bf6f7ab8ee7d6c9a0b394a29dd6f973e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1a35e4bd3e0d68cb05ae407a6df18365(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.28632837533950806, 0.14128106832504272, 0.2493639439344406, 0.345720499753952, 0.20566606521606445, 0.2737073600292206, 0.2740994691848755, 0.1319129914045334, 0.43545040488243103, 0.39978665113449097, 0.3408570885658264, 0.12128468602895737, 0.47730258107185364, 0.005106886383146048, 0.39203956723213196, 0.3586319386959076, 0.3630107045173645, 0.16483910381793976, 0.07819975912570953, 0.26583266258239746, 0.32701170444488525, 0.2616892457008362, 0.18025089800357819, 0.4905647039413452, 0.4066683053970337, 0.10244491696357727, 0.30671507120132446, 0.16134005784988403, 0.007610113359987736, 0.48379456996917725], dtype='float32').reshape([30]),
            paddle.to_tensor([0.31226497888565063, 0.24410192668437958, 0.48014771938323975, 0.13640087842941284, 0.1604175716638565, 0.42081478238105774, 0.0940498411655426, 0.45113834738731384, 0.012431250885128975, 0.3074324429035187, 0.44075626134872437, 0.19116725027561188, 0.2463790327310562, 0.49353018403053284, 0.46332141757011414, 0.12349491566419601, 0.31041646003723145, 0.28269413113594055, 0.15473128855228424, 0.3266838788986206, 0.4788303077220917, 0.27580785751342773, 0.36419618129730225, 0.3435961604118347, 0.24929632246494293, 0.4209553599357605, 0.1318824589252472, 0.3368256092071533, 0.28129497170448303, 0.24474464356899261], dtype='float32').reshape([30]),
            paddle.to_tensor([0.21631568670272827, 0.041163746267557144, 0.49168577790260315, 0.168253555893898, 0.36794164776802063, 0.3936631977558136, 0.11093170940876007, 0.12304194271564484, 0.15554790198802948, 0.36254486441612244, 0.006264525465667248, 0.37343066930770874, 0.37238380312919617, 0.36459416151046753, 0.25679701566696167, 0.3378460705280304, 0.2289934754371643, 0.3563367426395416, 0.4417405128479004, 0.48417359590530396, 0.354491263628006, 0.36415180563926697, 0.14865604043006897, 0.18035085499286652, 0.3607812821865082, 0.23040899634361267, 0.32328537106513977, 0.36939337849617004, 0.25951531529426575, 0.48407605290412903], dtype='float32').reshape([30]),
            paddle.to_tensor([0.37155917286872864, 0.4790911078453064, 0.1419016420841217, 0.047065917402505875, 0.07771960645914078, 0.21317359805107117, 0.3023163676261902, 0.07061141729354858, 0.07427775114774704, 0.1777678281068802, 0.04707559570670128, 0.3242088556289673, 0.4549206793308258, 0.3892363905906677, 0.39907628297805786, 0.18668557703495026, 0.33507704734802246, 0.17653968930244446, 0.42289796471595764, 0.31611767411231995, 0.09353134036064148, 0.4341999590396881, 0.4016473889350891, 0.44222113490104675, 0.4765552878379822, 0.08218319714069366, 0.07719479501247406, 0.3562677502632141, 0.017914045602083206, 0.018146755173802376], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f23763a3cace8ba81c8e2627ed031282(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 4, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9ae67bb515a7cd29d14650fe4a1492f2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1344, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1344], dtype='float32', min=0, max=0.5),
            paddle.uniform([1344], dtype='float32', min=0, max=0.5),
            paddle.uniform([1344], dtype='float32', min=0, max=0.5),
            paddle.uniform([1344], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_889f7e03798cab8767f6e2c1a0829b84(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 35, 35], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_89792713fe37a120ae7a473278cd0404(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 232, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_04d9f50998862810eaf29d37241fade8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a3ccb678001541d2502b2c9400458688(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b6c33a7e1e8c7a776a21c959e0bce6bf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_db4ed28f572276d1dfd149a9cf4ae9b5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.10899832099676132, 0.058641426265239716, 0.4680914878845215, 0.49764811992645264, 0.4089395999908447, 0.19575046002864838, 0.002506557386368513, 0.0680069625377655, 0.23905049264431, 0.08364303410053253, 0.15451104938983917, 0.16148129105567932], dtype='float32').reshape([12]),
            paddle.to_tensor([0.24611447751522064, 0.004131671041250229, 0.34462684392929077, 0.3928220272064209, 0.48850715160369873, 0.0107116112485528, 0.2759416699409485, 0.4848001003265381, 0.3443562686443329, 0.21954576671123505, 0.4041227102279663, 0.2082715630531311], dtype='float32').reshape([12]),
            paddle.to_tensor([0.49584120512008667, 0.02453133650124073, 0.36270177364349365, 0.40211260318756104, 0.44509074091911316, 0.3147653341293335, 0.4299190640449524, 0.018748680129647255, 0.4915767312049866, 0.051073554903268814, 0.4525124132633209, 0.14605359733104706], dtype='float32').reshape([12]),
            paddle.to_tensor([0.060496680438518524, 0.47099611163139343, 0.001930400962010026, 0.41627028584480286, 0.2798919975757599, 0.45019206404685974, 0.46268466114997864, 0.1460447758436203, 0.2567290961742401, 0.01710931584239006, 0.16381070017814636, 0.18618960678577423], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d28b17c1245e53df375ff4a0cb13afa4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_5e7cb23119f65b33bdbad71952f6dd12
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 512, 512], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_af6705d458db76e30c2f7d6b54962137(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b282b834dd4650606cb59822a044cc1c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4657667875289917, 0.445388525724411, 0.08012326806783676, 0.1743851602077484, 0.36935582756996155, 0.3585120141506195, 0.38934946060180664, 0.3859616816043854, 0.2124413102865219, 0.031106729060411453, 0.33093562722206116, 0.20284272730350494, 0.13131049275398254, 0.26470664143562317, 0.34076017141342163, 0.3953837454319, 0.4600703716278076, 0.448265016078949, 0.3494107127189636, 0.31591883301734924], dtype='float32').reshape([20]),
            paddle.to_tensor([0.2156522572040558, 0.08408421277999878, 0.209090456366539, 0.3348461985588074, 0.46937304735183716, 0.4521985650062561, 0.29451224207878113, 0.2317432314157486, 0.24500201642513275, 0.4266546964645386, 0.0005732902209274471, 0.06360822916030884, 0.05801507830619812, 0.21738234162330627, 0.07506414502859116, 0.23779134452342987, 0.443533718585968, 0.09349454194307327, 0.08041587471961975, 0.4906764328479767], dtype='float32').reshape([20]),
            paddle.to_tensor([0.30694377422332764, 0.3397088646888733, 0.3485954701900482, 0.021786078810691833, 0.30340293049812317, 0.09159071743488312, 0.4956936538219452, 0.4359346330165863, 0.010304446332156658, 0.2515478730201721, 0.4779409170150757, 0.04232025891542435, 0.025772931054234505, 0.07301938533782959, 0.043210387229919434, 0.32720065116882324, 0.1487170159816742, 0.2610475420951843, 0.2504526674747467, 0.029103828594088554], dtype='float32').reshape([20]),
            paddle.to_tensor([0.09606404602527618, 0.1806262880563736, 0.006684564985334873, 0.19392141699790955, 0.010293431580066681, 0.36065563559532166, 0.18809479475021362, 0.37173932790756226, 0.4077743589878082, 0.4569093883037567, 0.43689483404159546, 0.4813852906227112, 0.44392502307891846, 0.047166623175144196, 0.29971835017204285, 0.3904642164707184, 0.4737004041671753, 0.19185999035835266, 0.07461102306842804, 0.28434646129608154], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_925515d29d60fc29be68dc0c3b464edd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_959b2ca67282a9c7adeef590e1d96c15(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_efb8eadf8e2616d1e9fae027d7946b75(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3594430685043335, 0.35145026445388794, 0.20291206240653992, 0.4068829119205475, 0.4478452801704407, 0.14694543182849884, 0.32861965894699097, 0.12786483764648438, 0.30198967456817627, 0.4317781329154968, 0.19744160771369934, 0.3058277368545532, 0.15911173820495605, 0.007631268352270126, 0.3654508888721466, 0.08275505900382996], dtype='float32').reshape([16]),
            paddle.to_tensor([0.43717509508132935, 0.04976854845881462, 0.4072151184082031, 0.44566428661346436, 0.17829468846321106, 0.4839679002761841, 0.0185871385037899, 0.37285101413726807, 0.11549539119005203, 0.4655405879020691, 0.26392698287963867, 0.246123269200325, 0.333696573972702, 0.013175311498343945, 0.022244226187467575, 0.05488017201423645], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2055530548095703, 0.20342031121253967, 0.025027310475707054, 0.4170879125595093, 0.1641884595155716, 0.09181652218103409, 0.39807042479515076, 0.2663487195968628, 0.08807118237018585, 0.34875109791755676, 0.4983688294887543, 0.2393680363893509, 0.06059790402650833, 0.14015941321849823, 0.2746959626674652, 0.18756960332393646], dtype='float32').reshape([16]),
            paddle.to_tensor([0.42403262853622437, 0.4596876800060272, 0.4498903751373291, 0.33269256353378296, 0.22401560842990875, 0.2849830090999603, 0.14317293465137482, 0.17625795304775238, 0.010301463305950165, 0.0962367057800293, 0.18622568249702454, 0.05322995036840439, 0.3028433620929718, 0.38537850975990295, 0.19942770898342133, 0.3732358515262604], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d31a86b4694fd55f5c32dc43f8aed96a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5ed6de9e531cc6cc951a718a979a5d3c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a05e2f30ac6fac3cc1ccb22d88405d95(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_de5798b1afa75ada67eb12698088d33f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 2, 112, 199], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.30224061012268066, 0.03254084661602974], dtype='float32').reshape([2]),
            paddle.to_tensor([0.3550063371658325, 0.2618473172187805], dtype='float32').reshape([2]),
            paddle.to_tensor([0.33881255984306335, 0.27377790212631226], dtype='float32').reshape([2]),
            paddle.to_tensor([0.027170825749635696, 0.32545286417007446], dtype='float32').reshape([2]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b4ecc86a1e34bdabe624612b6f928c4c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d9169045ab642af435891b0879aadaa9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_381f0b5a6d42f83baba97e9057d16213(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_5e7cb23119f65b33bdbad71952f6dd12
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_50d223a0100c42923eb0df076c8c3721(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 100, 168], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c4698e8e66bf4ae39b260cdc515db379(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 100], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_623d84ccd2ed545d0bf6955d58c7d3e7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1104, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1104], dtype='float32', min=0, max=0.5),
            paddle.uniform([1104], dtype='float32', min=0, max=0.5),
            paddle.uniform([1104], dtype='float32', min=0, max=0.5),
            paddle.uniform([1104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_13d230d777b0b82757291afee8389fbf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 5, 5], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9d7466094bb60af57f138453f06ea9d3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 832, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([832], dtype='float32', min=0, max=0.5),
            paddle.uniform([832], dtype='float32', min=0, max=0.5),
            paddle.uniform([832], dtype='float32', min=0, max=0.5),
            paddle.uniform([832], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_553525f68ccf03d2e5c4688c02d3b7ac(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.168495774269104, 0.45027294754981995, 0.4037059247493744, 0.3872095048427582, 0.1062089055776596, 0.3606375753879547, 0.3104500472545624, 0.4336046576499939, 0.23918992280960083, 0.15693940222263336, 0.1365756392478943, 0.3983995318412781, 0.2965693473815918, 0.1830165535211563, 0.11627916246652603, 0.3565351963043213], dtype='float32').reshape([16]),
            paddle.to_tensor([0.1407838612794876, 0.36113399267196655, 0.44341546297073364, 0.05632919818162918, 0.08578456938266754, 0.4333077073097229, 0.22558347880840302, 0.2997073829174042, 0.1997213363647461, 0.25684309005737305, 0.12412168830633163, 0.3779727518558502, 0.295068621635437, 0.13552963733673096, 0.054655544459819794, 0.4129111170768738], dtype='float32').reshape([16]),
            paddle.to_tensor([0.06216885522007942, 0.2902412712574005, 0.273866206407547, 0.12308914959430695, 0.07524339854717255, 0.03603506088256836, 0.24875204265117645, 0.17994274199008942, 0.0800725445151329, 0.2430020123720169, 0.18128344416618347, 0.062434032559394836, 0.4577784836292267, 0.15206170082092285, 0.15681396424770355, 0.23045694828033447], dtype='float32').reshape([16]),
            paddle.to_tensor([0.09412878751754761, 0.13708913326263428, 0.12817683815956116, 0.16814695298671722, 0.2627894878387451, 0.13056232035160065, 0.2478124350309372, 0.3678111135959625, 0.2902405560016632, 0.32470938563346863, 0.049164898693561554, 0.38073447346687317, 0.32130753993988037, 0.409290611743927, 0.11373312026262283, 0.11178215593099594], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9f81b49402841acfaa692190e4de94e6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.20859821140766144, 0.19286102056503296, 0.42786651849746704, 0.18370012938976288, 0.2360146939754486, 0.4991688132286072, 0.004245898220688105, 0.3496803939342499, 0.46749696135520935, 0.3531562387943268, 0.15999363362789154, 0.1433997005224228, 0.04508150741457939, 0.3668760657310486, 0.014230131171643734, 0.030535545200109482, 0.36939072608947754, 0.05310244858264923], dtype='float32').reshape([18]),
            paddle.to_tensor([0.343072772026062, 0.49485859274864197, 0.11031368374824524, 0.2786392569541931, 0.4121670722961426, 0.485469788312912, 0.03587200492620468, 0.44238534569740295, 0.42835795879364014, 0.06740862876176834, 0.013983393087983131, 0.25978851318359375, 0.16217702627182007, 0.22094285488128662, 0.16143637895584106, 0.2945421636104584, 0.24347729980945587, 0.4696723520755768], dtype='float32').reshape([18]),
            paddle.to_tensor([0.22570079565048218, 0.26022815704345703, 0.25016820430755615, 0.2713734805583954, 0.2623152732849121, 0.3026962876319885, 0.009782657958567142, 0.43106189370155334, 0.10611522942781448, 0.05738377571105957, 0.34398674964904785, 0.02240699529647827, 0.2279987633228302, 0.057589076459407806, 0.17160752415657043, 0.4306522011756897, 0.24844622611999512, 0.3729492723941803], dtype='float32').reshape([18]),
            paddle.to_tensor([0.034513767808675766, 0.10093487799167633, 0.12321220338344574, 0.4897458851337433, 0.49061036109924316, 0.1501261591911316, 0.49658676981925964, 0.4224320650100708, 0.33626726269721985, 0.1294325441122055, 0.4184969663619995, 0.3720744848251343, 0.35341784358024597, 0.4432118833065033, 0.24377259612083435, 0.05855929106473923, 0.4369117021560669, 0.005266718566417694], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_18c85722b7f424e734508dd0a966147e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_189d59256fd9c694832cba0317019d71(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.36598604917526245, 0.05228954181075096, 0.4293106198310852, 0.330159455537796, 0.4848635792732239, 0.3127482831478119, 0.4756535589694977, 0.31325507164001465, 0.19514992833137512, 0.014605103991925716, 0.0034412178210914135, 0.4538869857788086, 0.3295830190181732, 0.2236286997795105, 0.2561025023460388, 0.2008422315120697, 0.38130107522010803, 0.17468559741973877, 0.4417874217033386, 0.34042832255363464, 0.1424786001443863, 0.0023439531214535236, 0.15973533689975739, 0.4716908931732178, 0.17349979281425476, 0.21686851978302002, 0.2254619300365448, 0.2028338462114334, 0.35472655296325684, 0.25539430975914], dtype='float32').reshape([30]),
            paddle.to_tensor([0.223370760679245, 0.0017020769882947206, 0.05683235824108124, 0.355141282081604, 0.21800681948661804, 0.2075585275888443, 0.3012845814228058, 0.07172615826129913, 0.2591599225997925, 0.2855493426322937, 0.23717470467090607, 0.48542243242263794, 0.32580044865608215, 0.07737146317958832, 0.10102510452270508, 0.48421216011047363, 0.23529693484306335, 0.12236654758453369, 0.4391002357006073, 0.14910545945167542, 0.0773862674832344, 0.08215721696615219, 0.4525041878223419, 0.27515125274658203, 0.4948280453681946, 0.11107844114303589, 0.18784977495670319, 0.36307772994041443, 0.09004922211170197, 0.41666698455810547], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3108045756816864, 0.2089431881904602, 0.22059676051139832, 0.25638899207115173, 0.3441062867641449, 0.4736747443675995, 0.11891782283782959, 0.13593323528766632, 0.10431888699531555, 0.30651599168777466, 0.050448738038539886, 0.0033744187094271183, 0.4815788269042969, 0.23677392303943634, 0.3836398124694824, 0.013680662959814072, 0.4254688024520874, 0.24298207461833954, 0.286152720451355, 0.34345924854278564, 0.03195996209979057, 0.46560198068618774, 0.25524434447288513, 0.17933093011379242, 0.35521262884140015, 0.25600478053092957, 0.12434606999158859, 0.4531959295272827, 0.12425293773412704, 0.06022530049085617], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3333151638507843, 0.013263843022286892, 0.3541349172592163, 0.3695056736469269, 0.3784995377063751, 0.28513678908348083, 0.25434932112693787, 0.029968131333589554, 0.4699254631996155, 0.44578224420547485, 0.20570993423461914, 0.357229620218277, 0.14448252320289612, 0.34582754969596863, 0.11797607690095901, 0.29756808280944824, 0.22961391508579254, 0.37879812717437744, 0.24340994656085968, 0.31574293971061707, 0.13245387375354767, 0.4149109721183777, 0.026516325771808624, 0.484280526638031, 0.3662954568862915, 0.08662891387939453, 0.08462487161159515, 0.18365827202796936, 0.16549447178840637, 0.31901177763938904], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_01a234bf8fb86b474035e84be1dd91cd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dbcc2c47ef76280e1d10826731204cc9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 75, 75], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ecb2cbe38894332a060082d507cc27f4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 2, 240, 240], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3941758871078491, 0.44622060656547546], dtype='float32').reshape([2]),
            paddle.to_tensor([0.10488570481538773, 0.3020337224006653], dtype='float32').reshape([2]),
            paddle.to_tensor([0.4118139445781708, 0.16887153685092926], dtype='float32').reshape([2]),
            paddle.to_tensor([0.05805171653628349, 0.014876659959554672], dtype='float32').reshape([2]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8cdff67216a4406d99f620d9181d898d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 4, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4119814932346344, 0.301640123128891, 0.25297728180885315, 0.16397833824157715], dtype='float32').reshape([4]),
            paddle.to_tensor([0.39527878165245056, 0.13570992648601532, 0.14499618113040924, 0.2620997428894043], dtype='float32').reshape([4]),
            paddle.to_tensor([0.27523186802864075, 0.1715800166130066, 0.38450121879577637, 0.16117994487285614], dtype='float32').reshape([4]),
            paddle.to_tensor([0.008192326873540878, 0.4921569526195526, 0.433409184217453, 0.18237319588661194], dtype='float32').reshape([4]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ce8140772952b94a036acd68b4f98378(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3777152895927429, 0.08005386590957642, 0.24916429817676544, 0.3431626856327057, 0.492319792509079, 0.3612518906593323, 0.08316666632890701, 0.251723051071167, 0.19899390637874603, 0.02859320491552353, 0.2916036546230316, 0.03081236220896244, 0.19171468913555145, 0.30904316902160645, 0.34762996435165405, 0.18180006742477417, 0.03556298837065697, 0.023115521296858788, 0.15608134865760803, 0.10500942915678024], dtype='float32').reshape([20]),
            paddle.to_tensor([0.35370030999183655, 0.21447114646434784, 0.48620113730430603, 0.34443530440330505, 0.010307824239134789, 0.07347846776247025, 0.3912593722343445, 0.2110980898141861, 0.03677137941122055, 0.3771616220474243, 0.07102909684181213, 0.1128835529088974, 0.24689246714115143, 0.2811128795146942, 0.34955209493637085, 0.3680473566055298, 0.1772938221693039, 0.08780407905578613, 0.3142191469669342, 0.3794931471347809], dtype='float32').reshape([20]),
            paddle.to_tensor([0.2925431430339813, 0.2702403664588928, 0.0023932820186018944, 0.22143752872943878, 0.21434012055397034, 0.3822135627269745, 0.09110590815544128, 0.4074828624725342, 0.40549197793006897, 0.14475417137145996, 0.1904360055923462, 0.4646743834018707, 0.1572047472000122, 0.016684526577591896, 0.33422380685806274, 0.4431517720222473, 0.48223403096199036, 0.3198045492172241, 0.20755797624588013, 0.14733023941516876], dtype='float32').reshape([20]),
            paddle.to_tensor([0.34537431597709656, 0.12966762483119965, 0.2792069911956787, 0.3137964606285095, 0.34126168489456177, 0.18957912921905518, 0.3634636700153351, 0.46387916803359985, 0.2585172951221466, 0.4458974003791809, 0.40724942088127136, 0.28366467356681824, 0.3320062756538391, 0.12400970607995987, 0.04915565252304077, 0.34987273812294006, 0.2979290187358856, 0.37220263481140137, 0.4406023323535919, 0.11704888939857483], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3ec96d28a70678ed9183463d1a62f9fb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 448, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([448], dtype='float32', min=0, max=0.5),
            paddle.uniform([448], dtype='float32', min=0, max=0.5),
            paddle.uniform([448], dtype='float32', min=0, max=0.5),
            paddle.uniform([448], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5899581a6144ae10fa72a202238d22cf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 300, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([300], dtype='float32', min=0, max=0.5),
            paddle.uniform([300], dtype='float32', min=0, max=0.5),
            paddle.uniform([300], dtype='float32', min=0, max=0.5),
            paddle.uniform([300], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9668e41c5601fb0a6edfdeb3ed5cbbfb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_d589af777f5a8ae699611515c41499fb
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_229c342c4d2a42c93b5574cac4614fe2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_60f8b95aba499b85b6b22561e5c747f1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_f78910a24ab00d45797c31273495c7e0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-06'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5ece8c3dabc22b8960ba3a97f449ebe9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_f78910a24ab00d45797c31273495c7e0
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1fa8320b5e3c67f9c47e5a1586a1dc75(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_deac690d00da6ed8a6e8b928460223f5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 122, 122], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2990320026874542, 0.16686217486858368, 0.24925236403942108, 0.15594987571239471, 0.3344300091266632, 0.3241084814071655, 0.2694866359233856, 0.4211658537387848, 0.49227383732795715, 0.4349578619003296, 0.016379542648792267, 0.13248026371002197, 0.4892207384109497, 0.3213573396205902, 0.22317855060100555, 0.3677797317504883], dtype='float32').reshape([16]),
            paddle.to_tensor([0.13913622498512268, 0.09680444747209549, 0.04884376376867294, 0.4071809649467468, 0.262160986661911, 0.13018858432769775, 0.0029688512440770864, 0.06103269010782242, 0.09655139595270157, 0.05530861020088196, 0.27943891286849976, 0.21331064403057098, 0.27030494809150696, 0.4981365203857422, 0.29954811930656433, 0.23613478243350983], dtype='float32').reshape([16]),
            paddle.to_tensor([0.01199547853320837, 0.1285930871963501, 0.40364354848861694, 0.28514230251312256, 0.41649776697158813, 0.10486886650323868, 0.22282153367996216, 0.44529327750205994, 0.3334721028804779, 0.09169784933328629, 0.4074445962905884, 0.3640289008617401, 0.013776463456451893, 0.21941597759723663, 0.3213496506214142, 0.4534880220890045], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4775794446468353, 0.0684949979186058, 0.18729226291179657, 0.28767162561416626, 0.48974788188934326, 0.3411691188812256, 0.4297363758087158, 0.01419773604720831, 0.30872997641563416, 0.09777574986219406, 0.2190455198287964, 0.3419100344181061, 0.2753508687019348, 0.0899607241153717, 0.4642995595932007, 0.1782807856798172], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cdcd10a9a13f61fccab1d926f6ceacdc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 240, 240], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_169c7e0144613ee97737ac81cc473615(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.43506115674972534, 0.021799268200993538, 0.24350766837596893, 0.4731212556362152, 0.03413749858736992, 0.1164255291223526, 0.4722309410572052, 0.045996636152267456, 0.34946587681770325, 0.1535879671573639, 0.220633864402771, 0.3637694716453552, 0.108400858938694, 0.2870623469352722, 0.34402498602867126, 0.23090466856956482, 0.31052166223526, 0.29214736819267273, 0.25394394993782043, 0.3271116018295288, 0.16171078383922577, 0.45405665040016174, 0.2348231077194214, 0.47510260343551636], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4598386585712433, 0.11653245240449905, 0.16693511605262756, 0.18184788525104523, 0.1155412346124649, 0.2549895644187927, 0.1559281349182129, 0.13066589832305908, 0.39184901118278503, 0.2678738832473755, 0.4706384837627411, 0.20293518900871277, 0.46743062138557434, 0.48272669315338135, 0.1278088539838791, 0.3369060456752777, 0.4021955728530884, 0.4209921658039093, 0.34261614084243774, 0.11912161111831665, 0.20894461870193481, 0.3905933201313019, 0.13244929909706116, 0.19000044465065002], dtype='float32').reshape([24]),
            paddle.to_tensor([0.08509759604930878, 0.3731417953968048, 0.3663046061992645, 0.2844570577144623, 0.23040321469306946, 0.22163701057434082, 0.2422511875629425, 0.22816459834575653, 0.4148380756378174, 0.2484769970178604, 0.2883985936641693, 0.47171005606651306, 0.17096053063869476, 0.05757493898272514, 0.3809784948825836, 0.4222746789455414, 0.38805869221687317, 0.3610861003398895, 0.38618171215057373, 0.3512231707572937, 0.21169275045394897, 0.4996509850025177, 0.15196122229099274, 0.43738049268722534], dtype='float32').reshape([24]),
            paddle.to_tensor([0.04823973774909973, 0.16646888852119446, 0.4895575940608978, 0.059915442019701004, 0.013578644022345543, 0.08878595381975174, 0.2821810841560364, 0.44016316533088684, 0.05937817692756653, 0.024739336222410202, 0.2455274760723114, 0.08173735439777374, 0.024153368547558784, 0.42381763458251953, 0.025762207806110382, 0.45133551955223083, 0.3690683841705322, 0.10155858844518661, 0.45697420835494995, 0.32350605726242065, 0.11238404363393784, 0.44071513414382935, 0.14766807854175568, 0.1964782029390335], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e37710a031515f2d119a0501331ae10d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1776, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1776], dtype='float32', min=0, max=0.5),
            paddle.uniform([1776], dtype='float32', min=0, max=0.5),
            paddle.uniform([1776], dtype='float32', min=0, max=0.5),
            paddle.uniform([1776], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1f1986bd7ae91af9bfb4ed6740c361b1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 1, 1], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_faa48a48d846cdbe9b992a121a0a9ee4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_895d2923c2ce77f9c2cd1e3094a503d5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 47, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([47], dtype='float32', min=0, max=0.5),
            paddle.uniform([47], dtype='float32', min=0, max=0.5),
            paddle.uniform([47], dtype='float32', min=0, max=0.5),
            paddle.uniform([47], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bbe9af8f06b58db52b978ea1089b990d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_00fe16de063510ec07aad52a136c8fdf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 122, 122], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_148e5e94a11833f351678c562ddc2b60(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 60, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_55f6309c904b67ae9325a31d81b30395(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a142c909e153d8fb714a0d19c8065431(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8907ebe5b54236f34c2c366f97ba615f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c0876997be363f1cdc0d5dbb3d88d4c2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cf7bff76bad3300714c9644a262d27c1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 244, 244], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ef19fc42a04b0b0f46f1fd8a157fa999(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e54cf321e9a40e7cf7f363082c996f64(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 768, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5d414d342c5a4ff2a46c716078f7dd36(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_81957a704c1947d4666f67228f9b85d1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8e0409ce6f1d287aed1b82add7f827e6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fb28723607a98df728971ae1d6338a1c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1632, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1632], dtype='float32', min=0, max=0.5),
            paddle.uniform([1632], dtype='float32', min=0, max=0.5),
            paddle.uniform([1632], dtype='float32', min=0, max=0.5),
            paddle.uniform([1632], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9fd32e34d5c30cbe1b04a7fbe57fa6f7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_181800d78ea15dc04599796758812f74(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 2, 27], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9bcf356110d246c8d7fb3a578628f16f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 112, 199], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3c0756486403c1b4c2d3e4a4276a17c9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 88, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_268ed7fe830c2a9b0807c0851cd9624d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2d645178b28894a4daab2a0a05c908b7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_145ad20e7a3b3c8aa31f6b40d4fe3d45
    def get_inputs(self):
        return [
            paddle.uniform([196, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fc6aed72fa99926f02de2499d1a96ea6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.25986331701278687, 0.43823230266571045, 0.028112368658185005, 0.28179624676704407, 0.3827641010284424, 0.48232707381248474, 0.28385892510414124, 0.013784192502498627, 0.3578951060771942, 0.03952093794941902, 0.3973453938961029, 0.4566037952899933, 0.007329720072448254, 0.3797299265861511, 0.38958460092544556, 0.345822274684906, 0.2231360226869583, 0.3715880215167999, 0.11002600938081741, 0.25094109773635864, 0.46323955059051514, 0.34493571519851685, 0.42048385739326477, 0.019665559753775597, 0.07435794919729233, 0.42876046895980835, 0.12119825929403305, 0.436015248298645, 0.3659762144088745, 0.2941727638244629], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2069305181503296, 0.019984036684036255, 0.3114710748195648, 0.12972332537174225, 0.01612112671136856, 0.4161266088485718, 0.37018507719039917, 0.2705472409725189, 0.3588477373123169, 0.22181102633476257, 0.0635269284248352, 0.026785586029291153, 0.23211368918418884, 0.21586453914642334, 0.09683329612016678, 0.007787102833390236, 0.3962724506855011, 0.46036475896835327, 0.3445013165473938, 0.2924237549304962, 0.25400224328041077, 0.24786275625228882, 0.2091493159532547, 0.4205494821071625, 0.3711554706096649, 0.3377589285373688, 0.267276793718338, 0.3705298602581024, 0.42542433738708496, 0.024734575301408768], dtype='float32').reshape([30]),
            paddle.to_tensor([0.15969476103782654, 0.1520879864692688, 0.1667984426021576, 0.0716630071401596, 0.24698254466056824, 0.17642083764076233, 0.2680627405643463, 0.2371547967195511, 0.42537227272987366, 0.22210325300693512, 0.33846136927604675, 0.41776424646377563, 0.2763899862766266, 0.20389559864997864, 0.05807529389858246, 0.11718566715717316, 0.08368539810180664, 0.31659767031669617, 0.1725725382566452, 0.43826571106910706, 0.48124265670776367, 0.3894129991531372, 0.006132147740572691, 0.42897704243659973, 0.035739969462156296, 0.25385940074920654, 0.08346621692180634, 0.12039797753095627, 0.47568371891975403, 0.39171019196510315], dtype='float32').reshape([30]),
            paddle.to_tensor([0.47499459981918335, 0.2044605016708374, 0.4789358079433441, 0.18021824955940247, 0.46062883734703064, 0.3723992109298706, 0.15942059457302094, 0.21915973722934723, 0.36462509632110596, 0.09884072095155716, 0.1446353793144226, 0.18101684749126434, 0.3866523504257202, 0.2990291118621826, 0.19130314886569977, 0.30244511365890503, 0.17049729824066162, 0.4492628276348114, 0.49672940373420715, 0.3369891047477722, 0.4854099750518799, 0.4424491226673126, 0.2207980751991272, 0.23445354402065277, 0.16631992161273956, 0.05857020244002342, 0.14647482335567474, 0.14116516709327698, 0.12679605185985565, 0.22209836542606354], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bc8d1f2c867d19d8c04cdc24bb7b3207(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 147, 147], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_96434fd86b2448f4325bb8ec9f5a58e7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 31, 31], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b0b029afc93270694dd297f9e6304933(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 64, 128], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.20590579509735107, 0.4847494959831238, 0.04708035662770271, 0.3402717709541321, 0.2968468964099884, 0.10633765906095505, 0.38747259974479675, 0.3157620131969452, 0.4398011267185211, 0.42890465259552, 0.11298716813325882, 0.044196926057338715, 0.46138375997543335, 0.30270785093307495, 0.15955354273319244, 0.20691335201263428, 0.43685904145240784, 0.19135284423828125], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4427923560142517, 0.15861861407756805, 0.31822681427001953, 0.3286589980125427, 0.1317663937807083, 0.11109120398759842, 0.04304921627044678, 0.1827152669429779, 0.18673716485500336, 0.13525448739528656, 0.4798341393470764, 0.36508622765541077, 0.010868473909795284, 0.23399287462234497, 0.05402661859989166, 0.4510630667209625, 0.24923822283744812, 0.027834370732307434], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3136368691921234, 0.10117294639348984, 0.006800642237067223, 0.18322813510894775, 0.3686447739601135, 0.2814770042896271, 0.07077866047620773, 0.3140571117401123, 0.27964261174201965, 0.2523314654827118, 0.19384926557540894, 0.18785060942173004, 0.4990817606449127, 0.16204199194908142, 0.2419704645872116, 0.07562405616044998, 0.09521826356649399, 0.3430609405040741], dtype='float32').reshape([18]),
            paddle.to_tensor([0.07716216146945953, 0.42329394817352295, 0.4608660638332367, 0.45019304752349854, 0.21094489097595215, 0.058116182684898376, 0.250590056180954, 0.20984573662281036, 0.18753071129322052, 0.4138912856578827, 0.1141546294093132, 0.3691003620624542, 0.18045926094055176, 0.29675188660621643, 0.009398587979376316, 0.21439428627490997, 0.3870432376861572, 0.11595200002193451], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ed63f6e2e57ab7d80ecf13063aadc452(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4597308337688446, 0.18770790100097656, 0.4687431752681732, 0.42816534638404846, 0.12045212835073471, 0.26744136214256287, 0.20194262266159058, 0.06895214319229126, 0.22583197057247162, 0.4435502290725708, 0.008202540688216686, 0.42513856291770935, 0.4526117742061615, 0.27578049898147583, 0.47634318470954895, 0.462629497051239, 0.4661516845226288, 0.22625993192195892, 0.26687121391296387, 0.42425012588500977, 0.37141090631484985, 0.44315242767333984, 0.26154184341430664, 0.45629194378852844], dtype='float32').reshape([24]),
            paddle.to_tensor([0.42745229601860046, 0.37733086943626404, 0.20129725337028503, 0.09786824136972427, 0.12256138771772385, 0.03882332891225815, 0.30040454864501953, 0.09266632050275803, 0.37866827845573425, 0.04064185172319412, 0.4504571855068207, 0.09703806042671204, 0.43326085805892944, 0.3480915129184723, 0.12716813385486603, 0.2847440242767334, 0.4913405776023865, 0.2742202877998352, 0.21451503038406372, 0.4624961316585541, 0.05435754731297493, 0.20573033392429352, 0.36380237340927124, 0.251407653093338], dtype='float32').reshape([24]),
            paddle.to_tensor([0.06476046144962311, 0.09494629502296448, 0.3368716239929199, 0.48532840609550476, 0.2341005951166153, 0.4206598699092865, 0.2931012511253357, 0.3193410336971283, 0.06071344390511513, 0.03175972402095795, 0.3445054590702057, 0.3400631546974182, 0.10875201225280762, 0.49325135350227356, 0.3182584345340729, 0.4016699194908142, 0.314245343208313, 0.49932950735092163, 0.42902910709381104, 0.07166916131973267, 0.08142377436161041, 0.24630481004714966, 0.12744095921516418, 0.24378779530525208], dtype='float32').reshape([24]),
            paddle.to_tensor([0.439792662858963, 0.27757272124290466, 0.09193336963653564, 0.4484293460845947, 0.48574408888816833, 0.04107671231031418, 0.27821940183639526, 0.48946037888526917, 0.1709812581539154, 0.024589210748672485, 0.4242574870586395, 0.2989162504673004, 0.04405717924237251, 0.1482311338186264, 0.487949401140213, 0.355716735124588, 0.3535022735595703, 0.0960685983300209, 0.41337358951568604, 0.04248129576444626, 0.1536162942647934, 0.23868463933467865, 0.08304237574338913, 0.15655364096164703], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5aa1d301a9c0bc7f66585b24d6eed571(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f47399c3420ff0d0ec4588d732f73d4c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 60, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_063df08bbc6af8992edd8e9c74182162(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5cbd2a5def56dcd3286531931b581b79(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 31, 31], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_30877f4063a08621273c9c9a5be40654(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 120, 120], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a206ffcb73dfc928c22c01d58854bd68(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 32, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.131561741232872, 0.09449553489685059, 0.23188456892967224, 0.1534268856048584, 0.13976377248764038, 0.40931403636932373, 0.42697522044181824, 0.17558585107326508], dtype='float32').reshape([8]),
            paddle.to_tensor([0.49557140469551086, 0.2477581948041916, 0.17711833119392395, 0.4824180006980896, 0.0013122609816491604, 0.09000260382890701, 0.11975488066673279, 0.163849875330925], dtype='float32').reshape([8]),
            paddle.to_tensor([0.4610351324081421, 0.4547572731971741, 0.11885730177164078, 0.22803737223148346, 0.02866187132894993, 0.4087020754814148, 0.04074695333838463, 0.2722623348236084], dtype='float32').reshape([8]),
            paddle.to_tensor([0.033624496310949326, 0.01507482398301363, 0.1302262842655182, 0.2883298397064209, 0.265112966299057, 0.24200472235679626, 0.020974241197109222, 0.030081698670983315], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8d524bc3a72041d016abab1faac5dd33(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 31, 31], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_da9c588a04f07026ec39a595362ab84b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 84, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([84], dtype='float32', min=0, max=0.5),
            paddle.uniform([84], dtype='float32', min=0, max=0.5),
            paddle.uniform([84], dtype='float32', min=0, max=0.5),
            paddle.uniform([84], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_76032b40d76dedd39f24d5c72aa0fd13(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 320, 320], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.43687883019447327, 0.43719932436943054, 0.26395267248153687, 0.030918359756469727, 0.388744592666626, 0.13499289751052856, 0.16669641435146332, 0.35702773928642273, 0.10072523355484009, 0.1425125002861023, 0.1283431500196457, 0.4374690353870392, 0.10476014018058777, 0.37782397866249084, 0.12289974093437195, 0.11517412215471268], dtype='float32').reshape([16]),
            paddle.to_tensor([0.33310624957084656, 0.18008938431739807, 0.47058308124542236, 0.11367284506559372, 0.16793674230575562, 0.06616386026144028, 0.4882330298423767, 0.05742579698562622, 0.37560588121414185, 0.33666670322418213, 0.2998669147491455, 0.09458357095718384, 0.2564433813095093, 0.21388015151023865, 0.15408781170845032, 0.025342347100377083], dtype='float32').reshape([16]),
            paddle.to_tensor([0.38383200764656067, 0.0833863615989685, 0.4280000329017639, 0.10363282263278961, 0.4955114722251892, 0.24609850347042084, 0.44987308979034424, 0.05307302251458168, 0.4443794786930084, 0.08764879405498505, 0.36176633834838867, 0.3371015787124634, 0.18721677362918854, 0.3747257888317108, 0.1359313279390335, 0.3120870590209961], dtype='float32').reshape([16]),
            paddle.to_tensor([0.12716951966285706, 0.4200979173183441, 0.4566292464733124, 0.2926531434059143, 0.3056398630142212, 0.26507389545440674, 0.24814850091934204, 0.19255764782428741, 0.055343274027109146, 0.282825231552124, 0.4154821038246155, 0.09133247286081314, 0.18348802626132965, 0.330654114484787, 0.11104318499565125, 0.1535942554473877], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c8cc0c86eda51cbcefdd1a941547ba04(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.03980453312397003, 0.31619808077812195, 0.273287832736969, 0.34749868512153625, 0.20609207451343536, 0.31516456604003906, 0.3294079899787903, 0.41039255261421204, 0.35507073998451233, 0.2370443493127823, 0.34880098700523376, 0.18772198259830475, 0.2869270443916321, 0.25710466504096985, 0.004404444247484207, 0.3632282316684723, 0.4430922865867615, 0.21196970343589783, 0.06201700121164322, 0.45461729168891907, 0.329468309879303, 0.26987871527671814, 0.3735305964946747, 0.11661543697118759, 0.23714913427829742, 0.281004399061203, 0.14420361816883087, 0.13157227635383606], dtype='float32').reshape([28]),
            paddle.to_tensor([0.017887823283672333, 0.29509809613227844, 0.10783026367425919, 0.2406163364648819, 0.028125490993261337, 0.3375689685344696, 0.21122872829437256, 0.13598251342773438, 0.3744308650493622, 0.08617017418146133, 0.41906118392944336, 0.21061336994171143, 0.27161160111427307, 0.030350875109434128, 0.4547114670276642, 0.3042958676815033, 0.012345867231488228, 0.042229533195495605, 0.025708038359880447, 0.26802724599838257, 0.04753970354795456, 0.44340500235557556, 0.27883172035217285, 0.4559679925441742, 0.12441577017307281, 0.1363680213689804, 0.16734936833381653, 0.19380970299243927], dtype='float32').reshape([28]),
            paddle.to_tensor([0.242343932390213, 0.1163322776556015, 0.4973642826080322, 0.43727484345436096, 0.3322020471096039, 0.3905533254146576, 0.3198823630809784, 0.3105839490890503, 0.16338174045085907, 0.12383744865655899, 0.19943533837795258, 0.3120983839035034, 0.15702635049819946, 0.12810058891773224, 0.3372724652290344, 0.48848170042037964, 0.4969286322593689, 0.22890624403953552, 0.27368679642677307, 0.2697399854660034, 0.1958024501800537, 0.32909849286079407, 0.02112787961959839, 0.14428745210170746, 0.41644221544265747, 0.410949170589447, 0.19642548263072968, 0.21578669548034668], dtype='float32').reshape([28]),
            paddle.to_tensor([0.3519272208213806, 0.0448688305914402, 0.23559939861297607, 0.04528781399130821, 0.14611835777759552, 0.3441820442676544, 0.31057196855545044, 0.22286836802959442, 0.43987223505973816, 0.4349954128265381, 0.17235209047794342, 0.36982327699661255, 0.17511528730392456, 0.3044319450855255, 0.16878265142440796, 0.20552048087120056, 0.060565412044525146, 0.15926003456115723, 0.4390913248062134, 0.4968467652797699, 0.25871747732162476, 0.32094550132751465, 0.49542853236198425, 0.1329106092453003, 0.06590767949819565, 0.3242497444152832, 0.21207647025585175, 0.4532657861709595], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_adc49fd1f7cc60f9458491425b0cfe7a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_35252d7289e9a4700b3ee6ee08beb8f2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 840, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([840], dtype='float32', min=0, max=0.5),
            paddle.uniform([840], dtype='float32', min=0, max=0.5),
            paddle.uniform([840], dtype='float32', min=0, max=0.5),
            paddle.uniform([840], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b628a784a403ca544f738436358df2cf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 75, 75], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f4f848d1badfe78fc33dc03b83f3858e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.06583740562200546, 0.23820945620536804, 0.1841266006231308, 0.49686160683631897, 0.45058542490005493, 0.26261085271835327, 0.4256831109523773, 0.10282613337039948, 0.16158510744571686, 0.24228839576244354, 0.21538768708705902, 0.23323148488998413, 0.09537092596292496, 0.3626510500907898, 0.32409563660621643, 0.2644776403903961, 0.21524985134601593, 0.14514079689979553], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4881894886493683, 0.053462304174900055, 0.3397030234336853, 0.14725805819034576, 0.016243694350123405, 0.049102481454610825, 0.33662739396095276, 0.38799136877059937, 0.054343607276678085, 0.04876801371574402, 0.40147465467453003, 0.49721968173980713, 0.21230104565620422, 0.24568110704421997, 0.4968847334384918, 0.39341604709625244, 0.36152544617652893, 0.15533053874969482], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3286198079586029, 0.039531126618385315, 0.4799058437347412, 0.33260759711265564, 0.3739774525165558, 0.38197529315948486, 0.01486955676227808, 0.15700379014015198, 0.10998853296041489, 0.01695547066628933, 0.016387054696679115, 0.32719334959983826, 0.33054471015930176, 0.1370261013507843, 0.14047136902809143, 0.4232602119445801, 0.34250420331954956, 0.043903738260269165], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4102322459220886, 0.36512264609336853, 0.4325436055660248, 0.49012577533721924, 0.34395450353622437, 0.16338951885700226, 0.2813754975795746, 0.41160517930984497, 0.03041965514421463, 0.023177633062005043, 0.18737328052520752, 0.39064958691596985, 0.4808104634284973, 0.16329160332679749, 0.3763502538204193, 0.250696063041687, 0.06023377925157547, 0.4166950285434723], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9a35ee208a6d0d28e7e95bea9ba909d4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_067890abfb83f8910a777be9c42a6152(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 4, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ca0e314a3a0b65ff777cce1240e4b4ce(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 144, 14, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2bba1baf959b6e4ac8a282c1b3494e9c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 122, 122], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_93525d94ea551542a1ce1b3867134a97(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_446dbd9ab454b2f4bb089621fcdc978d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 16, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1aca5b2f7d5193b8c956405eaa029246(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 31, 31], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9741da1500161c6c6d265764f2566383(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 75, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([75], dtype='float32', min=0, max=0.5),
            paddle.uniform([75], dtype='float32', min=0, max=0.5),
            paddle.uniform([75], dtype='float32', min=0, max=0.5),
            paddle.uniform([75], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_168a090c67fdc8203f3ac839a41317a5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 32, 64], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4781056046485901, 0.09295909106731415, 0.20265400409698486, 0.225518599152565, 0.3803684115409851, 0.16414497792720795, 0.3106452524662018, 0.04484536871314049, 0.05445510521531105, 0.391112357378006, 0.04549309238791466, 0.40341389179229736, 0.016502732411026955, 0.0017791071441024542, 0.15914660692214966, 0.22937527298927307, 0.2701152265071869, 0.3133670389652252], dtype='float32').reshape([18]),
            paddle.to_tensor([0.26708418130874634, 0.27300870418548584, 0.3531413674354553, 0.3511173129081726, 0.2519657611846924, 0.32627567648887634, 0.007244503125548363, 0.10700821876525879, 0.09252916276454926, 0.12087574601173401, 0.24125127494335175, 0.18785734474658966, 0.2570064067840576, 0.10775162279605865, 0.3216592073440552, 0.006775521207600832, 0.023662839084863663, 0.1519080102443695], dtype='float32').reshape([18]),
            paddle.to_tensor([0.03017732873558998, 0.24328561127185822, 0.11289440095424652, 0.10063818097114563, 0.3296104669570923, 0.19009780883789062, 0.07135572284460068, 0.3210153579711914, 0.22932195663452148, 0.06519468128681183, 0.181897833943367, 0.17943240702152252, 0.4626893401145935, 0.19258861243724823, 0.11716607958078384, 0.08913181722164154, 0.21765144169330597, 0.17260877788066864], dtype='float32').reshape([18]),
            paddle.to_tensor([0.19135457277297974, 0.1384550929069519, 0.024714868515729904, 0.28781190514564514, 0.2069232165813446, 0.3410429358482361, 0.09425343573093414, 0.10960867255926132, 0.21503448486328125, 0.18133492767810822, 0.399578332901001, 0.39705130457878113, 0.41751614212989807, 0.23023679852485657, 0.35064172744750977, 0.3029220402240753, 0.3697313666343689, 0.07753509283065796], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_53c03582fdbe946c81856c99c10397e5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c4980c5c596e34b1fc1c223d1c84e742(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 244, 244], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.062054697424173355, 0.09204066544771194, 0.2880668640136719, 0.10610457509756088, 0.06743636727333069, 0.10415779054164886, 0.22408895194530487, 0.21793533861637115, 0.4296610355377197, 0.2045304775238037, 0.4611082375049591, 0.47655418515205383, 0.04694230854511261, 0.19745394587516785, 0.06480975449085236, 0.009696396067738533], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3104470670223236, 0.26019081473350525, 0.29989513754844666, 0.16287550330162048, 0.4354768991470337, 0.4889301359653473, 0.3716649115085602, 0.32369133830070496, 0.09055269509553909, 0.4522773325443268, 0.26637178659439087, 0.006965285632759333, 0.1834956705570221, 0.0813397765159607, 0.2332780659198761, 0.0906975045800209], dtype='float32').reshape([16]),
            paddle.to_tensor([0.1764756441116333, 0.44123411178588867, 0.0015738941729068756, 0.48627492785453796, 0.09433116763830185, 0.28396207094192505, 0.19595442712306976, 0.2042422741651535, 0.43264323472976685, 0.23885385692119598, 0.4674360454082489, 0.021158671006560326, 0.016978727653622627, 0.16334973275661469, 0.11086190491914749, 0.13975591957569122], dtype='float32').reshape([16]),
            paddle.to_tensor([0.013842117972671986, 0.329415887594223, 0.1705685555934906, 0.10353952646255493, 0.03566073998808861, 0.45742806792259216, 0.311733216047287, 0.2681578993797302, 0.3180908262729645, 0.025672582909464836, 0.05261208862066269, 0.03496289253234863, 0.1644071340560913, 0.10698311030864716, 0.2706262469291687, 0.3026847541332245], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6fa22984d305dc31090619ba02a64a93(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d1a836f6b4f6bdd2f959c62ba0ca3fde(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_75da54914d772e3f7b62164c429b1883(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 2016, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([2016], dtype='float32', min=0, max=0.5),
            paddle.uniform([2016], dtype='float32', min=0, max=0.5),
            paddle.uniform([2016], dtype='float32', min=0, max=0.5),
            paddle.uniform([2016], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c44a22b74ee31fb7f1ce33abbdad5a27(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2428fe41e069348e823b33e06ac6ea70(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_420c34a900b204bc2025a0703f392c58(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e6184524e9dcb00cbd72629db575d52e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_78d05869059c0de0a3565dd91bb748aa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([8, 256, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_31d32d3b85bf9cb7867122f458154983(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 31, 31], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a4072b0552fbcd6e712cc5c5992de113(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 4, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9a1d1cb8bcbefc02a04c913a31b69141(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fd6a5fe6168a8069045ebea767579e4a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 480, 480], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.010450774803757668, 0.003220274345949292, 0.19658297300338745, 0.039867937564849854, 0.12764307856559753, 0.4073907434940338, 0.0006315415375865996, 0.25661686062812805], dtype='float32').reshape([8]),
            paddle.to_tensor([0.3413293659687042, 0.003997707739472389, 0.42056700587272644, 0.08052228391170502, 0.44809332489967346, 0.0749991238117218, 0.08526191860437393, 0.45610690116882324], dtype='float32').reshape([8]),
            paddle.to_tensor([0.41581249237060547, 0.4899568557739258, 0.3629833161830902, 0.34294310212135315, 0.3872171938419342, 0.08399098366498947, 0.17277181148529053, 0.10008802264928818], dtype='float32').reshape([8]),
            paddle.to_tensor([0.3331967294216156, 0.3470754623413086, 0.2925184667110443, 0.06584931164979935, 0.39469113945961, 0.4496955871582031, 0.4307473599910736, 0.28912875056266785], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_70ae2b697729967bdfca093f1f279c8e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.431867778301239, 0.40339919924736023, 0.18106995522975922, 0.4208586513996124, 0.36840224266052246, 0.48209261894226074, 0.32645857334136963, 0.19050580263137817, 0.4132843017578125, 0.2055962234735489, 0.3810732066631317, 0.11334135383367538, 0.3457266390323639, 0.39770686626434326, 0.021854691207408905, 0.19810576736927032, 0.08699127286672592, 0.4440856873989105, 0.4667195975780487, 0.40551358461380005, 0.0845843255519867, 0.3996383249759674, 0.10549387335777283, 0.39046239852905273], dtype='float32').reshape([24]),
            paddle.to_tensor([0.012772143818438053, 0.25447461009025574, 0.30321770906448364, 0.3570713996887207, 0.048542194068431854, 0.20198850333690643, 0.39612412452697754, 0.23123428225517273, 0.23164059221744537, 0.35323330760002136, 0.09088076651096344, 0.1650460660457611, 0.4426993429660797, 0.47719088196754456, 0.48215803503990173, 0.3050600588321686, 0.2826179265975952, 0.3345111608505249, 0.29416224360466003, 0.37920236587524414, 0.07230973988771439, 0.2545965611934662, 0.203896164894104, 0.03896499425172806], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3512837290763855, 0.08220825344324112, 0.07513769716024399, 0.1956709325313568, 0.27360695600509644, 0.03225712850689888, 0.31378108263015747, 0.028100259602069855, 0.20755444467067719, 0.15941709280014038, 0.3033170998096466, 0.22793643176555634, 0.1402292251586914, 0.3650277853012085, 0.4175083339214325, 0.38277506828308105, 0.489758163690567, 0.4068886935710907, 0.011529502458870411, 0.4346711039543152, 0.10555319488048553, 0.1585288643836975, 0.28676095604896545, 0.11944161355495453], dtype='float32').reshape([24]),
            paddle.to_tensor([0.41514289379119873, 0.14439837634563446, 0.41387617588043213, 0.0479186549782753, 0.3209390640258789, 0.211432084441185, 0.10820931196212769, 0.28039178252220154, 0.28536415100097656, 0.339759886264801, 0.12024447321891785, 0.08979424089193344, 0.015318048186600208, 0.32936328649520874, 0.44993162155151367, 0.3323860168457031, 0.34234049916267395, 0.19870217144489288, 0.2598876357078552, 0.4888439476490021, 0.1547376811504364, 0.030055265873670578, 0.25813671946525574, 0.11400303244590759], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f458041cc897cb31e866812589ef6e82(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 4, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b3cca576a17d82a8444b64b428f98715(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1280, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b296843461f4801635debe6dd56e3452(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 906, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([906], dtype='float32', min=0, max=0.5),
            paddle.uniform([906], dtype='float32', min=0, max=0.5),
            paddle.uniform([906], dtype='float32', min=0, max=0.5),
            paddle.uniform([906], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5907b03828c722d77bcaf3695ae404cf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_b4f4da3a8d44d1b5b3a2903a097df186
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dec101a99f4c8dced65bb7a3a28e3efa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_11dc0f839239edc26db3ec13c87c3cb1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_71298adb35dbb06a6d3ad8a560f5cedd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dc6e95c8341343313cd460ca93d3650a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 112, 64, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ea00d92eb13b92b6930b53cc3ccc47a0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fa66099d8184023be727a7708f014c3b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.43877652287483215, 0.16886746883392334, 0.0904618501663208, 0.1727827787399292, 0.49529126286506653, 0.3198532462120056, 0.07318098843097687, 0.2984827756881714, 0.07954106479883194, 0.009834883734583855, 0.13579802215099335, 0.46703892946243286, 0.055056508630514145, 0.24400970339775085, 0.2617006301879883, 0.17914187908172607, 0.34938034415245056, 0.46516114473342896, 0.1270623356103897, 0.19349020719528198, 0.37862369418144226, 0.25980809330940247, 0.3846559524536133, 0.029874756932258606, 0.2568959891796112, 0.17498138546943665, 0.3072289526462555, 0.26130878925323486, 0.2133997082710266, 0.13819479942321777], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4388198256492615, 0.4872233271598816, 0.15972377359867096, 0.1279563158750534, 0.1337932050228119, 0.4073774814605713, 0.00903838500380516, 0.04729333147406578, 0.22832638025283813, 0.37402281165122986, 0.04871971532702446, 0.22769924998283386, 0.020357690751552582, 0.3712783753871918, 0.4513412117958069, 0.33987322449684143, 0.10939330607652664, 0.1319677233695984, 0.29630956053733826, 0.028630157932639122, 0.4489993751049042, 0.010920938104391098, 0.37032097578048706, 0.16818824410438538, 0.32466921210289, 0.42238253355026245, 0.0009174954029731452, 0.4246695339679718, 0.31568560004234314, 0.4910368025302887], dtype='float32').reshape([30]),
            paddle.to_tensor([0.08732296526432037, 0.39782047271728516, 0.05168062448501587, 0.35182860493659973, 0.46901407837867737, 0.09169687330722809, 0.2547001540660858, 0.2664720118045807, 0.36604008078575134, 0.37510496377944946, 0.46969181299209595, 0.2767690122127533, 0.07227032631635666, 0.4089207053184509, 0.12495002895593643, 0.2010907530784607, 0.32280629873275757, 0.29058751463890076, 0.3024432361125946, 0.40341001749038696, 0.19267748296260834, 0.00453970767557621, 0.09399675577878952, 0.12458044290542603, 0.4874965250492096, 0.12611837685108185, 0.08932715654373169, 0.236582413315773, 0.2885896861553192, 0.46648508310317993], dtype='float32').reshape([30]),
            paddle.to_tensor([0.03891546279191971, 0.043086905032396317, 0.0602821484208107, 0.2546418011188507, 0.3723342716693878, 0.048659320920705795, 0.2421971559524536, 0.2045058310031891, 0.265590637922287, 0.4025583267211914, 0.19877837598323822, 0.26251471042633057, 0.4351423382759094, 0.3473031222820282, 0.15448851883411407, 0.026072699576616287, 0.04261082410812378, 0.1653841882944107, 0.043182622641325, 0.12107086926698685, 0.06118288263678551, 0.015985878184437752, 0.04154319688677788, 0.19779275357723236, 0.3930908739566803, 0.058252811431884766, 0.2072446495294571, 0.3638133406639099, 0.4857650399208069, 0.3419094681739807], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_036cc7e56a5cae37b6b9c43232c24c55(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.0408504493534565, 0.15962421894073486, 0.35814589262008667, 0.17997406423091888, 0.1964353770017624, 0.08725279569625854, 0.31447818875312805, 0.18745161592960358, 0.03487010300159454, 0.15033239126205444, 0.3669164180755615, 0.38407379388809204, 0.10264824330806732, 0.16989639401435852, 0.41067805886268616, 0.07797800004482269, 0.09634030610322952, 0.3128681182861328, 0.2780155539512634, 0.2603066563606262], dtype='float32').reshape([20]),
            paddle.to_tensor([0.2826961576938629, 0.31840071082115173, 0.17631377279758453, 0.08549290895462036, 0.263919860124588, 0.13216519355773926, 0.27001267671585083, 0.2798324227333069, 0.24724701046943665, 0.25261661410331726, 0.25927695631980896, 0.3313407301902771, 0.29365938901901245, 0.4017907381057739, 0.16880615055561066, 0.13931088149547577, 0.29371514916419983, 0.1707422286272049, 0.4971871078014374, 0.4096907079219818], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3839450180530548, 0.434489905834198, 0.44404029846191406, 0.27249711751937866, 0.46084603667259216, 0.05628461390733719, 0.3188941478729248, 0.3787013590335846, 0.10554289072751999, 0.4009974002838135, 0.06609378755092621, 0.09648362547159195, 0.27355754375457764, 0.22467473149299622, 0.03269805759191513, 0.42059773206710815, 0.1946949064731598, 0.05335776507854462, 0.3795408308506012, 0.2553105056285858], dtype='float32').reshape([20]),
            paddle.to_tensor([0.011971792206168175, 0.33201825618743896, 0.4320855140686035, 0.1152726262807846, 0.0008679583552293479, 0.3433787226676941, 0.17712613940238953, 0.3650672733783722, 0.024048414081335068, 0.470163494348526, 0.0290435291826725, 0.22818110883235931, 0.2810201644897461, 0.13597770035266876, 0.05873480439186096, 0.27742311358451843, 0.10735097527503967, 0.24224267899990082, 0.3085100054740906, 0.41138729453086853], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_565a3604b7f82964097be11ffb38d124(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0d1c2494883d6db2b9a1c77e21b68faa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.28592243790626526, 0.1649017333984375, 0.23348413407802582, 0.2805023491382599, 0.30906182527542114, 0.015399353578686714, 0.32885828614234924, 0.01532550249248743, 0.16094520688056946, 0.1981067955493927, 0.18740062415599823, 0.48462975025177, 0.10096590965986252, 0.31759098172187805, 0.2702125608921051, 0.0698760449886322, 0.4049953520298004, 0.3980761468410492], dtype='float32').reshape([18]),
            paddle.to_tensor([0.16246414184570312, 0.03505169227719307, 0.4005914330482483, 0.30657798051834106, 0.31232210993766785, 0.21026791632175446, 0.20216886699199677, 0.14002284407615662, 0.10112748295068741, 0.37118053436279297, 0.4062691330909729, 0.1487841010093689, 0.07118389755487442, 0.3833763003349304, 0.2850726544857025, 0.20993539690971375, 0.2949211597442627, 0.2309943288564682], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4388708472251892, 0.3602309823036194, 0.09303862601518631, 0.14236865937709808, 0.0807553231716156, 0.418141633272171, 0.38861310482025146, 0.22488030791282654, 0.3080224394798279, 0.43673276901245117, 0.21600079536437988, 0.0982319712638855, 0.4554620385169983, 0.44972604513168335, 0.3436598777770996, 0.0643261969089508, 0.33808633685112, 0.36006247997283936], dtype='float32').reshape([18]),
            paddle.to_tensor([0.49841535091400146, 0.24550308287143707, 0.12392014265060425, 0.4291035532951355, 0.4637508988380432, 0.13355642557144165, 0.47174298763275146, 0.49741652607917786, 0.4886012077331543, 0.20651262998580933, 0.44305485486984253, 0.2442433089017868, 0.10579396784305573, 0.41201022267341614, 0.4239732027053833, 0.3202022314071655, 0.27516090869903564, 0.17808666825294495], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_77e828d49e14ecc9c1b9df0cff1ff7bd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 30, 30], dtype='float16', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_691ab405a2b178f49d3ba797d79d53e4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 64, 128], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.20629936456680298, 0.017692923545837402, 0.4653850793838501, 0.41710662841796875, 0.1350001096725464, 0.3568778336048126, 0.02977043017745018, 0.4164797067642212, 0.3648427724838257, 0.327180415391922, 0.15311750769615173, 0.22881512343883514, 0.46086442470550537, 0.2206309735774994, 0.3915911614894867, 0.1652963012456894, 0.3798889219760895, 0.3273206055164337], dtype='float32').reshape([18]),
            paddle.to_tensor([0.07387271523475647, 0.11471422016620636, 0.4542422294616699, 0.21580995619297028, 0.3085404932498932, 0.2409411370754242, 0.4457555115222931, 0.357780396938324, 0.012949567288160324, 0.003924890421330929, 0.403362512588501, 0.42562583088874817, 0.1562110334634781, 0.1857200562953949, 0.39971399307250977, 0.3284018933773041, 0.4137105941772461, 0.15991409122943878], dtype='float32').reshape([18]),
            paddle.to_tensor([0.43140095472335815, 0.38697391748428345, 0.4435051381587982, 0.025296397507190704, 0.057611968368291855, 0.2937135398387909, 0.1552094966173172, 0.3529810309410095, 0.4639115035533905, 0.34212881326675415, 0.04942162707448006, 0.08448248356580734, 0.03515750542283058, 0.060600396245718, 0.07718445360660553, 0.0496479831635952, 0.3219528794288635, 0.36703312397003174], dtype='float32').reshape([18]),
            paddle.to_tensor([0.37419918179512024, 0.485976904630661, 0.13582001626491547, 0.2584637403488159, 0.44578251242637634, 0.352956622838974, 0.021835552528500557, 0.45503857731819153, 0.35489270091056824, 0.47641435265541077, 0.3867010474205017, 0.26752105355262756, 0.06445130705833435, 0.17283469438552856, 0.32727834582328796, 0.27638256549835205, 0.43311890959739685, 0.2967909276485443], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4b78c53abe29f15329dd30856e1c9e33(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 480, 480], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4375481605529785, 0.1417504996061325, 0.4371594190597534, 0.16224093735218048, 0.3295445740222931, 0.38742750883102417, 0.445823609828949, 0.28738459944725037], dtype='float32').reshape([8]),
            paddle.to_tensor([0.36526036262512207, 0.48696115612983704, 0.03957527130842209, 0.010267280973494053, 0.2747705280780792, 0.29662445187568665, 0.22253695130348206, 0.3732546865940094], dtype='float32').reshape([8]),
            paddle.to_tensor([0.2276366949081421, 0.47818922996520996, 0.43170762062072754, 0.22081157565116882, 0.42209240794181824, 0.44924196600914, 0.007951046340167522, 0.4896825850009918], dtype='float32').reshape([8]),
            paddle.to_tensor([0.2771124541759491, 0.08244840055704117, 0.15761302411556244, 0.3461557626724243, 0.11980313807725906, 0.4454009234905243, 0.10811582952737808, 0.1344381868839264], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_54d6614ae6f014a834e7e4f54305f068(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 320, 320], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4551810324192047, 0.09113523364067078, 0.42266175150871277, 0.17591947317123413, 0.32409918308258057, 0.09676307439804077, 0.11535600572824478, 0.19359241425991058], dtype='float32').reshape([8]),
            paddle.to_tensor([0.12223859876394272, 0.21348915994167328, 0.07508371025323868, 0.032550133764743805, 0.19267182052135468, 0.22140580415725708, 0.33191946148872375, 0.4537173807621002], dtype='float32').reshape([8]),
            paddle.to_tensor([0.4707787334918976, 0.4707505404949188, 0.29055339097976685, 0.409609317779541, 0.12113802134990692, 0.18986549973487854, 0.45245248079299927, 0.06660543382167816], dtype='float32').reshape([8]),
            paddle.to_tensor([0.39851370453834534, 0.035049568861722946, 0.19341827929019928, 0.08346635103225708, 0.005690871737897396, 0.34029409289360046, 0.30111315846443176, 0.31582972407341003], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9d768d3ce392fc4592db21b4383275c8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.17710454761981964, 0.021438119933009148, 0.21619829535484314, 0.18688836693763733, 0.18549451231956482, 0.1866222321987152, 0.48037081956863403, 0.23878724873065948, 0.07370614260435104, 0.2453971654176712, 0.06558803468942642, 0.48569634556770325, 0.1794315129518509, 0.2185589224100113, 0.2685951888561249, 0.045531004667282104, 0.23588243126869202, 0.019898854196071625, 0.36831024289131165, 0.2171739935874939, 0.04817957803606987, 0.21610654890537262, 0.02567608468234539, 0.08928962796926498, 0.45353853702545166, 0.25806719064712524, 0.3693859577178955, 0.07794642448425293, 0.01710672304034233, 0.005195898935198784], dtype='float32').reshape([30]),
            paddle.to_tensor([0.23848262429237366, 0.3617098331451416, 0.08773405849933624, 0.47086620330810547, 0.3361399471759796, 0.3734738528728485, 0.045636843889951706, 0.15544439852237701, 0.4595732092857361, 0.499347060918808, 0.13221891224384308, 0.28098589181900024, 0.1176053062081337, 0.4228074252605438, 0.30267319083213806, 0.05647801607847214, 0.49009862542152405, 0.13142305612564087, 0.4303187429904938, 0.26739567518234253, 0.437129408121109, 0.405538409948349, 0.44498568773269653, 0.4176657795906067, 0.20175111293792725, 0.2541486620903015, 0.35953155159950256, 0.015793003141880035, 0.05299515277147293, 0.05160422623157501], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3531034290790558, 0.14535605907440186, 0.4906904399394989, 0.30745023488998413, 0.18091048300266266, 0.4596438705921173, 0.0861838310956955, 0.4149380326271057, 0.3962022662162781, 0.05767675116658211, 0.10272105038166046, 0.21999678015708923, 0.28085577487945557, 0.3506166934967041, 0.28329429030418396, 0.2795585095882416, 0.2889803946018219, 0.49894317984580994, 0.44253474473953247, 0.2165289968252182, 0.2728930115699768, 0.34775158762931824, 0.2149636447429657, 0.10632938891649246, 0.32881954312324524, 0.053651340305805206, 0.30378004908561707, 0.39264267683029175, 0.2815040349960327, 0.1350776106119156], dtype='float32').reshape([30]),
            paddle.to_tensor([0.01061219535768032, 0.00034667906584218144, 0.24676047265529633, 0.056462984532117844, 0.25222277641296387, 0.28750067949295044, 0.03073204681277275, 0.4226183295249939, 0.28080639243125916, 0.4501965045928955, 0.1261269599199295, 0.218839630484581, 0.039679523557424545, 0.21849536895751953, 0.4738262891769409, 0.10549228638410568, 0.21839182078838348, 0.21595333516597748, 0.0640256404876709, 0.04040418192744255, 0.3749066889286041, 0.06049472093582153, 0.08140586316585541, 0.10236135870218277, 0.013107124716043472, 0.4678749442100525, 0.2682564854621887, 0.43582385778427124, 0.3223336935043335, 0.10192106664180756], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3b096f7743dfa54831b5c4d0127ec9f2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 16, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8b88e2a33baed31ae78a86b6716ba3ab(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.009729335084557533, 0.1783348172903061, 0.36160892248153687, 0.031264279037714005, 0.3890276551246643, 0.211801216006279, 0.09279539436101913, 0.06288595497608185, 0.1361798346042633, 0.14440783858299255, 0.3023133873939514, 0.016390200704336166, 0.35275885462760925, 0.2685743570327759, 0.17506690323352814, 0.016633329913020134, 0.3321085274219513, 0.20845864713191986, 0.31237491965293884, 0.23381157219409943, 0.12185899168252945, 0.035644471645355225, 0.44788089394569397, 0.3534754514694214], dtype='float32').reshape([24]),
            paddle.to_tensor([0.35810092091560364, 0.038287073373794556, 0.47120213508605957, 0.0847068652510643, 0.12603558599948883, 0.40999308228492737, 0.3427045941352844, 0.0747469961643219, 0.433247834444046, 0.17870359122753143, 0.11052025109529495, 0.2950829863548279, 0.033369723707437515, 0.25518521666526794, 0.4877708852291107, 0.1719963401556015, 0.09319702535867691, 0.4694790840148926, 0.23215605318546295, 0.38742122054100037, 0.02533867582678795, 0.4000507593154907, 0.3315126895904541, 0.22118695080280304], dtype='float32').reshape([24]),
            paddle.to_tensor([0.26236692070961, 0.2840336263179779, 0.006342867389321327, 0.27875474095344543, 0.24703748524188995, 0.2999517321586609, 0.2465846836566925, 0.044303420931100845, 0.18046580255031586, 0.05457741394639015, 0.35868045687675476, 0.27054640650749207, 0.24237902462482452, 0.3706640303134918, 0.27859237790107727, 0.23404978215694427, 0.37750890851020813, 0.16183467209339142, 0.2838789224624634, 0.24195751547813416, 0.16098551452159882, 0.16987229883670807, 0.268583208322525, 0.38496702909469604], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3735000193119049, 0.35498350858688354, 0.39097127318382263, 0.07216208428144455, 0.06607981771230698, 0.48589497804641724, 0.41740480065345764, 0.4111204147338867, 0.22958356142044067, 0.32448893785476685, 0.1389351338148117, 0.31939560174942017, 0.43959543108940125, 0.4337947964668274, 0.11819397658109665, 0.2583261728286743, 0.0321117639541626, 0.3100418746471405, 0.016191702336072922, 0.16430708765983582, 0.4922093451023102, 0.37069982290267944, 0.2678305208683014, 0.08556032925844193], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dcd4d369d4222203377cf2c9b6580ea7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 10, 10], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_032abec520a63f384b38f942aaac2398(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 16, 12], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d158c362eda895472cbb7071b8e730d4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0e3780199d6931d7b2877934a2cf5297(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 832, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([832], dtype='float32', min=0, max=0.5),
            paddle.uniform([832], dtype='float32', min=0, max=0.5),
            paddle.uniform([832], dtype='float32', min=0, max=0.5),
            paddle.uniform([832], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4342d2501d2cd0876870d79d2ab66e74(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5b997d12cc7e77b0652d1e13738496c3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3766333758831024, 0.24777165055274963, 0.3019944727420807, 0.010335056111216545, 0.0794665515422821, 0.16615194082260132, 0.021698247641324997, 0.10810460895299911, 0.3751205801963806, 0.31828463077545166, 0.0683181881904602, 0.4131630063056946, 0.1631356179714203, 0.013925572857260704, 0.4970289468765259, 0.24353094398975372, 0.2532738149166107, 0.009555921889841557, 0.04262399673461914, 0.10364874452352524, 0.07344972342252731, 0.4381772577762604, 0.4047088623046875, 0.4454900026321411], dtype='float32').reshape([24]),
            paddle.to_tensor([0.08282235264778137, 0.3458974063396454, 0.2354595959186554, 0.4408881366252899, 0.09723608940839767, 0.19136883318424225, 0.016562528908252716, 0.14390982687473297, 0.1771068572998047, 0.23072613775730133, 0.49413102865219116, 0.2504909634590149, 0.20323710143566132, 0.4091748893260956, 0.06847794353961945, 0.19594642519950867, 0.17665491998195648, 0.13318759202957153, 0.2632620632648468, 0.10018222033977509, 0.033238690346479416, 0.46977412700653076, 0.016113758087158203, 0.2281137853860855], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2540800869464874, 0.058459870517253876, 0.22848598659038544, 0.04272087663412094, 0.30452972650527954, 0.17283740639686584, 0.4570807218551636, 0.47518518567085266, 0.49214795231819153, 0.3032403588294983, 0.144976407289505, 0.24923402070999146, 0.006804468110203743, 0.15968938171863556, 0.3996600806713104, 0.2802720069885254, 0.2575173079967499, 0.020711196586489677, 0.05115557834506035, 0.3912901282310486, 0.07267948985099792, 0.2885401248931885, 0.24754689633846283, 0.29268530011177063], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4481979012489319, 0.07205232977867126, 0.3132251799106598, 0.48258453607559204, 0.32469642162323, 0.10505124181509018, 0.13775959610939026, 0.47378796339035034, 0.19301728904247284, 0.35130441188812256, 0.25573965907096863, 0.23305918276309967, 0.4752671420574188, 0.05215926468372345, 0.03706828132271767, 0.2608710527420044, 0.4142245650291443, 0.14650799334049225, 0.49200496077537537, 0.4789082109928131, 0.18364495038986206, 0.002624149201437831, 0.4299129247665405, 0.31023168563842773], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fa18b88c3e41b5a584191804386f7baf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4300483167171478, 0.2386118620634079, 0.364095002412796, 0.22138771414756775, 0.4019272029399872, 0.4898560643196106, 0.17395557463169098, 0.4793540835380554, 0.1497955173254013, 0.12727218866348267, 0.23346583545207977, 0.4950268268585205, 0.3106728792190552, 0.03202356770634651, 0.37578025460243225, 0.2789318561553955, 0.4710349440574646, 0.4658501446247101, 0.3442443311214447, 0.42263051867485046, 0.10730332136154175, 0.19509293138980865, 0.18118077516555786, 0.052168529480695724, 0.37073376774787903, 0.20256447792053223, 0.10577447712421417, 0.36305201053619385, 0.09315378218889236, 0.09988437592983246], dtype='float32').reshape([30]),
            paddle.to_tensor([0.24444058537483215, 0.012578687630593777, 0.31734228134155273, 0.09961573779582977, 0.3757748007774353, 0.08456873893737793, 0.40611109137535095, 0.06695076823234558, 0.17274846136569977, 0.31995242834091187, 0.23148982226848602, 0.4752095341682434, 0.11773711442947388, 0.1304907649755478, 0.43362224102020264, 0.21791408956050873, 0.3762969374656677, 0.006757499650120735, 0.33363988995552063, 0.4699890613555908, 0.484119713306427, 0.23018965125083923, 0.26999011635780334, 0.4891805648803711, 0.2697189450263977, 0.29414618015289307, 0.25906410813331604, 0.24855007231235504, 0.1987561583518982, 0.3716798722743988], dtype='float32').reshape([30]),
            paddle.to_tensor([0.01607401669025421, 0.11372210830450058, 0.2726866602897644, 0.26803621649742126, 0.28610286116600037, 0.04289877414703369, 0.32075223326683044, 0.20906966924667358, 0.305910587310791, 0.1769348829984665, 0.2950224280357361, 0.47500109672546387, 0.03830695152282715, 0.08780229836702347, 0.21733449399471283, 0.45416969060897827, 0.06970185041427612, 0.14415627717971802, 0.2141198366880417, 0.4771427810192108, 0.33407026529312134, 0.4949493408203125, 0.3825112581253052, 0.38744476437568665, 0.45567286014556885, 0.41461506485939026, 0.47711458802223206, 0.17068319022655487, 0.3491332232952118, 0.286220520734787], dtype='float32').reshape([30]),
            paddle.to_tensor([0.41099706292152405, 0.38546431064605713, 0.04248806834220886, 0.19679103791713715, 0.08482527732849121, 0.05253852531313896, 0.183616042137146, 0.39922261238098145, 0.23997919261455536, 0.08127619326114655, 0.19332735240459442, 0.41719359159469604, 0.10403699427843094, 0.11063244193792343, 0.3348810374736786, 0.2719435393810272, 0.48641237616539, 0.39419353008270264, 0.38291069865226746, 0.3530575633049011, 0.42241162061691284, 0.3812796473503113, 0.15020586550235748, 0.21400409936904907, 0.43740788102149963, 0.2549888491630554, 0.04609297215938568, 0.2403121143579483, 0.4971592128276825, 0.2912660837173462], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_789e33bc0c1aacba739b10dc985546cc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4058992266654968, 0.25276967883110046, 0.16691246628761292, 0.4177493751049042, 0.43073785305023193, 0.3156943917274475, 0.28814712166786194, 0.40460240840911865, 0.07233709841966629, 0.2431603968143463, 0.4617695212364197, 0.02755645103752613, 0.22377446293830872, 0.11933239549398422, 0.15908174216747284, 0.3842901289463043, 0.31659117341041565, 0.09253957122564316, 0.37425121665000916, 0.4935184419155121, 0.2811792492866516, 0.2631831467151642, 0.46986714005470276, 0.3116982579231262, 0.4529394507408142, 0.44440892338752747, 0.1808251589536667, 0.3177263140678406], dtype='float32').reshape([28]),
            paddle.to_tensor([0.08779456466436386, 0.4245498776435852, 0.2344997525215149, 0.33791235089302063, 0.04600416123867035, 0.1945406049489975, 0.062303923070430756, 0.2504977583885193, 0.4458494782447815, 0.057072438299655914, 0.4574729800224304, 0.30838924646377563, 0.07213342189788818, 0.20132474601268768, 0.22743342816829681, 0.4578610360622406, 0.25407642126083374, 0.1717541515827179, 0.4418030083179474, 0.04492733255028725, 0.1709233522415161, 0.26781588792800903, 0.48891398310661316, 0.3370373845100403, 0.4818156957626343, 0.3995887339115143, 0.1351502537727356, 0.3404076397418976], dtype='float32').reshape([28]),
            paddle.to_tensor([0.39703071117401123, 0.09910719841718674, 0.4188401997089386, 0.4464258849620819, 0.08566304296255112, 0.3747795820236206, 0.16577668488025665, 0.022503338754177094, 0.09430616348981857, 0.3491561710834503, 0.09500418603420258, 0.3432190418243408, 0.12979748845100403, 0.2196909785270691, 0.025992296636104584, 0.1170811802148819, 0.3917275369167328, 0.4051385521888733, 0.41443100571632385, 0.49290749430656433, 0.349321573972702, 0.39812907576560974, 0.3527868390083313, 0.48471251130104065, 0.22553850710391998, 0.4529002606868744, 0.35268306732177734, 0.15184235572814941], dtype='float32').reshape([28]),
            paddle.to_tensor([0.29298365116119385, 0.309574156999588, 0.015965517610311508, 0.37094464898109436, 0.3316134214401245, 0.3920632004737854, 0.026453085243701935, 0.39106205105781555, 0.44864535331726074, 0.2552104890346527, 0.0876469686627388, 0.12512870132923126, 0.4487670660018921, 0.20760172605514526, 0.31279274821281433, 0.4280533194541931, 0.12644946575164795, 0.01607833243906498, 0.4037202298641205, 0.39659783244132996, 0.3073403835296631, 0.2914513647556305, 0.2455868422985077, 0.12275823950767517, 0.30427291989326477, 0.1500036120414734, 0.3773117959499359, 0.4469732344150543], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0632cd3f9efb0880cf8bf3c800e29b12(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 2208, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([2208], dtype='float32', min=0, max=0.5),
            paddle.uniform([2208], dtype='float32', min=0, max=0.5),
            paddle.uniform([2208], dtype='float32', min=0, max=0.5),
            paddle.uniform([2208], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_609e2f21dc94b954f8fce3994ec6bcce(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2709459662437439, 0.011553794145584106, 0.1516907513141632, 0.2956019341945648, 0.3095751106739044, 0.01058407686650753, 0.47545498609542847, 0.017029820010066032, 0.024767015129327774, 0.07208438962697983, 0.14112763106822968, 0.13748925924301147], dtype='float32').reshape([12]),
            paddle.to_tensor([0.01878463104367256, 0.42795562744140625, 0.3135347366333008, 0.25514698028564453, 0.22612698376178741, 0.4284750819206238, 0.1455494910478592, 0.45887047052383423, 0.21788457036018372, 0.33510005474090576, 0.34242308139801025, 0.46883338689804077], dtype='float32').reshape([12]),
            paddle.to_tensor([0.11726633459329605, 0.31403759121894836, 0.3618794083595276, 0.03719373047351837, 0.4885713458061218, 0.18534936010837555, 0.45727813243865967, 0.33207303285598755, 0.14043168723583221, 0.031834010034799576, 0.03358519822359085, 0.09608326107263565], dtype='float32').reshape([12]),
            paddle.to_tensor([0.4165721833705902, 0.23318670690059662, 0.19853775203227997, 0.20214278995990753, 0.23943887650966644, 0.3878849744796753, 0.32010823488235474, 0.019229047000408173, 0.0966387391090393, 0.1810462325811386, 0.38386404514312744, 0.25676119327545166], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_73ab9604261c0a0ca87fddd644773ed9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3369610011577606, 0.1551283299922943, 0.3160500228404999, 0.2842651307582855, 0.3101363182067871, 0.4061834216117859, 0.05908627808094025, 0.12055306881666183, 0.2527883052825928, 0.256818026304245, 0.49889323115348816, 0.44483238458633423, 0.17392779886722565, 0.2613135278224945, 0.46897533535957336, 0.14204852283000946], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4991055428981781, 0.3495332598686218, 0.2311936914920807, 0.45823588967323303, 0.024976344779133797, 0.4412432610988617, 0.20720060169696808, 0.1450222283601761, 0.49716827273368835, 0.22119370102882385, 0.34028053283691406, 0.3285873532295227, 0.1388804316520691, 0.20147815346717834, 0.1548716127872467, 0.06956710666418076], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3331014811992645, 0.4177953600883484, 0.41280558705329895, 0.1509319543838501, 0.2975313663482666, 0.07728197425603867, 0.4542951285839081, 0.3198132812976837, 0.4154464900493622, 0.4265690743923187, 0.3736456036567688, 0.14311495423316956, 0.010117094032466412, 0.13574224710464478, 0.4393106997013092, 0.37404295802116394], dtype='float32').reshape([16]),
            paddle.to_tensor([0.15121352672576904, 0.24456264078617096, 0.31261351704597473, 0.2797739803791046, 0.3565290570259094, 0.4255712628364563, 0.3902391493320465, 0.04395477473735809, 0.22727814316749573, 0.22828459739685059, 0.1855020672082901, 0.12223128229379654, 0.15574422478675842, 0.1190171167254448, 0.2573956549167633, 0.4049729108810425], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e1b94dde5dca1c3112f9023eea859684(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.24544881284236908, 0.05735711380839348, 0.48951926827430725, 0.42282187938690186, 0.05695777386426926, 0.18520788848400116, 0.2520943880081177, 0.32014039158821106, 0.0005504264845512807, 0.4751436412334442, 0.16976475715637207, 0.22785496711730957, 0.38804957270622253, 0.30528634786605835, 0.3080410063266754, 0.458739310503006, 0.2624090909957886, 0.3562459647655487, 0.22146138548851013, 0.3085399568080902], dtype='float32').reshape([20]),
            paddle.to_tensor([0.28359729051589966, 0.43999242782592773, 0.01961156539618969, 0.3482072055339813, 0.3386693596839905, 0.49311038851737976, 0.41178733110427856, 0.3270173966884613, 0.2154098004102707, 0.10171607881784439, 0.07860511541366577, 0.30407533049583435, 0.006839710287749767, 0.1071988046169281, 0.013196260668337345, 0.07264961302280426, 0.09393256157636642, 0.12794385850429535, 0.2461816817522049, 0.018943222239613533], dtype='float32').reshape([20]),
            paddle.to_tensor([0.0335143581032753, 0.09698928892612457, 0.09006907045841217, 0.27414631843566895, 0.20797757804393768, 0.38411274552345276, 0.30824506282806396, 0.1894744634628296, 0.09895797818899155, 0.39571836590766907, 0.47320497035980225, 0.23987312614917755, 0.16457219421863556, 0.3737714886665344, 0.2676371932029724, 0.31118783354759216, 0.03861415386199951, 0.3208126127719879, 0.3659359812736511, 0.337970495223999], dtype='float32').reshape([20]),
            paddle.to_tensor([0.23368152976036072, 0.057602304965257645, 0.26893535256385803, 0.2519696056842804, 0.3703771233558655, 0.3813123404979706, 0.29855188727378845, 0.42179933190345764, 0.05148888751864433, 0.2393791377544403, 0.36487051844596863, 0.05298537015914917, 0.49374449253082275, 0.42235851287841797, 0.17631661891937256, 0.19755934178829193, 0.24351932108402252, 0.25352099537849426, 0.07139643281698227, 0.029227403923869133], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a1976c0bec8abf262286a06ba546e5c9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.26927217841148376, 0.047924842685461044, 0.14243853092193604, 0.034410540014505386, 0.4270583987236023, 0.2798895537853241, 0.0026870251167565584, 0.007237771525979042, 0.1665182113647461, 0.24386028945446014, 0.3277844190597534, 0.08912024646997452, 0.46058890223503113, 0.36065009236335754, 0.14820775389671326, 0.3286377191543579, 0.17755447328090668, 0.49588173627853394, 0.021608863025903702, 0.447856068611145, 0.37114250659942627, 0.12370846420526505, 0.17821182310581207, 0.22456282377243042, 0.37987497448921204, 0.04261070862412453, 0.38485100865364075, 0.40306925773620605, 0.3408046066761017, 0.2334040254354477], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3095861077308655, 0.10650526732206345, 0.3730640411376953, 0.4241630434989929, 0.43100932240486145, 0.07857993990182877, 0.3833833336830139, 0.26579925417900085, 0.19737550616264343, 0.04450152814388275, 0.14146505296230316, 0.44809046387672424, 0.4275045096874237, 0.28465259075164795, 0.07819216698408127, 0.4597005248069763, 0.24774639308452606, 0.14510004222393036, 0.32970309257507324, 0.14682161808013916, 0.20413917303085327, 0.3865138292312622, 0.24149078130722046, 0.3131915330886841, 0.13450491428375244, 0.17095421254634857, 0.3475712835788727, 0.40081867575645447, 0.2433934360742569, 0.26615583896636963], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2823505997657776, 0.24835118651390076, 0.27105847001075745, 0.49402791261672974, 0.2815761864185333, 0.07188431173563004, 0.32823604345321655, 0.4957878887653351, 0.09955578297376633, 0.06266699731349945, 0.28307950496673584, 0.06614521145820618, 0.0576031468808651, 0.3555469512939453, 0.4431776702404022, 0.3938022553920746, 0.35204052925109863, 0.25793251395225525, 0.11429879814386368, 0.47304603457450867, 0.2912016808986664, 0.4400899112224579, 0.3920373022556305, 0.10893996804952621, 0.11382186412811279, 0.36025798320770264, 0.12780626118183136, 0.4443889260292053, 0.18845234811306, 0.10555370151996613], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4655185639858246, 0.49787795543670654, 0.10794653743505478, 0.06964161247015, 0.001297323266044259, 0.33526548743247986, 0.4749613106250763, 0.07313866913318634, 0.4215172529220581, 0.08260694891214371, 0.16232247650623322, 0.46633297204971313, 0.01847868226468563, 0.00849435105919838, 0.35775214433670044, 0.4112084209918976, 0.4524327218532562, 0.0872875303030014, 0.11836889386177063, 0.18465721607208252, 0.04185955971479416, 0.29004937410354614, 0.0008746558451093733, 0.17051462829113007, 0.3106081187725067, 0.13547714054584503, 0.07126376777887344, 0.201449453830719, 0.029641151428222656, 0.08813126385211945], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b99bb7da10ca78bdf23edbd28d11a066(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4084513187408447, 0.1105782613158226, 0.04656362161040306, 0.49858489632606506, 0.1901564747095108, 0.48837119340896606, 0.11678789556026459, 0.21821905672550201, 0.05135798081755638, 0.44399771094322205, 0.4269811809062958, 0.04969477653503418, 0.30848684906959534, 0.125192791223526, 0.4951242506504059, 0.027139738202095032, 0.21970456838607788, 0.014152662828564644], dtype='float32').reshape([18]),
            paddle.to_tensor([0.10244498401880264, 0.301789790391922, 0.36982637643814087, 0.08263686299324036, 0.131082683801651, 0.07845845818519592, 0.44055554270744324, 0.4492746889591217, 0.09481348842382431, 0.39924895763397217, 0.412203311920166, 0.16766870021820068, 0.37348952889442444, 0.3732612729072571, 0.22515857219696045, 0.3939156234264374, 0.4913378357887268, 0.4582289755344391], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3385915458202362, 0.296019583940506, 0.17261134088039398, 0.4650479853153229, 0.17814427614212036, 0.10221412032842636, 0.383400559425354, 0.013661609031260014, 0.31930410861968994, 0.4335945248603821, 0.15040607750415802, 0.46394193172454834, 0.03662965074181557, 0.16882744431495667, 0.3397838771343231, 0.3343592882156372, 0.29608625173568726, 0.38931378722190857], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4009034037590027, 0.38162368535995483, 0.3046085238456726, 0.18408067524433136, 0.21005520224571228, 0.27185148000717163, 0.024500669911503792, 0.23790490627288818, 0.46033790707588196, 0.23858805000782013, 0.23678496479988098, 0.10925537347793579, 0.4363497793674469, 0.3079456686973572, 0.46008792519569397, 0.3856726586818695, 0.23414282500743866, 0.4435308873653412], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bd6e0669c42e2722bb6a19dc9b64ecb4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([8, 128, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_91a9e2d82dac75a22a91fd1a7718312b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3c40ca3978927e1afd76a391eb588eeb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.010657240636646748, 0.43055838346481323, 0.4344034194946289, 0.2767208516597748, 0.36060112714767456, 0.044281795620918274, 0.09490490704774857, 0.16523563861846924, 0.4316573739051819, 0.2952840328216553, 0.0028529490809887648, 0.08115315437316895, 0.40931954979896545, 0.29802772402763367, 0.4912685453891754, 0.4892636239528656, 0.4647410809993744, 0.46628516912460327, 0.2773807644844055, 0.21803787350654602, 0.44217708706855774, 0.44494888186454773, 0.2858993411064148, 0.06773955374956131, 0.12991787493228912, 0.14126724004745483, 0.10087123513221741, 0.03398669883608818], dtype='float32').reshape([28]),
            paddle.to_tensor([0.36659327149391174, 0.3840346932411194, 0.10715930908918381, 0.3162813186645508, 0.14466705918312073, 0.20215339958667755, 0.2798398733139038, 0.04815445840358734, 0.20506539940834045, 0.4656132757663727, 0.3971865475177765, 0.25503745675086975, 0.27348464727401733, 0.14145809412002563, 0.4549615979194641, 0.44862592220306396, 0.13498607277870178, 0.31256306171417236, 0.3911004364490509, 0.03988010436296463, 0.47478175163269043, 0.19847312569618225, 1.4240504242479801e-06, 0.41346079111099243, 0.2746325433254242, 0.29094260931015015, 0.48324424028396606, 0.1594763845205307], dtype='float32').reshape([28]),
            paddle.to_tensor([0.14045776426792145, 0.20405149459838867, 0.23220360279083252, 0.354110985994339, 0.23431389033794403, 0.14595885574817657, 0.054320186376571655, 0.27479472756385803, 0.3436914086341858, 0.10007335245609283, 0.057301074266433716, 0.18349537253379822, 0.19151854515075684, 0.11211273074150085, 0.12637576460838318, 0.46846356987953186, 0.03523518145084381, 0.45790258049964905, 0.34957244992256165, 0.3499075770378113, 0.009467042982578278, 0.05177461728453636, 0.36695387959480286, 0.2666556239128113, 0.3524658679962158, 0.19390279054641724, 0.4604523777961731, 0.24670520424842834], dtype='float32').reshape([28]),
            paddle.to_tensor([0.37370601296424866, 0.48225468397140503, 0.31683099269866943, 0.232291579246521, 0.004846701864153147, 0.26963284611701965, 0.3794442415237427, 0.2804884612560272, 0.4992981553077698, 0.24510109424591064, 0.4704667031764984, 0.4346747398376465, 0.3096827566623688, 0.24977988004684448, 0.44939011335372925, 0.43114158511161804, 0.3021182715892792, 0.1591605395078659, 0.242176815867424, 0.07215819507837296, 0.19586890935897827, 0.4150073230266571, 0.09367918968200684, 0.40300485491752625, 0.4522252678871155, 0.49746495485305786, 0.2515815496444702, 0.13820049166679382], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1ca4bd1a15edcc549c7130b8003c40b8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 6, 6], dtype='float16', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_071fddb81e10a655f9d26c8d920bdc17(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([8, 512, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_218d9f0dcbfe880db124463f56d7db14(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_fc6b6917e04b7b7cf9fdb39dde97ec40
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_12259f3c70e2f420dc382250e53c38a4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 35, 35], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ea9f98a236307734504362036f512508(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1200, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_11bea1026744e3807e4c3ecdfbc2d62e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 256, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f25f9caa9e9434e0f63fdd006f77a9d6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_51fbf3b8996ef65f43a437fd2c6b0d44(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1952, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1952], dtype='float32', min=0, max=0.5),
            paddle.uniform([1952], dtype='float32', min=0, max=0.5),
            paddle.uniform([1952], dtype='float32', min=0, max=0.5),
            paddle.uniform([1952], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_85086989d63185813228fb58ce28c230(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1152, 6, 6], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bf6bb842a03f16570ef598b7fc789a32(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 1, 1], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_cdc3b0a148b551f86897e4668e0bdb6b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_272123507c34111166d6f9d764fcb4b8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cdc3b0a148b551f86897e4668e0bdb6b
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 4], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9f48d70023c69288d00c6f0215424282(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 768, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fe53feab6295160b163666f79f5bcc04(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 88, 61, 61], dtype='float16', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_202f85c3df9444b589eb6245a79d95d0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 350, 25], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_89eade693c25932d04c67f3c7ba6845d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 50, 84], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1ac35ae2ef76f20e202d7e448de22c22(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 64, 128], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.10093249380588531, 0.2547796070575714, 0.3248707056045532, 0.1528095155954361, 0.1131463497877121, 0.29941362142562866, 0.3129069209098816, 0.1993662714958191, 0.28440210223197937, 0.1262945830821991, 0.40202459692955017, 0.49441900849342346, 0.2318931370973587, 0.07164351642131805, 0.4956905245780945, 0.21368208527565002, 0.16254998743534088, 0.2998797595500946], dtype='float32').reshape([18]),
            paddle.to_tensor([0.11722858250141144, 0.4403810501098633, 0.18393807113170624, 0.17502163350582123, 0.46649593114852905, 0.22186921536922455, 0.39229682087898254, 0.02325550839304924, 0.31555813550949097, 0.4836064875125885, 0.023328963667154312, 0.006427638232707977, 0.3676964342594147, 0.22160644829273224, 0.1548251062631607, 0.47213831543922424, 0.16715086996555328, 0.054881781339645386], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4347486197948456, 0.35837221145629883, 0.36947885155677795, 0.13358452916145325, 0.28543832898139954, 0.44361114501953125, 0.2966299057006836, 0.07583320885896683, 0.387795627117157, 0.3443896174430847, 0.38760146498680115, 0.25113946199417114, 0.17837147414684296, 0.3764147162437439, 0.324546754360199, 0.06004151329398155, 0.41399839520454407, 0.2184351235628128], dtype='float32').reshape([18]),
            paddle.to_tensor([0.017930371686816216, 0.021370798349380493, 0.21289867162704468, 0.07434006035327911, 0.3785417675971985, 0.3206343650817871, 0.09554853290319443, 0.32992440462112427, 0.28807300329208374, 0.345307856798172, 0.2016633301973343, 0.44204312562942505, 0.2279307246208191, 0.30670255422592163, 0.2926877737045288, 0.3782842457294464, 0.3062813878059387, 0.36251506209373474], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f50cc3fd399db6c10ce83b87a8e97109(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 200, 64, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9d3d9307638e54f3a3be3459544e804d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 4, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e188bcc482351a5581983ba472c5bce5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 31, 31], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4fdb3881e7b2d726103f2a845a036f4d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 32, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.47687262296676636, 0.3591068685054779, 0.41629400849342346, 0.45932990312576294, 0.48299169540405273, 0.029028285294771194, 0.021794447675347328, 0.49192872643470764, 0.0634104460477829, 0.37643513083457947, 0.27188268303871155, 0.09249630570411682, 0.4061472713947296, 0.013153034262359142, 0.06055857986211777, 0.21056410670280457], dtype='float32').reshape([16]),
            paddle.to_tensor([0.06914195418357849, 0.4984966218471527, 0.1316920816898346, 0.21236121654510498, 0.432183176279068, 0.07715525478124619, 0.24984847009181976, 0.27959007024765015, 0.4967336356639862, 0.036886055022478104, 0.026502855122089386, 0.399088978767395, 0.3764457106590271, 0.11288939416408539, 0.2425636351108551, 0.07787448167800903], dtype='float32').reshape([16]),
            paddle.to_tensor([0.20027630031108856, 0.17226551473140717, 0.18726037442684174, 0.03068068064749241, 0.1827358603477478, 0.19151200354099274, 0.491802841424942, 0.09280750900506973, 0.21692752838134766, 0.0016077267937362194, 0.4145355820655823, 0.02240373194217682, 0.3905239701271057, 0.3309049904346466, 0.48480188846588135, 0.4547642171382904], dtype='float32').reshape([16]),
            paddle.to_tensor([0.18466386198997498, 0.2698788642883301, 0.07746028900146484, 0.2822376787662506, 0.48448967933654785, 0.14320673048496246, 0.2188715636730194, 0.48841702938079834, 0.20501305162906647, 0.3761899769306183, 0.4314180910587311, 0.33990368247032166, 0.3607354760169983, 0.26871779561042786, 0.013970382511615753, 0.06753849983215332], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3106022402bdaee543e2f23d4d55902a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 480, 480], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.48703911900520325, 0.20758689939975739, 0.10175846517086029, 0.01644456572830677, 0.1810215562582016, 0.07110852003097534, 0.40013110637664795, 0.3575015664100647, 0.14619269967079163, 0.3986910283565521, 0.3112196624279022, 0.24698249995708466, 0.31701767444610596, 0.11650361865758896, 0.45625123381614685, 0.20207954943180084, 0.2732625901699066, 0.41235899925231934, 0.011597178876399994, 0.2838418781757355, 0.1532578468322754, 0.33050939440727234, 0.21758447587490082, 0.14525064826011658], dtype='float32').reshape([24]),
            paddle.to_tensor([0.08809610456228256, 0.10736501961946487, 0.38333162665367126, 0.411704957485199, 0.38768213987350464, 0.008013738319277763, 0.05729631707072258, 0.32797566056251526, 0.3601425886154175, 0.38727083802223206, 0.39261889457702637, 0.37058424949645996, 0.06444479525089264, 0.11432954668998718, 0.16869540512561798, 0.29835882782936096, 0.3399697542190552, 0.2815256416797638, 0.26559174060821533, 0.1669035702943802, 0.4575903117656708, 0.4143071174621582, 0.03426036238670349, 0.09121342748403549], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4129653871059418, 0.07351359724998474, 0.3668099045753479, 0.39184510707855225, 0.37821775674819946, 0.11000727862119675, 0.342107892036438, 0.24341340363025665, 0.2810216248035431, 0.06208927556872368, 0.2196083962917328, 0.4118652045726776, 0.37525674700737, 0.11578649282455444, 0.4535713493824005, 0.33129167556762695, 0.2683340609073639, 0.11677169054746628, 0.06672518700361252, 0.09176138043403625, 0.35260796546936035, 0.4307761490345001, 0.011387620121240616, 0.4942355155944824], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4602997303009033, 0.488683819770813, 0.3068878650665283, 0.26195526123046875, 0.13402792811393738, 0.05449149012565613, 0.467213898897171, 0.11208345741033554, 0.4397180676460266, 0.05669854208827019, 0.30772337317466736, 0.4213583171367645, 0.43838319182395935, 0.3816811144351959, 0.12281624972820282, 0.35201454162597656, 0.16505232453346252, 0.02897224947810173, 0.06786088645458221, 0.007296617142856121, 0.11252917349338531, 0.26027801632881165, 0.42912304401397705, 0.18423007428646088], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_000345d3fef3fda8308d6553c21137f6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ccf9425df6e4f694d7f66dcd745f6183(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 240, 240], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.42569518089294434, 0.29051896929740906, 0.19253365695476532, 0.30391889810562134, 0.34939756989479065, 0.3402082026004791, 0.22594895958900452, 0.11089698225259781, 0.12664827704429626, 0.22824016213417053, 0.43348070979118347, 0.12059126794338226, 0.1756635457277298, 0.12620647251605988, 0.46354439854621887, 0.2557487189769745], dtype='float32').reshape([16]),
            paddle.to_tensor([0.03372349217534065, 0.17385028302669525, 0.10315932333469391, 0.40441274642944336, 0.11197991669178009, 0.48999688029289246, 0.15710584819316864, 0.19525837898254395, 0.036811355501413345, 0.32099485397338867, 0.2615680992603302, 0.19893409311771393, 0.49549704790115356, 0.24514393508434296, 0.26651617884635925, 0.4251062870025635], dtype='float32').reshape([16]),
            paddle.to_tensor([0.40285512804985046, 0.003608076134696603, 0.4269254207611084, 0.32693082094192505, 0.07535595446825027, 0.12731195986270905, 0.2587261199951172, 0.18495753407478333, 0.39493119716644287, 0.31630244851112366, 0.24796055257320404, 0.13727620244026184, 0.18895605206489563, 0.3628722131252289, 0.3588707745075226, 0.33891522884368896], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3690759837627411, 0.293570339679718, 0.14704546332359314, 0.2685762643814087, 0.07311827689409256, 0.35295939445495605, 0.07630050927400589, 0.28057315945625305, 0.13861322402954102, 0.228464275598526, 0.37102892994880676, 0.4423789978027344, 0.4206673204898834, 0.14261725544929504, 0.19662298262119293, 0.2244943231344223], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_27698e8eadf8c5c1306dcd38bdc10b38(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 320, 320], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.022958144545555115, 0.12790267169475555, 0.1072227954864502, 0.4966221749782562, 0.4336743652820587, 0.34130218625068665, 0.1466396152973175, 0.2362191379070282], dtype='float32').reshape([8]),
            paddle.to_tensor([0.20460371673107147, 0.2907209098339081, 0.28121548891067505, 0.1289921998977661, 0.06959328800439835, 0.336306095123291, 0.32503896951675415, 0.3588123917579651], dtype='float32').reshape([8]),
            paddle.to_tensor([0.08706928789615631, 0.3472652733325958, 0.4124930500984192, 0.3512091040611267, 0.015256250277161598, 0.3388502299785614, 0.40081265568733215, 0.06988159567117691], dtype='float32').reshape([8]),
            paddle.to_tensor([0.23846124112606049, 0.4151635766029358, 0.4647173285484314, 0.45421141386032104, 0.07949741184711456, 0.23435907065868378, 0.43207451701164246, 0.4738318622112274], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_277124e2c59d7fa72d1379c1ea24365a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 15, 15], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_696ac6583a0c789762ac239e1e0c7431(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 76, 76], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b72e2d0206a37c493150ccac781d4706(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.11481925845146179, 0.017511997371912003, 0.24826407432556152, 0.3094697594642639, 0.05489460006356239, 0.36561810970306396, 0.07526752352714539, 0.1742127239704132, 0.4821827709674835, 0.39730754494667053, 0.21037063002586365, 0.3615647852420807, 0.480351060628891, 0.18514883518218994, 0.37009358406066895, 0.2859303951263428], dtype='float32').reshape([16]),
            paddle.to_tensor([0.14862549304962158, 0.08880297094583511, 0.362562894821167, 0.21156471967697144, 0.4340841472148895, 0.06166632100939751, 0.08346552401781082, 0.46189290285110474, 0.33197253942489624, 0.38537341356277466, 0.2387136071920395, 0.11352023482322693, 0.42541295289993286, 0.3773079514503479, 0.4604070782661438, 0.43284451961517334], dtype='float32').reshape([16]),
            paddle.to_tensor([0.21876302361488342, 0.4799439609050751, 0.20000940561294556, 0.18784581124782562, 0.3348020017147064, 0.2551001012325287, 0.15246842801570892, 0.057625144720077515, 0.09712084382772446, 0.1920451521873474, 0.2182978093624115, 0.48092493414878845, 0.478337824344635, 0.21845410764217377, 0.2650023400783539, 0.43931037187576294], dtype='float32').reshape([16]),
            paddle.to_tensor([0.21983987092971802, 0.3578875660896301, 0.21016718447208405, 0.38216808438301086, 0.42940032482147217, 0.33126646280288696, 0.4522480070590973, 0.1358444094657898, 0.24875392019748688, 0.42485448718070984, 0.027598395943641663, 0.391674667596817, 0.24355100095272064, 0.00376893300563097, 0.48729127645492554, 0.0045894477516412735], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fc0aa0142911eaf92826031fc61b8c8e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 300, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([300], dtype='float32', min=0, max=0.5),
            paddle.uniform([300], dtype='float32', min=0, max=0.5),
            paddle.uniform([300], dtype='float32', min=0, max=0.5),
            paddle.uniform([300], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f0ae0f87f76b233219df6346b38c9132(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1248, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1248], dtype='float32', min=0, max=0.5),
            paddle.uniform([1248], dtype='float32', min=0, max=0.5),
            paddle.uniform([1248], dtype='float32', min=0, max=0.5),
            paddle.uniform([1248], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d9c34e6cdfef867934789908592b675a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 2, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9fd003444bad580c146954150c333c1d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_973010d737b2fea524b0d0b791e42cfd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1152, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e1b75669a79ba755159a560e8fa1e491(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 61, 61], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ed232fb3e433d33acdbecc73f5a92193(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 4, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b994ae6234c632e4a2a2b1d534a64f4f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 32, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a379564024e08d8601b8326c4a24a84e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 48, 48], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f2292afb0c54437f367224fa6ac5b5af(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_45552f33ce2418d32d098e27a2fc4a21(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5f5281b792ccc920bc3c90cf7d0ea893(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3a64c8fc4a714c7fd12844580968c099(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 120, 120], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.36939629912376404, 0.1253475844860077, 0.4673890173435211, 0.4404527544975281, 0.47400885820388794, 0.2820507884025574, 0.08345595002174377, 0.3362365663051605, 0.3948863446712494, 0.2274894118309021, 0.42152684926986694, 0.10664655268192291, 0.40692728757858276, 0.18688726425170898, 0.015742652118206024, 0.31257641315460205, 0.2057955265045166, 0.3414371609687805, 0.08132341504096985, 0.020969217643141747, 0.05704272910952568, 0.39715874195098877, 0.21390143036842346, 0.09442398697137833], dtype='float32').reshape([24]),
            paddle.to_tensor([0.15957756340503693, 0.39464449882507324, 0.3830263018608093, 0.3211023807525635, 0.09071412682533264, 0.3853786885738373, 0.1469154804944992, 0.15001393854618073, 0.07999560981988907, 0.18368574976921082, 0.28121721744537354, 0.3009974956512451, 0.36422619223594666, 0.28984975814819336, 0.27710607647895813, 0.4678992033004761, 0.039773136377334595, 0.13640174269676208, 0.2893019914627075, 0.46347421407699585, 0.22706156969070435, 0.029911240562796593, 0.1577170193195343, 0.12772396206855774], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4310465157032013, 0.1374901831150055, 0.2571975588798523, 0.4819916784763336, 0.14846988022327423, 0.1452610045671463, 0.2640911638736725, 0.34052711725234985, 0.20043742656707764, 0.22355887293815613, 0.0896526426076889, 0.222377210855484, 0.1642514318227768, 0.07985148578882217, 0.00468401238322258, 0.29779765009880066, 0.06324833631515503, 0.45583033561706543, 0.4925582706928253, 0.2230806201696396, 0.058910880237817764, 0.011528895236551762, 0.46115463972091675, 0.016298633068799973], dtype='float32').reshape([24]),
            paddle.to_tensor([0.39041486382484436, 0.21615101397037506, 0.19882208108901978, 0.43154099583625793, 0.24627093970775604, 0.29374217987060547, 0.1829380840063095, 0.2984994351863861, 0.4433540999889374, 0.18164925277233124, 0.32922932505607605, 0.15035954117774963, 0.3715493679046631, 0.1810879111289978, 0.017850272357463837, 0.23171183466911316, 0.18103466928005219, 0.051499247550964355, 0.1876153200864792, 0.35147950053215027, 0.42001354694366455, 0.1656692624092102, 0.2426266074180603, 0.30044564604759216], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_802888ebeb83c657512a18e38bcc362a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 38, 38], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6be3a5d5032d77d89ea93f6cdfc9a5fa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 50, 84], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3e1ba267c5f7db2b900b7f0f775c085f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.002798224799335003, 0.4735492169857025, 0.13946007192134857, 0.22037050127983093, 0.19093886017799377, 0.37837275862693787, 0.21425479650497437, 0.16001836955547333, 0.18436747789382935, 0.20456001162528992, 0.1727142035961151, 0.12179957330226898, 0.2613188922405243, 0.0841994658112526, 0.13298079371452332, 0.26596319675445557, 0.2958836853504181, 0.09203191101551056, 0.0538208931684494, 0.38898786902427673, 0.10600803047418594, 0.0402442142367363, 0.0710134357213974, 0.2357977330684662], dtype='float32').reshape([24]),
            paddle.to_tensor([0.19549314677715302, 0.2380969226360321, 0.4404134750366211, 0.07258713245391846, 0.4541061222553253, 0.08865159749984741, 0.4946686029434204, 0.1439875066280365, 0.12965849041938782, 0.23442858457565308, 0.3150291442871094, 0.0033710822463035583, 0.14754828810691833, 0.12443134933710098, 0.4523462951183319, 0.41115739941596985, 0.38001391291618347, 0.38246244192123413, 0.07238908857107162, 0.327120304107666, 0.07020319253206253, 0.38248881697654724, 0.3799394369125366, 0.089304618537426], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2528645098209381, 0.0448770634829998, 0.4162311851978302, 0.22754937410354614, 0.3286378085613251, 0.31972500681877136, 0.37555041909217834, 0.20872721076011658, 0.2731587886810303, 0.21500302851200104, 0.4468117654323578, 0.33537304401397705, 0.25174298882484436, 0.36918991804122925, 0.19400110840797424, 0.1913924515247345, 0.19034139811992645, 0.4476431906223297, 0.4283616244792938, 0.12887011468410492, 0.43151694536209106, 0.42429664731025696, 0.262361079454422, 0.2367260456085205], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4712621867656708, 0.3221092224121094, 0.029921630397439003, 0.16822566092014313, 0.3314383625984192, 0.21490758657455444, 0.4413946270942688, 0.0533914752304554, 0.40307188034057617, 0.4847303330898285, 0.4558282494544983, 0.3975268304347992, 0.2841505706310272, 0.3413681089878082, 0.2603923976421356, 0.42670631408691406, 0.3676007091999054, 0.3527841567993164, 0.3974780738353729, 0.03207377716898918, 0.19116412103176117, 0.3408888280391693, 0.15766151249408722, 0.20920433104038239], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_688d1223a457607d1f272357c350ca62(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cdc3b0a148b551f86897e4668e0bdb6b
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9fd732d4cc77c74518da9da772679d35(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.16946598887443542, 0.4657563269138336, 0.4732358157634735, 0.2696385383605957, 0.05638394132256508, 0.35813578963279724, 0.08326549082994461, 0.3194078803062439, 0.029985414817929268, 0.25577977299690247, 0.10304775834083557, 0.10360800474882126, 0.3841719925403595, 0.19606776535511017, 0.4864826798439026, 0.19240406155586243, 0.1290353536605835, 0.09690040349960327, 0.11560995131731033, 0.2716086506843567], dtype='float32').reshape([20]),
            paddle.to_tensor([0.024773074313998222, 0.4939720332622528, 0.3788311779499054, 0.20484988391399384, 0.3184986710548401, 0.31499823927879333, 0.47872471809387207, 0.19357004761695862, 0.1740712821483612, 0.23734188079833984, 0.43445390462875366, 0.2858830392360687, 0.032628338783979416, 0.059548862278461456, 0.41698217391967773, 0.31396278738975525, 0.1718088686466217, 0.23139910399913788, 0.33681586384773254, 0.12577886879444122], dtype='float32').reshape([20]),
            paddle.to_tensor([0.34901192784309387, 0.28901585936546326, 0.14536067843437195, 0.3275311291217804, 0.44032546877861023, 0.1632431000471115, 0.02343791536986828, 0.2673388123512268, 0.4588717818260193, 0.028965236619114876, 0.3407517671585083, 0.11374904215335846, 0.33376508951187134, 0.3440871834754944, 0.15485937893390656, 0.2926453649997711, 0.07863077521324158, 0.006896648555994034, 0.06336667388677597, 0.08058743178844452], dtype='float32').reshape([20]),
            paddle.to_tensor([0.13898195326328278, 0.04310377687215805, 0.35832008719444275, 0.14868134260177612, 0.2727292478084564, 0.04094178229570389, 0.2179681807756424, 0.06630359590053558, 0.1966373473405838, 0.4196740686893463, 0.09987831115722656, 0.4825532138347626, 0.16699974238872528, 0.04644850268959999, 0.42879921197891235, 0.09476365894079208, 0.41575318574905396, 0.4683551490306854, 0.46152088046073914, 0.45488762855529785], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f2f4ebae1e4886e48c34ba5c8a99b823(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bfdbc31c54556d244ebaa48ff6258665(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 2, 4], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_44f098eda57946d262b4a5cfb3757d4b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 972, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([972], dtype='float32', min=0, max=0.5),
            paddle.uniform([972], dtype='float32', min=0, max=0.5),
            paddle.uniform([972], dtype='float32', min=0, max=0.5),
            paddle.uniform([972], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d119b6afa91ce6c31ac0b78d8b3ca506(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 120, 120], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5af480c9fc6bdf26ce9c7e4bbca9cb72(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([8, 64, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_41e182f653c1286e965f3f9b16334ca3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.17156283557415009, 0.17428405582904816, 0.26282837986946106, 0.0018975224811583757, 0.40650269389152527, 0.05534980818629265, 0.361116886138916, 0.1592046469449997, 0.3104338049888611, 0.13532006740570068, 0.4331675171852112, 0.4188482165336609, 0.17449550330638885, 0.04233093932271004, 0.024517251178622246, 0.10185945779085159, 0.10538296401500702, 0.05950411409139633], dtype='float32').reshape([18]),
            paddle.to_tensor([0.40974751114845276, 0.1878674328327179, 0.09044528007507324, 0.2571714222431183, 0.2024785876274109, 0.04667475447058678, 0.3194847106933594, 0.43838533759117126, 0.353839248418808, 0.4812324345111847, 0.3527659475803375, 0.049585577100515366, 0.4781131446361542, 0.28615501523017883, 0.14331699907779694, 0.2101045399904251, 0.12992945313453674, 0.16458618640899658], dtype='float32').reshape([18]),
            paddle.to_tensor([0.23971976339817047, 0.23399308323860168, 0.14509566128253937, 0.3925781846046448, 0.06556537002325058, 0.4409458339214325, 0.2721312940120697, 0.25824761390686035, 0.3277953565120697, 0.300365149974823, 0.41509315371513367, 0.1568196713924408, 0.053763680160045624, 0.39370331168174744, 0.11878050118684769, 0.4933910071849823, 0.1389319896697998, 0.38948261737823486], dtype='float32').reshape([18]),
            paddle.to_tensor([0.47330713272094727, 0.3431345820426941, 0.04922548681497574, 0.22613880038261414, 0.2701359689235687, 0.40671202540397644, 0.37783125042915344, 0.0951516330242157, 0.2746879756450653, 0.07129164040088654, 0.36735740303993225, 0.10677972435951233, 0.3438660204410553, 0.09558653086423874, 0.11824086308479309, 0.4120330810546875, 0.10218247026205063, 0.232034370303154], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3280af3ab59c76d7160af9f763414991(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 26, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.06979581713676453, 0.342334121465683, 0.44504034519195557, 0.11657297611236572, 0.32415375113487244, 0.4846748411655426, 0.38389232754707336, 0.4422566294670105, 0.24860474467277527, 0.43455472588539124, 0.3681907057762146, 0.00018343800911679864, 0.2691427171230316, 0.25391823053359985, 0.41035082936286926, 0.10370508581399918, 0.1348896324634552, 0.14286509156227112, 0.26550960540771484, 0.058624133467674255, 0.08046600967645645, 0.09957931190729141, 0.00857219472527504, 0.3122197687625885, 0.10998914390802383, 0.349284291267395], dtype='float32').reshape([26]),
            paddle.to_tensor([0.21520154178142548, 0.2900048494338989, 0.2859202027320862, 0.17126771807670593, 0.07309902459383011, 0.26282212138175964, 0.2856777012348175, 0.34229776263237, 0.29283931851387024, 0.2041088491678238, 0.3049016296863556, 0.15143877267837524, 0.13010811805725098, 0.4394051432609558, 0.4353164732456207, 0.47218096256256104, 0.459021657705307, 0.37293824553489685, 0.4392184913158417, 0.3627532720565796, 0.1376606673002243, 0.2043309360742569, 0.3599303364753723, 0.2759748697280884, 0.10616063326597214, 0.4080446660518646], dtype='float32').reshape([26]),
            paddle.to_tensor([0.3171997368335724, 0.47683069109916687, 0.25296804308891296, 0.3432396352291107, 0.29959022998809814, 0.22456054389476776, 0.1323489248752594, 0.15226881206035614, 0.04180561751127243, 0.09372904151678085, 0.2902172803878784, 0.4918777644634247, 0.2867613136768341, 0.2384769767522812, 0.014197589829564095, 0.052263785153627396, 0.23893174529075623, 0.2065904438495636, 0.41501888632774353, 0.3216063678264618, 0.43080443143844604, 0.3502615988254547, 0.08850417286157608, 0.31101688742637634, 0.11845379322767258, 0.09644687920808792], dtype='float32').reshape([26]),
            paddle.to_tensor([0.14618656039237976, 0.1820187121629715, 0.22847920656204224, 0.2471291869878769, 0.3499084413051605, 0.19073368608951569, 0.25035348534584045, 0.4357112646102905, 0.21994532644748688, 0.34573158621788025, 0.004912219941616058, 0.3566384017467499, 0.46668779850006104, 0.29709282517433167, 0.18632808327674866, 0.388014554977417, 0.3276594579219818, 0.023843761533498764, 0.12058936059474945, 0.14161784946918488, 0.3070306181907654, 0.3541814982891083, 0.3397943079471588, 0.35786592960357666, 0.4564227759838104, 0.22653989493846893], dtype='float32').reshape([26]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a9f827cb5f8925b7150dda492c7ac7a8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.1487126350402832, 0.34458646178245544, 0.3905547261238098, 0.208818718791008, 0.32655787467956543, 0.06564337015151978, 0.2160779982805252, 0.4716254472732544, 0.33953002095222473, 0.3538002371788025, 0.039944905787706375, 0.4809325337409973, 0.3073996901512146, 0.15151724219322205, 0.24960891902446747, 0.41400691866874695, 0.25429365038871765, 0.12353143841028214], dtype='float32').reshape([18]),
            paddle.to_tensor([0.11590611934661865, 0.08363772183656693, 0.45276549458503723, 0.34041327238082886, 0.474599152803421, 0.4468936324119568, 0.06588566303253174, 0.24827581644058228, 0.49729210138320923, 0.4649428725242615, 0.292642205953598, 0.22993165254592896, 0.39038848876953125, 0.3899802565574646, 0.1278197318315506, 0.04023144021630287, 0.48148906230926514, 0.4541860520839691], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2235611528158188, 0.4878050982952118, 0.21760019659996033, 0.4223465919494629, 0.17178142070770264, 0.47165849804878235, 0.2680981755256653, 0.45548009872436523, 0.17148834466934204, 0.44690293073654175, 0.13673734664916992, 0.14399275183677673, 0.07788926362991333, 0.27094006538391113, 0.4499325156211853, 0.0809272974729538, 0.2766554057598114, 0.2465791553258896], dtype='float32').reshape([18]),
            paddle.to_tensor([0.24363279342651367, 0.43045902252197266, 0.1452483981847763, 0.3050425350666046, 0.20676283538341522, 0.3654646575450897, 0.4414055049419403, 0.07304675132036209, 0.36374470591545105, 0.4868881404399872, 0.2858283519744873, 0.2548372447490692, 0.34937548637390137, 0.3721981942653656, 0.015093259513378143, 0.3856019973754883, 0.26649171113967896, 0.4188653528690338], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b7b8f50031b81075172b3259fab10871(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8cf7316702ee2c8c243bc7088e656a3e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1f9467a3dadd9f4760532f14e907fa47(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([8, 512, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_017ec56b902df5417299af4e9fcf0fe1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e9e03ce9eb9db3725cf5453210511178(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_af2733185c41942b46884b6528d9b35e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 864, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([864], dtype='float32', min=0, max=0.5),
            paddle.uniform([864], dtype='float32', min=0, max=0.5),
            paddle.uniform([864], dtype='float32', min=0, max=0.5),
            paddle.uniform([864], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_681e2eedd991f9b469fd3b05e0832ae4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 570, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([570], dtype='float32', min=0, max=0.5),
            paddle.uniform([570], dtype='float32', min=0, max=0.5),
            paddle.uniform([570], dtype='float32', min=0, max=0.5),
            paddle.uniform([570], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_eeeeca2dda4d38b2f54c2bbdefdbb1f2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_88156497569287f5c01399c7e6489fbb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 10, 10], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_82e4eeaa6471083fb8d5e1b145b0d696(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_518dd84bf1496114b85042442c0ff8c0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 480, 480], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4544292986392975, 0.3207375407218933, 0.19146575033664703, 0.35797351598739624, 0.15164895355701447, 0.3501657545566559, 0.21208423376083374, 0.11607928574085236], dtype='float32').reshape([8]),
            paddle.to_tensor([0.2756797969341278, 0.13426893949508667, 0.4918965995311737, 0.09130152314901352, 0.023076675832271576, 0.024785879999399185, 0.12085912376642227, 0.39326801896095276], dtype='float32').reshape([8]),
            paddle.to_tensor([0.23897968232631683, 0.4212518036365509, 0.12699730694293976, 0.26636192202568054, 0.47416460514068604, 0.2470441609621048, 0.4511239528656006, 0.18246902525424957], dtype='float32').reshape([8]),
            paddle.to_tensor([0.48258116841316223, 0.3448585867881775, 0.46598219871520996, 0.0879727452993393, 0.4667583405971527, 0.27129408717155457, 0.3849337100982666, 0.4130862355232239], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_94a80e1b76e17f6a7460b6a582397f82(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 32, 64], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3413233458995819, 0.22539643943309784, 0.2002672404050827, 0.4932827353477478, 0.41060906648635864, 0.35534775257110596, 0.32917049527168274, 0.06380926072597504, 0.23755863308906555, 0.4623221457004547, 0.21210555732250214, 0.11389677226543427, 0.10802572220563889, 0.39376920461654663, 0.49003341794013977, 0.3072156608104706, 0.13288027048110962, 0.06329090148210526], dtype='float32').reshape([18]),
            paddle.to_tensor([0.25806939601898193, 0.33614256978034973, 0.3323332667350769, 0.48784318566322327, 0.12291142344474792, 0.2977884113788605, 0.1319553107023239, 0.09449076652526855, 0.43209677934646606, 0.38219743967056274, 0.14317266643047333, 0.2769750952720642, 0.29188042879104614, 0.20132765173912048, 0.36195141077041626, 0.3744351267814636, 0.42104092240333557, 0.4406972825527191], dtype='float32').reshape([18]),
            paddle.to_tensor([0.1504538655281067, 0.07079772651195526, 0.10458077490329742, 0.39963191747665405, 0.22851285338401794, 0.019583873450756073, 0.09178240597248077, 0.2552163004875183, 0.3609541058540344, 0.10096818953752518, 0.0644582211971283, 0.3354368805885315, 0.332330584526062, 0.02680099382996559, 0.2738220989704132, 0.14683474600315094, 0.03712335601449013, 0.47723114490509033], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3671475946903229, 0.47063836455345154, 0.4313102066516876, 0.016480963677167892, 0.3752344250679016, 0.33268406987190247, 0.04659197852015495, 0.48688462376594543, 0.3494415283203125, 0.3478809595108032, 0.3975943624973297, 0.42603442072868347, 0.44535940885543823, 0.20928995311260223, 0.3643242418766022, 0.3979877829551697, 0.3644763231277466, 0.20944680273532867], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_20a4151a7d8d2f84eda8efbfc3dd3382(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_523959420b2303693f728d445dac6d32(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0b7584ea6d84752ca0780906fa8237c4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 175, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fe497642d62913c54b5e621201897ca9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 8, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f8a8dd35be62d5174a829c2d9a123d4a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ed165700b5329da03cb6b81fd0321eb2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 122, 122], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_801b8785b644913d7dccb58d4e873f76(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.41191366314888, 0.4430559277534485, 0.08123418688774109, 0.0423981249332428, 0.009050034917891026, 0.21969041228294373, 0.45275911688804626, 0.13801290094852448, 0.1804627627134323, 0.3269241750240326, 0.22097943723201752, 0.0525897853076458, 0.43007662892341614, 0.14687594771385193, 0.25613632798194885, 0.3355729579925537], dtype='float32').reshape([16]),
            paddle.to_tensor([0.48556140065193176, 0.022475162521004677, 0.23389950394630432, 0.029793066903948784, 0.021170662716031075, 0.04125002771615982, 0.4017741084098816, 0.3860619068145752, 0.07357896119356155, 0.17352405190467834, 0.1725047081708908, 0.38928189873695374, 0.2260424643754959, 0.46478015184402466, 0.21511757373809814, 0.27120187878608704], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4120693504810333, 0.08415040373802185, 0.06382916867733002, 0.43307361006736755, 0.40286263823509216, 0.42286866903305054, 0.4707416892051697, 0.22153209149837494, 0.4367533326148987, 0.3784216642379761, 0.4952360689640045, 0.18454603850841522, 0.009808482602238655, 0.2129337191581726, 0.2128019630908966, 0.3252687454223633], dtype='float32').reshape([16]),
            paddle.to_tensor([0.09434357285499573, 0.4024406671524048, 0.13970032334327698, 0.2776254415512085, 0.17496518790721893, 0.2744436264038086, 0.11569831520318985, 0.07049523293972015, 0.11718451231718063, 0.23765982687473297, 0.2500755786895752, 0.36273816227912903, 0.13624952733516693, 0.24222280085086823, 0.09329462796449661, 0.10476206988096237], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7be91c291ee0e057e053c9e0399780c4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1248, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1248], dtype='float32', min=0, max=0.5),
            paddle.uniform([1248], dtype='float32', min=0, max=0.5),
            paddle.uniform([1248], dtype='float32', min=0, max=0.5),
            paddle.uniform([1248], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_adbf939a1b5e3c3f5a447ec9ee78813f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_58108d30d0de6cb7e7d13696ae52d2ba(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 30, 30], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_68f2cd7a65d59005d894f319802fd6ab(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ef898b02c7d645fc360c1daf96998b13(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 60, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e5d5652f25661cbfaf1e5dd40e48d16d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.32712888717651367, 0.08020037412643433, 0.1109742820262909, 0.47762107849121094, 0.045360319316387177, 0.13990257680416107, 0.025815457105636597, 0.097676120698452, 0.47654616832733154, 0.20740440487861633, 0.1247575655579567, 0.30949485301971436, 0.03820335492491722, 0.40963122248649597, 0.025248084217309952, 0.08653126657009125, 0.07332932949066162, 0.4399334788322449, 0.1291992962360382, 0.027880771085619926, 0.3647991418838501, 0.34966883063316345, 0.48629841208457947, 0.46664488315582275], dtype='float32').reshape([24]),
            paddle.to_tensor([0.27643609046936035, 0.2238360196352005, 0.3278704881668091, 0.10043206810951233, 0.22960592806339264, 0.4295630156993866, 0.31436461210250854, 0.4039093554019928, 0.45520085096359253, 0.05754606053233147, 0.06719601899385452, 0.08039774745702744, 0.13674016296863556, 0.41553425788879395, 0.4134094715118408, 0.013702772557735443, 0.37040114402770996, 0.14327487349510193, 0.49219080805778503, 0.4573954939842224, 0.3943988084793091, 0.2628119885921478, 0.2911595106124878, 0.17885510623455048], dtype='float32').reshape([24]),
            paddle.to_tensor([0.02948407456278801, 0.38069742918014526, 0.02565232291817665, 0.013394168578088284, 0.23529113829135895, 0.3826436400413513, 0.3441731631755829, 0.16876409947872162, 0.3563874363899231, 0.3513210713863373, 0.019503764808177948, 0.16704195737838745, 0.14132273197174072, 0.1559200882911682, 0.2618209421634674, 0.22086380422115326, 0.166103333234787, 0.26554036140441895, 0.364544153213501, 0.32237040996551514, 0.4678459167480469, 0.16962559521198273, 0.3501408100128174, 0.14423851668834686], dtype='float32').reshape([24]),
            paddle.to_tensor([0.1126718819141388, 0.3496050536632538, 0.3316773474216461, 0.4034930467605591, 0.21445906162261963, 0.33954837918281555, 0.49849510192871094, 0.2909211218357086, 0.07357143610715866, 0.1211322769522667, 0.0019298838451504707, 0.03467770293354988, 0.24557912349700928, 0.4912591576576233, 0.4664590358734131, 0.4699341356754303, 0.23135995864868164, 0.26029908657073975, 0.17563356459140778, 0.07087988406419754, 0.33701255917549133, 0.02320249378681183, 0.30149105191230774, 0.43955710530281067], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e104150fb708b1969d840531f36f31e2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 106, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([106], dtype='float32', min=0, max=0.5),
            paddle.uniform([106], dtype='float32', min=0, max=0.5),
            paddle.uniform([106], dtype='float32', min=0, max=0.5),
            paddle.uniform([106], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a8022d81b2c3dc5886d9c77ad5511b75(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_34bd9fea540da17a48d9a3f426b1b296(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_5e7cb23119f65b33bdbad71952f6dd12
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_39f301f0b7726800c3f64144f3a058ec(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 26], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e1403194e8f79cff300d293a3bced478(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 26, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2943490147590637, 0.3452131152153015, 0.07033391296863556, 0.2630724608898163, 0.24365010857582092, 0.32385456562042236, 0.037849489599466324, 0.327264666557312, 0.22300799190998077, 0.18687555193901062, 0.28804054856300354, 0.13169512152671814, 0.011564643122255802, 0.3599376380443573, 0.10959241539239883, 0.25938719511032104, 0.3380585014820099, 0.4096847176551819, 0.46087032556533813, 0.21498863399028778, 0.17136111855506897, 0.16735287010669708, 0.10933006554841995, 0.4907160997390747, 0.4672940969467163, 0.32237809896469116], dtype='float32').reshape([26]),
            paddle.to_tensor([0.09210927039384842, 0.353631854057312, 0.4150717556476593, 0.4128127694129944, 0.19037532806396484, 0.3296164572238922, 0.30855676531791687, 0.19527122378349304, 0.06275057792663574, 0.40857771039009094, 0.008263320662081242, 0.21933624148368835, 0.40115785598754883, 0.2782895863056183, 0.21949666738510132, 0.2770785689353943, 0.4959392547607422, 0.4376643896102905, 0.12724068760871887, 0.011210762895643711, 0.25012099742889404, 0.1182040125131607, 0.24598272144794464, 0.07585898786783218, 0.46651479601860046, 0.05927678197622299], dtype='float32').reshape([26]),
            paddle.to_tensor([0.09955625236034393, 0.20731686055660248, 0.43994972109794617, 0.3555634319782257, 0.3617360591888428, 0.1372610330581665, 0.42103883624076843, 0.23451478779315948, 0.387204110622406, 0.23250627517700195, 0.49121198058128357, 0.312823623418808, 0.16979841887950897, 0.10079056024551392, 0.1974596530199051, 0.2954716682434082, 0.0994928777217865, 0.19529791176319122, 0.10080961883068085, 0.1612546294927597, 0.18369360268115997, 0.19892947375774384, 0.459519624710083, 0.4953535199165344, 0.4133688807487488, 0.35929396748542786], dtype='float32').reshape([26]),
            paddle.to_tensor([0.3356212079524994, 0.19257456064224243, 0.3454614281654358, 0.411453515291214, 0.07535713165998459, 0.2163449078798294, 0.22453804314136505, 0.07412081211805344, 0.49736273288726807, 0.23353272676467896, 0.27482473850250244, 0.0042339046485722065, 0.46435773372650146, 0.38928842544555664, 0.30469098687171936, 0.41004878282546997, 0.31968173384666443, 0.2813655138015747, 0.1246233806014061, 0.21406061947345734, 0.12109244614839554, 0.4628661274909973, 0.3759188652038574, 0.13194094598293304, 0.2430896759033203, 0.02569129876792431], dtype='float32').reshape([26]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4b85f3928f27b90b1b093a0adaf57dbc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 320, 320], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.39691659808158875, 0.0998644158244133, 0.024072706699371338, 0.2513355016708374, 0.15082649886608124, 0.2192571759223938, 0.08145661652088165, 0.04198823124170303, 0.19864583015441895, 0.06058192625641823, 0.3024556636810303, 0.4502607583999634, 0.16541488468647003, 0.35434210300445557, 0.3025333285331726, 0.48758795857429504], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3291131556034088, 0.36857131123542786, 0.04632359743118286, 0.017112568020820618, 0.3651345372200012, 0.14545422792434692, 0.12047453969717026, 0.3716808259487152, 0.3132190406322479, 0.0014602143783122301, 0.06582317501306534, 0.4474688172340393, 0.2888438105583191, 0.358830064535141, 0.1389106959104538, 0.1847113072872162], dtype='float32').reshape([16]),
            paddle.to_tensor([0.15356270968914032, 0.013192366808652878, 0.2933504283428192, 0.13356204330921173, 0.08313320577144623, 0.30959856510162354, 0.33441805839538574, 0.01467184629291296, 0.0704701766371727, 0.024081524461507797, 0.037588223814964294, 0.14589254558086395, 0.49538981914520264, 0.2625828683376312, 0.06226707249879837, 0.21135587990283966], dtype='float32').reshape([16]),
            paddle.to_tensor([0.23501251637935638, 0.09109257906675339, 0.025844911113381386, 0.37386056780815125, 0.3708018660545349, 0.26430612802505493, 0.33753496408462524, 0.08770215511322021, 0.0959247350692749, 0.009418445639312267, 0.31098631024360657, 0.06988953053951263, 0.4698837697505951, 0.07230998575687408, 0.05417373403906822, 0.16076192259788513], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ccc19c878f54c46dd79cf292711fed17(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 728, 38, 38], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0725043ab09ccf9e1775d3b0afe76c08(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_96a16a730cb55bf28e6b0c242f8e2b7e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 4, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f6f03fcaa1316f27821173938a7f7ef1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 200, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c1dd6eced1c6e9e59a9a86daa7299e61(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 64, 128], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.11141117662191391, 0.4464627504348755, 0.47758620977401733, 0.0680541843175888, 0.20206312835216522, 0.4633614420890808, 0.2190927118062973, 0.2834630608558655, 0.08124381303787231, 0.30890971422195435, 0.2504715025424957, 0.144719660282135, 0.0030703626107424498, 0.366914302110672, 0.3136443793773651, 0.3007471263408661, 0.37283557653427124, 0.399662584066391], dtype='float32').reshape([18]),
            paddle.to_tensor([0.39132195711135864, 0.32156193256378174, 0.13150031864643097, 0.28750622272491455, 0.007046246901154518, 0.1451074481010437, 0.06530067324638367, 0.4810643196105957, 0.4539560377597809, 0.4084809124469757, 0.40999749302864075, 0.28018832206726074, 0.19971036911010742, 0.3574035167694092, 0.3445335328578949, 0.45782652497291565, 0.2845211923122406, 0.404936820268631], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4910587668418884, 0.3863629102706909, 0.08625569194555283, 0.23565462231636047, 0.36070531606674194, 0.2683856189250946, 0.006501454859972, 0.24443046748638153, 0.3363285958766937, 0.16827261447906494, 0.4797889292240143, 0.2291731834411621, 0.28480708599090576, 0.4017248749732971, 0.4719368517398834, 0.33144059777259827, 0.1713513731956482, 0.05979023873806], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3623535931110382, 0.34643155336380005, 0.02480263262987137, 0.12399829924106598, 0.42509719729423523, 0.3354383707046509, 0.38968193531036377, 0.08254045248031616, 0.43083152174949646, 0.15667635202407837, 0.1347704827785492, 0.4150580167770386, 0.1409251093864441, 0.4279084801673889, 0.3203798532485962, 0.16312924027442932, 0.46554797887802124, 0.024148128926753998], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_f5f2213be92f20fda5378beeed72688f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5b7edc0ccd81b717909287901b4505d0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_f5f2213be92f20fda5378beeed72688f
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7909f581ffb536120875c6fbeb4ebae6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 16, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1ea15a11da4b34878330ac6353203487(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 352, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([352], dtype='float32', min=0, max=0.5),
            paddle.uniform([352], dtype='float32', min=0, max=0.5),
            paddle.uniform([352], dtype='float32', min=0, max=0.5),
            paddle.uniform([352], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_671f65bef89c3c4f27342b5d17d57656(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.29924964904785156, 0.12939277291297913, 0.17722438275814056, 0.3326844871044159, 0.11380332708358765, 0.22305090725421906, 0.27639660239219666, 0.15570694208145142, 0.4054626226425171, 0.3089938759803772, 0.33949020504951477, 0.2959540784358978, 0.39332377910614014, 0.1901824176311493, 0.4834146201610565, 0.14355771243572235, 0.02778785303235054, 0.04587912559509277, 0.36542952060699463, 0.17439700663089752, 0.47505858540534973, 0.3859604299068451, 0.1621434986591339, 0.0636458471417427, 0.13349702954292297, 0.39750394225120544, 0.3430236577987671, 0.06832318007946014], dtype='float32').reshape([28]),
            paddle.to_tensor([0.47660669684410095, 0.4874456822872162, 0.004674824420362711, 0.3991202414035797, 0.3599752187728882, 0.47699955105781555, 0.35726746916770935, 0.22299465537071228, 0.3996591866016388, 0.3006826937198639, 0.010735156014561653, 0.2973937690258026, 0.05337820574641228, 0.06002390757203102, 0.4013821482658386, 0.13349191844463348, 0.11565551906824112, 0.04236910864710808, 0.49237585067749023, 0.33773231506347656, 0.31277191638946533, 0.17738887667655945, 0.14529123902320862, 0.27600279450416565, 0.4904822111129761, 0.014871369116008282, 0.40959396958351135, 0.4021207094192505], dtype='float32').reshape([28]),
            paddle.to_tensor([0.18991631269454956, 0.1972775161266327, 0.20287016034126282, 0.14740906655788422, 0.14730453491210938, 0.021452808752655983, 0.03427773714065552, 0.2034536451101303, 0.31382304430007935, 0.12311425805091858, 0.12463810294866562, 0.15124507248401642, 0.24499988555908203, 0.3807791471481323, 0.476963609457016, 0.11665139347314835, 0.46320152282714844, 0.13626320660114288, 0.20872770249843597, 0.3289496600627899, 0.4964028298854828, 0.2017187625169754, 0.0391509123146534, 0.33363670110702515, 0.32784977555274963, 0.21978406608104706, 0.17049729824066162, 0.0013697254471480846], dtype='float32').reshape([28]),
            paddle.to_tensor([0.4553021490573883, 0.18955712020397186, 0.07474865019321442, 0.1205177903175354, 0.22415420413017273, 0.1760520040988922, 0.07614536583423615, 0.25521230697631836, 0.3733534812927246, 0.4033527970314026, 0.09997802972793579, 0.3480638265609741, 0.30318668484687805, 0.3173738121986389, 0.24677178263664246, 0.025371240451931953, 0.4305582046508789, 0.17197205126285553, 0.4255552291870117, 0.36089861392974854, 0.1534646600484848, 0.4505205452442169, 0.18992815911769867, 0.22706659138202667, 0.4128333032131195, 0.26931455731391907, 0.31866908073425293, 0.3843823969364166], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e6c834fbf0f4ae6b696c03db59b2994d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.056739892810583115, 0.036354340612888336, 0.328591525554657, 0.3732374906539917, 0.3912983536720276, 0.4083751440048218, 0.14060568809509277, 0.01662648469209671, 0.01104305312037468, 0.11761725693941116, 0.3793804943561554, 0.44490501284599304, 0.47998756170272827, 0.38849297165870667, 0.23870335519313812, 0.05408508703112602, 0.3502674400806427, 0.09982018917798996, 0.3621249198913574, 0.4342467784881592, 0.20760288834571838, 0.48448097705841064, 0.32095590233802795, 0.24578498303890228], dtype='float32').reshape([24]),
            paddle.to_tensor([0.036712806671857834, 0.16248875856399536, 0.4548518657684326, 0.3818114101886749, 0.4435950815677643, 0.4981291890144348, 0.17004120349884033, 0.08217588067054749, 0.0758686363697052, 0.4121563136577606, 0.4713101089000702, 0.08393441140651703, 0.2214898020029068, 0.09916581958532333, 0.39598339796066284, 0.008774650283157825, 0.14298632740974426, 0.18065087497234344, 0.189060240983963, 0.30739837884902954, 0.07987755537033081, 0.46504852175712585, 0.11933370679616928, 0.49679630994796753], dtype='float32').reshape([24]),
            paddle.to_tensor([0.23675070703029633, 0.44564712047576904, 0.17401885986328125, 0.26116374135017395, 0.35135385394096375, 0.3836611807346344, 0.09779791533946991, 0.07891461253166199, 0.49972105026245117, 0.16914990544319153, 0.01984170265495777, 0.2971477210521698, 0.22762921452522278, 0.20874997973442078, 0.06628058105707169, 0.4428636133670807, 0.22710104286670685, 0.04157264903187752, 0.1995915323495865, 0.29358166456222534, 0.1787724494934082, 0.13680166006088257, 0.1113053560256958, 0.34063220024108887], dtype='float32').reshape([24]),
            paddle.to_tensor([0.03174931928515434, 0.12664322555065155, 0.007059184834361076, 0.32903432846069336, 0.019634846597909927, 0.02174302004277706, 0.17877241969108582, 0.31712743639945984, 0.32154184579849243, 0.4419596791267395, 0.07690123468637466, 0.4389168918132782, 0.3123347759246826, 0.00578575674444437, 0.19054445624351501, 0.14292702078819275, 0.42990604043006897, 0.4088262915611267, 0.41032934188842773, 0.14793187379837036, 0.08785892277956009, 0.14710763096809387, 0.10126704722642899, 0.09634027630090714], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_73d5548b72efc5bcec7e2ef79bd45479(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.39636269211769104, 0.1944407969713211, 0.2988562285900116, 0.3602859079837799, 0.1577557772397995, 0.16408760845661163, 0.26569947600364685, 0.2775789499282837, 0.235543891787529, 0.4583068788051605, 0.47419294714927673, 0.0015723691321909428, 0.44149065017700195, 0.4457622170448303, 0.0994730293750763, 0.2985946536064148, 0.23197826743125916, 0.47301438450813293, 0.08472476899623871, 0.26081767678260803, 0.22057129442691803, 0.15797369182109833, 0.47647425532341003, 0.4342457354068756, 0.3683406412601471, 0.3460652828216553, 0.2483147233724594, 0.3067186176776886, 0.20208539068698883, 0.12398266792297363], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4446758031845093, 0.4888641834259033, 0.347297728061676, 0.3149648606777191, 0.2308548390865326, 0.1310504823923111, 0.22899141907691956, 0.29783856868743896, 0.11111291497945786, 0.10962103307247162, 0.21961013972759247, 0.44778645038604736, 0.015882184728980064, 0.15999583899974823, 0.03910971060395241, 0.487580806016922, 0.3646022379398346, 0.19553861021995544, 0.4489285945892334, 0.04589664191007614, 0.12599146366119385, 0.4027980864048004, 0.04800288379192352, 0.3156557083129883, 0.45256030559539795, 0.45429298281669617, 0.3655024766921997, 0.0069436910562217236, 0.09413719177246094, 0.1129145547747612], dtype='float32').reshape([30]),
            paddle.to_tensor([0.40109819173812866, 0.4584578573703766, 0.16206900775432587, 0.3285623788833618, 0.4711065888404846, 0.4307354688644409, 0.139828160405159, 0.3810751140117645, 0.3074982762336731, 0.06847600638866425, 0.06288164108991623, 0.16231419146060944, 0.11039391905069351, 0.4450835883617401, 0.47120752930641174, 0.19250014424324036, 0.2774563431739807, 0.1809794157743454, 0.07443378865718842, 0.29019588232040405, 0.041992656886577606, 0.1241140216588974, 0.3638309836387634, 0.43386173248291016, 0.3478497862815857, 0.38475120067596436, 0.4411035478115082, 0.07856062054634094, 0.05277127027511597, 0.1923888623714447], dtype='float32').reshape([30]),
            paddle.to_tensor([0.29766422510147095, 0.08191084116697311, 0.16607321798801422, 0.4617835283279419, 0.18779510259628296, 0.1666405349969864, 0.16439516842365265, 0.1503116637468338, 0.36160826683044434, 0.05127517133951187, 0.3695460855960846, 0.47939205169677734, 0.2173246145248413, 0.2485622614622116, 0.3255051076412201, 0.03594350069761276, 0.31057631969451904, 0.3564133942127228, 0.28558358550071716, 0.3912128806114197, 0.22530068457126617, 0.47409093379974365, 0.36848893761634827, 0.10090065747499466, 0.4766152501106262, 0.4991281032562256, 0.31302565336227417, 0.46551597118377686, 0.4195324182510376, 0.17405427992343903], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_818fcec8a73a139b187129182d9957e8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 360, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([360], dtype='float32', min=0, max=0.5),
            paddle.uniform([360], dtype='float32', min=0, max=0.5),
            paddle.uniform([360], dtype='float32', min=0, max=0.5),
            paddle.uniform([360], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6d776a4944b1367335a29ea8d976cd39(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 640, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_eac502bad7ee4bc47a14963db719cbb0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 12, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8ad6206a5135403d69bbf467dd03e3f6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b274ea83c927a37d520664248e3b7925(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 152, 152], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_775d0d09940f6f652b7193572281ccaf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1044, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1044], dtype='float32', min=0, max=0.5),
            paddle.uniform([1044], dtype='float32', min=0, max=0.5),
            paddle.uniform([1044], dtype='float32', min=0, max=0.5),
            paddle.uniform([1044], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_26c63e8cdc316719fc0b123f1f056a03(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_009776204df1c246ace59041cd5974e1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.40444648265838623, 0.1642134189605713, 0.20824047923088074, 0.32378265261650085, 0.024820785969495773, 0.06474718451499939, 0.47656363248825073, 0.2426866590976715, 0.4259227216243744, 0.4594555199146271, 0.022711334750056267, 0.4513692557811737, 0.45482733845710754, 0.09847258031368256, 0.3648192882537842, 0.08589161187410355], dtype='float32').reshape([16]),
            paddle.to_tensor([0.41735681891441345, 0.20136472582817078, 0.48436304926872253, 0.39412933588027954, 0.1634155958890915, 0.007100961171090603, 0.3409581482410431, 0.18491420149803162, 0.0657007098197937, 0.49098560214042664, 0.154167041182518, 0.035112883895635605, 0.08760292083024979, 0.2617150843143463, 0.2569213807582855, 0.10705646872520447], dtype='float32').reshape([16]),
            paddle.to_tensor([0.333996057510376, 0.2114337682723999, 0.47231796383857727, 0.4660785496234894, 0.45405641198158264, 0.2535516321659088, 0.24905288219451904, 0.09523702412843704, 0.25921720266342163, 0.30919626355171204, 0.22969773411750793, 0.07622290402650833, 0.18664051592350006, 0.2480805218219757, 0.3345911502838135, 0.33274367451667786], dtype='float32').reshape([16]),
            paddle.to_tensor([0.23249474167823792, 0.397338330745697, 0.4690687954425812, 0.42782115936279297, 0.026110723614692688, 0.19706463813781738, 0.34727853536605835, 0.04706191271543503, 0.36339160799980164, 0.4051813781261444, 0.48215532302856445, 0.4997803568840027, 0.059584081172943115, 0.25161221623420715, 0.44369587302207947, 0.3764418363571167], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_99fe90696970908a799e2b36ff07c60f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6d2554ee8ed65d66dea15c3b791c97a1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_0edd578f48a07aa5d1d79f9f678b472e
    def get_inputs(self):
        return [
            paddle.uniform([16, 768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_92401916800b421a48c54bfd781a24bc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 76, 76], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1a56c4085ca5984bfaad2385de195548(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2825179398059845, 0.3611309826374054, 0.38085421919822693, 0.44189199805259705, 0.23165297508239746, 0.4513006806373596, 0.306689590215683, 0.07420535385608673, 0.11713745445013046, 0.15472733974456787, 0.23852360248565674, 0.250595360994339, 0.47006988525390625, 0.11584671586751938, 0.048587147146463394, 0.34609508514404297, 0.23464542627334595, 0.38293856382369995, 0.09837745130062103, 0.29385122656822205, 0.04128022491931915, 0.26396721601486206, 0.2393074631690979, 0.39727142453193665, 0.3515373468399048, 0.4703615605831146, 0.1649683713912964, 0.3291098177433014], dtype='float32').reshape([28]),
            paddle.to_tensor([0.4873466193675995, 0.43145817518234253, 0.15502850711345673, 0.47857576608657837, 0.4504517912864685, 0.4211081862449646, 0.2558848261833191, 0.30314335227012634, 0.46807175874710083, 0.41091394424438477, 0.09529013186693192, 0.2199396938085556, 0.42261189222335815, 0.2645862400531769, 0.1405922919511795, 0.29980000853538513, 0.3515583574771881, 0.1879679262638092, 0.35397031903266907, 0.45065799355506897, 0.16833212971687317, 0.06446414440870285, 0.07500690966844559, 0.15646210312843323, 0.17570365965366364, 0.2874872088432312, 0.12705062329769135, 0.3220846652984619], dtype='float32').reshape([28]),
            paddle.to_tensor([0.10847840458154678, 0.4118714928627014, 0.006830450613051653, 0.461819052696228, 0.03441102057695389, 0.16523616015911102, 0.33621451258659363, 0.2980584502220154, 0.4061851501464844, 0.36320969462394714, 0.2563912868499756, 0.46655890345573425, 0.028448058292269707, 0.04423234984278679, 0.46025872230529785, 0.44721484184265137, 0.45962947607040405, 0.4610702395439148, 0.23117850720882416, 0.3262370824813843, 0.32326576113700867, 0.22452564537525177, 0.04004991427063942, 0.4427689015865326, 0.4679918587207794, 0.060814931988716125, 0.40821677446365356, 0.19606712460517883], dtype='float32').reshape([28]),
            paddle.to_tensor([0.06573740392923355, 0.4159516394138336, 0.2612203359603882, 0.2953939437866211, 0.21236275136470795, 0.24090048670768738, 0.30334049463272095, 0.019197477027773857, 0.34631702303886414, 0.11653946340084076, 0.33035773038864136, 0.1291400045156479, 0.14664727449417114, 0.23464509844779968, 0.3013899028301239, 0.4279104173183441, 0.3455071449279785, 0.03763933479785919, 0.13027308881282806, 0.08498676866292953, 0.42203184962272644, 0.2110084593296051, 0.13425779342651367, 0.1041506975889206, 0.03591945394873619, 0.05990797281265259, 0.24147120118141174, 0.06687768548727036], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_04da8f80618372f4d3b53fc1487c4566(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3771187663078308, 0.25669172406196594, 0.21105724573135376, 0.4906562268733978, 0.3234219253063202, 0.3888687789440155, 0.35331445932388306, 0.057403314858675, 0.25351423025131226, 0.14426684379577637, 0.02396601065993309, 0.18201686441898346, 0.15565736591815948, 0.03819291666150093, 0.3862954378128052, 0.1898522824048996, 0.19200383126735687, 0.1590294986963272], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4059303104877472, 0.2774569094181061, 0.4889037311077118, 0.12090691924095154, 0.4997672736644745, 0.0382302887737751, 0.18585410714149475, 0.09912332892417908, 0.3568951487541199, 0.08683844655752182, 0.22047902643680573, 0.3831591010093689, 0.49999505281448364, 0.3955845236778259, 0.27152925729751587, 0.038254182785749435, 0.13884049654006958, 0.07845437526702881], dtype='float32').reshape([18]),
            paddle.to_tensor([0.21622343361377716, 0.0766526311635971, 0.3935736417770386, 0.1729288250207901, 0.11622242629528046, 0.3660374581813812, 0.37023115158081055, 0.462762713432312, 0.2768547236919403, 0.4901021122932434, 0.29405784606933594, 0.3019063472747803, 0.3135538697242737, 0.1453407108783722, 0.08222243189811707, 0.17779135704040527, 0.49578070640563965, 0.46688202023506165], dtype='float32').reshape([18]),
            paddle.to_tensor([0.21592417359352112, 0.4263836145401001, 0.12472067773342133, 0.0014892334584146738, 0.053128860890865326, 0.4845564067363739, 0.12289251387119293, 0.23152732849121094, 0.1440948247909546, 0.14717289805412292, 0.08207876980304718, 0.48475486040115356, 0.19797298312187195, 0.20787225663661957, 0.0478917695581913, 0.15694715082645416, 0.09600517898797989, 0.024764228612184525], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aed205faaacc92c0e8cddb4b41452599(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1008, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1008], dtype='float32', min=0, max=0.5),
            paddle.uniform([1008], dtype='float32', min=0, max=0.5),
            paddle.uniform([1008], dtype='float32', min=0, max=0.5),
            paddle.uniform([1008], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cfca0abcb28d56c468721ebd0aeedf64(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c182d0d414eff0cdd3eea0a8f368c4e0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cdc3b0a148b551f86897e4668e0bdb6b
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cdca329a884c530c9e37a97c84353454(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a56f45f32136d8931f4bd1978a289cf6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9e7c17f9ba7391848adf4d3942ea6d30(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_51d4383fbf12e184cee6696049d34f6c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 35, 35], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9b5291d5f02effffa005bad1992eccd6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.25253450870513916, 0.40743958950042725, 0.3683123290538788, 0.3159712553024292, 0.46061068773269653, 0.355289489030838, 0.06862588971853256, 0.035033781081438065, 0.4437981843948364, 0.18485362827777863, 0.3893531858921051, 0.11548017710447311, 0.4519514739513397, 0.2550727427005768, 0.4662603735923767, 0.002597441431134939, 0.3661985993385315, 0.43855610489845276, 0.03839461877942085, 0.07214076071977615, 0.2946086823940277, 0.1668500453233719, 0.4802235960960388, 0.37173113226890564, 0.2048361599445343, 0.014571974985301495, 0.27780017256736755, 0.3012891709804535], dtype='float32').reshape([28]),
            paddle.to_tensor([0.4669032096862793, 0.2319057285785675, 0.14629019796848297, 0.0006373384385369718, 0.29904934763908386, 0.14924383163452148, 0.3671583831310272, 0.30385011434555054, 0.4122982323169708, 0.38165366649627686, 0.07469213008880615, 0.13809889554977417, 0.01994527317583561, 0.18050965666770935, 0.36449745297431946, 0.38462281227111816, 0.4597998261451721, 0.45790985226631165, 0.24004919826984406, 0.20328395068645477, 0.0992349311709404, 0.4133724868297577, 0.44035762548446655, 0.01976834237575531, 0.1566949039697647, 0.2389651983976364, 0.4661858379840851, 0.3668908178806305], dtype='float32').reshape([28]),
            paddle.to_tensor([0.17089293897151947, 0.46173006296157837, 0.27447518706321716, 0.3972393572330475, 0.1624496728181839, 0.16211313009262085, 0.4554864168167114, 0.16245347261428833, 0.36552518606185913, 0.13224785029888153, 0.4543880820274353, 0.157333642244339, 0.2982858419418335, 0.0794752687215805, 0.32726237177848816, 0.37739548087120056, 0.47070151567459106, 0.14805318415164948, 0.12109092622995377, 0.21036437153816223, 0.45000749826431274, 0.4745914340019226, 0.030060702934861183, 0.26632779836654663, 0.12841840088367462, 0.3478197157382965, 0.2448696345090866, 0.004781054332852364], dtype='float32').reshape([28]),
            paddle.to_tensor([0.0377718023955822, 0.13207781314849854, 0.20910637080669403, 0.19486813247203827, 0.4816499352455139, 0.009714774787425995, 0.03106442093849182, 0.018623553216457367, 0.08373527228832245, 0.03147753328084946, 0.4003541171550751, 0.004279803950339556, 0.35567131638526917, 0.10006333142518997, 0.03330722451210022, 0.3306058943271637, 0.33184629678726196, 0.11629720777273178, 0.1439826786518097, 0.06907278299331665, 0.04050515964627266, 0.32762446999549866, 0.38493216037750244, 0.0715482085943222, 0.20435410737991333, 0.25106021761894226, 0.07362467050552368, 0.016222137957811356], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6b030111afebde44cc008a81877ecf9a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2380441427230835, 0.24070581793785095, 0.020829085260629654, 0.0430096797645092, 0.11059027165174484, 0.19600756466388702, 0.3738662600517273, 0.1566154658794403, 0.20065157115459442, 0.2199573963880539, 0.25039294362068176, 0.2838548719882965], dtype='float32').reshape([12]),
            paddle.to_tensor([0.4514855146408081, 0.4194748103618622, 0.12964487075805664, 0.4308520257472992, 0.17432720959186554, 0.050024840980768204, 0.2526906430721283, 0.36951014399528503, 0.31809645891189575, 0.27146416902542114, 0.2552608549594879, 0.1760004758834839], dtype='float32').reshape([12]),
            paddle.to_tensor([0.28074946999549866, 0.047992974519729614, 0.11848378926515579, 0.34080585837364197, 0.27425891160964966, 0.4154495596885681, 0.05213841050863266, 0.06975051760673523, 0.19422778487205505, 0.07830202579498291, 0.039810072630643845, 0.12439116835594177], dtype='float32').reshape([12]),
            paddle.to_tensor([0.20629051327705383, 0.3499221205711365, 0.06126909703016281, 0.18964728713035583, 0.10583575069904327, 0.43038442730903625, 0.03611128032207489, 0.1415613889694214, 0.18095265328884125, 0.35947179794311523, 0.28331732749938965, 0.2651826739311218], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_92f31a4adb928839bfa85e646d6a25bb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_b4f4da3a8d44d1b5b3a2903a097df186
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b31b3e648db70d57414d1c4a080c40a0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([8, 256, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cedd74ee0f0ee9d73c1569accd1e80b0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 2560, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([2560], dtype='float32', min=0, max=0.5),
            paddle.uniform([2560], dtype='float32', min=0, max=0.5),
            paddle.uniform([2560], dtype='float32', min=0, max=0.5),
            paddle.uniform([2560], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d7a8d6faa735c9663483a6e184463e36(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_47025a9d4e87f018677db5b418c84c08(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 16, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7f491496970c567562a5dc2ca893bf2f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a81f8e974259482675dca744840ce209(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 62, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([62], dtype='float32', min=0, max=0.5),
            paddle.uniform([62], dtype='float32', min=0, max=0.5),
            paddle.uniform([62], dtype='float32', min=0, max=0.5),
            paddle.uniform([62], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8ff0ae41db8f55bc93b7c7070c98b569(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_b4f4da3a8d44d1b5b3a2903a097df186
    def get_inputs(self):
        return [
            paddle.uniform([1, 640, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8ca51cbf600a2490b7ca40b7d02a9741(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_64fc992ace21c4cd05dc667fed51454a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1296, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1296], dtype='float32', min=0, max=0.5),
            paddle.uniform([1296], dtype='float32', min=0, max=0.5),
            paddle.uniform([1296], dtype='float32', min=0, max=0.5),
            paddle.uniform([1296], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_60ac304885dcb2d44ec7dc1a1edcf864(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1152, 6, 6], dtype='float16', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9c49c3d4cdd1a3e7e6672d44cea385a3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 4, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9e50accb72d9b23ae570d17c7d3f34b6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 240, 240], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.27514755725860596, 0.010681802406907082, 0.11832964420318604, 0.19789041578769684, 0.19108180701732635, 0.04752655699849129, 0.031201649457216263, 0.015025729313492775, 0.11667695641517639, 0.2895217835903168, 0.11857251077890396, 0.33376166224479675, 0.3409620225429535, 0.1758786290884018, 0.3602648675441742, 0.15396063029766083, 0.4096298813819885, 0.17409394681453705, 0.44153910875320435, 0.3717848062515259, 0.2637440860271454, 0.15198005735874176, 0.06602217257022858, 0.47824910283088684], dtype='float32').reshape([24]),
            paddle.to_tensor([0.008156145922839642, 0.2899649441242218, 0.02298041619360447, 0.2031600922346115, 0.3334250748157501, 0.3049527108669281, 0.09472215175628662, 0.1994687169790268, 0.45010337233543396, 0.13459281623363495, 0.02130161039531231, 0.43175703287124634, 0.4423057734966278, 0.4528564512729645, 0.33994153141975403, 0.3563011586666107, 0.305303156375885, 0.19464822113513947, 0.12471454590559006, 0.3213741183280945, 0.42311641573905945, 0.41478070616722107, 0.18733203411102295, 0.03718108311295509], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4774242639541626, 0.32866618037223816, 0.261353999376297, 0.24544091522693634, 0.09913162887096405, 0.2680313289165497, 0.23583835363388062, 0.29859474301338196, 0.26203659176826477, 0.44517016410827637, 0.290149062871933, 0.3042537271976471, 0.3891666829586029, 0.3381907045841217, 0.1311604529619217, 0.3385544717311859, 0.09220709651708603, 0.3085227608680725, 0.1507551074028015, 0.4728846549987793, 0.3259742856025696, 0.33054009079933167, 0.06020510941743851, 0.23315511643886566], dtype='float32').reshape([24]),
            paddle.to_tensor([0.40588444471359253, 0.160117045044899, 0.15598458051681519, 0.2004399597644806, 0.18247240781784058, 0.4265640377998352, 0.1375969499349594, 0.3594572842121124, 0.019465172663331032, 0.4416930377483368, 0.4690857529640198, 0.01422447431832552, 0.49725082516670227, 0.1634104996919632, 0.41254299879074097, 0.4604416787624359, 0.1354219764471054, 0.41216006875038147, 0.4134741425514221, 0.00041675049578770995, 0.19227784872055054, 0.38350778818130493, 0.24751247465610504, 0.41794857382774353], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3b1cabd9723129e1c2fa8cb0c70a355a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.12631948292255402, 0.17509247362613678, 0.1039135679602623, 0.3621671199798584, 0.13018031418323517, 0.35511380434036255, 0.43341922760009766, 0.3173801898956299, 0.2877046465873718, 0.18538634479045868, 0.348252534866333, 0.29092007875442505, 0.01678873784840107, 0.03155304864048958, 0.3086051046848297, 0.11609495431184769, 0.4632321298122406, 0.16772980988025665, 0.1574506014585495, 0.09139465540647507, 0.17011912167072296, 0.49511802196502686, 0.4558210074901581, 0.4119786322116852, 0.33294355869293213, 0.35392776131629944, 0.43120330572128296, 0.48602917790412903], dtype='float32').reshape([28]),
            paddle.to_tensor([0.1821223646402359, 0.20914971828460693, 0.04067838191986084, 0.3145451247692108, 0.14751121401786804, 0.3133503794670105, 0.3455469012260437, 0.08313591033220291, 0.022734953090548515, 0.14418154954910278, 0.03442702814936638, 0.05874694511294365, 0.22309236228466034, 0.3142607808113098, 0.3253522515296936, 0.14028596878051758, 0.4909714162349701, 0.0009321472025476396, 0.0054343705996870995, 0.1820461004972458, 0.21956031024456024, 0.37295714020729065, 0.4147716164588928, 0.029538823291659355, 0.0270013976842165, 0.37222805619239807, 0.39939597249031067, 0.21342733502388], dtype='float32').reshape([28]),
            paddle.to_tensor([0.15170830488204956, 0.1998646855354309, 0.2759104371070862, 0.26737505197525024, 0.34366151690483093, 0.08289721608161926, 0.07293616980314255, 0.16193364560604095, 0.39115551114082336, 0.07599607110023499, 0.3470858335494995, 0.29636645317077637, 0.02929382584989071, 0.1580764651298523, 0.32747212052345276, 0.39645227789878845, 0.31278911232948303, 0.029412565752863884, 0.04326228052377701, 0.35039329528808594, 0.47435224056243896, 0.03507721796631813, 0.24413390457630157, 0.29211118817329407, 0.3481810986995697, 0.4427469074726105, 0.24974292516708374, 0.1466646045446396], dtype='float32').reshape([28]),
            paddle.to_tensor([0.03610798344016075, 0.08335787802934647, 0.19504393637180328, 0.32880139350891113, 0.35128170251846313, 0.271210640668869, 0.04730476811528206, 0.22199752926826477, 0.46892574429512024, 0.16511325538158417, 0.408064603805542, 0.2843720614910126, 0.04580670967698097, 0.06411200016736984, 0.38071656227111816, 0.3960585594177246, 0.47270482778549194, 0.4091624319553375, 0.47421231865882874, 0.33073943853378296, 0.033617399632930756, 0.3084338903427124, 0.2625970244407654, 0.16431176662445068, 0.06483349204063416, 0.14949792623519897, 0.181051105260849, 0.3782511055469513], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fafbe349162b1f4d19565bac48d08588(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a5541b615fc18b2de60c00e5d76110f2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 112, 12, 12], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f79cd68f447c3558499dbff26a3ee317(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0ab6ca355d887643a90820031f070b42(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_d589af777f5a8ae699611515c41499fb
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6a473721f585872e363ac533b29fb0c5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.15337641537189484, 0.13846175372600555, 0.498357355594635, 0.07495513558387756, 0.24819453060626984, 0.15262800455093384, 0.25695639848709106, 0.08723995834589005, 0.4512028098106384, 0.4050271511077881, 0.21363826096057892, 0.34893184900283813, 0.2982326149940491, 0.39611753821372986, 0.4969111680984497, 0.4663469195365906, 0.3859737515449524, 0.12961043417453766, 0.21290378272533417, 0.24438276886940002, 0.028567897155880928, 0.3991151452064514, 0.195331871509552, 0.42600753903388977], dtype='float32').reshape([24]),
            paddle.to_tensor([0.1655295193195343, 0.04639467969536781, 0.18262405693531036, 0.37672126293182373, 0.29366621375083923, 0.44088873267173767, 0.12847594916820526, 0.29001107811927795, 0.09051316976547241, 0.3965558409690857, 0.47647082805633545, 0.017268242314457893, 0.14329823851585388, 0.21662889420986176, 0.15724660456180573, 0.2826477289199829, 0.16062305867671967, 0.4092840850353241, 0.4002339839935303, 0.46348127722740173, 0.1391151249408722, 0.2594500184059143, 0.24176478385925293, 0.3152790665626526], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3120987117290497, 0.4886346757411957, 0.2571735382080078, 0.47802549600601196, 0.46205347776412964, 0.057531777769327164, 0.16597530245780945, 0.02152370847761631, 0.1526656448841095, 0.40647801756858826, 0.18339839577674866, 0.4968929886817932, 0.12808451056480408, 0.2232593148946762, 0.11296035349369049, 0.1292767971754074, 0.06102798506617546, 0.024595219641923904, 0.46244603395462036, 0.3136824667453766, 0.46862003207206726, 0.20383530855178833, 0.4438217878341675, 0.37141892313957214], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4551178514957428, 0.28544244170188904, 0.19045022130012512, 0.23933635652065277, 0.121292345225811, 0.16907085478305817, 0.08706432580947876, 0.08785044401884079, 0.28574079275131226, 0.31080180406570435, 0.34017214179039, 0.08865921944379807, 0.10045666992664337, 0.3220156133174896, 0.01496164407581091, 0.12483417242765427, 0.215279683470726, 0.31432223320007324, 0.24688394367694855, 0.27311477065086365, 0.48812809586524963, 0.30464375019073486, 0.02643115445971489, 0.13130436837673187], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_650e564858b03d74a0b2d1b4291ab2ad(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.05300386995077133, 0.44805482029914856, 0.37707167863845825, 0.4919348359107971, 0.19443050026893616, 0.09986371546983719, 0.2024303376674652, 0.015652822330594063, 0.3637419641017914, 0.254921019077301, 0.19345180690288544, 0.09290791302919388, 0.13839463889598846, 0.04834820702672005, 0.4224355220794678, 0.013557974249124527, 0.043878886848688126, 0.32604366540908813, 0.09432625770568848, 0.4184577465057373, 0.30596715211868286, 0.1200152337551117, 0.449313223361969, 0.40619492530822754, 0.1876814067363739, 0.3513278365135193, 0.13021239638328552, 0.11862531304359436, 0.10447470843791962, 0.16089992225170135], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3821217119693756, 0.19735966622829437, 0.20572854578495026, 0.2595403790473938, 0.21814754605293274, 0.19671018421649933, 0.05527108162641525, 0.2575327157974243, 0.0692816972732544, 0.0370759554207325, 0.16763803362846375, 0.24013714492321014, 0.267090380191803, 0.4103919565677643, 0.15432077646255493, 0.0021244576200842857, 0.45226818323135376, 0.24494041502475739, 0.049536410719156265, 0.41224783658981323, 0.2700177729129791, 0.1785164624452591, 0.035059694200754166, 0.14423014223575592, 0.4360829293727875, 0.37314173579216003, 0.40597161650657654, 0.2574782967567444, 0.2430959939956665, 0.09968926757574081], dtype='float32').reshape([30]),
            paddle.to_tensor([0.28434595465660095, 0.2887583076953888, 0.48426470160484314, 0.04905429854989052, 0.45909178256988525, 0.06351316720247269, 0.25048086047172546, 0.05629674345254898, 0.030049098655581474, 0.01591970957815647, 0.08895265311002731, 0.044278018176555634, 0.40148794651031494, 0.37194138765335083, 0.012273076921701431, 0.0408221073448658, 0.05293707922101021, 0.2795292139053345, 0.4973258674144745, 0.23053795099258423, 0.19285143911838531, 0.03648083657026291, 0.2898876965045929, 0.4986802935600281, 0.4089749753475189, 0.0035695822443813086, 0.015340548940002918, 0.2781420946121216, 0.3720240294933319, 0.02154175564646721], dtype='float32').reshape([30]),
            paddle.to_tensor([0.01342822052538395, 0.27952417731285095, 0.2656647264957428, 0.29435423016548157, 0.15488828718662262, 0.24584655463695526, 0.3799365758895874, 0.005782215856015682, 0.2338113635778427, 0.21314622461795807, 0.029105180874466896, 0.2573621869087219, 0.20046284794807434, 0.2780896723270416, 0.2678825855255127, 0.01684221252799034, 0.14547060430049896, 0.03633001074194908, 0.20258255302906036, 0.4908284842967987, 0.15546675026416779, 0.35737282037734985, 0.06527327746152878, 0.2332220822572708, 0.08504641056060791, 0.16530610620975494, 0.22345581650733948, 0.47010689973831177, 0.3047386705875397, 0.45885366201400757], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e98ca2b3f7fbfd2040a237455a4a77a6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7dbebcbbed3230f6a2407e4f14f0ecee(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.02862209640443325, 0.2357429563999176, 0.06444458663463593, 0.19988539814949036, 0.1345585286617279, 0.041081417351961136, 0.39590340852737427, 0.08395050466060638, 0.2690584361553192, 0.015404685400426388, 0.16165095567703247, 0.07644089311361313, 0.060858745127916336, 0.22641080617904663, 0.08223125338554382, 0.25630101561546326, 0.19098766148090363, 0.010294770821928978, 0.0302584245800972, 0.2193749099969864, 0.01862087845802307, 0.023500846698880196, 0.23527760803699493, 0.4095635712146759, 0.23647543787956238, 0.15560181438922882, 0.486395001411438, 0.1770869344472885], dtype='float32').reshape([28]),
            paddle.to_tensor([0.17854462563991547, 0.35488396883010864, 0.02755413018167019, 0.15518172085285187, 0.38752683997154236, 0.4872186779975891, 0.42323824763298035, 0.42012345790863037, 0.49538594484329224, 0.4425162672996521, 0.05936405807733536, 0.10043949633836746, 0.08474859595298767, 0.08054371923208237, 0.3166996240615845, 0.23838070034980774, 0.4373857378959656, 0.4730817675590515, 0.007760645821690559, 0.1571122705936432, 0.18737071752548218, 0.10571949183940887, 0.3772473633289337, 0.2152152955532074, 0.014440927654504776, 0.041702382266521454, 0.1112518161535263, 0.240387424826622], dtype='float32').reshape([28]),
            paddle.to_tensor([0.3000747561454773, 0.48728224635124207, 0.4564399719238281, 0.29482153058052063, 0.049531493335962296, 0.1707383692264557, 0.13622300326824188, 0.2623102068901062, 0.2772817313671112, 0.2143351435661316, 0.19588369131088257, 0.1080799326300621, 0.03705453500151634, 0.4823180139064789, 0.18507231771945953, 0.006259966176003218, 0.29885390400886536, 0.45451584458351135, 0.042503342032432556, 0.15004396438598633, 0.003924296237528324, 0.12292969971895218, 0.05317679047584534, 0.03953571617603302, 0.1518878936767578, 0.20684154331684113, 0.4635736346244812, 0.010877418331801891], dtype='float32').reshape([28]),
            paddle.to_tensor([0.41964977979660034, 0.27812129259109497, 0.13587942719459534, 0.41987934708595276, 0.38686445355415344, 0.34463706612586975, 0.3207756578922272, 0.44336965680122375, 0.4949394166469574, 0.19218936562538147, 0.012109042145311832, 0.22062405943870544, 0.2730357348918915, 0.04637014865875244, 0.21972155570983887, 0.12559349834918976, 0.4428226947784424, 0.15245017409324646, 0.4205878674983978, 0.103484608232975, 0.4348007142543793, 0.4725707471370697, 0.4621685743331909, 0.13341903686523438, 0.4128560721874237, 0.0934450700879097, 0.040678706020116806, 0.3441635072231293], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4fb2a7437e1344f05e67c61b6d4e9052(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 4, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.12500214576721191, 0.15937119722366333, 0.09811483323574066, 0.450875461101532], dtype='float32').reshape([4]),
            paddle.to_tensor([0.15262456238269806, 0.4391401708126068, 0.09145837277173996, 0.21192777156829834], dtype='float32').reshape([4]),
            paddle.to_tensor([0.16110940277576447, 0.10762476921081543, 0.3078920841217041, 0.3875574767589569], dtype='float32').reshape([4]),
            paddle.to_tensor([0.22182834148406982, 0.4080149233341217, 0.13292983174324036, 0.2921484112739563], dtype='float32').reshape([4]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0553766af004ab2053e7e77bda0f041d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 120, 120], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_55ea71e9885c1555403a7ad470b92623(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1872, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1872], dtype='float32', min=0, max=0.5),
            paddle.uniform([1872], dtype='float32', min=0, max=0.5),
            paddle.uniform([1872], dtype='float32', min=0, max=0.5),
            paddle.uniform([1872], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_03f0e9119729c32121674c2c8638a0ce(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.19216661155223846, 0.442065954208374, 0.18679626286029816, 0.4887293875217438, 0.02337569184601307, 0.08070161193609238, 0.4138333201408386, 0.43266987800598145, 0.0724508985877037, 0.20477046072483063, 0.38622355461120605, 0.4259287118911743, 0.42083868384361267, 0.4759860634803772, 0.09059500694274902, 0.15083245933055878], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3962118923664093, 0.2907028794288635, 0.4005204737186432, 0.16326642036437988, 0.22604580223560333, 0.057342518121004105, 0.017732204869389534, 0.021463455632328987, 0.08662929385900497, 0.1527019888162613, 0.4171351492404938, 0.12736152112483978, 0.34664443135261536, 0.1194169819355011, 0.24531003832817078, 0.28527945280075073], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4282192587852478, 0.09687643498182297, 0.36845943331718445, 0.3473086357116699, 0.1827961951494217, 0.21149124205112457, 0.2821044921875, 0.3088398277759552, 0.0624518021941185, 0.14499709010124207, 0.3436557650566101, 0.4006175994873047, 0.15762436389923096, 0.0053519075736403465, 0.44498035311698914, 0.4016993045806885], dtype='float32').reshape([16]),
            paddle.to_tensor([0.1883184164762497, 0.28020504117012024, 0.0458032488822937, 0.46000936627388, 0.023917624726891518, 0.25386500358581543, 0.2991625666618347, 0.05705021321773529, 0.11917603760957718, 0.07900574058294296, 0.49433138966560364, 0.32293227314949036, 0.35496172308921814, 0.2699199318885803, 0.36839422583580017, 0.08251667767763138], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_29ba421363074794880f1450daf40bdc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.47752493619918823, 0.40960049629211426, 0.4297265112400055, 0.22277070581912994, 0.19905075430870056, 0.4829905331134796, 0.1163838654756546, 0.01806327886879444, 0.27145180106163025, 0.34917914867401123, 0.3541397154331207, 0.34246835112571716, 0.1563810557126999, 0.28193992376327515, 0.11086951941251755, 0.49301716685295105], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2667732536792755, 0.4532015025615692, 0.021606849506497383, 0.06622231006622314, 0.01585337333381176, 0.39830881357192993, 0.20502513647079468, 0.4064653813838959, 0.23613858222961426, 0.04594454541802406, 0.32064956426620483, 0.21043196320533752, 0.4185836613178253, 0.2981560230255127, 0.3518519997596741, 0.03956761956214905], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2781763970851898, 0.016442740336060524, 0.39441072940826416, 0.2736133337020874, 0.22304116189479828, 0.4035690724849701, 0.428389310836792, 0.47520577907562256, 0.024485277011990547, 0.4633064568042755, 0.47948870062828064, 0.4346868097782135, 0.09491302818059921, 0.49696195125579834, 0.14191365242004395, 0.2805473208427429], dtype='float32').reshape([16]),
            paddle.to_tensor([0.43362680077552795, 0.19220051169395447, 0.11150042712688446, 0.08804447203874588, 0.20464788377285004, 0.3784356117248535, 0.0551019087433815, 0.411954402923584, 0.20996788144111633, 0.24206328392028809, 0.34325653314590454, 0.23591122031211853, 0.16190290451049805, 0.13586682081222534, 0.25971657037734985, 0.35561227798461914], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7d7fc51e65ccbd9a2c21e22a7c425b62(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 4, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d3f51daf20078d925a2b616f5f78870f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7cb3b90995db8331acc569a5556639d8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1296, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1296], dtype='float32', min=0, max=0.5),
            paddle.uniform([1296], dtype='float32', min=0, max=0.5),
            paddle.uniform([1296], dtype='float32', min=0, max=0.5),
            paddle.uniform([1296], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4ea669bf34021b132b3c36471aa0e272(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c77ea578c6f5b0d1e938ebb3ab598d17(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_145ad20e7a3b3c8aa31f6b40d4fe3d45
    def get_inputs(self):
        return [
            paddle.uniform([49, 1280], dtype='float16', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7a6f2111856c6a0ebb06dea327dcebba(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.46813562512397766, 0.15498198568820953, 0.219598650932312, 0.3924938142299652, 0.45776838064193726, 0.29997438192367554, 0.21556809544563293, 0.4414508640766144, 0.28812268376350403, 0.4071647524833679, 0.4848882257938385, 0.19771622121334076, 0.2082517296075821, 0.18009606003761292, 0.10371145606040955, 0.4642101526260376, 0.30104339122772217, 0.3239491581916809, 0.4802866280078888, 0.13853341341018677, 0.01987946592271328, 0.12634430825710297, 0.26134800910949707, 0.0717371329665184, 0.04857432469725609, 0.3172968327999115, 0.4716813266277313, 0.2606126368045807, 0.47394075989723206, 0.2415705919265747], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4458880126476288, 0.3670632243156433, 0.18361537158489227, 0.22606082260608673, 0.3185786008834839, 0.45862695574760437, 0.1211465522646904, 0.44052815437316895, 0.4670715630054474, 0.016504358500242233, 0.47988319396972656, 0.3289351165294647, 0.3563263416290283, 0.07388602197170258, 0.09574602544307709, 0.09992679953575134, 0.13896048069000244, 0.06822588294744492, 0.0020568016916513443, 0.1645859181880951, 0.4181567132472992, 0.11521081626415253, 0.07420452684164047, 0.2888129651546478, 0.31981492042541504, 0.08840038627386093, 0.39039936661720276, 0.37411805987358093, 0.24826593697071075, 0.3421567678451538], dtype='float32').reshape([30]),
            paddle.to_tensor([0.19427981972694397, 0.14473819732666016, 0.3524247407913208, 0.0022437258157879114, 0.3808214068412781, 0.4782400131225586, 0.35400259494781494, 0.22693465650081635, 0.4395173490047455, 0.17849436402320862, 0.2781895399093628, 0.11752799153327942, 0.14435121417045593, 0.10537078231573105, 0.43130964040756226, 0.19254666566848755, 0.262139230966568, 0.30701786279678345, 0.39793407917022705, 0.24551507830619812, 0.15343888103961945, 0.19116118550300598, 0.09077854454517365, 0.2034277766942978, 0.1039610430598259, 0.2591715455055237, 0.06735692173242569, 0.3175957500934601, 0.49972957372665405, 0.4626785218715668], dtype='float32').reshape([30]),
            paddle.to_tensor([0.36894747614860535, 0.06632307171821594, 0.16607803106307983, 0.44819989800453186, 0.050483591854572296, 0.11803776025772095, 0.2333647459745407, 0.3003743588924408, 0.27180930972099304, 0.1980522871017456, 0.2985289692878723, 0.21031254529953003, 0.358656108379364, 0.2687881290912628, 0.4269235134124756, 0.24277165532112122, 0.06378589570522308, 0.1377488374710083, 0.012867487035691738, 0.2935529351234436, 0.026611745357513428, 0.36322495341300964, 0.49490809440612793, 0.25399836897850037, 0.3916129469871521, 0.0710049569606781, 0.33404383063316345, 0.3170296251773834, 0.06022600457072258, 0.2796020805835724], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_59984a112d7c9fa1470b7f9be32bb7c5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 8, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_af7555471a5fa47dd9d2d25adeb36ed1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 30, 30], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_972534663799a28f3f84ce6eac5c4db2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e807c6e14ed741012fda1fd83392478f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 832, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([832], dtype='float32', min=0, max=0.5),
            paddle.uniform([832], dtype='float32', min=0, max=0.5),
            paddle.uniform([832], dtype='float32', min=0, max=0.5),
            paddle.uniform([832], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_230d2719f862e7eb2b0d6a05fd139f65(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 4, 3], dtype='float16', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0e1b9603e1e0c39c75f8ff2f25fd20a4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.012841269373893738, 0.23011407256126404, 0.16752073168754578, 0.08607427030801773, 0.0023212460801005363, 0.37139594554901123, 0.26471683382987976, 0.30654048919677734, 0.10010543465614319, 0.31050392985343933, 0.4354132413864136, 0.0690077692270279, 0.33711349964141846, 0.2193729132413864, 0.2505956292152405, 0.2718788683414459, 0.1746131181716919, 0.2784681022167206, 0.021894413977861404, 0.21139837801456451], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4517415761947632, 0.23471590876579285, 0.21547268331050873, 0.004682464525103569, 0.4190027415752411, 0.04059873893857002, 0.46352118253707886, 0.316853791475296, 0.42904824018478394, 0.0547090508043766, 0.39627668261528015, 0.13024799525737762, 0.45109137892723083, 0.025510938838124275, 0.029943039640784264, 0.2131657898426056, 0.00257564103230834, 0.46517878770828247, 0.43291324377059937, 0.11210157722234726], dtype='float32').reshape([20]),
            paddle.to_tensor([0.16016113758087158, 0.2864055633544922, 0.4576023519039154, 0.33838117122650146, 0.06182584539055824, 0.32692795991897583, 0.3706357181072235, 0.4048711955547333, 0.36487704515457153, 0.4107988476753235, 0.12133995443582535, 0.4170701205730438, 0.03632703423500061, 0.3140888512134552, 0.44652560353279114, 0.4456273019313812, 0.007110415026545525, 0.24032950401306152, 0.3032089173793793, 0.26319390535354614], dtype='float32').reshape([20]),
            paddle.to_tensor([0.14878493547439575, 0.06249377503991127, 0.03038272075355053, 0.49731066823005676, 0.09536141157150269, 0.0060660745948553085, 0.273011714220047, 0.11502877622842789, 0.24645237624645233, 0.044287748634815216, 0.15087270736694336, 0.1415381133556366, 0.2804888188838959, 0.20392493903636932, 0.3020695447921753, 0.128024622797966, 0.382179856300354, 0.24240663647651672, 0.4400545656681061, 0.21697354316711426], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9d50efd64a22cf30ab2e30245b155c69(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c39274b51611c3487a46588573ee57c5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_57d3d4d9c33460df5b5c5150bdc26f8a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.010602354072034359, 0.17633657157421112, 0.028337199240922928, 0.22427624464035034, 0.19178766012191772, 0.40379148721694946, 0.43809786438941956, 0.3751097023487091, 0.036964334547519684, 0.34819695353507996, 0.04778497666120529, 0.37902089953422546, 0.03892780467867851, 0.25821343064308167, 0.07940446585416794, 0.36328399181365967], dtype='float32').reshape([16]),
            paddle.to_tensor([0.33578020334243774, 0.45994505286216736, 0.398135244846344, 0.4057665765285492, 0.4053908884525299, 0.2479114979505539, 0.19579558074474335, 0.21863214671611786, 0.2073570340871811, 0.15134276449680328, 0.3069952428340912, 0.029734406620264053, 0.4792526364326477, 0.18123237788677216, 0.47515735030174255, 0.38992226123809814], dtype='float32').reshape([16]),
            paddle.to_tensor([0.21750874817371368, 0.188412606716156, 0.2937774062156677, 0.2709735631942749, 0.37434127926826477, 0.33981314301490784, 0.22868920862674713, 0.12028825283050537, 0.2601124048233032, 0.2193428874015808, 0.10016026347875595, 0.49873340129852295, 0.3875599205493927, 0.08673510700464249, 0.1734178513288498, 0.4083656370639801], dtype='float32').reshape([16]),
            paddle.to_tensor([0.18669572472572327, 0.03350319713354111, 0.34406349062919617, 0.292448490858078, 0.27969062328338623, 0.008128117769956589, 0.08872505277395248, 0.2765377163887024, 0.4335760474205017, 0.205070361495018, 0.33043423295021057, 0.22539766132831573, 0.24274912476539612, 0.06635814905166626, 0.2988666892051697, 0.23050661385059357], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_618426c1ee0a4782a9c8da27fe24310d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d093bb5b3176553b1ae4908ad48037c3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_9d5dde09230a5209a57565a3b7304f49(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_95e811825649fd76b0bef5efe3ef21de(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_9d5dde09230a5209a57565a3b7304f49
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 49], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_19bddc23a2b77d1fe640a3f162fe3a88(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 19, 19], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ec492dbf540096e2ceb232baab76a4c2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.25065210461616516, 0.3284100294113159, 0.10985023528337479, 0.42515039443969727, 0.19186827540397644, 0.072844497859478, 0.30763542652130127, 0.06588686257600784, 0.4447804391384125, 0.13781329989433289, 0.4867425262928009, 0.06332120299339294, 0.4563586413860321, 0.48765721917152405, 0.04280857369303703, 0.4894305467605591, 0.22749008238315582, 0.3528677225112915, 0.060402654111385345, 0.40473470091819763, 0.11177269369363785, 0.3993442952632904, 0.10091368854045868, 0.2510700821876526], dtype='float32').reshape([24]),
            paddle.to_tensor([0.14409339427947998, 0.08505522459745407, 0.11363306641578674, 0.2360047847032547, 0.4462186396121979, 0.17532230913639069, 0.22541682422161102, 0.22261153161525726, 0.16218455135822296, 0.09016971290111542, 0.4148802161216736, 0.48048537969589233, 0.19322676956653595, 0.12230382114648819, 0.16677924990653992, 0.3559323847293854, 0.2863028943538666, 0.289089560508728, 0.06796076148748398, 0.4753221273422241, 0.34674957394599915, 0.13635174930095673, 0.1188032403588295, 0.05729512497782707], dtype='float32').reshape([24]),
            paddle.to_tensor([0.16576726734638214, 0.33013349771499634, 0.41440847516059875, 0.40709754824638367, 0.38873666524887085, 0.31547674536705017, 0.24144315719604492, 0.016324300318956375, 0.07133002579212189, 0.28795966506004333, 0.21242305636405945, 0.3059830665588379, 0.09409354627132416, 0.044324636459350586, 0.035963572561740875, 0.49657541513442993, 0.40746116638183594, 0.16636337339878082, 0.21273016929626465, 0.27252474427223206, 0.053096041083335876, 0.04110041633248329, 0.31108951568603516, 0.37607553601264954], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3647152781486511, 0.4770892262458801, 0.18375475704669952, 0.10954684764146805, 0.04950663074851036, 0.06875188648700714, 0.24801571667194366, 0.09922223538160324, 0.08451525121927261, 0.3361044228076935, 0.4386906325817108, 0.3610130548477173, 0.14320744574069977, 0.06703083217144012, 0.1731927990913391, 0.11239071935415268, 0.33249592781066895, 0.3447772264480591, 0.39458516240119934, 0.06591885536909103, 0.4669700562953949, 0.23449048399925232, 0.010612249374389648, 0.24690966308116913], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a2de474ff22cd1ba05d164d0b9d0a341(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1824, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1824], dtype='float32', min=0, max=0.5),
            paddle.uniform([1824], dtype='float32', min=0, max=0.5),
            paddle.uniform([1824], dtype='float32', min=0, max=0.5),
            paddle.uniform([1824], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_77eb72090d9031d6a9ec5700f4d325b9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d50e16b59f4aff0a851efd3ea18258fa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 1, 26], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_34519629d2b01e9728f353054bfb1426(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0b1a882a6be9d3c1225b20b65ccfe938(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 64, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1e966303f32aa257387f5c5bb15c2710(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3388795554637909, 0.43316754698753357, 0.35930559039115906, 0.05696140229701996, 0.45549654960632324, 0.3200497031211853, 0.15826715528964996, 0.4898972511291504, 0.3961392343044281, 0.015581161715090275, 0.48800477385520935, 0.13814868032932281, 0.0625726729631424, 0.41706329584121704, 0.33031371235847473, 0.11599899083375931, 0.4652545154094696, 0.287346214056015, 0.12086229026317596, 0.26187247037887573, 0.21373850107192993, 0.40326371788978577, 0.06613022089004517, 0.015078564174473286], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4112892150878906, 0.43830373883247375, 0.47930052876472473, 0.3443506062030792, 0.2876014709472656, 0.22431010007858276, 0.4914844036102295, 0.13323795795440674, 0.24552829563617706, 0.10694512724876404, 0.21177056431770325, 0.2132318615913391, 0.4350847601890564, 0.2780388593673706, 0.2151053249835968, 0.0644945278763771, 0.03754067420959473, 0.2949206233024597, 0.46031200885772705, 0.4306054413318634, 0.007871373556554317, 0.006325509864836931, 0.32385483384132385, 0.20659229159355164], dtype='float32').reshape([24]),
            paddle.to_tensor([0.23646380007266998, 0.2793128490447998, 0.4097123444080353, 0.474921852350235, 0.25390613079071045, 0.46251973509788513, 0.47846707701683044, 0.13302816450595856, 0.03363843634724617, 0.18141818046569824, 0.40014955401420593, 0.275439977645874, 0.43796050548553467, 0.4135667085647583, 0.26895350217819214, 0.42950335144996643, 0.21492734551429749, 0.040445610880851746, 0.18708118796348572, 0.262674480676651, 0.13239316642284393, 0.2532801628112793, 0.1925482600927353, 0.46861428022384644], dtype='float32').reshape([24]),
            paddle.to_tensor([0.1600538045167923, 0.11584051698446274, 0.2930136024951935, 0.3990907669067383, 0.3199382722377777, 0.26860636472702026, 0.45459210872650146, 0.02204064466059208, 0.07686101645231247, 0.3004034757614136, 0.4623197913169861, 0.2956424355506897, 0.46770331263542175, 0.33623823523521423, 0.08893228322267532, 0.4177589416503906, 0.1788700520992279, 0.44039276242256165, 0.42638981342315674, 0.18911218643188477, 0.150140181183815, 0.23790328204631805, 0.0001354398555122316, 0.14808721840381622], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a9d995cd107bf2c85f74ba83b519d991(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_746b018111f4c2dde21b76990c15f2d0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 320, 320], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.06924660503864288, 0.3955422639846802, 0.225335955619812, 0.2451748549938202, 0.3795120120048523, 0.22734752297401428, 0.12514834105968475, 0.47673892974853516], dtype='float32').reshape([8]),
            paddle.to_tensor([0.11597831547260284, 0.13596051931381226, 0.005309592001140118, 0.18060268461704254, 0.4389170706272125, 0.16944408416748047, 0.20013630390167236, 0.23309265077114105], dtype='float32').reshape([8]),
            paddle.to_tensor([0.37403419613838196, 0.3182019889354706, 0.1764608472585678, 0.059281233698129654, 0.3064245283603668, 0.49466773867607117, 0.36096885800361633, 0.47877010703086853], dtype='float32').reshape([8]),
            paddle.to_tensor([0.162317156791687, 0.28286921977996826, 0.45135384798049927, 0.11338310688734055, 0.03357014060020447, 0.36102741956710815, 0.05006973817944527, 0.011483141221106052], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d4800b31909582e10a9f4eb4f8bf1014(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2441697120666504, 0.3567935824394226, 0.27189087867736816, 0.3237172067165375, 0.14670425653457642, 0.055298954248428345, 0.21142610907554626, 0.3886651396751404, 0.03938595578074455, 0.40689778327941895, 0.3972180187702179, 0.32812824845314026, 0.397928386926651, 0.0579303503036499, 0.4743598997592926, 0.4870418608188629, 0.33940210938453674, 0.2226998656988144], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4388028681278229, 0.4127487242221832, 0.28612855076789856, 0.056374091655015945, 0.25131070613861084, 0.3337872624397278, 0.19175808131694794, 0.12083075940608978, 0.17548657953739166, 0.33916589617729187, 0.4107542634010315, 0.234847292304039, 0.42556288838386536, 0.4115009605884552, 0.06331590563058853, 0.3917120099067688, 0.49790048599243164, 0.13242831826210022], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2399422973394394, 0.2763650715351105, 0.19833876192569733, 0.48974350094795227, 0.47397732734680176, 0.43838727474212646, 0.25783100724220276, 0.45312076807022095, 0.19865210354328156, 0.25644028186798096, 0.46950626373291016, 0.06397909671068192, 0.42228391766548157, 0.3622452914714813, 0.1373644769191742, 0.06120360642671585, 0.3376251459121704, 0.09058290719985962], dtype='float32').reshape([18]),
            paddle.to_tensor([0.11159517616033554, 0.3162076473236084, 0.16593855619430542, 0.23140175640583038, 0.4632205367088318, 0.1999455839395523, 0.03137417882680893, 0.3136556148529053, 0.050604335963726044, 0.3758532702922821, 0.20249734818935394, 0.13497334718704224, 0.33701077103614807, 0.003872114000841975, 0.16127248108386993, 0.06927712261676788, 0.06843988597393036, 0.22406113147735596], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_88139ea8d86d2d77eef6165dbe4da8f7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.17230218648910522, 0.3138042390346527, 0.2977410852909088, 0.45613786578178406, 0.00603731581941247, 0.07558091729879379, 0.4644564688205719, 0.08851984143257141], dtype='float32').reshape([8]),
            paddle.to_tensor([0.3216527998447418, 0.03167492896318436, 0.16056691110134125, 0.1940210908651352, 0.18948248028755188, 0.4880235195159912, 0.4346088767051697, 0.2998039424419403], dtype='float32').reshape([8]),
            paddle.to_tensor([0.4248977303504944, 0.1405440717935562, 0.12724654376506805, 0.23638004064559937, 0.010244008153676987, 0.0727856382727623, 0.06247173622250557, 0.4354969561100006], dtype='float32').reshape([8]),
            paddle.to_tensor([0.2425106167793274, 0.48841753602027893, 0.4544825851917267, 0.28853997588157654, 0.2242748886346817, 0.27965816855430603, 0.039294201880693436, 0.46887412667274475], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3f1af0edc55578d283898971c75ec701(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bd78e080520db67791c9f82f48afc04c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3976c4eb08613cad08878c78a534385f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e8f2d39d1442385cb29afde2e9e7cbf4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 200, 336], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_303e28bf8daa20964b62c99c19c50cd4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c53ef2a043aeaded99c3650289c6624f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4256795644760132, 0.2526317536830902, 0.46322882175445557, 0.39710259437561035, 0.003130383789539337, 0.2000572830438614, 0.3954034745693207, 0.26862433552742004, 0.2263014018535614, 0.358694851398468, 0.4036516845226288, 0.4102773070335388, 0.011589663103222847, 0.24495142698287964, 0.01800030842423439, 0.1556219756603241, 0.31951773166656494, 0.10179546475410461], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3878733813762665, 0.4233708679676056, 0.05548648163676262, 0.0458989143371582, 0.23417040705680847, 0.05471925064921379, 0.027229685336351395, 0.4382250905036926, 0.25835996866226196, 0.48931658267974854, 0.38165283203125, 0.021247658878564835, 0.31586676836013794, 0.49358493089675903, 0.12808158993721008, 0.3495149612426758, 0.4853236675262451, 0.2452852874994278], dtype='float32').reshape([18]),
            paddle.to_tensor([0.26746585965156555, 0.36649781465530396, 0.2744845151901245, 0.41843292117118835, 0.19411277770996094, 0.01651616394519806, 0.1900944858789444, 0.3054395616054535, 0.38867607712745667, 0.1690821647644043, 0.32463058829307556, 0.42598938941955566, 0.07411102950572968, 0.4408389925956726, 0.2943194508552551, 0.4541589915752411, 0.10464067757129669, 0.14729832112789154], dtype='float32').reshape([18]),
            paddle.to_tensor([0.16861891746520996, 0.3312654197216034, 0.11436948925256729, 0.3224509060382843, 0.3155380189418793, 0.1270727664232254, 0.39772942662239075, 0.20293742418289185, 0.15905146300792694, 0.10112165659666061, 0.1889956146478653, 0.44230058789253235, 0.48713502287864685, 0.4963810443878174, 0.4935525953769684, 0.42320629954338074, 0.08066485822200775, 0.4161730110645294], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_54a62a2b6084718ed110d0fd0c7d4061(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4d7b7d81a837269c3e4ec09bb17ee163(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_df61c1dec3d44b2fc02aae8842728722(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 2, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4301975667476654, 0.11383649706840515], dtype='float32').reshape([2]),
            paddle.to_tensor([0.23133796453475952, 0.39996466040611267], dtype='float32').reshape([2]),
            paddle.to_tensor([0.15506352484226227, 0.14785891771316528], dtype='float32').reshape([2]),
            paddle.to_tensor([0.05091734230518341, 0.38045793771743774], dtype='float32').reshape([2]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f87edcb9bea35e0a2bc6fe97dee4ceea(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.33237937092781067, 0.47941482067108154, 0.10654833167791367, 0.3254301846027374, 0.4173491299152374, 0.2925291657447815, 0.2050013691186905, 0.1449204981327057, 0.34291499853134155, 0.43714070320129395, 0.42677873373031616, 0.38584521412849426, 0.06901988387107849, 0.443445086479187, 0.07485093176364899, 0.17361271381378174, 0.38526445627212524, 0.36355939507484436, 0.2248384952545166, 0.20303739607334137, 0.44628721475601196, 0.4855528473854065, 0.1529981940984726, 0.011171717196702957, 0.40875187516212463, 0.21674449741840363, 0.4338189959526062, 0.43592220544815063, 0.22722359001636505, 0.31870245933532715], dtype='float32').reshape([30]),
            paddle.to_tensor([0.05794094130396843, 0.3979818522930145, 0.33887794613838196, 0.3360941410064697, 0.40329509973526, 0.4453009068965912, 0.35691291093826294, 0.09001276642084122, 0.34228166937828064, 0.08077386766672134, 0.020681282505393028, 0.4803659915924072, 0.40845853090286255, 0.4821563959121704, 0.20967410504817963, 0.004611694719642401, 0.002012243028730154, 0.142318457365036, 0.07753165811300278, 0.3324059247970581, 0.21351632475852966, 0.17685912549495697, 0.02157656103372574, 0.13865630328655243, 0.1214371845126152, 0.3792242407798767, 0.0400654561817646, 0.4132317006587982, 0.1623697578907013, 0.16901470720767975], dtype='float32').reshape([30]),
            paddle.to_tensor([0.1772487461566925, 0.10190277546644211, 0.02625274658203125, 0.30251556634902954, 0.48338404297828674, 0.3400510549545288, 0.36154353618621826, 0.2671535015106201, 0.13385134935379028, 0.040682319551706314, 0.18739522993564606, 0.2210693657398224, 0.4658098816871643, 0.39710986614227295, 0.09068816900253296, 0.2960607707500458, 0.26724913716316223, 0.3479069471359253, 0.3642345070838928, 0.4459049105644226, 0.045468784868717194, 0.4888477325439453, 0.3855280876159668, 0.2529626786708832, 0.03262120857834816, 0.02764788642525673, 0.41434770822525024, 0.12874431908130646, 0.3845362961292267, 0.4584288001060486], dtype='float32').reshape([30]),
            paddle.to_tensor([0.22444452345371246, 0.31154102087020874, 0.46418851613998413, 0.4718688130378723, 0.1724209487438202, 0.4430682361125946, 0.3772128224372864, 0.05126302316784859, 0.2153414785861969, 0.1725262552499771, 0.3502499461174011, 0.4394969642162323, 0.27031978964805603, 0.030317489057779312, 0.004677135031670332, 0.4437238276004791, 0.015715807676315308, 0.465694785118103, 0.2482301890850067, 0.12699376046657562, 0.318058043718338, 0.0332987979054451, 0.1745569109916687, 0.3348392844200134, 0.008282024413347244, 0.3244074881076813, 0.3945680856704712, 0.44853347539901733, 0.2715417146682739, 0.23539380729198456], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bd8f33fd506f7f76fe0e0fb18663eee8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 960, 32, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bb66abaf33964acf8d2499cd219c5e82(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 19, 19], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_847ccdfb38fbdcf20620631f1e4cc076(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_0edd578f48a07aa5d1d79f9f678b472e
    def get_inputs(self):
        return [
            paddle.uniform([1, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_457e6d15decd13cffd7eb24d8e667f0e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 100, 168], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5ad62675175507f7bde17294093b834a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 32, 64], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.1897599846124649, 0.3951004147529602, 0.0179421566426754, 0.1262272596359253, 0.18811549246311188, 0.4076215922832489, 0.22896002233028412, 0.45381101965904236, 0.3790205419063568, 0.32582035660743713, 0.06360764801502228, 0.13419200479984283, 0.04356992989778519, 0.26902180910110474, 0.13960842788219452, 0.20547625422477722, 0.25946298241615295, 0.21796223521232605], dtype='float32').reshape([18]),
            paddle.to_tensor([0.43442559242248535, 0.0027092299424111843, 0.05418245866894722, 0.4340568482875824, 0.3756723403930664, 0.2579084038734436, 0.379558801651001, 0.26051953434944153, 0.3410758078098297, 0.13322141766548157, 0.2790173590183258, 0.033159490674734116, 0.31736597418785095, 0.37808188796043396, 0.3623403012752533, 0.02054479718208313, 0.23551024496555328, 0.34084174036979675], dtype='float32').reshape([18]),
            paddle.to_tensor([0.28895509243011475, 0.13246819376945496, 0.0038357307203114033, 0.4221310019493103, 0.06542667001485825, 0.41754692792892456, 0.029151951894164085, 0.14219778776168823, 0.14563769102096558, 0.1637209951877594, 0.011945012025535107, 0.0451454259455204, 0.19935494661331177, 0.3787517845630646, 0.4864235520362854, 0.40744996070861816, 0.08362134546041489, 0.03132934868335724], dtype='float32').reshape([18]),
            paddle.to_tensor([0.49751487374305725, 0.020387884229421616, 0.24546019732952118, 0.2939421832561493, 0.23114587366580963, 0.0017989384941756725, 0.41636064648628235, 0.49924248456954956, 0.342769056558609, 0.4597424566745758, 0.10853667557239532, 0.4460025131702423, 0.09211057424545288, 0.15468889474868774, 0.46509629487991333, 0.37296757102012634, 0.03078458458185196, 0.14309675991535187], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3c7a8ffd1ead8988bc507c4f9f823003(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 4, 12], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.1883576214313507, 0.33236637711524963, 0.020704111084342003, 0.1780894249677658, 0.26515090465545654, 0.36392742395401, 0.4388059079647064, 0.10615716874599457, 0.29668495059013367, 0.013330847024917603, 0.08160239458084106, 0.27623751759529114, 0.4253883361816406, 0.3817481994628906, 0.2513463795185089, 0.32176393270492554], dtype='float32').reshape([16]),
            paddle.to_tensor([0.43575429916381836, 0.15217100083827972, 0.3561353087425232, 0.48491862416267395, 0.13596683740615845, 0.2785923182964325, 0.11789599061012268, 0.20170600712299347, 0.2781524360179901, 0.41323086619377136, 0.2799609303474426, 0.1902720034122467, 0.056343868374824524, 0.34885701537132263, 0.3166376054286957, 0.15435513854026794], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2164180427789688, 0.19787971675395966, 0.041126858443021774, 0.044339846819639206, 0.08824742585420609, 0.40076014399528503, 0.0010275044478476048, 0.05319555103778839, 0.13631467521190643, 0.21700023114681244, 0.2647758722305298, 0.137870192527771, 0.49424096941947937, 0.39755159616470337, 0.2725406587123871, 0.2566292881965637], dtype='float32').reshape([16]),
            paddle.to_tensor([0.12105157971382141, 0.40375733375549316, 0.35793787240982056, 0.3663594424724579, 0.4048117697238922, 0.37225788831710815, 0.06444110721349716, 0.1958465874195099, 0.1469200998544693, 0.4764691889286041, 0.18842434883117676, 0.014274329878389835, 0.31848713755607605, 0.08325157314538956, 0.3475590944290161, 0.3251554071903229], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b94bde94e4209f64018ef01420864fda(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4426807165145874, 0.2677467465400696, 0.41010454297065735, 0.1950421780347824, 0.17248214781284332, 0.013268769718706608, 0.4838275611400604, 0.336325466632843, 0.2493312656879425, 0.11623307317495346, 0.42165061831474304, 0.4144043028354645, 0.13557757437229156, 0.23880834877490997, 0.02466518059372902, 0.13952742516994476, 0.060275599360466, 0.3539149761199951, 0.41954144835472107, 0.39420291781425476], dtype='float32').reshape([20]),
            paddle.to_tensor([0.18052314221858978, 0.22778353095054626, 0.42957019805908203, 0.31081074476242065, 0.4769696295261383, 0.32859620451927185, 0.22971399128437042, 0.029966803267598152, 0.24275445938110352, 0.06408441811800003, 0.4437749981880188, 0.07141292095184326, 0.30991676449775696, 0.3681947588920593, 0.28204700350761414, 0.3155456483364105, 0.2023445963859558, 0.1930759996175766, 0.16834969818592072, 0.01607736013829708], dtype='float32').reshape([20]),
            paddle.to_tensor([0.1981266289949417, 0.0048752729780972, 0.07488278299570084, 0.17532075941562653, 0.21415138244628906, 0.4916374981403351, 0.3153528869152069, 0.14384831488132477, 0.2810685336589813, 0.46207836270332336, 0.22809912264347076, 0.06701647490262985, 0.32634076476097107, 0.09519822895526886, 0.22949336469173431, 0.3102864623069763, 0.18831674754619598, 0.08326584100723267, 0.10427260398864746, 0.08372016996145248], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3988977074623108, 0.49620428681373596, 0.4436032474040985, 0.054059240967035294, 0.47531014680862427, 0.4373720586299896, 0.3461393415927887, 0.16396760940551758, 0.4268341064453125, 0.41794630885124207, 0.0011731539852917194, 0.018002791330218315, 0.2458716183900833, 0.17050418257713318, 0.32116249203681946, 0.27326449751853943, 0.3687642812728882, 0.21293911337852478, 0.15218059718608856, 0.30445119738578796], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7d2fa8f3ce84f40cb62becb6003b49ba(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 16, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_24f23bf3e825e53c0f9a189bdae892ea(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 528, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([528], dtype='float32', min=0, max=0.5),
            paddle.uniform([528], dtype='float32', min=0, max=0.5),
            paddle.uniform([528], dtype='float32', min=0, max=0.5),
            paddle.uniform([528], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9c52fb9aa9c59110f4b270132ac8e55e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_09e7061f89f8e6e5faa8aa2d08e3358e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_50d6ce2427ec9c3f1d9a88519a9ee400(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 16, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_722f2cb8b98ab9ec4cbb829019bd6fe9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 32, 64], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3475712537765503, 0.4238034784793854, 0.45694273710250854, 0.26372212171554565, 0.219978466629982, 0.3508472740650177, 0.27393606305122375, 0.3470025658607483, 0.2909359037876129, 0.4382112920284271, 0.3650854229927063, 0.11416681110858917, 0.04862478747963905, 0.43169671297073364, 0.4841020107269287, 0.3953784704208374, 0.2277955859899521, 0.2764706611633301], dtype='float32').reshape([18]),
            paddle.to_tensor([0.11476490646600723, 0.3749507665634155, 0.03638804331421852, 0.0787816196680069, 0.009155163541436195, 0.3883898854255676, 0.12358985096216202, 0.02854510024189949, 0.26276499032974243, 0.014823663048446178, 0.09222110360860825, 0.2116813361644745, 0.4051464796066284, 0.37027686834335327, 0.498852014541626, 0.10884810984134674, 0.37463998794555664, 0.42040157318115234], dtype='float32').reshape([18]),
            paddle.to_tensor([0.024201462045311928, 0.24707697331905365, 0.14299197494983673, 0.05597473308444023, 0.41706445813179016, 0.16158051788806915, 0.25560465455055237, 0.31093713641166687, 0.1419541984796524, 0.45221206545829773, 0.2767394185066223, 0.11865907907485962, 0.15926122665405273, 0.0678308829665184, 0.2526954412460327, 0.21422681212425232, 0.07746311277151108, 0.42029696702957153], dtype='float32').reshape([18]),
            paddle.to_tensor([0.27801668643951416, 0.2421480268239975, 0.4722391366958618, 0.060467302799224854, 0.07223276793956757, 0.03632473200559616, 0.40278589725494385, 0.03308947756886482, 0.259393572807312, 0.2611452341079712, 0.2072763741016388, 0.15732622146606445, 0.13218386471271515, 0.020596178248524666, 0.33279669284820557, 0.35206812620162964, 0.40073826909065247, 0.4165421426296234], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2b7f07d774f253a3746276b272567692(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.14983651041984558, 0.13013088703155518, 0.2310158759355545, 0.43329712748527527, 0.28867366909980774, 0.4998684823513031, 0.2967994213104248, 0.0125530194491148, 0.345813125371933, 0.040429942309856415, 0.25044524669647217, 0.4049381613731384, 0.4513874053955078, 0.2092527598142624, 0.30426308512687683, 0.2921818196773529, 0.27583086490631104, 0.41720321774482727, 0.345204621553421, 0.4676196575164795, 0.12870703637599945, 0.4542393088340759, 0.4400501549243927, 0.03500792756676674, 0.4889652132987976, 0.13082651793956757, 0.02695958875119686, 0.4067475497722626, 0.09709935635328293, 0.15714092552661896], dtype='float32').reshape([30]),
            paddle.to_tensor([0.097419872879982, 0.007149737328290939, 0.019525550305843353, 0.2520202696323395, 0.25927498936653137, 0.19943144917488098, 0.3498378098011017, 0.07529878616333008, 0.16316525638103485, 0.12192410230636597, 0.2664985656738281, 0.055542346090078354, 0.3509034812450409, 0.4356270432472229, 0.356585830450058, 0.4790288209915161, 0.11939764022827148, 0.02480459026992321, 0.09738129377365112, 0.14142270386219025, 0.06555089354515076, 0.15475204586982727, 0.3981194794178009, 0.2076597660779953, 0.11732632666826248, 0.3774050772190094, 0.34076976776123047, 0.12900637090206146, 0.12891507148742676, 0.4281197786331177], dtype='float32').reshape([30]),
            paddle.to_tensor([0.003685424104332924, 0.17768597602844238, 0.021198362112045288, 0.07049763947725296, 0.4322519898414612, 0.20848767459392548, 0.2282279133796692, 0.16301874816417694, 0.0038280945736914873, 0.16567304730415344, 0.3091700077056885, 0.16160938143730164, 0.4167570173740387, 0.24744895100593567, 0.4869498312473297, 0.2641389071941376, 0.1253374069929123, 0.4356008768081665, 0.028078950941562653, 0.05676141008734703, 0.0676378458738327, 0.37550073862075806, 0.26153334975242615, 0.483536958694458, 0.06579618155956268, 0.47435587644577026, 0.1750989854335785, 0.040069352835416794, 0.4706187844276428, 0.009522088803350925], dtype='float32').reshape([30]),
            paddle.to_tensor([0.09208782762289047, 0.41060101985931396, 0.45723316073417664, 0.2354375571012497, 0.18489067256450653, 0.07381892949342728, 0.016635874286293983, 0.13916729390621185, 0.3282584547996521, 0.18522395193576813, 0.288467139005661, 0.3353772759437561, 0.05976181477308273, 0.4986996352672577, 0.4962969422340393, 0.17314474284648895, 0.19255836308002472, 0.4557264745235443, 0.4058198630809784, 0.24733532965183258, 0.11006391048431396, 0.19234108924865723, 0.39485400915145874, 0.22975006699562073, 0.47832217812538147, 0.04168464243412018, 0.19891130924224854, 0.36814653873443604, 0.19931383430957794, 0.2960839867591858], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_80a922d01c51b42428c4a611167a7986(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 16, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6342ea3e755cb1d8e1411dc21aeb04f0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 32, 100], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ec82175f2fe8c89e54a7aa6733150e57(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_f5f2213be92f20fda5378beeed72688f
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cea755f1cc0433dc592f68439bbd5e53(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.27563321590423584, 0.03368476778268814, 0.40424180030822754, 0.15632911026477814, 0.34466353058815, 0.20483766496181488, 0.2789612412452698, 0.46951428055763245, 0.010175219736993313, 0.27038222551345825, 0.15650151669979095, 0.11340408772230148, 0.3670556843280792, 0.4158352315425873, 0.02097151055932045, 0.09140384942293167, 0.2319619357585907, 0.034556347876787186, 0.29941341280937195, 0.052919838577508926], dtype='float32').reshape([20]),
            paddle.to_tensor([0.10295622050762177, 0.21195384860038757, 0.050651952624320984, 0.23685681819915771, 0.15471722185611725, 0.2566218674182892, 0.189963698387146, 0.37301549315452576, 0.20885688066482544, 0.2252049446105957, 0.246944859623909, 0.4592978060245514, 0.2146574854850769, 0.13425852358341217, 0.12690739333629608, 0.1759076714515686, 0.3631608188152313, 0.03311344236135483, 0.44596660137176514, 0.164566770195961], dtype='float32').reshape([20]),
            paddle.to_tensor([0.27320119738578796, 0.4229142963886261, 0.18214261531829834, 0.2079571634531021, 0.3238223195075989, 0.09052626043558121, 0.17857195436954498, 0.10120541602373123, 0.2344764769077301, 0.42226889729499817, 0.025447838008403778, 0.03262149915099144, 0.2351139485836029, 0.0386342778801918, 0.3006305396556854, 0.21458162367343903, 0.2877315878868103, 0.31819605827331543, 0.24994519352912903, 0.10232242196798325], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4001252055168152, 0.18419750034809113, 0.1158432587981224, 0.08552126586437225, 0.09059213846921921, 0.31022027134895325, 0.4661601781845093, 0.1381923258304596, 0.045536208897829056, 0.40988531708717346, 0.32545822858810425, 0.4931485950946808, 0.3745570778846741, 0.308554470539093, 0.27881625294685364, 0.22787900269031525, 0.40988481044769287, 0.36536741256713867, 0.2874665856361389, 0.2522886395454407], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d681682d8cd99b38c1cfe3605e3c21dd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 19, 19], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1eb19d705202bf9fdff1b3ebac70c482(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2534830868244171, 0.10740629583597183, 0.2273891717195511, 0.06858754903078079, 0.36194750666618347, 0.19181299209594727, 0.36005493998527527, 0.38210728764533997, 0.39427900314331055, 0.48546749353408813, 0.08560243993997574, 0.19262447953224182, 0.007577856071293354, 0.29125988483428955, 0.25460848212242126, 0.2562706172466278, 0.3651820123195648, 0.3385864496231079, 0.3604726493358612, 0.12494910508394241, 0.10318457335233688, 0.13916702568531036, 0.11811240762472153, 0.13190019130706787, 0.13006262481212616, 0.24699260294437408, 0.3517218828201294, 0.31118568778038025, 0.22213901579380035, 0.382507860660553], dtype='float32').reshape([30]),
            paddle.to_tensor([0.055999454110860825, 0.3650344908237457, 0.340900719165802, 0.12098734825849533, 0.06522105634212494, 0.09421826899051666, 0.014023327268660069, 0.16243715584278107, 0.24327068030834198, 0.0125197134912014, 0.25403621792793274, 0.05728420242667198, 0.10336548089981079, 0.06380023807287216, 0.05304291471838951, 0.04523627832531929, 0.3464124798774719, 0.2488352656364441, 0.306416392326355, 0.06332454830408096, 0.22280655801296234, 0.3699707090854645, 0.2622967064380646, 0.1548880934715271, 0.09799784421920776, 0.4700636565685272, 0.08477289229631424, 0.27493518590927124, 0.1108289361000061, 0.0892106220126152], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2230343222618103, 0.1498640775680542, 0.03669535368680954, 0.21732720732688904, 0.15206503868103027, 0.41325852274894714, 0.1250065565109253, 0.01373014971613884, 0.40177661180496216, 0.3514028787612915, 0.39747974276542664, 0.4153778851032257, 0.3213275671005249, 0.2817309498786926, 0.3461615741252899, 0.4370684027671814, 0.02695329859852791, 0.2588632106781006, 0.3158821165561676, 0.1496676653623581, 0.006134177092462778, 0.40544936060905457, 0.2622045576572418, 0.1564885675907135, 0.05824233591556549, 0.2386007457971573, 0.4278918504714966, 0.2530982792377472, 0.07852800190448761, 0.14681075513362885], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3565445840358734, 0.4455295205116272, 0.36200761795043945, 0.19225819408893585, 0.40289679169654846, 0.4607517719268799, 0.12238433957099915, 0.4501589238643646, 0.4977962076663971, 0.033970464020967484, 0.06924687325954437, 0.08538250625133514, 0.24361677467823029, 0.08298434317111969, 0.4136480689048767, 0.09951778501272202, 0.05453947186470032, 0.2079785019159317, 0.29757583141326904, 0.32063430547714233, 0.4109489321708679, 0.47759172320365906, 0.24885983765125275, 0.14488224685192108, 0.412049263715744, 0.20841911435127258, 0.06561785191297531, 0.1691478043794632, 0.46564340591430664, 0.04583653062582016], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_02098c7ecbcb9cf768b71f969bf00536(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.37896785140037537, 0.39583778381347656, 0.2678925395011902, 0.03997251018881798, 0.26720479130744934, 0.02803381346166134, 0.2613063454627991, 0.3471791446208954, 0.053109023720026016, 0.4617995023727417, 0.21472831070423126, 0.02931806817650795, 0.28666597604751587, 0.19802334904670715, 0.4384440779685974, 0.3381373882293701, 0.41639280319213867, 0.15698017179965973, 0.10723895579576492, 0.008164452388882637, 0.39996689558029175, 0.13135012984275818, 0.4026288390159607, 0.2753736674785614], dtype='float32').reshape([24]),
            paddle.to_tensor([0.25252506136894226, 0.3038040101528168, 0.003677080851048231, 0.0030901702120900154, 0.0640762448310852, 0.4262551963329315, 0.3070201277732849, 0.2398395985364914, 0.1857326477766037, 0.11159269511699677, 0.13459113240242004, 0.411137193441391, 0.08439246565103531, 0.41564130783081055, 0.43993037939071655, 0.39273735880851746, 0.16213227808475494, 0.11301340162754059, 0.31013643741607666, 0.3024922013282776, 0.16817615926265717, 0.371685653924942, 0.3801322281360626, 0.22007589042186737], dtype='float32').reshape([24]),
            paddle.to_tensor([0.39920300245285034, 0.46208566427230835, 0.08756903558969498, 0.24650341272354126, 0.09977979958057404, 0.16089002788066864, 0.09090732038021088, 0.4296162724494934, 0.03168663755059242, 0.00028262537671253085, 0.2224348783493042, 0.09544714540243149, 0.424731582403183, 0.16006731986999512, 0.022835250943899155, 0.30033913254737854, 0.09498030692338943, 0.19348442554473877, 0.49027585983276367, 0.03676385432481766, 0.2582714855670929, 0.28859055042266846, 0.2617335319519043, 0.04067869111895561], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4067196547985077, 0.05706268921494484, 0.3480947017669678, 0.055575091391801834, 0.26714080572128296, 0.050490785390138626, 0.10109485685825348, 0.2320685237646103, 0.008208063431084156, 0.22232714295387268, 0.18535253405570984, 0.399512380361557, 0.2666134536266327, 0.29824307560920715, 0.25116756558418274, 0.10415687412023544, 0.16130706667900085, 0.0625435933470726, 0.33584097027778625, 0.2985112965106964, 0.4285967946052551, 0.060421716421842575, 0.3173196613788605, 0.3915071189403534], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_83be1bf2aef8bf703a73cff065bb0389(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2167907953262329, 0.459511399269104, 0.3775317668914795, 0.19796635210514069, 0.45537057518959045, 0.12280800193548203, 0.1206628605723381, 0.365540087223053, 0.291942834854126, 0.2768687307834625, 0.03887617960572243, 0.13364513218402863, 0.21680720150470734, 0.4170324206352234, 0.0629611387848854, 0.36618322134017944, 0.4707919657230377, 0.32106712460517883], dtype='float32').reshape([18]),
            paddle.to_tensor([0.01548498310148716, 0.11278164386749268, 0.12982089817523956, 0.17190419137477875, 0.1320493519306183, 0.052350129932165146, 0.33023273944854736, 0.25645655393600464, 0.17423345148563385, 0.16103461384773254, 0.30720192193984985, 0.20216962695121765, 0.23660138249397278, 0.1371431201696396, 0.4934970736503601, 0.2437242716550827, 0.1730615198612213, 0.3190056383609772], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3575354218482971, 0.25353458523750305, 0.050642263144254684, 0.19414561986923218, 0.19962789118289948, 0.08767609298229218, 0.13408683240413666, 0.17085282504558563, 0.3226912021636963, 0.4758303165435791, 0.42925167083740234, 0.13181789219379425, 0.2758886218070984, 0.4028592109680176, 0.13048110902309418, 0.29909566044807434, 0.3176707327365875, 0.29898256063461304], dtype='float32').reshape([18]),
            paddle.to_tensor([0.43787452578544617, 0.473775178194046, 0.3667493164539337, 0.020200835540890694, 0.3201214671134949, 0.2265760898590088, 0.14187544584274292, 0.11288817971944809, 0.032107554376125336, 0.12955999374389648, 0.10579041391611099, 0.3992244303226471, 0.4932786822319031, 0.03963466361165047, 0.1539674997329712, 0.0604555681347847, 0.032246943563222885, 0.22025632858276367], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8aaae76e57d21c4bb098adfe73863483(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 480, 480], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2895117998123169, 0.4016754925251007, 0.06541910022497177, 0.16866767406463623, 0.13798736035823822, 0.3113562762737274, 0.44885125756263733, 0.13342136144638062], dtype='float32').reshape([8]),
            paddle.to_tensor([0.4657141864299774, 0.1676887720823288, 0.1641506850719452, 0.0825849249958992, 0.3311871290206909, 0.13938380777835846, 0.3834645450115204, 0.4251166582107544], dtype='float32').reshape([8]),
            paddle.to_tensor([0.48641523718833923, 0.15447908639907837, 0.3269798159599304, 0.4679078459739685, 0.46100956201553345, 0.3064754009246826, 0.18965403735637665, 0.006066457834094763], dtype='float32').reshape([8]),
            paddle.to_tensor([0.3766482174396515, 0.08277073502540588, 0.25276094675064087, 0.44553765654563904, 0.15939772129058838, 0.2633870840072632, 0.121910959482193, 0.22494088113307953], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fbe125843e9c83c28d8e7b378ea6879b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3104635775089264, 0.3920905292034149, 0.29400548338890076, 0.3090893626213074, 0.3873220682144165, 0.3130742013454437, 0.48069217801094055, 0.3662164509296417, 0.05103497952222824, 0.4793068468570709, 0.4644717276096344, 0.05080125108361244, 0.4998103976249695, 0.020598191767930984, 0.00346646411344409, 0.4384765028953552, 0.004454833921045065, 0.08228762447834015], dtype='float32').reshape([18]),
            paddle.to_tensor([0.08676914125680923, 0.010489298030734062, 0.37816178798675537, 0.371892511844635, 0.12037400156259537, 0.18437333405017853, 0.4423702359199524, 0.14330138266086578, 0.449208527803421, 0.2666718661785126, 0.2656903564929962, 0.23971541225910187, 0.36176344752311707, 0.19133935868740082, 0.01893780194222927, 0.32560357451438904, 0.17472203075885773, 0.3037380278110504], dtype='float32').reshape([18]),
            paddle.to_tensor([0.43342697620391846, 0.31300628185272217, 0.1824270635843277, 0.06432262808084488, 0.37289878726005554, 0.3543975055217743, 0.2632609009742737, 0.4308431148529053, 0.4755007028579712, 0.0721442922949791, 0.47550585865974426, 0.0439264178276062, 0.04062814265489578, 0.48018893599510193, 0.3181496262550354, 0.024504143744707108, 0.4130125641822815, 0.0871824100613594], dtype='float32').reshape([18]),
            paddle.to_tensor([0.12227262556552887, 0.23401959240436554, 0.22800801694393158, 0.009610959328711033, 0.05822071433067322, 0.1682310551404953, 0.2870809733867645, 0.45600444078445435, 0.19493213295936584, 0.43163397908210754, 0.04644891992211342, 0.46329939365386963, 0.03588324785232544, 0.008767067454755306, 0.2987072467803955, 0.07034603506326675, 0.42769551277160645, 0.4310160279273987], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d417a85ad18af683b0a1756e8a992acb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 122, 122], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_433c0397ab823465c5848d084a4a7858(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([8, 512, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_865c5d1141f900f77f68bb396806143e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.26276886463165283, 0.46045804023742676, 0.01820443384349346, 0.35198163986206055, 0.2487650066614151, 0.27742576599121094, 0.22680513560771942, 0.4700157940387726, 0.3643586039543152, 0.43046388030052185, 0.03921886533498764, 0.014569736085832119, 0.06222463399171829, 0.050630420446395874, 0.11512989550828934, 0.24877919256687164, 0.43566229939460754, 0.4007219970226288, 0.12617407739162445, 0.33143338561058044], dtype='float32').reshape([20]),
            paddle.to_tensor([0.11988935619592667, 0.34745901823043823, 0.26678454875946045, 0.3643694818019867, 0.4556379020214081, 0.16345065832138062, 0.3999277651309967, 0.3995154798030853, 0.4219153821468353, 0.23627936840057373, 0.49834126234054565, 0.14623747766017914, 0.1555686742067337, 0.060162708163261414, 0.11395994573831558, 0.1791967749595642, 0.2507834732532501, 0.36632704734802246, 0.10546110570430756, 0.41095831990242004], dtype='float32').reshape([20]),
            paddle.to_tensor([0.2690432667732239, 0.2586216926574707, 0.24402040243148804, 0.003789555514231324, 0.3262545168399811, 0.2982226014137268, 0.14232467114925385, 0.14230585098266602, 0.4465027153491974, 0.12651327252388, 0.46347886323928833, 0.28772133588790894, 0.10733997076749802, 0.2931637167930603, 0.01762702502310276, 0.4854554235935211, 0.4613797962665558, 0.11599075049161911, 0.4332698583602905, 0.3447376489639282], dtype='float32').reshape([20]),
            paddle.to_tensor([0.22142906486988068, 0.4520077705383301, 0.2500459849834442, 0.10468373447656631, 0.43867942690849304, 0.36185261607170105, 0.24916063249111176, 0.4258345067501068, 0.2164357453584671, 0.33616960048675537, 0.18632818758487701, 0.34116074442863464, 0.44896459579467773, 0.0583328902721405, 0.2465628981590271, 0.11023496836423874, 0.27048754692077637, 0.07210655510425568, 0.33470189571380615, 0.31083184480667114], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_34268752954160fd5dfbe9ac934f2f5c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.15595312416553497, 0.1548856645822525, 0.06847437471151352, 0.4661942720413208, 0.3794420659542084, 0.022333795204758644, 0.4950506389141083, 0.06524453312158585, 0.1282893866300583, 0.3841252624988556, 0.1482216715812683, 0.439151406288147], dtype='float32').reshape([12]),
            paddle.to_tensor([0.3011971414089203, 0.3255883455276489, 0.2639169991016388, 0.1943993717432022, 0.4943884015083313, 0.42703959345817566, 0.2666592299938202, 0.1768491566181183, 0.4937567710876465, 0.18372973799705505, 0.24193191528320312, 0.3241076171398163], dtype='float32').reshape([12]),
            paddle.to_tensor([0.19947722554206848, 0.1989554613828659, 0.25219759345054626, 0.07273034006357193, 0.15107150375843048, 0.29438331723213196, 0.16799168288707733, 0.11004969477653503, 0.12882816791534424, 0.4157923460006714, 0.14743976294994354, 0.19022667407989502], dtype='float32').reshape([12]),
            paddle.to_tensor([0.4569885730743408, 0.12015967071056366, 0.21796870231628418, 0.3418501317501068, 0.26550012826919556, 0.4084225296974182, 0.022084558382630348, 0.35970115661621094, 0.429008424282074, 0.3346732258796692, 0.2533871829509735, 0.16540315747261047], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_54ea3d5f6e734140fce1bb7c816486ef(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_79fd552d1a7561c7bde5bb8ae4d3b774(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 128, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_12de28554061a33e3c070c052e56e32b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_507a87343923c61335885b77917b6d47(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 244, 244], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.18896572291851044, 0.3432774841785431, 0.24942530691623688, 0.10049308836460114, 0.24885809421539307, 0.3608073890209198, 0.18999110162258148, 0.06902853399515152, 0.06647329032421112, 0.1117890328168869, 0.3530142903327942, 0.08329454809427261, 0.10595716536045074, 0.058960042893886566, 0.1034497544169426, 0.19443437457084656], dtype='float32').reshape([16]),
            paddle.to_tensor([0.026037605479359627, 0.15278767049312592, 0.1506820172071457, 0.06970765441656113, 0.32911616563796997, 0.15555517375469208, 0.10524386912584305, 0.3797288239002228, 0.039361752569675446, 0.45921406149864197, 0.1894914209842682, 0.41634535789489746, 0.19028621912002563, 0.35976773500442505, 0.43935254216194153, 0.26030048727989197], dtype='float32').reshape([16]),
            paddle.to_tensor([0.07536249607801437, 0.3639916777610779, 0.28469008207321167, 0.2583928108215332, 0.37641337513923645, 0.136993870139122, 0.18467657268047333, 0.2297874242067337, 0.2996119558811188, 0.0952119305729866, 0.11014270782470703, 0.09046443551778793, 0.42313283681869507, 0.3042008876800537, 0.4176707863807678, 0.058812662959098816], dtype='float32').reshape([16]),
            paddle.to_tensor([0.03552395850419998, 0.3721517324447632, 0.14748477935791016, 0.09275360405445099, 0.28915277123451233, 0.3523575961589813, 0.13758979737758636, 0.24547064304351807, 0.03052656352519989, 0.011898470111191273, 0.14436647295951843, 0.4377472400665283, 0.28862980008125305, 0.2565401494503021, 0.26150885224342346, 0.48572516441345215], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a80c2da173f5133913e3a93614b65b41(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 512, 1024], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3558959662914276, 0.15451626479625702, 0.3302529752254486, 0.29128047823905945, 0.029972298070788383, 0.04259872809052467, 0.03722706437110901, 0.34490737318992615, 0.008392248302698135, 0.1917049139738083, 0.32391318678855896, 0.13220494985580444, 0.12435448169708252, 0.13756348192691803, 0.25670063495635986, 0.03281005844473839], dtype='float32').reshape([16]),
            paddle.to_tensor([0.30466651916503906, 0.4832902252674103, 0.27291399240493774, 0.3680175840854645, 0.04017793387174606, 0.34637823700904846, 0.025992469862103462, 0.3938978910446167, 0.011022224090993404, 0.18772193789482117, 0.20849300920963287, 0.04548289626836777, 0.010422452352941036, 0.28135329484939575, 0.41633519530296326, 0.12362322956323624], dtype='float32').reshape([16]),
            paddle.to_tensor([0.03372933715581894, 0.3998206555843353, 0.033968210220336914, 0.40009748935699463, 0.2716124355792999, 0.2976842224597931, 0.12835751473903656, 0.4402724504470825, 0.014838024042546749, 0.04793887212872505, 0.044758331030607224, 0.47724151611328125, 0.2513812184333801, 0.15993182361125946, 0.4186510145664215, 0.25908994674682617], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2411702275276184, 0.0927354097366333, 0.3570655286312103, 0.3068978786468506, 0.19856275618076324, 0.4664953947067261, 0.16753262281417847, 0.08839863538742065, 0.02648359164595604, 0.24317216873168945, 0.19745798408985138, 0.39287513494491577, 0.4021732807159424, 0.48238828778266907, 0.03203853964805603, 0.286701500415802], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ed17ad7fe268955d66003dae66b6ba7c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.12753449380397797, 0.41394948959350586, 0.326312392950058, 0.034724753350019455, 0.22098828852176666, 0.35923662781715393, 0.05046851187944412, 0.06842958927154541, 0.4090441167354584, 0.3308078646659851, 0.008877060376107693, 0.27916428446769714, 0.17730367183685303, 0.4951757490634918, 0.38458141684532166, 0.42957308888435364, 0.37680551409721375, 0.10617265105247498, 0.04954327642917633, 0.15489430725574493], dtype='float32').reshape([20]),
            paddle.to_tensor([0.2117871195077896, 0.35924655199050903, 0.3202684223651886, 0.29607945680618286, 0.06290022283792496, 0.4325515627861023, 0.48338231444358826, 0.42375919222831726, 0.3316577672958374, 0.1447552740573883, 0.2839718163013458, 0.43753954768180847, 0.2573489546775818, 0.19017671048641205, 0.28292298316955566, 0.4495321810245514, 0.39692723751068115, 0.38751646876335144, 0.15116173028945923, 0.13071854412555695], dtype='float32').reshape([20]),
            paddle.to_tensor([0.46350330114364624, 0.06307554990053177, 0.2572452127933502, 0.10641805827617645, 0.23518891632556915, 0.33336424827575684, 0.026934737339615822, 0.0695587620139122, 0.15577349066734314, 0.15193907916545868, 0.3310261368751526, 0.3180235028266907, 0.038411252200603485, 0.48098018765449524, 0.4665161669254303, 0.3326447904109955, 0.15535014867782593, 0.45219531655311584, 0.1490005999803543, 0.47380051016807556], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3780466616153717, 0.2852058708667755, 0.4123196303844452, 0.46712180972099304, 0.24284270405769348, 0.3243701159954071, 0.49016183614730835, 0.3146977424621582, 0.3018997013568878, 0.11479675769805908, 0.09498269110918045, 0.43453407287597656, 0.05755621939897537, 0.042848795652389526, 0.09202840924263, 0.1656447947025299, 0.08112017810344696, 0.4226285219192505, 0.251904159784317, 0.2882370948791504], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b80ae4e260cda84a803e2503ecb4ef36(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 12, 12], dtype='float16', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_063dc7e01f236c8604db4f7a20cdb29e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 61, 61], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_110ca97956a25964b235eacc240d66d5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2804001271724701, 0.10726630687713623, 0.09688518941402435, 0.0413542315363884, 0.2595252990722656, 0.0065805017948150635, 0.35268887877464294, 0.24427145719528198, 0.12136190384626389, 0.08892420679330826, 0.24185369908809662, 0.16873779892921448, 0.28546813130378723, 0.4012186825275421, 0.4812954366207123, 0.23699012398719788, 0.430744469165802, 0.19751091301441193], dtype='float32').reshape([18]),
            paddle.to_tensor([0.05514470115303993, 0.43723374605178833, 0.4961261451244354, 0.11201032251119614, 0.32983332872390747, 0.3621375560760498, 0.29713621735572815, 0.05930108577013016, 0.1504950225353241, 0.4780808985233307, 0.18856920301914215, 0.486772745847702, 0.12102861702442169, 0.12186506390571594, 0.26527103781700134, 0.2983989417552948, 0.07332152873277664, 0.3042030334472656], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4668586552143097, 0.0880189761519432, 0.45542457699775696, 0.4574577510356903, 0.1551228165626526, 0.11921282112598419, 0.1257365196943283, 0.23839010298252106, 0.09084949642419815, 0.1306835561990738, 0.4871032238006592, 0.49384647607803345, 0.27799171209335327, 0.10863404721021652, 0.05582587793469429, 0.12571656703948975, 0.373980313539505, 0.13154719769954681], dtype='float32').reshape([18]),
            paddle.to_tensor([0.22884419560432434, 0.4810682237148285, 0.4016503095626831, 0.07882558554410934, 0.24548645317554474, 0.364830881357193, 0.05991654098033905, 0.037408195436000824, 0.09703581780195236, 0.24185635149478912, 0.2515874207019806, 0.08491002023220062, 0.13978998363018036, 0.3413124084472656, 0.05052882805466652, 0.46361151337623596, 0.1027732789516449, 0.11882050335407257], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1cca9b05a96a5910e0f54cac541a1bcb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 4, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_60d97c6ba1391dc0c0dc72235997d16e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b463814a66dfaeb349b9474725a40e4b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 320, 320], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4317985475063324, 0.336946964263916, 0.446126788854599, 0.128691166639328, 0.11729748547077179, 0.31993523240089417, 0.00863699708133936, 0.09157628566026688], dtype='float32').reshape([8]),
            paddle.to_tensor([0.36168602108955383, 0.48104017972946167, 0.20115749537944794, 0.16761185228824615, 0.23869666457176208, 0.16019974648952484, 0.2936311960220337, 0.45240533351898193], dtype='float32').reshape([8]),
            paddle.to_tensor([0.07655809819698334, 0.07641290128231049, 0.34410205483436584, 0.42845913767814636, 0.09799657762050629, 0.35679489374160767, 0.3038654625415802, 0.3176037073135376], dtype='float32').reshape([8]),
            paddle.to_tensor([0.3397425413131714, 0.4795161187648773, 0.49396413564682007, 0.2510721981525421, 0.4027712047100067, 0.19641420245170593, 0.17350074648857117, 0.47289514541625977], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2b2cb8352087c2be15248588c6702baa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2667315900325775, 0.08437273651361465, 0.4944077134132385, 0.4363080859184265, 0.1883758157491684, 0.12067899852991104, 0.16266578435897827, 0.20583412051200867, 0.33665692806243896, 0.2516017556190491, 0.15827707946300507, 0.4043967127799988, 0.26316970586776733, 0.4199329614639282, 0.15847639739513397, 0.30698883533477783, 0.31707003712654114, 0.10426061600446701, 0.3487107753753662, 0.16719000041484833, 0.3489517569541931, 0.3566037118434906, 0.3523795008659363, 0.18730495870113373, 0.35550153255462646, 0.12941473722457886, 0.09124637395143509, 0.24236434698104858, 0.22548651695251465, 0.20338371396064758], dtype='float32').reshape([30]),
            paddle.to_tensor([0.20806032419204712, 0.39325425028800964, 0.29992541670799255, 0.21725156903266907, 0.2971479296684265, 0.14553315937519073, 0.10752226412296295, 0.32488715648651123, 0.2802717983722687, 0.050351403653621674, 0.3448610305786133, 0.029536226764321327, 0.24738678336143494, 0.013099313713610172, 0.24841821193695068, 0.09322090446949005, 0.342161625623703, 0.20667289197444916, 0.023524124175310135, 0.131983682513237, 0.018879558891057968, 0.38429251313209534, 0.08230075985193253, 0.31542786955833435, 0.23160184919834137, 0.2974090576171875, 0.4129889905452728, 0.20300740003585815, 0.4998217821121216, 0.06426456570625305], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3689853549003601, 0.08916166424751282, 0.08537711948156357, 0.4182060956954956, 0.3045141100883484, 0.040231671184301376, 0.30118077993392944, 0.2222297191619873, 0.42421916127204895, 0.19215665757656097, 0.29665154218673706, 0.2040829360485077, 0.0370892696082592, 0.31728363037109375, 0.2953353524208069, 0.3738809823989868, 0.12534260749816895, 0.3812102973461151, 0.44282111525535583, 0.16460435092449188, 0.45212599635124207, 0.3248969614505768, 0.1743793487548828, 0.30054420232772827, 0.17730453610420227, 0.4504455327987671, 0.09149124473333359, 0.1247711256146431, 0.17140091955661774, 0.13590221107006073], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3107607960700989, 0.148000106215477, 0.2462122142314911, 0.06300274282693863, 0.18625271320343018, 0.3812699019908905, 0.3075064420700073, 0.24196967482566833, 0.4458949863910675, 0.45110273361206055, 0.33260107040405273, 0.02543964982032776, 0.04057532176375389, 0.29246237874031067, 0.09023699909448624, 0.19753071665763855, 0.04640834406018257, 0.32345521450042725, 0.02637726254761219, 0.06894927471876144, 0.16781648993492126, 0.13822011649608612, 0.3820723295211792, 0.31443706154823303, 0.10779092460870743, 0.0016654375940561295, 0.4582454264163971, 0.12161792069673538, 0.420180082321167, 0.45088478922843933], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_83a513c035ef0f9d35a3e2b69487e779(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 25, 42], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_db3307622c8de0068b4d8e56e0f4fe68(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9507d2c9bd78a8c106f98786da18b846(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0cfa176e260cfbf04d01e6196989c538(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_f78910a24ab00d45797c31273495c7e0
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_157a00729c898c8994289990d1cee6ab(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_12d34811fb0f715838c846227b41d3fb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_fc6b6917e04b7b7cf9fdb39dde97ec40
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_14cd2430129ad120f4d011c17b1b697b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4dfaf4a7fae4cc4cd824d86a15a6a55c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.1376204788684845, 0.1808018982410431, 0.27168843150138855, 0.05482619255781174, 0.23868899047374725, 0.3748219907283783, 0.358996719121933, 0.2599068880081177, 0.4892514646053314, 0.2389233410358429, 0.2075536698102951, 0.12757670879364014, 0.1653614342212677, 0.17711560428142548, 0.20578239858150482, 0.4700106978416443, 0.06700399518013, 0.35362449288368225, 0.44347813725471497, 0.3703344762325287, 0.09888054430484772, 0.08399289846420288, 0.37505313754081726, 0.463785856962204], dtype='float32').reshape([24]),
            paddle.to_tensor([0.09942655265331268, 0.23285238444805145, 0.47107994556427, 0.42412102222442627, 0.3004894554615021, 0.06271705031394958, 0.28292185068130493, 0.46966028213500977, 0.375220388174057, 0.3554093837738037, 0.18080973625183105, 0.15803934633731842, 0.07786919176578522, 0.27844586968421936, 0.40611836314201355, 0.08329717069864273, 0.045358806848526, 0.30405399203300476, 0.1232779324054718, 0.3133053183555603, 0.0021784240379929543, 0.32168465852737427, 0.45501551032066345, 0.39777299761772156], dtype='float32').reshape([24]),
            paddle.to_tensor([0.05509045347571373, 0.3943619728088379, 0.4684980809688568, 0.25117671489715576, 0.14123190939426422, 0.4821588397026062, 0.15390928089618683, 0.16932238638401031, 0.209568053483963, 0.4139590561389923, 0.32415860891342163, 0.3425541818141937, 0.31208536028862, 0.4744502007961273, 0.268065482378006, 0.34934431314468384, 0.08543188124895096, 0.1871587336063385, 0.031022297218441963, 0.11696606874465942, 0.38388144969940186, 0.03208790719509125, 0.3966180682182312, 0.11517748236656189], dtype='float32').reshape([24]),
            paddle.to_tensor([0.19915196299552917, 0.4549729526042938, 0.4378536343574524, 0.03654792159795761, 0.21988502144813538, 0.31321337819099426, 0.34319865703582764, 0.30574774742126465, 0.18060147762298584, 0.2552248239517212, 0.26845258474349976, 0.4251168370246887, 0.47250646352767944, 0.25662562251091003, 0.40065762400627136, 0.3962070643901825, 0.38676315546035767, 0.13377250730991364, 0.4462469816207886, 0.23579974472522736, 0.17879566550254822, 0.14331859350204468, 0.30770307779312134, 0.16682378947734833], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f53a80c1faf4bc16242ddf168b186247(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dbc805aaf9a3720fa287c2e5d6468b97(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4b2d22b62c99b47c892c1c475132c954(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.21373054385185242, 0.0005071851774118841, 0.17424163222312927, 0.4760344326496124, 0.04318917915225029, 0.002928373869508505, 0.00035178271355107427, 0.30015048384666443, 0.1951807588338852, 0.1839907169342041, 0.39723536372184753, 0.28499263525009155, 0.09162634611129761, 0.3378962278366089, 0.3291451036930084, 0.009843707084655762], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2184363752603531, 0.030504725873470306, 0.10358348488807678, 0.44924718141555786, 0.1279090940952301, 0.07138035446405411, 0.3230534791946411, 0.06905204802751541, 0.2615509033203125, 0.3491014242172241, 0.14445070922374725, 0.2679794132709503, 0.3789125382900238, 0.2515018582344055, 0.2549770772457123, 0.1316211074590683], dtype='float32').reshape([16]),
            paddle.to_tensor([0.31122690439224243, 0.2974383533000946, 0.1307554990053177, 0.1801670342683792, 0.44638270139694214, 0.015325442887842655, 0.30235159397125244, 0.22975362837314606, 0.20326484739780426, 0.27059826254844666, 0.24560481309890747, 0.39765262603759766, 0.15529511868953705, 0.2539237439632416, 0.29929426312446594, 0.25917109847068787], dtype='float32').reshape([16]),
            paddle.to_tensor([0.11074168235063553, 0.1852140873670578, 0.06800512969493866, 0.41640332341194153, 0.09187626838684082, 0.34202590584754944, 0.1542264223098755, 0.16953903436660767, 0.29081106185913086, 0.4560296833515167, 0.198166623711586, 0.25207293033599854, 0.3755137622356415, 0.2826286554336548, 0.43686383962631226, 0.06729044020175934], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a67df917b369c4b85e45ceca9040d2d1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3434891700744629, 0.09754735231399536, 0.47806963324546814, 0.04844783991575241, 0.10900607705116272, 0.15349051356315613, 0.12167181074619293, 0.2985553443431854, 0.4975317716598511, 0.15367060899734497, 0.2148032784461975, 0.21803882718086243, 0.20176248252391815, 0.4970986843109131, 0.2813686728477478, 0.1307617574930191], dtype='float32').reshape([16]),
            paddle.to_tensor([0.06588498502969742, 0.2313941866159439, 0.2852160632610321, 0.38588085770606995, 0.18719050288200378, 0.19221407175064087, 0.47406429052352905, 0.12928368151187897, 0.2826635241508484, 0.1704985499382019, 0.19789618253707886, 0.02466445229947567, 0.38049760460853577, 0.330277681350708, 0.42630434036254883, 0.41758328676223755], dtype='float32').reshape([16]),
            paddle.to_tensor([0.16325539350509644, 0.34353771805763245, 0.46832454204559326, 0.424528032541275, 0.13698501884937286, 0.11000478267669678, 0.3765316605567932, 0.38954704999923706, 0.39740175008773804, 0.04702312871813774, 0.13337527215480804, 0.012765521183609962, 0.46652114391326904, 0.11860056221485138, 0.31217414140701294, 0.28063225746154785], dtype='float32').reshape([16]),
            paddle.to_tensor([0.21137776970863342, 0.10340550541877747, 0.10168345272541046, 0.04149075597524643, 0.18475821614265442, 0.12920072674751282, 0.2881348729133606, 0.10636574774980545, 0.42164120078086853, 0.10950999706983566, 0.22801101207733154, 0.16680653393268585, 0.25159725546836853, 0.0007145825657062232, 0.0037960801273584366, 0.12240498512983322], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_754cf3adee4fd03f0fe313736991c4ee(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 2, 2], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_018c90d2406f056ca90ca40699938dc9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 12, 12], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1b73be516a2595c2a5bd5ff15aa5a346(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 168, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([168], dtype='float32', min=0, max=0.5),
            paddle.uniform([168], dtype='float32', min=0, max=0.5),
            paddle.uniform([168], dtype='float32', min=0, max=0.5),
            paddle.uniform([168], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3484a0a8ca1e2f5bb3bedea577b2eb11(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.14680606126785278, 0.3454975485801697, 0.08447373658418655, 0.3267437517642975, 0.4069688320159912, 0.12420535832643509, 0.20587074756622314, 0.4283539056777954, 0.19407829642295837, 0.029631804674863815, 0.20752713084220886, 0.03551559895277023, 0.31703388690948486, 0.32750293612480164, 0.15965043008327484, 0.22230003774166107, 0.305370956659317, 0.3011496663093567, 0.09082102030515671, 0.043569695204496384, 0.40997594594955444, 0.15836670994758606, 0.11871011555194855, 0.15971963107585907], dtype='float32').reshape([24]),
            paddle.to_tensor([0.23977433145046234, 0.40258127450942993, 0.3132118284702301, 0.17113932967185974, 0.062063612043857574, 0.31381499767303467, 0.23634082078933716, 0.046594299376010895, 0.1685463935136795, 0.08181317895650864, 0.2504519522190094, 0.3539559841156006, 0.20217345654964447, 0.2442074418067932, 0.2099192887544632, 0.32342255115509033, 0.19179069995880127, 0.38796645402908325, 0.2581498920917511, 0.12782908976078033, 0.48789113759994507, 0.2459249645471573, 0.29445725679397583, 0.43146854639053345], dtype='float32').reshape([24]),
            paddle.to_tensor([0.26537859439849854, 0.2484484314918518, 0.23030813038349152, 0.20807695388793945, 0.0373477078974247, 0.051432956010103226, 0.10634872317314148, 0.2750347852706909, 0.3746412396430969, 0.4650253355503082, 0.14528989791870117, 0.3609524667263031, 0.03631981462240219, 0.39196300506591797, 0.36760640144348145, 0.13662734627723694, 0.13995760679244995, 0.09274499118328094, 0.3273981809616089, 0.20526526868343353, 0.4001702070236206, 0.48207566142082214, 0.4667103588581085, 0.016645504161715508], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3469173312187195, 0.0865669772028923, 0.05807575210928917, 0.42318272590637207, 0.4874710738658905, 0.2097090482711792, 0.41272228956222534, 0.1825464814901352, 0.06419272720813751, 0.12848106026649475, 0.13320574164390564, 0.3042176067829132, 0.4184945821762085, 0.41195008158683777, 0.1413034200668335, 0.3032141923904419, 0.16543534398078918, 0.12751008570194244, 0.4788779020309448, 0.10394813865423203, 0.44203102588653564, 0.39115142822265625, 0.2769100069999695, 0.39505085349082947], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f217177a14842d7c786b37fe70e207bf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 112, 199], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f67aaf10828d2d53d7e48405f98d5357(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d1406e8c5be2673e895036958812b2b2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a55e3b5f07afb933a82493b7f65d9409(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 1152, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1ffa5384829b2f3d9f3a83be7d363910(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 31, 31], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5f7d4591ab0b12280abc4cb98151ee1b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fdc6b9458d729e692d90a13e565de75b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.15534724295139313, 0.04900270327925682, 0.48136454820632935, 0.14750321209430695, 0.4358651340007782, 0.16792012751102448, 0.4571271240711212, 0.03772527724504471, 0.14501792192459106, 0.16379843652248383, 0.05951749533414841, 0.05605730414390564, 0.11495571583509445, 0.4955363869667053, 0.4145999252796173, 0.040410082787275314], dtype='float32').reshape([16]),
            paddle.to_tensor([0.024524526670575142, 0.2511797547340393, 0.0036897831596434116, 0.3500187397003174, 0.30056726932525635, 0.1792834997177124, 0.16390085220336914, 0.18669827282428741, 0.41491156816482544, 0.18635787069797516, 0.23170576989650726, 0.2634878158569336, 0.1370571255683899, 0.0031412371899932623, 0.3535723090171814, 0.16393503546714783], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3611390292644501, 0.45485278964042664, 0.31425145268440247, 0.24113832414150238, 0.07537060230970383, 0.13201814889907837, 0.31379953026771545, 0.4689532220363617, 0.14327725768089294, 0.22957631945610046, 0.26288044452667236, 0.061952780932188034, 0.2704300880432129, 0.005066379439085722, 0.009534966200590134, 0.2038259655237198], dtype='float32').reshape([16]),
            paddle.to_tensor([0.47740450501441956, 0.4158172607421875, 0.365486741065979, 0.08835495263338089, 0.07623566687107086, 0.4807842969894409, 0.4758523404598236, 0.057203907519578934, 0.09303577989339828, 0.16821244359016418, 0.13716289401054382, 0.1460769921541214, 0.005338466726243496, 0.35303401947021484, 0.008464922197163105, 0.29934680461883545], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_195a5ba30923791d038a2db9e039bcb9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2051285207271576, 0.080255888402462, 0.006900068372488022, 0.42098912596702576, 0.38020017743110657, 0.04800425469875336, 0.22040323913097382, 0.27695032954216003, 0.35810190439224243, 0.10914456844329834, 0.29951655864715576, 0.4771358072757721, 0.022836802527308464, 0.14836491644382477, 0.08176452666521072, 0.21322409808635712, 0.34479448199272156, 0.4773631691932678, 0.22481141984462738, 0.08560306578874588, 0.4213271141052246, 0.3370765447616577, 0.28706011176109314, 0.1322828084230423, 0.26048263907432556, 0.07388260960578918, 0.27694642543792725, 0.19384120404720306, 0.37132322788238525, 0.05158499255776405], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4279344379901886, 0.08970221877098083, 0.11663790792226791, 0.31687235832214355, 0.4244747757911682, 0.12744028866291046, 0.19189314544200897, 0.07577301561832428, 0.09681978821754456, 0.17252306640148163, 0.1581897884607315, 0.4979753792285919, 0.0396423302590847, 0.4389200806617737, 0.3763829171657562, 0.0790180116891861, 0.3798956274986267, 0.4703674018383026, 0.44289568066596985, 0.35154926776885986, 0.04696003720164299, 0.18037576973438263, 0.44678202271461487, 0.34461578726768494, 0.35812750458717346, 0.1721285730600357, 0.38534659147262573, 0.33255505561828613, 0.4528498649597168, 0.34883609414100647], dtype='float32').reshape([30]),
            paddle.to_tensor([0.02749958075582981, 0.15747466683387756, 0.2763797640800476, 0.49841925501823425, 0.43453511595726013, 0.009651592001318932, 0.3900010585784912, 0.3139083981513977, 0.09327852725982666, 0.26006001234054565, 0.41639742255210876, 0.4456501603126526, 0.2847665548324585, 0.15422385931015015, 0.13892818987369537, 0.20788446068763733, 0.3186793327331543, 0.05050184577703476, 0.15203769505023956, 0.32897165417671204, 0.3697283864021301, 0.4494653046131134, 0.007204726338386536, 0.03075012005865574, 0.24859264492988586, 0.1739596128463745, 0.40044447779655457, 0.41625547409057617, 0.31836068630218506, 0.4852154552936554], dtype='float32').reshape([30]),
            paddle.to_tensor([0.161763995885849, 0.45775657892227173, 0.2582031488418579, 0.3393685221672058, 0.4729345440864563, 0.0940491333603859, 0.27811741828918457, 0.46827444434165955, 0.25855183601379395, 0.2543666362762451, 0.261765718460083, 0.21045933663845062, 0.36201316118240356, 0.20186832547187805, 0.216423898935318, 0.40549927949905396, 0.037984222173690796, 0.4684237837791443, 0.31439271569252014, 0.3024071156978607, 0.2306833416223526, 0.11145852506160736, 0.2808593511581421, 0.2532985210418701, 0.47800007462501526, 0.19459643959999084, 0.4835335612297058, 0.12927919626235962, 0.31767043471336365, 0.2402232438325882], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_98edef8dc64cc56432351b9e658c82c4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 64, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e7a9764e0434639b5aa0462bb4e10c00(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 1, 1], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_51e82c1ad49fe4fae5b8868bb23106f6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.24074791371822357, 0.44353318214416504, 0.061413779854774475, 0.3020043671131134, 0.4913196861743927, 0.28868040442466736, 0.10068148374557495, 0.08027366548776627, 0.34107285737991333, 0.21872270107269287, 0.2665846645832062, 0.3461863696575165, 0.36363840103149414, 0.13303500413894653, 0.13313648104667664, 0.46179136633872986, 0.35978248715400696, 0.017185669392347336, 0.4572184979915619, 0.3426753282546997], dtype='float32').reshape([20]),
            paddle.to_tensor([0.10472337156534195, 0.302833616733551, 0.03871571272611618, 0.017650585621595383, 0.2061450034379959, 0.3964332342147827, 0.32072025537490845, 0.2635049819946289, 0.09118340909481049, 0.394581139087677, 0.16617554426193237, 0.03264199197292328, 0.23343566060066223, 0.3354444205760956, 0.4282039701938629, 0.4311591684818268, 0.34092676639556885, 0.3725922405719757, 0.2577074468135834, 0.009057140909135342], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3179296851158142, 0.4676324129104614, 0.2687012553215027, 0.09058765321969986, 0.003311609150841832, 0.31574568152427673, 0.15052342414855957, 0.14138163626194, 0.48976919054985046, 0.16680070757865906, 0.14849011600017548, 0.04466678574681282, 0.21790780127048492, 0.48680371046066284, 0.1785297691822052, 0.28378626704216003, 0.35833024978637695, 0.2928934395313263, 0.20952507853507996, 0.29196685552597046], dtype='float32').reshape([20]),
            paddle.to_tensor([0.17343148589134216, 0.34230175614356995, 0.3486623764038086, 0.24757760763168335, 0.44699910283088684, 0.49373483657836914, 0.2311987578868866, 0.39748215675354004, 0.07231123000383377, 0.38066723942756653, 0.2503238618373871, 0.4030599892139435, 0.15379077196121216, 0.15492293238639832, 0.19938723742961884, 0.3355432450771332, 0.25115537643432617, 0.38004451990127563, 0.3586477041244507, 0.16383583843708038], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_79294185693258ddb0892abc7051278f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2290087193250656, 0.057406552135944366, 0.18001455068588257, 0.3033427298069, 0.2586038112640381, 0.25705772638320923, 0.1438995599746704, 0.2215808629989624, 0.34042268991470337, 0.31296223402023315, 0.4264417290687561, 0.3846830427646637, 0.47306501865386963, 0.25921204686164856, 0.042660992592573166, 0.40197867155075073, 0.3722694516181946, 0.07183220982551575, 0.08915731310844421, 0.0978710874915123, 0.4905887246131897, 0.058152951300144196, 0.10939095169305801, 0.13098467886447906, 0.41609954833984375, 0.2970096170902252, 0.30503854155540466, 0.4229448139667511, 0.04491749405860901, 0.10911550372838974], dtype='float32').reshape([30]),
            paddle.to_tensor([0.26420825719833374, 0.29331716895103455, 0.37288564443588257, 0.4441335201263428, 0.29685017466545105, 0.3828546106815338, 0.015065200626850128, 0.1045261025428772, 0.3177863359451294, 0.3473335802555084, 0.31964462995529175, 0.08685080707073212, 0.07710081338882446, 0.48015740513801575, 0.48468905687332153, 0.012041470035910606, 0.14506199955940247, 0.21334807574748993, 0.44481611251831055, 0.49711865186691284, 0.3347623944282532, 0.2773447036743164, 0.2115069031715393, 0.3977634310722351, 0.25941213965415955, 0.12588626146316528, 0.4097016155719757, 0.01867835968732834, 0.18948647379875183, 0.23505988717079163], dtype='float32').reshape([30]),
            paddle.to_tensor([0.05276954919099808, 0.02426285296678543, 0.26627808809280396, 0.3556343615055084, 0.21117247641086578, 0.3984948694705963, 0.41123417019844055, 0.08528932929039001, 0.1525123566389084, 0.24269448220729828, 0.1765507161617279, 0.1269509643316269, 0.3763318955898285, 0.23345333337783813, 0.37189632654190063, 0.43783092498779297, 0.4283077120780945, 0.36542823910713196, 0.36704355478286743, 0.15074682235717773, 0.27227428555488586, 0.10334915667772293, 0.057807594537734985, 0.4370964467525482, 0.04455699399113655, 0.3706745207309723, 0.27237606048583984, 0.2377486526966095, 0.28930309414863586, 0.25500109791755676], dtype='float32').reshape([30]),
            paddle.to_tensor([0.07876337319612503, 0.3353630304336548, 0.4548417925834656, 0.016312142834067345, 0.12525388598442078, 0.2575150430202484, 0.33307400345802307, 0.04871449992060661, 0.28372320532798767, 0.25528329610824585, 0.1843743473291397, 0.19518549740314484, 0.2881513237953186, 0.26524874567985535, 0.001646190183237195, 0.19352151453495026, 0.27814769744873047, 0.09460975974798203, 0.399968683719635, 0.3446178734302521, 0.39689937233924866, 0.4018744230270386, 0.47203803062438965, 0.23723378777503967, 0.13464856147766113, 0.14879922568798065, 0.3487211763858795, 0.18753936886787415, 0.28928840160369873, 0.28857293725013733], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0cb619b3c87fd20e766b170517f46a22(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8d2a39122b7a526c9e2e4525cdb487bb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 102, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([102], dtype='float32', min=0, max=0.5),
            paddle.uniform([102], dtype='float32', min=0, max=0.5),
            paddle.uniform([102], dtype='float32', min=0, max=0.5),
            paddle.uniform([102], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_88a3fd38a816cfe59a1cde875b0a5aa3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 48, 48], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_59f91f47acaab12904be0b2a2fe24bd0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 112, 199], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.0736379623413086, 0.3083580434322357, 0.30120304226875305, 0.3863343298435211, 0.19810181856155396, 0.3739544153213501, 0.2665212154388428, 0.16314996778964996, 0.3651651442050934, 0.44149067997932434, 0.4358469843864441, 0.12617480754852295, 0.12854889035224915, 0.38138946890830994, 0.3873077630996704, 0.04903775081038475, 0.024602409452199936, 0.11851676553487778], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2984674870967865, 0.40678125619888306, 0.4741946756839752, 0.22869879007339478, 0.22578559815883636, 0.031232742592692375, 0.052402298897504807, 0.40540874004364014, 0.39601728320121765, 0.14058472216129303, 0.3094783425331116, 0.30212005972862244, 0.3572908341884613, 0.3042624592781067, 0.12043830007314682, 0.47582319378852844, 0.36789992451667786, 0.08376064896583557], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4075753092765808, 0.012689761817455292, 0.20301596820354462, 0.19158244132995605, 0.028768938034772873, 0.3269825577735901, 0.1142609566450119, 0.27693474292755127, 0.2402629256248474, 0.3857743740081787, 0.3847852945327759, 0.25337526202201843, 0.41737961769104004, 0.26954036951065063, 0.3781200051307678, 0.35713618993759155, 0.27618226408958435, 0.09385903924703598], dtype='float32').reshape([18]),
            paddle.to_tensor([0.05047319829463959, 0.18896447122097015, 0.2704962491989136, 0.21301323175430298, 0.34077879786491394, 0.45345351099967957, 0.36550000309944153, 0.31904301047325134, 0.4725848436355591, 0.05561268702149391, 0.06439974159002304, 0.008689007721841335, 0.20786072313785553, 0.03066886216402054, 0.4113539159297943, 0.49680033326148987, 0.18275876343250275, 0.15606078505516052], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_eb2a519f04601244cfdb59c497a966d4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2a274714bbe1f0c866ce7d05a20668d4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 31, 31], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ce6bd89590c51c6dd50c8bc9651792f6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.08228226006031036, 0.3683041036128998, 0.1837337762117386, 0.2136581540107727, 0.3993990123271942, 0.3265051543712616, 0.003689680015668273, 0.0754752904176712, 0.48565784096717834, 0.14294622838497162, 0.3937748074531555, 0.038971856236457825, 0.41997256875038147, 0.0882301852107048, 0.165359228849411, 0.4356442987918854, 0.1251235455274582, 0.44466525316238403, 0.38201025128364563, 0.3259056806564331, 0.06429148465394974, 0.14244292676448822, 0.12118123471736908, 0.48576560616493225], dtype='float32').reshape([24]),
            paddle.to_tensor([0.20446501672267914, 0.0007397807785309851, 0.41291508078575134, 0.395102858543396, 0.299057275056839, 0.20443463325500488, 0.21087945997714996, 0.49251464009284973, 0.09918861836194992, 0.27989324927330017, 0.20321126282215118, 0.22189363837242126, 0.1746414303779602, 0.4384641647338867, 0.45769280195236206, 0.20252424478530884, 0.35110220313072205, 0.37368232011795044, 0.4717209041118622, 0.22924378514289856, 0.056050773710012436, 0.22335714101791382, 0.4268476366996765, 0.38141945004463196], dtype='float32').reshape([24]),
            paddle.to_tensor([0.01459913793951273, 0.018311019986867905, 0.317089706659317, 0.35702091455459595, 0.20459091663360596, 0.23268651962280273, 0.3448798656463623, 0.18029247224330902, 0.2907383441925049, 0.0007170369499363005, 0.08766013383865356, 0.05709369480609894, 0.08797318488359451, 0.39680176973342896, 0.4221961796283722, 0.41009247303009033, 0.20251667499542236, 0.14155609905719757, 0.044626373797655106, 0.3612709939479828, 0.2613179087638855, 0.21135039627552032, 0.3420521020889282, 0.37135306000709534], dtype='float32').reshape([24]),
            paddle.to_tensor([0.46366822719573975, 0.46944165229797363, 0.28025883436203003, 0.41488948464393616, 0.24139182269573212, 0.3752545416355133, 0.4311864674091339, 0.48082098364830017, 0.06904834508895874, 0.22999180853366852, 0.06328600645065308, 0.2837117314338684, 0.3982428014278412, 0.18763062357902527, 0.3615495562553406, 0.032882992178201675, 0.30510884523391724, 0.20180654525756836, 0.45104825496673584, 0.07388473302125931, 0.08742643147706985, 0.1844685971736908, 0.3863651156425476, 0.4841463565826416], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f6defe9e993db7618f276cba0b505e17(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.0017567032482475042, 0.09396591782569885, 0.31263816356658936, 0.2418970763683319, 0.1382823884487152, 0.12737210094928741, 0.056912560015916824, 0.312319278717041, 0.17929676175117493, 0.29415982961654663, 0.4475909471511841, 0.21449841558933258, 0.44617030024528503, 0.48195716738700867, 0.4243377149105072, 0.09266068786382675, 0.09756568819284439, 0.10436181724071503, 0.43406131863594055, 0.1313561201095581], dtype='float32').reshape([20]),
            paddle.to_tensor([0.12711593508720398, 0.031952157616615295, 0.434684157371521, 0.48908573389053345, 0.3110896646976471, 0.10389232635498047, 0.45222482085227966, 0.34332525730133057, 0.4292026162147522, 0.19471798837184906, 0.04803387075662613, 0.38243982195854187, 0.06340436637401581, 0.03153505176305771, 0.17197412252426147, 0.18486355245113373, 0.17776921391487122, 0.26775985956192017, 0.21549105644226074, 0.2517598867416382], dtype='float32').reshape([20]),
            paddle.to_tensor([0.15943095088005066, 0.28743040561676025, 0.40716028213500977, 0.08638565987348557, 0.4967024326324463, 0.3745046555995941, 0.4653741121292114, 0.13537883758544922, 0.07303902506828308, 0.4289481043815613, 0.3841032385826111, 0.12343501299619675, 0.2823054790496826, 0.26858487725257874, 0.14135277271270752, 0.497057169675827, 0.49631020426750183, 0.1761379837989807, 0.06250984221696854, 0.4007900059223175], dtype='float32').reshape([20]),
            paddle.to_tensor([0.16362108290195465, 0.15266916155815125, 0.4956226646900177, 0.39851680397987366, 0.3688809871673584, 0.1862189620733261, 0.4623057246208191, 0.003515082411468029, 0.12172985821962357, 0.060243796557188034, 0.188795804977417, 0.10657081007957458, 0.32972943782806396, 0.06223607063293457, 0.4488683342933655, 0.022001339122653008, 0.4485391080379486, 0.030209243297576904, 0.06922178715467453, 0.1528305858373642], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ee4cf5bdaef62283da8f709dfb955533(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fee8e06f7df46e36bc40d7292b833836(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 24, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f5a63c8367b4ebf3ff6c3b47ed78a531(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b84af26dcf954250078ef094fd8d452a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.29156777262687683, 0.0014075357466936111, 0.25472962856292725, 0.33416685461997986, 0.061892811208963394, 0.32043707370758057, 0.3258856534957886, 0.32071763277053833, 0.15424410998821259, 0.17224138975143433, 0.32129567861557007, 0.10726509243249893, 0.33775973320007324, 0.09863796085119247, 0.03698447346687317, 0.36104151606559753, 0.4128798544406891, 0.31848233938217163, 0.2625393271446228, 0.014314527623355389, 0.48902371525764465, 0.04176100343465805, 0.14346258342266083, 0.4633726477622986, 0.37935367226600647, 0.43266725540161133, 0.2988157868385315, 0.11005812138319016], dtype='float32').reshape([28]),
            paddle.to_tensor([0.400726854801178, 0.23812028765678406, 0.33950912952423096, 0.03992227837443352, 0.10345716029405594, 0.10074138641357422, 0.2604033350944519, 0.49887990951538086, 0.24347218871116638, 0.4053538739681244, 0.271322101354599, 0.38887298107147217, 0.022007230669260025, 0.337030291557312, 0.09219057112932205, 0.0314176082611084, 0.2534868121147156, 0.40293216705322266, 0.39645513892173767, 0.14698730409145355, 0.20306551456451416, 0.1427120566368103, 0.41154763102531433, 0.1057981327176094, 0.10773997008800507, 0.14912095665931702, 0.07828549295663834, 0.22073961794376373], dtype='float32').reshape([28]),
            paddle.to_tensor([0.3835720717906952, 0.14988070726394653, 0.4907921850681305, 0.24816785752773285, 0.12573176622390747, 0.11046959459781647, 0.3676002323627472, 0.33880874514579773, 0.07729387283325195, 0.16657647490501404, 0.2803055942058563, 0.49592527747154236, 0.0118730952963233, 0.07837511599063873, 0.014264591038227081, 0.3021746277809143, 0.31051766872406006, 0.4550554156303406, 0.47199249267578125, 0.47287365794181824, 0.2715742588043213, 0.1430174708366394, 0.18871352076530457, 0.4714662730693817, 0.15370546281337738, 0.012181080877780914, 0.3619578778743744, 0.03772812709212303], dtype='float32').reshape([28]),
            paddle.to_tensor([0.3908870220184326, 0.3651558756828308, 0.30836108326911926, 0.33914270997047424, 0.28067365288734436, 0.38813066482543945, 0.1707553267478943, 0.07300463318824768, 0.24074310064315796, 0.15844842791557312, 0.4511002004146576, 0.38855162262916565, 0.3028707504272461, 0.24632902443408966, 0.19870556890964508, 0.013986973091959953, 0.423489511013031, 0.0658717229962349, 0.1937900334596634, 0.395747572183609, 0.4290314316749573, 0.4059971272945404, 0.1356900930404663, 0.22565941512584686, 0.44439995288848877, 0.22403474152088165, 0.17712892591953278, 0.4226837456226349], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b2a353948af1ecdd657793221473e1fc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1152, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9b6f069c9cd733951ad51390c2706339(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_5e7cb23119f65b33bdbad71952f6dd12
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 256, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7c4eefeee543a858ed28d58eee0d7a2e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 150, 150], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_313cac7372f7a45a3800a21e81914d64(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bfc0293787b7cdcf292bb5dc509b8b92(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.08266740292310715, 0.3326117992401123, 0.07167389988899231, 0.10600130259990692, 0.07830946892499924, 0.2360459268093109, 0.12355240434408188, 0.18019327521324158, 0.23764856159687042, 0.4533138573169708, 0.10124162584543228, 0.4227214753627777, 0.44156041741371155, 0.40970712900161743, 0.2130749225616455, 0.4988507926464081, 0.4962197542190552, 0.4196777939796448, 0.45270535349845886, 0.33981797099113464, 0.19836178421974182, 0.3628980219364166, 0.02442532405257225, 0.18715102970600128, 0.2246405929327011, 0.15270541608333588, 0.09650278836488724, 0.3702944219112396, 0.12013506889343262, 0.3193557560443878], dtype='float32').reshape([30]),
            paddle.to_tensor([0.040291689336299896, 0.4460015892982483, 0.33406949043273926, 0.32723745703697205, 0.10534680634737015, 0.48861026763916016, 0.333077996969223, 0.13132859766483307, 0.04126731678843498, 0.04485857114195824, 0.002769152633845806, 0.06527003645896912, 0.4659237265586853, 0.2595478594303131, 0.4611443281173706, 0.17471250891685486, 0.3542982041835785, 0.45183080434799194, 0.0614946149289608, 0.21623419225215912, 0.3144015371799469, 0.38296470046043396, 0.39116427302360535, 0.39263391494750977, 0.14517219364643097, 0.13320353627204895, 0.4793875515460968, 0.06929554790258408, 0.34184107184410095, 0.09990071505308151], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2799417972564697, 0.04613993689417839, 0.3056589663028717, 0.40490883588790894, 0.23823167383670807, 0.3796955347061157, 0.20210421085357666, 0.24856998026371002, 0.35802286863327026, 0.14177533984184265, 0.09875629842281342, 0.3536297678947449, 0.2506760060787201, 0.21003291010856628, 0.2465067356824875, 0.3740497827529907, 0.33580613136291504, 0.06492390483617783, 0.05200868099927902, 0.048601288348436356, 0.15500310063362122, 0.18016403913497925, 0.2931024432182312, 0.053734175860881805, 0.18989509344100952, 0.020107336342334747, 0.08951228111982346, 0.4368230104446411, 0.11200962215662003, 0.4305684566497803], dtype='float32').reshape([30]),
            paddle.to_tensor([0.1663808524608612, 0.10701990127563477, 0.33556002378463745, 0.46417662501335144, 0.4118967354297638, 0.48860684037208557, 0.46013137698173523, 0.14238834381103516, 0.2959190607070923, 0.42268308997154236, 0.05723860487341881, 0.12170786410570145, 0.22984175384044647, 0.33236822485923767, 0.03696383163332939, 0.30001384019851685, 0.19736602902412415, 0.19625349342823029, 0.04035539552569389, 0.056582868099212646, 0.19218894839286804, 0.01859576813876629, 0.14119406044483185, 0.022678043693304062, 0.4648502469062805, 0.4333100914955139, 0.2563020586967468, 0.23946519196033478, 0.41593360900878906, 0.20260857045650482], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_86d25ff0e75f7f04238ba70428b54e78(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 740, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([740], dtype='float32', min=0, max=0.5),
            paddle.uniform([740], dtype='float32', min=0, max=0.5),
            paddle.uniform([740], dtype='float32', min=0, max=0.5),
            paddle.uniform([740], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7e8a295152984ff9fd4da842361e1aa1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_92d9b3ec959a89ae4943a28edf2009c7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3942674994468689, 0.41117703914642334, 0.39867809414863586, 0.05724943429231644, 0.46806278824806213, 0.4043314456939697, 0.42480599880218506, 0.28157612681388855, 0.27825087308883667, 0.29241713881492615, 0.05196133628487587, 0.3533118665218353, 0.3113365173339844, 0.449753075838089, 0.40228280425071716, 0.42244523763656616, 0.30505409836769104, 0.2797969877719879, 0.2051093578338623, 0.1708393394947052, 0.21230252087116241, 0.1250525712966919, 0.2769201993942261, 0.30474793910980225, 0.3925994336605072, 0.24918372929096222, 0.03126772120594978, 0.2807517647743225, 0.37485164403915405, 0.11380793899297714], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2532396912574768, 0.1042376384139061, 0.45429807901382446, 0.48088228702545166, 0.08256671577692032, 0.428146630525589, 0.12958021461963654, 0.13269184529781342, 0.06717493385076523, 0.25914132595062256, 0.22847475111484528, 0.15865874290466309, 0.15378741919994354, 0.3612782955169678, 0.47469598054885864, 0.19642353057861328, 0.31675484776496887, 0.03231598436832428, 0.19533483684062958, 0.4604489803314209, 0.21971362829208374, 0.33152395486831665, 0.44482505321502686, 0.19155508279800415, 0.21883684396743774, 0.19777552783489227, 0.07350970059633255, 0.27094876766204834, 0.029554255306720734, 0.25237828493118286], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3823098838329315, 0.09548832476139069, 0.1261126548051834, 0.1333061009645462, 0.3653515577316284, 0.3628090023994446, 0.2597837448120117, 0.3485141396522522, 0.006405817810446024, 0.49802422523498535, 0.22773277759552002, 0.02055632695555687, 0.18449415266513824, 0.38306480646133423, 0.3439948856830597, 0.3608899712562561, 0.3734813630580902, 0.09585332870483398, 0.37695592641830444, 0.1809365600347519, 0.14536777138710022, 0.38896697759628296, 0.09313210844993591, 0.1425667107105255, 0.2051343321800232, 0.10483654588460922, 0.10373890399932861, 0.23907600343227386, 0.27074864506721497, 0.05986243486404419], dtype='float32').reshape([30]),
            paddle.to_tensor([0.19527725875377655, 0.26123908162117004, 0.29035496711730957, 0.022691823542118073, 0.14646278321743011, 0.06992804259061813, 0.1965627372264862, 0.2881525158882141, 0.24201476573944092, 0.48333391547203064, 0.01851324737071991, 0.3324272334575653, 0.28025978803634644, 0.027434997260570526, 0.3413279950618744, 0.4316202402114868, 0.28000691533088684, 0.35831961035728455, 0.28516173362731934, 0.084372378885746, 0.12641440331935883, 0.14052653312683105, 0.48074325919151306, 0.471101850271225, 0.17000386118888855, 0.35553741455078125, 0.2674332857131958, 0.34947746992111206, 0.3609377443790436, 0.08933274447917938], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_75309ca4ee267497a0d0747a166771ba(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 4, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cd190daaaf836f21385cf3a7fc435599(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bcbc422043b1e98fe1533870f3face89(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0b3f6bd7ee97b587529b425ef40cb600(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 120, 120], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.42637595534324646, 0.14940190315246582, 0.30586668848991394, 0.2881331443786621, 0.028715146705508232, 0.4338342249393463, 0.3040919005870819, 0.1274074763059616, 0.049421586096286774, 0.3955332636833191, 0.07511714100837708, 0.42590293288230896, 0.13956168293952942, 0.3879309594631195, 0.36839544773101807, 0.47705501317977905, 0.158372163772583, 0.30151107907295227, 0.3449192941188812, 0.14333075284957886, 0.24746179580688477, 0.11995340883731842, 0.02549869380891323, 0.4584708511829376], dtype='float32').reshape([24]),
            paddle.to_tensor([0.42893174290657043, 0.4889124631881714, 0.2444901019334793, 0.49964508414268494, 0.3965003192424774, 0.4817289412021637, 0.37621575593948364, 0.3918575644493103, 0.20993143320083618, 0.492885023355484, 0.3289145231246948, 0.22504374384880066, 0.33613717555999756, 0.19042184948921204, 0.47270625829696655, 0.4507433772087097, 0.4372551739215851, 0.02643832005560398, 0.011023675091564655, 0.2329326719045639, 0.35384780168533325, 0.31212565302848816, 0.08738621324300766, 0.3757111728191376], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4096032381057739, 0.31712856888771057, 0.19889308512210846, 0.29764029383659363, 0.08668730407953262, 0.3026635944843292, 0.2008892297744751, 0.170473113656044, 0.3519105315208435, 0.2742384076118469, 0.044565387070178986, 0.4332265555858612, 0.40677422285079956, 0.13679777085781097, 0.31902971863746643, 0.16398575901985168, 0.1815553903579712, 0.3966962695121765, 0.481859415769577, 0.14843468368053436, 0.3347838819026947, 0.4038395583629608, 0.17742891609668732, 0.24729835987091064], dtype='float32').reshape([24]),
            paddle.to_tensor([0.21783801913261414, 0.26249179244041443, 0.42360755801200867, 0.49905499815940857, 0.06841637194156647, 0.1093105897307396, 0.20047542452812195, 0.0804874375462532, 0.18134750425815582, 0.4440312087535858, 0.001128421165049076, 0.4057962894439697, 0.012339584529399872, 0.25145402550697327, 0.007516694720834494, 0.19756995141506195, 0.23469796776771545, 0.24517104029655457, 0.42971691489219666, 0.2556772232055664, 0.01417792309075594, 0.01302388682961464, 0.3459291160106659, 0.010221459902822971], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_15cf28a93b8bcf836ba6763219208029(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.37206828594207764, 0.15072709321975708, 0.12195159494876862, 0.28047823905944824, 0.17344212532043457, 0.4759823977947235, 0.10363581031560898, 0.33985692262649536, 0.1357024610042572, 0.40106654167175293, 0.19071927666664124, 0.08390071988105774, 0.04128669202327728, 0.3487485647201538, 0.08737199753522873, 0.04964311420917511, 0.3192896842956543, 0.4598069190979004, 0.4438543915748596, 0.49915769696235657, 0.16893205046653748, 0.29933592677116394, 0.01568407192826271, 0.07996028661727905], dtype='float32').reshape([24]),
            paddle.to_tensor([0.12385350465774536, 0.30346453189849854, 0.4639785587787628, 0.48881930112838745, 0.09660856425762177, 0.010136702097952366, 0.07847388833761215, 0.39991632103919983, 0.45484212040901184, 0.02949502319097519, 0.24446268379688263, 0.26128312945365906, 0.4694211184978485, 0.11419495195150375, 0.3601366877555847, 0.30397218465805054, 0.07567790150642395, 0.49125269055366516, 0.016828520223498344, 0.2483620047569275, 0.29726505279541016, 0.2915956974029541, 0.013383982703089714, 0.3436981737613678], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2321106493473053, 0.3168782591819763, 0.23498530685901642, 0.3278248906135559, 0.32201600074768066, 0.41435614228248596, 0.16663473844528198, 0.4992527961730957, 0.0891592875123024, 0.2553395926952362, 0.1537356674671173, 0.09796447306871414, 0.42364344000816345, 0.41840630769729614, 0.13412019610404968, 0.4973457157611847, 0.07049518823623657, 0.31185901165008545, 0.10206357389688492, 0.2938089370727539, 0.38846179842948914, 0.34851548075675964, 0.2544538378715515, 0.3228921592235565], dtype='float32').reshape([24]),
            paddle.to_tensor([0.18749257922172546, 0.3711487948894501, 0.29993936419487, 0.3141605854034424, 0.07024718821048737, 0.25466760993003845, 0.02190433070063591, 0.3916429579257965, 0.3984019458293915, 0.3667357265949249, 0.4797421991825104, 0.10281436145305634, 0.4563201367855072, 0.3711916208267212, 0.17772623896598816, 0.07594567537307739, 0.09625150263309479, 0.02571965381503105, 0.015366719104349613, 0.2771857678890228, 0.3606129288673401, 0.4132366478443146, 0.05388397350907326, 0.15516524016857147], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e734d68a46b3e29944359d6743a396be(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 636, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([636], dtype='float32', min=0, max=0.5),
            paddle.uniform([636], dtype='float32', min=0, max=0.5),
            paddle.uniform([636], dtype='float32', min=0, max=0.5),
            paddle.uniform([636], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_328399dbbca2bf289b6ad58bb03c8842(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2726260721683502, 0.12494658678770065, 0.3804405927658081, 0.32246705889701843, 0.3797352612018585, 0.26107126474380493, 0.044899750500917435, 0.06521977484226227, 0.4977486729621887, 0.09736978262662888, 0.35437533259391785, 0.30065932869911194, 0.13519099354743958, 0.3226129412651062, 0.3572188913822174, 0.13322040438652039, 0.22254163026809692, 0.15906541049480438], dtype='float32').reshape([18]),
            paddle.to_tensor([0.07952851057052612, 0.35955122113227844, 0.2575135827064514, 0.41612008213996887, 0.4228430390357971, 0.3539094924926758, 0.307253897190094, 0.043081846088171005, 0.14811673760414124, 0.0916365534067154, 0.29526522755622864, 0.4640685021877289, 0.32161954045295715, 0.17381131649017334, 0.12114525586366653, 0.029377521947026253, 0.2716074585914612, 0.08927369117736816], dtype='float32').reshape([18]),
            paddle.to_tensor([0.18043628334999084, 0.1319061815738678, 0.10219193994998932, 0.404034286737442, 0.21471452713012695, 0.21469958126544952, 0.3489225506782532, 0.41833916306495667, 0.30991047620773315, 0.02284957282245159, 0.18588055670261383, 0.34326401352882385, 0.45629626512527466, 0.33665189146995544, 0.4510478973388672, 0.24314023554325104, 0.15829330682754517, 0.0923147052526474], dtype='float32').reshape([18]),
            paddle.to_tensor([0.26383525133132935, 0.1111050546169281, 0.20237451791763306, 0.4703989028930664, 0.2281714379787445, 0.2948072552680969, 0.4785386025905609, 0.49443161487579346, 0.06180458143353462, 0.43302807211875916, 0.2046285718679428, 0.1343582272529602, 0.05815261974930763, 0.30291885137557983, 0.4365880489349365, 0.22964346408843994, 0.3828798234462738, 0.4046010375022888], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8d1a047d253956a336af89f5fe33454d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 512], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.36318308115005493, 0.3971062898635864, 0.41969946026802063, 0.32588905096054077, 0.25730353593826294, 0.2807202935218811, 0.43877124786376953, 0.10294516384601593, 0.17366334795951843, 0.11526444554328918, 0.4863100051879883, 0.4414203464984894, 0.47268733382225037, 0.4042694568634033, 0.15314532816410065, 0.4952511489391327], dtype='float32').reshape([16]),
            paddle.to_tensor([0.11434301733970642, 0.3865504264831543, 0.27524229884147644, 0.3317108154296875, 0.32235652208328247, 0.2717705965042114, 0.14912453293800354, 0.42951422929763794, 0.20410066843032837, 0.1477343738079071, 0.1284995973110199, 0.3419526219367981, 0.2187502235174179, 0.36811748147010803, 0.04499730467796326, 0.4285838007926941], dtype='float32').reshape([16]),
            paddle.to_tensor([0.05492553859949112, 0.33018261194229126, 0.482780396938324, 0.459139883518219, 0.31841057538986206, 0.22572538256645203, 0.27213582396507263, 0.46881812810897827, 0.4863521158695221, 0.4212214946746826, 0.3874368667602539, 0.06832288950681686, 0.4095570743083954, 0.21371887624263763, 0.31956905126571655, 0.17474372684955597], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4935001730918884, 0.09453772753477097, 0.32641953229904175, 0.15762139856815338, 0.15112219750881195, 0.0654248297214508, 0.48521700501441956, 0.10305237770080566, 0.48340272903442383, 0.34845271706581116, 0.010074072517454624, 0.20267178118228912, 0.2243635058403015, 0.3860896825790405, 0.13952185213565826, 0.43289119005203247], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_caaaadf6391a7bf35596d2670aa88e4b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2658534646034241, 0.3525153696537018, 0.1236303299665451, 0.27164754271507263, 0.35521969199180603, 0.13548298180103302, 0.07361213862895966, 0.3784219026565552, 0.1426185816526413, 0.43420180678367615, 0.17898721992969513, 0.24632862210273743, 0.2933080792427063, 0.09204285591840744, 0.06485813856124878, 0.04852929338812828, 0.499632328748703, 0.246236190199852], dtype='float32').reshape([18]),
            paddle.to_tensor([0.39837440848350525, 0.10604868829250336, 0.20729278028011322, 0.26960301399230957, 0.35721510648727417, 0.405490905046463, 0.033685676753520966, 0.4625250995159149, 0.36902371048927307, 0.30540144443511963, 0.0366462878882885, 0.4070347547531128, 0.29168498516082764, 0.49825531244277954, 0.3340182602405548, 0.005103553179651499, 0.17958985269069672, 0.46981093287467957], dtype='float32').reshape([18]),
            paddle.to_tensor([0.35772940516471863, 0.4577823281288147, 0.22134900093078613, 0.14495788514614105, 0.15036892890930176, 0.08733787387609482, 0.05153905227780342, 0.24442699551582336, 0.08351561427116394, 0.02965461276471615, 0.36179524660110474, 0.023149259388446808, 0.3066636621952057, 0.3782614469528198, 0.3361015319824219, 0.31126782298088074, 0.17148412764072418, 0.0010176096111536026], dtype='float32').reshape([18]),
            paddle.to_tensor([0.06843443214893341, 0.059793438762426376, 0.2045600712299347, 0.34648802876472473, 0.45207950472831726, 0.33988431096076965, 0.2894091010093689, 0.19212904572486877, 0.3991115391254425, 0.3666251003742218, 0.2723828852176666, 0.43216827511787415, 0.007440469227731228, 0.36724430322647095, 0.3310566544532776, 0.15546974539756775, 0.2664032280445099, 0.434488445520401], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bf41b311c937ca7f26e1042bd814576d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 2, 27], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d85f1846ebf8e9d1cb3cd6dc8cb87070(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 64, 128], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.08830179274082184, 0.23572558164596558, 0.34726861119270325, 0.33840298652648926, 0.3741897642612457, 0.23203954100608826, 0.09327661991119385, 0.4502229690551758, 0.3621754050254822, 0.29501548409461975, 0.16914132237434387, 0.4054817259311676, 0.3733672797679901, 0.1862630844116211, 0.43151140213012695, 0.39756059646606445, 0.3810594379901886, 0.1706181764602661], dtype='float32').reshape([18]),
            paddle.to_tensor([0.36149466037750244, 0.0018827952444553375, 0.22561761736869812, 0.27552536129951477, 0.40013670921325684, 0.0982983410358429, 0.4088909327983856, 0.12012514472007751, 0.3106161653995514, 0.31011322140693665, 0.3631768524646759, 0.25646793842315674, 0.12249689549207687, 0.3942480981349945, 0.028838980942964554, 0.24303637444972992, 0.1199532300233841, 0.43643245100975037], dtype='float32').reshape([18]),
            paddle.to_tensor([0.30035462975502014, 0.46189138293266296, 0.07903297990560532, 0.06921744346618652, 0.039990223944187164, 0.13772502541542053, 0.4194990396499634, 0.41962844133377075, 0.4783308207988739, 0.2411242425441742, 0.027610890567302704, 0.4779480993747711, 0.07250603288412094, 0.16896413266658783, 0.18484075367450714, 0.4154278039932251, 0.17490175366401672, 0.21531526744365692], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2096322923898697, 0.44591253995895386, 0.09165648370981216, 0.13840201497077942, 0.07616684585809708, 0.3379780352115631, 0.4501338005065918, 0.4728783667087555, 0.21599923074245453, 0.32797858119010925, 0.04090748727321625, 0.0789223462343216, 0.4515133798122406, 0.4420877993106842, 0.4219169020652771, 0.08229459077119827, 0.14828631281852722, 0.17380762100219727], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_59c1f93d091bddb470240444ad035b2e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 512], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.08724578469991684, 0.22391502559185028, 0.2921609580516815, 0.12506970763206482, 0.28362220525741577, 0.06897164136171341, 0.19365903735160828, 0.2575545012950897, 0.4287346601486206, 0.11703341454267502, 0.4524993896484375, 0.13196028769016266, 0.2669548988342285, 0.4371451735496521, 0.4029473662376404, 0.16859257221221924], dtype='float32').reshape([16]),
            paddle.to_tensor([0.06289244443178177, 0.49765869975090027, 0.09797854721546173, 0.20488162338733673, 0.37353429198265076, 0.16528816521167755, 0.3588975965976715, 0.40628498792648315, 0.4768999516963959, 0.013068661093711853, 0.1616111695766449, 0.30946385860443115, 0.17617206275463104, 0.0039017880335450172, 0.14632150530815125, 0.26419562101364136], dtype='float32').reshape([16]),
            paddle.to_tensor([0.37134870886802673, 0.21721094846725464, 0.4547221064567566, 0.24829965829849243, 0.438607394695282, 0.11366365104913712, 0.1439381092786789, 0.43920180201530457, 0.012082516215741634, 0.4241442084312439, 0.09600191563367844, 0.39761829376220703, 0.40213772654533386, 0.39038610458374023, 0.10685057938098907, 0.2854120433330536], dtype='float32').reshape([16]),
            paddle.to_tensor([0.38124001026153564, 0.2739957273006439, 0.06792182475328445, 0.05347457900643349, 0.03243828937411308, 0.21478544175624847, 0.2846694886684418, 0.18631723523139954, 0.47044721245765686, 0.4064471125602722, 0.2972317934036255, 0.3289816975593567, 0.08060409128665924, 0.29261183738708496, 0.4958791732788086, 0.15457841753959656], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a9dee00dcca4c65b550187393e2d0ffd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 228, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([228], dtype='float32', min=0, max=0.5),
            paddle.uniform([228], dtype='float32', min=0, max=0.5),
            paddle.uniform([228], dtype='float32', min=0, max=0.5),
            paddle.uniform([228], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9822818a17920b1c2cfeeaad3bc862a1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_58d1065aba52e05d23a9eefdd612bf64(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 2, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_59711850262fd7baf8a5cfeef5779ac6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 4, 256, 512], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2056197226047516, 0.49910905957221985, 0.16178593039512634, 0.2731609642505646], dtype='float32').reshape([4]),
            paddle.to_tensor([0.3253297805786133, 0.4064253866672516, 0.24512547254562378, 0.03038686141371727], dtype='float32').reshape([4]),
            paddle.to_tensor([0.040694091469049454, 0.3854121267795563, 0.4627656042575836, 0.45210421085357666], dtype='float32').reshape([4]),
            paddle.to_tensor([0.07678058743476868, 0.08336684852838516, 0.13275456428527832, 0.3294254541397095], dtype='float32').reshape([4]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_309803cf404c981daf2e6c4dfddd6df7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 16, 50], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.24881142377853394, 0.4315139651298523, 0.2012159526348114, 0.4028458893299103, 0.4409763813018799, 0.34292933344841003, 0.012702560052275658, 0.3170943856239319], dtype='float32').reshape([8]),
            paddle.to_tensor([0.4769926071166992, 0.4110977351665497, 0.2260337620973587, 0.31483981013298035, 0.26374733448028564, 0.2102142870426178, 0.46244633197784424, 0.2838236391544342], dtype='float32').reshape([8]),
            paddle.to_tensor([0.40304210782051086, 0.42426398396492004, 0.3243057131767273, 0.39462000131607056, 0.3162097930908203, 0.36557677388191223, 0.3517792522907257, 0.15463249385356903], dtype='float32').reshape([8]),
            paddle.to_tensor([0.24772502481937408, 0.32721152901649475, 0.41840434074401855, 0.4571097791194916, 0.3635985851287842, 0.23594705760478973, 0.2628810405731201, 0.17523545026779175], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aa327a7f23305956e64e10916f3912a9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 120, 120], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.26974138617515564, 0.21913538873195648, 0.014060945250093937, 0.3311190903186798, 0.39920663833618164, 0.24949854612350464, 0.2695566713809967, 0.05600353702902794, 0.498158723115921, 0.1397704780101776, 0.0829244926571846, 0.3690219521522522, 0.4692615270614624, 0.18797551095485687, 0.3953356146812439, 0.34811973571777344, 0.31865787506103516, 0.03844810649752617, 0.08381995558738708, 0.21545542776584625, 0.2277928590774536, 0.30248525738716125, 0.4253983795642853, 0.4317779839038849], dtype='float32').reshape([24]),
            paddle.to_tensor([0.21400803327560425, 0.35056623816490173, 0.2893453538417816, 0.16614417731761932, 0.026327790692448616, 0.002786471974104643, 0.276481568813324, 0.0525578148663044, 0.0320693738758564, 0.2965221703052521, 0.09904949367046356, 0.10109002143144608, 0.3527502715587616, 0.4503720998764038, 0.4905766248703003, 0.08349096775054932, 0.21404685080051422, 0.12273234128952026, 0.4540914297103882, 0.3029356896877289, 0.04069199413061142, 0.4239339530467987, 0.17683035135269165, 0.21681080758571625], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4960777461528778, 0.47417762875556946, 0.20165356993675232, 0.41800475120544434, 0.1934182494878769, 0.1339852213859558, 0.26699212193489075, 0.12420061230659485, 0.38714078068733215, 0.48141831159591675, 0.3424554765224457, 0.4836178123950958, 0.20550373196601868, 0.32452598214149475, 0.36614570021629333, 0.014262094162404537, 0.4701522886753082, 0.156798854470253, 0.08312787115573883, 0.3251611888408661, 0.010495800524950027, 0.4243156611919403, 0.2988864481449127, 0.2460075318813324], dtype='float32').reshape([24]),
            paddle.to_tensor([0.27729088068008423, 0.06605091691017151, 0.437784343957901, 0.0472148172557354, 0.23791153728961945, 0.4221474528312683, 0.48932063579559326, 0.22477714717388153, 0.3703114986419678, 0.21135644614696503, 0.43039870262145996, 0.3963230550289154, 0.3527139127254486, 0.324859082698822, 0.47694456577301025, 0.3380140960216522, 0.03112633340060711, 0.49518871307373047, 0.438754141330719, 0.44218701124191284, 0.3076378107070923, 0.30508187413215637, 0.20468156039714813, 0.29216209053993225], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_96252a2e08cc53dd15a2e261d25114db(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.02502109296619892, 0.18564777076244354, 0.38547471165657043, 0.40371423959732056, 0.021208710968494415, 0.4953954815864563, 0.37099528312683105, 0.4346822500228882, 0.15389786660671234, 0.24652519822120667, 0.4773864448070526, 0.38680341839790344, 0.15685106813907623, 0.042118072509765625, 0.39601850509643555, 0.38026732206344604, 0.3007480502128601, 0.029546815901994705, 0.33113527297973633, 0.012698261998593807, 0.318571001291275, 0.1600002944469452, 0.2599238455295563, 0.37060943245887756, 0.4292602241039276, 0.09829191863536835, 0.06992893666028976, 0.2309103161096573], dtype='float32').reshape([28]),
            paddle.to_tensor([0.45142635703086853, 0.38721272349357605, 0.13831022381782532, 0.1978125125169754, 0.4389391243457794, 0.2337663322687149, 0.10144484043121338, 0.48596706986427307, 0.17133434116840363, 0.05733763799071312, 0.2212095707654953, 0.1067931056022644, 0.49315667152404785, 0.20695504546165466, 0.15966899693012238, 0.045675359666347504, 0.34368452429771423, 0.45593222975730896, 0.11498654633760452, 0.309394508600235, 0.097194142639637, 0.4841243326663971, 0.0616406612098217, 0.2406294196844101, 0.05642667040228844, 0.26484227180480957, 0.11099985241889954, 0.01158563606441021], dtype='float32').reshape([28]),
            paddle.to_tensor([0.10323427617549896, 0.404062420129776, 0.07504140585660934, 0.18628865480422974, 0.3503941595554352, 0.35340556502342224, 0.02574990689754486, 0.24129565060138702, 0.14751404523849487, 0.20131896436214447, 0.4218291938304901, 0.18004652857780457, 0.31689295172691345, 0.102700375020504, 0.3929620683193207, 0.3370153307914734, 0.454306036233902, 0.06588482111692429, 0.05771618336439133, 0.41180992126464844, 0.25383174419403076, 0.1740713268518448, 0.053375598043203354, 0.01817386969923973, 0.42324721813201904, 0.056331053376197815, 0.22255539894104004, 0.12880221009254456], dtype='float32').reshape([28]),
            paddle.to_tensor([0.20253561437129974, 0.22507213056087494, 0.33742356300354004, 0.0608067587018013, 0.3378477096557617, 0.3557877540588379, 0.4763294756412506, 0.147035151720047, 0.2492312788963318, 0.32586753368377686, 0.3054201304912567, 0.009825430810451508, 0.07314606010913849, 0.29963743686676025, 0.37721508741378784, 0.2029089778661728, 0.23163467645645142, 0.16428862512111664, 0.41732239723205566, 0.294879674911499, 0.398943156003952, 0.069069504737854, 0.38711249828338623, 0.29535648226737976, 0.004638517275452614, 0.46828070282936096, 0.25180599093437195, 0.3540080487728119], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5c3145ad40d1a6c4cd5c7c9fce94e43d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 320, 320], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.004517389927059412, 0.1613626927137375, 0.086527518928051, 0.49976974725723267, 0.2126629501581192, 0.11170902848243713, 0.299379825592041, 0.4579058885574341], dtype='float32').reshape([8]),
            paddle.to_tensor([0.1851528435945511, 0.47202983498573303, 0.457177996635437, 0.11550448089838028, 0.357665091753006, 0.14839480817317963, 0.21807900071144104, 0.1066323071718216], dtype='float32').reshape([8]),
            paddle.to_tensor([0.16318711638450623, 0.4992254674434662, 0.21026380360126495, 0.28395336866378784, 0.20181426405906677, 0.13212178647518158, 0.15169955790042877, 0.17357800900936127], dtype='float32').reshape([8]),
            paddle.to_tensor([0.012609456665813923, 0.14826510846614838, 0.28109970688819885, 0.05583838000893593, 0.1420629620552063, 0.0841028243303299, 0.35568955540657043, 0.2181779146194458], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4662d5d8de6519984cba9195918fb771(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4021516442298889, 0.006534121930599213, 0.11555151641368866, 0.48292848467826843, 0.014350319281220436, 0.22041507065296173, 0.3769086003303528, 0.40431368350982666, 0.13553646206855774, 0.10678501427173615, 0.2036401629447937, 0.3716942071914673, 0.0075631760992109776, 0.10795733332633972, 0.1482629030942917, 0.2500045597553253, 0.21678927540779114, 0.0472588874399662, 0.12269698083400726, 0.3093068301677704, 0.4759654402732849, 0.05632619932293892, 0.34042394161224365, 0.31703564524650574, 0.4770187437534332, 0.2629423439502716, 0.4404863119125366, 0.346566379070282, 0.228271022439003, 0.32355427742004395], dtype='float32').reshape([30]),
            paddle.to_tensor([0.22870008647441864, 0.25742387771606445, 0.30012762546539307, 0.13826613128185272, 0.17619799077510834, 0.35494062304496765, 0.06359607726335526, 0.11145197600126266, 0.4597333073616028, 0.2451573610305786, 0.12588219344615936, 0.1434991955757141, 0.3217664957046509, 0.4795064628124237, 0.42123690247535706, 0.3094013035297394, 0.05492570996284485, 0.17415203154087067, 0.1895800232887268, 0.21948076784610748, 0.42196404933929443, 0.41984453797340393, 0.39093270897865295, 0.1602475792169571, 0.09137799590826035, 0.2711956799030304, 0.2935984134674072, 0.10128793120384216, 0.4277116358280182, 0.32212942838668823], dtype='float32').reshape([30]),
            paddle.to_tensor([0.010043434798717499, 0.13607783615589142, 0.31112799048423767, 0.10316573828458786, 0.1359836906194687, 0.015324452891945839, 0.4699769914150238, 0.41811808943748474, 0.006986335851252079, 0.19689136743545532, 0.3270913064479828, 0.06711283326148987, 0.46043914556503296, 0.3976007103919983, 0.24481803178787231, 0.22679604589939117, 0.10746293514966965, 0.34262263774871826, 0.16462084650993347, 0.3212030231952667, 0.390468567609787, 0.07047542929649353, 0.09173766523599625, 0.282775342464447, 0.13069260120391846, 0.22878792881965637, 0.04668214172124863, 0.15129141509532928, 0.25039929151535034, 0.230825737118721], dtype='float32').reshape([30]),
            paddle.to_tensor([0.07774971425533295, 0.2597658336162567, 0.0755014419555664, 0.2510565519332886, 0.3110072612762451, 0.12026473879814148, 0.28765928745269775, 0.09944987297058105, 0.4477187395095825, 0.224410742521286, 0.10950466245412827, 0.1467767059803009, 0.3485620319843292, 0.09357450902462006, 0.14287850260734558, 0.1636277139186859, 0.10610464960336685, 0.01052207313477993, 0.2785511314868927, 0.04415522888302803, 0.16675323247909546, 0.44389835000038147, 0.410417377948761, 0.3957000970840454, 0.32539570331573486, 0.28743627667427063, 0.0481124110519886, 0.012873448431491852, 0.2773367464542389, 0.23312903940677643], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d8c735f7710bb5543088561920d59fe0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 122, 122], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d67ebb77c8963001d70e9bedeb759bd8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5eded01ee2065c01459106086e7272d1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2366163432598114, 0.48829203844070435, 0.3401624262332916, 0.08558619022369385, 0.23110125958919525, 0.26990121603012085, 0.3576425015926361, 0.1652248501777649, 0.4402904808521271, 0.10081321746110916, 0.11670586466789246, 0.3919980823993683, 0.43171119689941406, 0.20623892545700073, 0.36564281582832336, 0.4004974067211151, 0.4534798860549927, 0.1706765592098236, 0.33446893095970154, 0.36608967185020447], dtype='float32').reshape([20]),
            paddle.to_tensor([0.33608362078666687, 0.4373284578323364, 0.15779633820056915, 0.07890264689922333, 0.3892994523048401, 0.12803474068641663, 0.05379734933376312, 0.4731951057910919, 0.4470405876636505, 0.10329043865203857, 0.09405752271413803, 0.4783460795879364, 0.4752756953239441, 0.07581964135169983, 0.31847450137138367, 0.40558934211730957, 0.04152870178222656, 0.2077997773885727, 0.4527624249458313, 0.44657886028289795], dtype='float32').reshape([20]),
            paddle.to_tensor([0.23030129075050354, 0.1706889420747757, 0.024017520248889923, 0.34051769971847534, 0.33602556586265564, 0.11918141692876816, 0.16017042100429535, 0.4984910786151886, 0.09184929728507996, 0.4947552978992462, 0.1509530395269394, 0.23133409023284912, 0.1985270231962204, 0.46858683228492737, 0.22136595845222473, 0.2036704272031784, 0.4136519432067871, 0.04319039732217789, 0.04780685529112816, 0.35761550068855286], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4461251199245453, 0.21668782830238342, 0.2948550283908844, 0.47463706135749817, 0.4356338381767273, 0.41470304131507874, 0.3238699734210968, 0.041116416454315186, 0.4723573625087738, 0.23622575402259827, 0.08294206857681274, 0.22356270253658295, 0.20196375250816345, 0.4680069088935852, 0.0770358145236969, 0.2433466762304306, 0.14321203529834747, 0.2868545353412628, 0.25207948684692383, 0.4667971432209015], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ad5a173832ef499559512046e4464e66(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.17032857239246368, 0.2915742099285126, 0.29050859808921814, 0.3345533013343811, 0.025209657847881317, 0.4651901125907898, 0.1475590467453003, 0.2078465223312378, 0.24399004876613617, 0.12320473045110703, 0.10023067891597748, 0.2899816632270813, 0.3322734832763672, 0.1378895342350006, 0.4733951985836029, 0.30887046456336975], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4814269244670868, 0.14350953698158264, 0.34882164001464844, 0.26099908351898193, 0.2449628859758377, 0.4990530014038086, 0.435594767332077, 0.2884463965892792, 0.2775326371192932, 0.37634149193763733, 0.005997646600008011, 0.3980027139186859, 0.1704498827457428, 0.2459382861852646, 0.37487363815307617, 0.3405008614063263], dtype='float32').reshape([16]),
            paddle.to_tensor([0.34270021319389343, 0.1276799589395523, 0.06160816177725792, 0.2688855230808258, 0.04896115884184837, 0.14782938361167908, 0.45253217220306396, 0.17009608447551727, 0.10233491659164429, 0.05214615911245346, 0.15113048255443573, 0.23501697182655334, 0.3288686275482178, 0.3195801079273224, 0.34115472435951233, 0.16619272530078888], dtype='float32').reshape([16]),
            paddle.to_tensor([0.37169307470321655, 0.4914824068546295, 0.1488903909921646, 0.30759161710739136, 0.2285841405391693, 0.34805434942245483, 0.06331980973482132, 0.4256711006164551, 0.4029938280582428, 0.4242723882198334, 0.4130209982395172, 0.06707317382097244, 0.408409982919693, 0.37914493680000305, 0.3308315873146057, 0.3565681278705597], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2b206aec1f440ef43cd77a185160fdc2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 31, 31], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_13093d081c073c8f88a3005da2d48a70(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 320, 320], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.33560746908187866, 0.15889635682106018, 0.4810957908630371, 0.3275509476661682, 0.18408896028995514, 0.015233049169182777, 0.2117256075143814, 0.030172595754265785], dtype='float32').reshape([8]),
            paddle.to_tensor([0.0186985582113266, 0.23276101052761078, 0.2087249904870987, 0.27584174275398254, 0.13890935480594635, 0.04071784019470215, 0.44706594944000244, 0.4464760422706604], dtype='float32').reshape([8]),
            paddle.to_tensor([0.1684921681880951, 0.03232245892286301, 0.4888756275177002, 0.18383562564849854, 0.07849989086389542, 0.48409682512283325, 0.22236014902591705, 0.12990052998065948], dtype='float32').reshape([8]),
            paddle.to_tensor([0.22140534222126007, 0.3796386420726776, 0.3304790258407593, 0.0829581692814827, 0.26180046796798706, 0.3368920087814331, 0.12347830086946487, 0.3022972643375397], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c94aeeb24aa908019158afc87c7e1db2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_5e7cb23119f65b33bdbad71952f6dd12
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 1, 1], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9677fc4c8dfb93fd81b71474bd5576ea(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1f6a740214568bfffc91c14bcd016a71(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 6, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_06b056356b38bde0ab281511c212d85b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.36387234926223755, 0.10971072316169739, 0.023315023630857468, 0.47040897607803345, 0.12834134697914124, 0.34211263060569763, 0.3602901101112366, 0.056362349539995193, 0.2950419485569, 0.05781487748026848, 0.2733556032180786, 0.12352646142244339, 0.22785642743110657, 0.49913257360458374, 0.0733875259757042, 0.28260499238967896], dtype='float32').reshape([16]),
            paddle.to_tensor([0.44939520955085754, 0.35905441641807556, 0.009321656078100204, 0.14911094307899475, 0.37138962745666504, 0.02927386574447155, 0.34746304154396057, 0.4115685522556305, 0.4374260902404785, 0.2007829248905182, 0.2395544946193695, 0.018337484449148178, 0.2628971338272095, 0.3532768487930298, 0.4935590326786041, 0.0143587039783597], dtype='float32').reshape([16]),
            paddle.to_tensor([0.25497645139694214, 0.34898221492767334, 0.1234838142991066, 0.4407278895378113, 0.42747753858566284, 0.222124844789505, 0.08843725919723511, 0.4585980772972107, 0.2785551846027374, 0.33474627137184143, 0.021137937903404236, 0.046192195266485214, 0.25017449259757996, 0.3451487720012665, 0.38763660192489624, 0.1561509072780609], dtype='float32').reshape([16]),
            paddle.to_tensor([0.43787118792533875, 0.0431307889521122, 0.29739058017730713, 0.18054789304733276, 0.019374597817659378, 0.4643935263156891, 0.25145700573921204, 0.2067590057849884, 0.4217367470264435, 0.36179906129837036, 0.475715309381485, 0.36649590730667114, 0.13057945668697357, 0.12147422134876251, 0.42129719257354736, 0.13145627081394196], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c35e0df8fa6d9af4c18295029f2fd979(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 75, 75], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5ee53e87e3f357385ff61235dbedba1b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3485841751098633, 0.15602469444274902, 0.05642937496304512, 0.3947772681713104, 0.3937263488769531, 0.4960971474647522, 0.4626069664955139, 0.44909659028053284, 0.12171941250562668, 0.4313916563987732, 0.4052213132381439, 0.34585142135620117, 0.49326926469802856, 0.4772909879684448, 0.17521165311336517, 0.06176222115755081, 0.4995841085910797, 0.33809518814086914], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4456441104412079, 0.25962767004966736, 0.2708456218242645, 0.07371868938207626, 0.4354732036590576, 0.4406301975250244, 0.4610651433467865, 0.4974587559700012, 0.4746517539024353, 0.1078934594988823, 0.3953624665737152, 0.2898519039154053, 0.1774984896183014, 0.3964877128601074, 0.14365573227405548, 0.38482677936553955, 0.38246941566467285, 0.33239543437957764], dtype='float32').reshape([18]),
            paddle.to_tensor([0.0955808013677597, 0.11841398477554321, 0.3322729766368866, 0.16572244465351105, 0.24805626273155212, 0.4456835687160492, 0.28012120723724365, 0.32317644357681274, 0.16729876399040222, 0.01674746535718441, 0.1754511296749115, 0.2356008142232895, 0.4453357458114624, 0.2927207946777344, 0.36711275577545166, 0.19680099189281464, 0.3186336159706116, 0.08504051715135574], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2031092792749405, 0.4058345556259155, 0.2578708529472351, 0.43422549962997437, 0.05950313061475754, 0.3584403693675995, 0.20642627775669098, 0.0496852770447731, 0.07442601025104523, 0.27342432737350464, 0.06858769804239273, 0.42508044838905334, 0.2913971543312073, 0.04339755326509476, 0.33016738295555115, 0.261842280626297, 0.2692071199417114, 0.4942094087600708], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c45ff138eb9b9ae2bc6e7c9ac3976d0c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 720, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c5336c9c634f5b4100d61916b4996df7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 122, 122], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4542640447616577, 0.2233237326145172, 0.2788946330547333, 0.043399736285209656, 0.29297590255737305, 0.12081984430551529, 0.4258504807949066, 0.4414939880371094, 0.212912455201149, 0.47963857650756836, 0.08844500035047531, 0.34109771251678467, 0.1288922280073166, 0.07664437592029572, 0.020574122667312622, 0.4680425822734833], dtype='float32').reshape([16]),
            paddle.to_tensor([0.34595584869384766, 0.05041324347257614, 0.31408432126045227, 0.35663750767707825, 0.43537765741348267, 0.40447649359703064, 0.46249455213546753, 0.27987608313560486, 0.07550642639398575, 0.4508391320705414, 0.34206148982048035, 0.358233243227005, 0.20191195607185364, 0.0863535925745964, 0.42186903953552246, 0.1570708006620407], dtype='float32').reshape([16]),
            paddle.to_tensor([0.26193472743034363, 0.14930343627929688, 0.29354313015937805, 0.48161864280700684, 0.18211200833320618, 0.2609497308731079, 0.26641395688056946, 0.04146289825439453, 0.4870017468929291, 0.09470292925834656, 0.3938390910625458, 0.4305405020713806, 0.0004160076496191323, 0.488193541765213, 0.4563960134983063, 0.19044387340545654], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4218587279319763, 0.1843581348657608, 0.28791874647140503, 0.31068456172943115, 0.061808500438928604, 0.24591153860092163, 0.16084414720535278, 0.12204910814762115, 0.07468800246715546, 0.3325767517089844, 0.2920130789279938, 0.19572938978672028, 0.4522601366043091, 0.2902202904224396, 0.21521297097206116, 0.07678381353616714], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fce65fb236704f8df35b21778860d531(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 17, 17], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9b03429b390873b3d834279af08c438c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.05145762115716934, 0.4362355172634125, 0.3320738971233368, 0.34613052010536194, 0.03706630691885948, 0.008618766441941261, 0.21787944436073303, 0.09660176932811737, 0.24167470633983612, 0.28443917632102966, 0.3119720220565796, 0.36190128326416016, 0.22260810434818268, 0.49911901354789734, 0.09477139264345169, 0.3713550865650177, 0.20501424372196198, 0.06428373605012894, 0.04067568853497505, 0.14009413123130798], dtype='float32').reshape([20]),
            paddle.to_tensor([0.39138901233673096, 0.010913874953985214, 0.010548904538154602, 0.03343209996819496, 0.18550552427768707, 0.40148162841796875, 0.3950009346008301, 0.17411543428897858, 0.020178912207484245, 0.17684940993785858, 0.4778587520122528, 0.4705238342285156, 0.17223572731018066, 0.12952359020709991, 0.3060671389102936, 0.11296340823173523, 0.4211275577545166, 0.3425860106945038, 0.15298894047737122, 0.02356787770986557], dtype='float32').reshape([20]),
            paddle.to_tensor([0.20160050690174103, 0.31267353892326355, 0.00433665607124567, 0.21422816812992096, 0.16513141989707947, 0.4747307002544403, 0.4093828499317169, 0.4127509295940399, 0.011555265635251999, 0.4971519410610199, 0.26609110832214355, 0.17657968401908875, 0.34308838844299316, 0.26896828413009644, 0.41995689272880554, 0.22449375689029694, 0.040274132043123245, 0.2522364854812622, 0.10130152106285095, 0.4077768623828888], dtype='float32').reshape([20]),
            paddle.to_tensor([0.37228432297706604, 0.4338195025920868, 0.3936992287635803, 0.15668220818042755, 0.4720284044742584, 0.14814071357250214, 0.3281020522117615, 0.3370000720024109, 0.12869182229042053, 0.21334205567836761, 0.03305468708276749, 0.41928547620773315, 0.1366361677646637, 0.4616115391254425, 0.030119849368929863, 0.013880498707294464, 0.36807745695114136, 0.41935425996780396, 0.26065686345100403, 0.3075753450393677], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c54541fac09d4234cd8280aeba8d76f0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_87f30fcdfd98b9977b4493f6d30db5d1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4875464141368866, 0.1857491135597229, 0.03369737043976784, 0.003187716007232666, 0.30275899171829224, 0.18855442106723785, 0.15402880311012268, 0.14086520671844482, 0.46239209175109863, 0.15760013461112976, 0.23946814239025116, 0.2562108635902405], dtype='float32').reshape([12]),
            paddle.to_tensor([0.004701882600784302, 0.13434578478336334, 0.08790092915296555, 0.35877370834350586, 0.23707471787929535, 0.2939445376396179, 0.4213683605194092, 0.06661588698625565, 0.24429792165756226, 0.2790313959121704, 0.038675226271152496, 0.18515925109386444], dtype='float32').reshape([12]),
            paddle.to_tensor([0.03799980878829956, 0.3436416983604431, 0.12664180994033813, 0.3956579566001892, 0.25612565875053406, 0.02418309450149536, 0.4170668423175812, 0.30110445618629456, 0.1893661916255951, 0.26373544335365295, 0.39688917994499207, 0.0225161612033844], dtype='float32').reshape([12]),
            paddle.to_tensor([0.05776249244809151, 0.3713456392288208, 0.20549167692661285, 0.40571659803390503, 0.25153470039367676, 0.32149791717529297, 0.4811698794364929, 0.18073983490467072, 0.12444132566452026, 0.37670865654945374, 0.11936701089143753, 0.266423761844635], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_26efaf8d5bcabc8f8b9ee03ab166b082(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 16, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f0ee75dbd23e2c0d57701a454c5ff2ae(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 432, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_23c2b10d7a8abf9a67611a173566f3d2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e080da6ae5a1ec4373ce5d8dc69a9edb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.19987818598747253, 0.3507937788963318, 0.3460952043533325, 0.12873703241348267, 0.2194591909646988, 0.026677072048187256, 0.029944144189357758, 0.47120338678359985, 0.4896712601184845, 0.4086358845233917, 0.42983922362327576, 0.15745136141777039, 0.2000001072883606, 0.1839534491300583, 0.007117083761841059, 0.06627611070871353, 0.03575488179922104, 0.08214614540338516, 0.10369855910539627, 0.13404090702533722, 0.3939386308193207, 0.12179350107908249, 0.43929922580718994, 0.06374818086624146, 0.2957797944545746, 0.419425368309021, 0.4743346869945526, 0.1336696296930313, 0.2021740972995758, 0.4517301917076111], dtype='float32').reshape([30]),
            paddle.to_tensor([0.13564392924308777, 0.255569726228714, 0.3937496244907379, 0.2843586206436157, 0.06014568358659744, 0.015978731215000153, 0.1588791012763977, 0.16141845285892487, 0.38280487060546875, 0.4451638162136078, 0.2545248866081238, 0.14836831390857697, 0.4561672508716583, 0.08758396655321121, 0.3298121392726898, 0.265542596578598, 0.20414172112941742, 0.20442746579647064, 0.33265578746795654, 0.13448677957057953, 0.022906553000211716, 0.46136337518692017, 0.15375636518001556, 0.38949355483055115, 0.480660080909729, 0.15164297819137573, 0.43783125281333923, 0.4531463086605072, 0.4839109480381012, 0.38418546319007874], dtype='float32').reshape([30]),
            paddle.to_tensor([0.15108130872249603, 0.39818355441093445, 0.4179007112979889, 0.41523095965385437, 0.02690766006708145, 0.4619990587234497, 0.27683958411216736, 0.44864892959594727, 0.26510557532310486, 0.34240344166755676, 0.20586764812469482, 0.3299236595630646, 0.04491451755166054, 0.2454485446214676, 0.3001929819583893, 0.332384318113327, 0.07914125919342041, 0.005491870921105146, 0.15903711318969727, 0.294598788022995, 0.03328901156783104, 0.1850833147764206, 0.28094053268432617, 0.28151294589042664, 0.3964580297470093, 0.4650106132030487, 0.16747061908245087, 0.06950654834508896, 0.28954729437828064, 0.3175862729549408], dtype='float32').reshape([30]),
            paddle.to_tensor([0.03079526126384735, 0.16486532986164093, 0.2869861125946045, 0.38036873936653137, 0.29957929253578186, 0.3369346857070923, 0.10700704157352448, 0.32718658447265625, 0.021352441981434822, 0.1586420089006424, 0.1001531183719635, 0.2972052991390228, 0.49193188548088074, 0.33929377794265747, 0.2808956801891327, 0.14637556672096252, 0.25116077065467834, 0.05894358828663826, 0.37731117010116577, 0.36771225929260254, 0.3019388020038605, 0.2523133158683777, 0.06480849534273148, 0.042957425117492676, 0.4443734288215637, 0.1289726197719574, 0.06316057592630386, 0.09749314188957214, 0.45184269547462463, 0.0021426258608698845], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e62051d30473a8835c28f0200d96df4f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 10, 10], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a1772a19394a88905242a47ade435330(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 64, 128], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2048390507698059, 0.06112600490450859, 0.07491500675678253, 0.11194673925638199, 0.378814697265625, 0.35251879692077637, 0.09034979343414307, 0.24855399131774902, 0.03262976557016373, 0.44428691267967224, 0.48383742570877075, 0.0008547225152142346, 0.014496229588985443, 0.17474114894866943, 0.13718844950199127, 0.18708954751491547, 0.29348716139793396, 0.06003760173916817], dtype='float32').reshape([18]),
            paddle.to_tensor([0.019534125924110413, 0.28519749641418457, 0.1649978905916214, 0.39771145582199097, 0.1982206404209137, 0.47596943378448486, 0.019675379619002342, 0.49162766337394714, 0.4738527238368988, 0.26730430126190186, 0.23934423923492432, 0.06081881374120712, 0.14326006174087524, 0.33321163058280945, 0.2769409120082855, 0.05198736488819122, 0.3097449839115143, 0.3163200616836548], dtype='float32').reshape([18]),
            paddle.to_tensor([0.41386678814888, 0.19628925621509552, 0.13655003905296326, 0.2969512641429901, 0.4155202805995941, 0.3990997076034546, 0.3712550401687622, 0.0678415521979332, 0.3578298091888428, 0.3886222839355469, 0.08036662638187408, 0.03331141546368599, 0.20534084737300873, 0.13697898387908936, 0.0423455610871315, 0.2866290509700775, 0.22764840722084045, 0.07952350378036499], dtype='float32').reshape([18]),
            paddle.to_tensor([0.11371907591819763, 0.44684869050979614, 0.17794162034988403, 0.0797581747174263, 0.2479066103696823, 0.10577213764190674, 0.4342803955078125, 0.21416333317756653, 0.12775076925754547, 0.3013792932033539, 0.3567408621311188, 0.01760568469762802, 0.2557511031627655, 0.04907623678445816, 0.2219400554895401, 0.3756687045097351, 0.11686637997627258, 0.34635528922080994], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5e2e08e2645e18408a487c1ec78022c9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.44193047285079956, 0.4631202816963196, 0.2654035985469818, 0.19931289553642273, 0.4777919054031372, 0.30191680788993835, 0.4889245927333832, 0.26278960704803467, 0.16169314086437225, 0.17090852558612823, 0.32022395730018616, 0.38036519289016724], dtype='float32').reshape([12]),
            paddle.to_tensor([0.26556992530822754, 0.2106226086616516, 0.019618218764662743, 0.4959261417388916, 0.2739666998386383, 0.06019440293312073, 0.3535417318344116, 0.2349887490272522, 0.28454795479774475, 0.23248720169067383, 0.31690913438796997, 0.27353793382644653], dtype='float32').reshape([12]),
            paddle.to_tensor([0.10136524587869644, 0.1734035164117813, 0.2708395719528198, 0.3096989095211029, 0.3383750319480896, 0.3005821406841278, 0.4200224280357361, 0.4688390791416168, 0.32565414905548096, 0.4565167725086212, 0.2058558613061905, 0.4853266775608063], dtype='float32').reshape([12]),
            paddle.to_tensor([0.4125807285308838, 0.2877490222454071, 0.43626341223716736, 0.3398520350456238, 0.23732784390449524, 0.3090634346008301, 0.08180427551269531, 0.4423612654209137, 0.4327460825443268, 0.12316584587097168, 0.3830150365829468, 0.2496756613254547], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_43ebdddd7cf19351a1d937c54892bb6c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 960, 32, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_56f21387da49e397a1180e73fd587236(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.43956029415130615, 0.2618945837020874, 0.33270958065986633, 0.0764089897274971, 0.23841415345668793, 0.19743819534778595, 0.19762961566448212, 0.32940444350242615, 0.07403285801410675, 0.0363231897354126, 0.45662838220596313, 0.17618101835250854, 0.1682659089565277, 0.260684609413147, 0.005387783981859684, 0.08250013738870621, 0.0832662582397461, 0.3738735020160675, 0.26519879698753357, 0.48356375098228455, 0.1708218902349472, 0.3390096127986908, 0.47291114926338196, 0.37432581186294556], dtype='float32').reshape([24]),
            paddle.to_tensor([0.18078291416168213, 0.04446719214320183, 0.4711315631866455, 0.4940779209136963, 0.19509382545948029, 0.07130814343690872, 0.060990773141384125, 0.4492567181587219, 0.43736371397972107, 0.3671230971813202, 0.025312960147857666, 0.11586582660675049, 0.40903711318969727, 0.10549363493919373, 0.3722045421600342, 0.36921605467796326, 0.06613767892122269, 0.07386814802885056, 0.22426414489746094, 0.27944329380989075, 0.004551881458610296, 0.3447515368461609, 0.44411125779151917, 0.27018582820892334], dtype='float32').reshape([24]),
            paddle.to_tensor([0.40962812304496765, 0.06917614489793777, 0.17513437569141388, 0.2772964537143707, 0.1714654266834259, 0.1521741896867752, 0.28980568051338196, 0.07912041246891022, 0.055638447403907776, 0.24453741312026978, 0.3327365517616272, 0.12453848123550415, 0.029534943401813507, 0.42154839634895325, 0.20757940411567688, 0.2752752900123596, 0.14969278872013092, 0.384770005941391, 0.12082535773515701, 0.31496959924697876, 0.17619894444942474, 0.24261197447776794, 0.4553678631782532, 0.2505110204219818], dtype='float32').reshape([24]),
            paddle.to_tensor([0.25900837779045105, 0.11456315964460373, 0.1914728581905365, 0.14898964762687683, 0.3863222002983093, 0.2442643642425537, 0.06240740418434143, 0.4687534272670746, 0.46178025007247925, 0.39478927850723267, 0.2310628890991211, 0.015343272127211094, 0.42205408215522766, 0.028375044465065002, 0.12169558554887772, 0.2464599609375, 0.2086537778377533, 0.39178603887557983, 0.3163089156150818, 0.32475054264068604, 0.09825575351715088, 0.31394481658935547, 0.3109091818332672, 0.17237624526023865], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8c6d8a8c933f2a5c08148ba995f6599c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_42523e384eb13a180c8cb3c4a2fe5651(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 224, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bac329050ce353d50389c16d6ba69d7f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 720, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_62bf3a0a0c2ff399b45a5a03384da9a0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e1c984bc7e4322cf21a1844397da5b8d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 224, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cfad5db87791c62f91fd9d9604dfc71c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 4, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cd756beb96205ff7e06e05da5dc43cd7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 128, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4e739d3155eaae0cb16d4fbc11edc39c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.06020493432879448, 0.1778704822063446, 0.3200996220111847, 0.33132773637771606, 0.2679298520088196, 0.4980018436908722, 0.18144267797470093, 0.1893560141324997, 0.13193051517009735, 0.051897794008255005, 0.29538393020629883, 0.23182673752307892, 0.29836297035217285, 0.17498156428337097, 0.3257097899913788, 0.491686075925827, 0.14432649314403534, 0.42467981576919556], dtype='float32').reshape([18]),
            paddle.to_tensor([0.04558555781841278, 0.06692884862422943, 0.07125784456729889, 0.4450182020664215, 0.42059487104415894, 0.34011101722717285, 0.17976491153240204, 0.35668450593948364, 0.057188790291547775, 0.05143535137176514, 0.47429293394088745, 0.33533865213394165, 0.33547186851501465, 0.32894062995910645, 0.315989226102829, 0.45401087403297424, 0.18577280640602112, 0.22010886669158936], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4565608501434326, 0.03671389818191528, 0.4351940453052521, 0.38676831126213074, 0.4508010149002075, 0.3268791139125824, 0.03230594843626022, 0.31984642148017883, 0.349384605884552, 0.0403604730963707, 0.17030733823776245, 0.20766353607177734, 0.4327874779701233, 0.38122448325157166, 0.26635053753852844, 0.2229325771331787, 0.3500029444694519, 0.21518918871879578], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2308180183172226, 0.04707056283950806, 0.3930886387825012, 0.2130453884601593, 0.23133350908756256, 0.41719427704811096, 0.4430675506591797, 0.2263357937335968, 0.03742056339979172, 0.40070438385009766, 0.15529142320156097, 0.09559866040945053, 0.41711723804473877, 0.289772629737854, 0.09047102183103561, 0.2077723890542984, 0.06434068828821182, 0.2895796298980713], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ce047025a4980fdcd1b7180727b084e2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 64, 128], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3349933624267578, 0.16783660650253296, 0.038353756070137024, 0.3587666153907776, 0.33246615529060364, 0.27969059348106384, 0.1512577086687088, 0.13258826732635498, 0.05041671544313431, 0.1425596922636032, 0.054761674255132675, 0.21093568205833435, 0.34859684109687805, 0.45601874589920044, 0.4562479257583618, 0.3963727653026581, 0.2564408779144287, 0.15969014167785645], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3886531889438629, 0.03571959212422371, 0.2818920612335205, 0.4086408019065857, 0.3343619704246521, 0.44698551297187805, 0.35784631967544556, 0.11229302734136581, 0.10694149136543274, 0.481152206659317, 0.28146541118621826, 0.42005655169487, 0.49091851711273193, 0.008450529538094997, 0.4943361282348633, 0.05329982563853264, 0.45349499583244324, 0.24981360137462616], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4490726590156555, 0.4354665279388428, 0.12204866111278534, 0.09665228426456451, 0.18607285618782043, 0.25927528738975525, 0.1515912264585495, 0.31266915798187256, 0.3212765157222748, 0.42546990513801575, 0.2749137282371521, 0.20242692530155182, 0.1211627647280693, 0.4419942796230316, 0.48325225710868835, 0.19326287508010864, 0.4585714638233185, 0.1896917223930359], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4190032184123993, 0.11564133316278458, 0.32042911648750305, 0.05408761277794838, 0.33695077896118164, 0.03120323270559311, 0.013979473151266575, 0.025743689388036728, 0.13039395213127136, 0.29701030254364014, 0.47182798385620117, 0.3072027862071991, 0.3847019076347351, 0.4591963291168213, 0.12217704951763153, 0.10764104872941971, 0.4303581118583679, 0.06525381654500961], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_da6a42a843861ff6964e6a12c5522cdf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 480, 480], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.01862477883696556, 0.08454956114292145, 0.4933224022388458, 0.19541111588478088, 0.07697126269340515, 0.04487411305308342, 0.14967575669288635, 0.10017766058444977], dtype='float32').reshape([8]),
            paddle.to_tensor([0.026004768908023834, 0.2520751655101776, 0.35902395844459534, 0.24336989223957062, 0.4083203971385956, 0.36893296241760254, 0.15596255660057068, 0.3249524235725403], dtype='float32').reshape([8]),
            paddle.to_tensor([0.23212343454360962, 0.30324995517730713, 0.15701790153980255, 0.39866089820861816, 0.23456528782844543, 0.41942766308784485, 0.059370122849941254, 0.3336818814277649], dtype='float32').reshape([8]),
            paddle.to_tensor([0.32993438839912415, 0.2536609470844269, 0.059920139610767365, 0.3319425582885742, 0.09294430166482925, 0.09736023843288422, 0.11071489751338959, 0.3909634053707123], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_56ba344d63318f77354f22823329049c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 10, 10], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9899ad36d165d01c98441394d4d347b5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 512, 512], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2476549595594406, 0.15866953134536743, 0.18790319561958313, 0.035809606313705444, 0.48744234442710876, 0.39407312870025635, 0.2045973241329193, 0.08221636712551117], dtype='float32').reshape([8]),
            paddle.to_tensor([0.48663899302482605, 0.4695424735546112, 0.13217005133628845, 0.49973931908607483, 0.4159238338470459, 0.24447418749332428, 0.16940294206142426, 0.47569406032562256], dtype='float32').reshape([8]),
            paddle.to_tensor([0.30473798513412476, 0.37336021661758423, 0.1570843607187271, 0.13838106393814087, 0.33400779962539673, 0.3937869071960449, 0.0772354006767273, 0.16712164878845215], dtype='float32').reshape([8]),
            paddle.to_tensor([0.41577738523483276, 0.22338689863681793, 0.07284499704837799, 0.13310249149799347, 0.2779545485973358, 0.46260878443717957, 0.17648166418075562, 0.12561675906181335], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d01d12106e4628ede8a538dd21906a14(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e98414991b013adf834a6e9968fe028a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.009390393272042274, 0.35012465715408325, 0.17271187901496887, 0.0911674052476883, 0.24416808784008026, 0.0480111762881279, 0.334568589925766, 0.2679591774940491, 0.4049544632434845, 0.06099552661180496, 0.055896300822496414, 0.4037972092628479, 0.20152007043361664, 0.33979883790016174, 0.4191574454307556, 0.4373581111431122, 0.3779507577419281, 0.35178542137145996, 0.13390907645225525, 0.1601615846157074, 0.3506978750228882, 0.35923147201538086, 0.22209542989730835, 0.031040094792842865, 0.12352709472179413, 0.3695752024650574, 0.2243245244026184, 0.10598855465650558], dtype='float32').reshape([28]),
            paddle.to_tensor([0.2719328701496124, 0.08418796211481094, 0.29651010036468506, 0.22506259381771088, 0.25305449962615967, 0.15283745527267456, 0.20348094403743744, 0.04387187212705612, 0.3472205698490143, 0.10351359844207764, 0.42896440625190735, 0.14787130057811737, 0.29689761996269226, 0.4469172954559326, 0.15771882236003876, 0.14589153230190277, 0.020399395376443863, 0.4433722198009491, 0.3790603578090668, 0.2401091307401657, 0.07257129997015, 0.07142292708158493, 0.42903974652290344, 0.23356103897094727, 0.08495361357927322, 0.201354518532753, 0.27516135573387146, 0.0952683836221695], dtype='float32').reshape([28]),
            paddle.to_tensor([0.09855351597070694, 0.2711191475391388, 0.14338918030261993, 0.01723775453865528, 0.1338512897491455, 0.06347361207008362, 0.1181226521730423, 0.030325470492243767, 0.10994736105203629, 0.4827248454093933, 0.3566919267177582, 0.11878903210163116, 0.3738926947116852, 0.06335712969303131, 0.059060126543045044, 0.19002893567085266, 0.12171981483697891, 0.11759354919195175, 0.2269233614206314, 0.3404915928840637, 0.41514623165130615, 0.14967124164104462, 0.2579173147678375, 0.04516788199543953, 0.0367593988776207, 0.35327762365341187, 0.02622293494641781, 0.2426902800798416], dtype='float32').reshape([28]),
            paddle.to_tensor([0.009911123663187027, 0.15213970839977264, 0.029266197234392166, 0.26444098353385925, 0.12711858749389648, 0.035499151796102524, 0.00940407533198595, 0.40697968006134033, 0.24386590719223022, 0.0012659477069973946, 0.17993603646755219, 0.36093616485595703, 0.32057225704193115, 0.03132252022624016, 0.3012673556804657, 0.266499787569046, 0.2022961676120758, 0.4872473180294037, 0.3517465591430664, 0.43853023648262024, 0.302907794713974, 0.24719183146953583, 0.16202007234096527, 0.373589426279068, 0.40665745735168457, 0.4286687970161438, 0.10698604583740234, 0.43342769145965576], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9c681ed43e80a75a4ce3eac352fc99b4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 256, 512], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_45a61101c044f7bf6622cdd807c27f03(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 30, 30], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9d23d218a1b5bc0395840a0f1e9d1738(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 168, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([168], dtype='float32', min=0, max=0.5),
            paddle.uniform([168], dtype='float32', min=0, max=0.5),
            paddle.uniform([168], dtype='float32', min=0, max=0.5),
            paddle.uniform([168], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_26da245594c1a1e70b6c38ade6c3ffe7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.16047725081443787, 0.2539287805557251, 0.23043249547481537, 0.048020195215940475, 0.3991543650627136, 0.49452516436576843, 0.04062175005674362, 0.43216150999069214, 0.37442347407341003, 0.32163500785827637, 0.29129910469055176, 0.1775319129228592, 0.3840915560722351, 0.3340137004852295, 0.046646613627672195, 0.2990754246711731, 0.06592469662427902, 0.16222570836544037, 0.031282417476177216, 0.37216275930404663], dtype='float32').reshape([20]),
            paddle.to_tensor([0.46081826090812683, 0.05331883952021599, 0.45130711793899536, 0.26384666562080383, 0.12862582504749298, 0.1447637975215912, 0.4375377297401428, 0.3240983486175537, 0.1539696604013443, 0.17154282331466675, 0.25806713104248047, 0.022411124780774117, 0.4709419906139374, 0.013289516791701317, 0.4973859488964081, 0.09389770776033401, 0.37098705768585205, 0.3074762523174286, 0.04365628957748413, 0.4032570421695709], dtype='float32').reshape([20]),
            paddle.to_tensor([0.0413324236869812, 0.12649302184581757, 0.44622865319252014, 0.16550596058368683, 0.19059938192367554, 0.15532490611076355, 0.19840790331363678, 0.23650586605072021, 0.25217947363853455, 0.07071149349212646, 0.25430575013160706, 0.23018090426921844, 0.45942163467407227, 0.07578227669000626, 0.27431538701057434, 0.18416470289230347, 0.2157433032989502, 0.24778752028942108, 0.30129101872444153, 0.3292422294616699], dtype='float32').reshape([20]),
            paddle.to_tensor([0.30029284954071045, 0.32480260729789734, 0.10182588547468185, 0.05021623149514198, 0.2641124725341797, 0.382010817527771, 0.2576029300689697, 0.2730543613433838, 0.06937967985868454, 0.10860557854175568, 0.3286263346672058, 0.33412793278694153, 0.3037545680999756, 0.40727800130844116, 0.010724389925599098, 0.3445625603199005, 0.20688340067863464, 0.03244640305638313, 0.2926183342933655, 0.25542333722114563], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_19f7e47d154db487e5312153c1bf281c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.47903120517730713, 0.46523723006248474, 0.26520654559135437, 0.19384640455245972, 0.45080286264419556, 0.347614049911499, 0.4231887757778168, 0.4711073338985443, 0.2611682713031769, 0.3300354778766632, 0.32201361656188965, 0.1994200199842453, 0.044995665550231934, 0.14589335024356842, 0.3036760985851288, 0.12081405520439148, 0.24073426425457, 0.11873146146535873], dtype='float32').reshape([18]),
            paddle.to_tensor([0.31232312321662903, 0.14738789200782776, 0.24722585082054138, 0.4667108356952667, 0.3008178770542145, 0.12778997421264648, 0.057847727090120316, 0.41656431555747986, 0.11536726355552673, 0.39661723375320435, 0.4567408859729767, 0.09256842732429504, 0.44775089621543884, 0.4586847424507141, 0.4224865734577179, 0.21428148448467255, 0.3164803683757782, 0.12994235754013062], dtype='float32').reshape([18]),
            paddle.to_tensor([0.16319532692432404, 0.08919733762741089, 0.04909856244921684, 0.008475832641124725, 0.1276932805776596, 0.06506285071372986, 0.4287542402744293, 0.47411206364631653, 0.26005882024765015, 0.0929519459605217, 0.33724096417427063, 0.3045566976070404, 0.25296685099601746, 0.22695574164390564, 0.2442999929189682, 0.33157458901405334, 0.4996464252471924, 0.4783613681793213], dtype='float32').reshape([18]),
            paddle.to_tensor([0.1062716394662857, 0.40140220522880554, 0.1545165628194809, 0.28929516673088074, 0.09936954826116562, 0.042257364839315414, 0.07545167207717896, 0.20577457547187805, 0.42300328612327576, 0.27236664295196533, 0.4757458567619324, 0.4630720913410187, 0.050818003714084625, 0.420951247215271, 0.4816335439682007, 0.32521337270736694, 0.3864329755306244, 0.30884212255477905], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_024cb53f41e072a6c202ba35abf0ee32(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.06899487972259521, 0.34500280022621155, 0.49310919642448425, 0.22328677773475647, 0.3601999580860138, 0.29896703362464905, 0.12592685222625732, 0.04105770215392113, 0.36248573660850525, 0.17850270867347717, 0.06835929304361343, 0.022170593962073326, 0.36817577481269836, 0.48092857003211975, 0.26966962218284607, 0.14983119070529938, 0.4430248439311981, 0.325265496969223, 0.3389781415462494, 0.18071839213371277, 0.15811221301555634, 0.29130738973617554, 0.4292758107185364, 0.4019119143486023, 0.3323175609111786, 0.07252940535545349, 0.05246899276971817, 0.38350558280944824, 0.3179260790348053, 0.36618223786354065], dtype='float32').reshape([30]),
            paddle.to_tensor([0.1195274293422699, 0.29772743582725525, 0.281615674495697, 0.3833591639995575, 0.40712007880210876, 0.011071768589317799, 0.05137772858142853, 0.20113718509674072, 0.3237121105194092, 0.1420450359582901, 0.1462095081806183, 0.11885201185941696, 0.2365449070930481, 0.3125300705432892, 0.23662787675857544, 0.17220157384872437, 0.43097999691963196, 0.1482701152563095, 0.27905791997909546, 0.0630241334438324, 0.4029234051704407, 0.0006652698502875865, 0.366543710231781, 0.324162095785141, 0.2928691804409027, 0.3601043224334717, 0.21288833022117615, 0.27265000343322754, 0.15930812060832977, 0.25584182143211365], dtype='float32').reshape([30]),
            paddle.to_tensor([0.1408601850271225, 0.23736347258090973, 0.15553998947143555, 0.13234280049800873, 0.1703428477048874, 0.4076083302497864, 0.46846872568130493, 0.04196501895785332, 0.19963392615318298, 0.3207247853279114, 0.18032206594944, 0.2629598081111908, 0.48031923174858093, 0.022372528910636902, 0.06329981237649918, 0.08267022669315338, 0.2739575505256653, 0.4947754442691803, 0.42340195178985596, 0.1418374627828598, 0.14656303822994232, 0.3621181547641754, 0.3596993684768677, 0.29614436626434326, 0.05408736318349838, 0.4204173982143402, 0.47247564792633057, 0.21332503855228424, 0.29353979229927063, 0.0724833607673645], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2626141309738159, 0.007306325249373913, 0.18831679224967957, 0.184341698884964, 0.02768080309033394, 0.30021369457244873, 0.15120644867420197, 0.16767509281635284, 0.10355806350708008, 0.10384974628686905, 0.29474467039108276, 0.06865987926721573, 0.02140689827501774, 0.4637221097946167, 0.019040429964661598, 0.32586154341697693, 0.2622959613800049, 0.3726694583892822, 0.27056291699409485, 0.47272989153862, 0.18643121421337128, 0.4547257423400879, 0.25793907046318054, 0.28697460889816284, 0.1601923257112503, 0.09155597537755966, 0.02405596897006035, 0.3324953019618988, 0.10775010287761688, 0.4820946156978607], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_86da30f7829434ed595e9f5d588b0929(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_765095a358e3cb8a79ab260d3782d182(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 32, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_76aea88705aaf130f6dd99b40c72dd19(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_85b798c303135d19ff37608537e732da(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 144, 31, 31], dtype='float16', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bdefdba51422b69e21a836ec80f7a922(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 32, 64], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.19060665369033813, 0.33366715908050537, 0.40382617712020874, 0.41705435514450073, 0.16724145412445068, 0.06974181532859802, 0.28002557158470154, 0.11855398118495941, 0.10017932206392288, 0.2936258018016815, 0.2671436667442322, 0.13468779623508453, 0.2163892239332199, 0.3521365225315094, 0.22092510759830475, 0.31613796949386597, 0.13040457665920258, 0.37873154878616333], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3313925564289093, 0.044665105640888214, 0.42015981674194336, 0.0004928854177705944, 0.29412633180618286, 0.2110845446586609, 0.004681551828980446, 0.23786677420139313, 0.4118663966655731, 0.015896430239081383, 0.11097965389490128, 0.16310212016105652, 0.09769469499588013, 0.06353702396154404, 0.2153928577899933, 0.2739355266094208, 0.19121946394443512, 0.0350981168448925], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3798633813858032, 0.06793050467967987, 0.4219689965248108, 0.058163661509752274, 0.16352619230747223, 0.4416974186897278, 0.02485453337430954, 0.12069176882505417, 0.47791406512260437, 0.4672180414199829, 0.23512691259384155, 0.38995951414108276, 0.4815472662448883, 0.11419104784727097, 0.34245991706848145, 0.45246630907058716, 0.10197971761226654, 0.39577192068099976], dtype='float32').reshape([18]),
            paddle.to_tensor([0.47027602791786194, 0.19488035142421722, 0.19527088105678558, 0.17044958472251892, 0.4467618465423584, 0.13999173045158386, 0.4600048065185547, 0.33758652210235596, 0.03618309274315834, 0.3508695960044861, 0.47380101680755615, 0.496966689825058, 0.2136446237564087, 0.03234166279435158, 0.2499559074640274, 0.18949554860591888, 0.2701544761657715, 0.43609386682510376], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_554f5a573087e07cce72d818e0ec3493(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1621d0a3ea680e10f096df61191c8779(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3108263909816742, 0.21286998689174652, 0.026193538680672646, 0.49911415576934814, 0.056516874581575394, 0.13985984027385712, 0.20802059769630432, 0.3341194987297058, 0.4484526813030243, 0.19976288080215454, 0.22025851905345917, 0.11251598596572876, 0.4225276708602905, 0.17897580564022064, 0.39141905307769775, 0.4103076756000519, 0.39069265127182007, 0.09911295771598816], dtype='float32').reshape([18]),
            paddle.to_tensor([0.14800617098808289, 0.08922913670539856, 0.29882919788360596, 0.17190665006637573, 0.0308961383998394, 0.3827401101589203, 0.4726419746875763, 0.3697056174278259, 0.3831462860107422, 0.3810502290725708, 0.1855786293745041, 0.3057388961315155, 0.49401092529296875, 0.3671770393848419, 0.03398836404085159, 0.22416134178638458, 0.35236942768096924, 0.1408323347568512], dtype='float32').reshape([18]),
            paddle.to_tensor([0.14968614280223846, 0.18435661494731903, 0.02833469957113266, 0.30888622999191284, 0.36835259199142456, 0.3526528775691986, 0.26544708013534546, 0.442383348941803, 0.4536881148815155, 0.4836583733558655, 0.3238871991634369, 0.10226383060216904, 0.07849101722240448, 0.03236677497625351, 0.2944987416267395, 0.12153225392103195, 0.21737344563007355, 0.2744850814342499], dtype='float32').reshape([18]),
            paddle.to_tensor([0.060529373586177826, 0.4128088653087616, 0.14883174002170563, 0.3790269196033478, 0.27561092376708984, 0.358760803937912, 0.01967015117406845, 0.09713204205036163, 0.05797167867422104, 0.24044349789619446, 0.10374689847230911, 0.3671092391014099, 0.23026414215564728, 0.4010832607746124, 0.3318589925765991, 0.2854384481906891, 0.4708862006664276, 0.13934840261936188], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3da388fec608d0b53db3f2482c4c6e3f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1344, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1344], dtype='float32', min=0, max=0.5),
            paddle.uniform([1344], dtype='float32', min=0, max=0.5),
            paddle.uniform([1344], dtype='float32', min=0, max=0.5),
            paddle.uniform([1344], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ac2dc0dcb2c9677e66cde6ebfaa80abb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 32, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7e9edbbf5027fa44ac84bbdf4ac0f7d7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3067606985569, 0.14981745183467865, 0.4337328374385834, 0.3309730887413025, 0.46167901158332825, 0.4196750521659851, 0.41656994819641113, 0.025982273742556572, 0.2189241349697113, 0.03072437271475792, 0.43409132957458496, 0.04833964258432388, 0.4315384030342102, 0.44842588901519775, 0.08311150223016739, 0.04791664704680443, 0.08073275536298752, 0.02622050605714321], dtype='float32').reshape([18]),
            paddle.to_tensor([0.38851258158683777, 0.29572445154190063, 0.49035805463790894, 0.13340289890766144, 0.21403741836547852, 0.29293280839920044, 0.023282162845134735, 0.034373361617326736, 0.13095985352993011, 0.2635875642299652, 0.37806999683380127, 0.3487306833267212, 0.30956435203552246, 0.3175775110721588, 0.46912243962287903, 0.34036120772361755, 0.22771824896335602, 0.03822450712323189], dtype='float32').reshape([18]),
            paddle.to_tensor([0.10650458931922913, 0.42723241448402405, 0.12092188745737076, 0.2833707928657532, 0.4644189774990082, 0.42825475335121155, 0.08141971379518509, 0.11475386470556259, 0.10321018844842911, 0.19392353296279907, 0.3575975000858307, 0.008102074265480042, 0.012959604151546955, 0.4268961548805237, 0.13858573138713837, 0.1516241580247879, 0.06770557165145874, 0.38548728823661804], dtype='float32').reshape([18]),
            paddle.to_tensor([0.1630677580833435, 0.2795916795730591, 0.16258880496025085, 0.46246108412742615, 0.03439286723732948, 0.016192978248000145, 0.1742464303970337, 0.19525855779647827, 0.3208329677581787, 0.4460248053073883, 0.21464379131793976, 0.42180371284484863, 0.080724336206913, 0.3343736231327057, 0.3321946859359741, 0.2715418040752411, 0.218034029006958, 0.339359849691391], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_29e681ae259034abb6c338d3778755a2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.03842997923493385, 0.2860228717327118, 0.14749157428741455, 0.1256801337003708, 0.37921440601348877, 0.410266637802124, 0.30841064453125, 0.4768499732017517, 0.2813507914543152, 0.343258798122406, 0.0008147373446263373, 0.08352164924144745, 0.2899407148361206, 0.42079028487205505, 0.002044101944193244, 0.16926774382591248, 0.3943096101284027, 0.18161450326442719], dtype='float32').reshape([18]),
            paddle.to_tensor([0.10569915920495987, 0.4600001275539398, 0.4289357364177704, 0.21902239322662354, 0.235423281788826, 0.2781001329421997, 0.05100739002227783, 0.33589616417884827, 0.11385656148195267, 0.0394892543554306, 0.35454121232032776, 0.11736290901899338, 0.40164250135421753, 0.13290159404277802, 0.24722333252429962, 0.25081509351730347, 0.1172867938876152, 0.2613333761692047], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3602536916732788, 0.17495359480381012, 0.37836018204689026, 0.14925412833690643, 0.2534213066101074, 0.407093346118927, 0.33194342255592346, 0.3324241638183594, 0.10610929131507874, 0.22170305252075195, 0.2793434262275696, 0.48235663771629333, 0.4167793393135071, 0.49144798517227173, 0.22415205836296082, 0.2174384742975235, 0.056864362210035324, 0.012847130186855793], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2466006875038147, 0.09072349965572357, 0.14137454330921173, 0.18857043981552124, 0.1440904438495636, 0.15976424515247345, 0.18378117680549622, 0.4135521650314331, 0.036553673446178436, 0.05646774172782898, 0.22940513491630554, 0.09879753738641739, 0.4704124927520752, 0.15846170485019684, 0.4724152088165283, 0.18571631610393524, 0.02470533736050129, 0.03617098927497864], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4c34af641aa3289578d1eac7a962a61e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 52, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4c0de217e84174d5e697640e3737f85b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 200, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9a8877f824e28a02e6c222882a9fd3d2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.34066617488861084, 0.06795289367437363, 0.4274185597896576, 0.373289555311203, 0.015074516646564007, 0.22589966654777527, 0.30229470133781433, 0.19985370337963104, 0.06107465177774429, 0.32845085859298706, 0.434311181306839, 0.2279013842344284, 0.27776119112968445, 0.09001617878675461, 0.4821193814277649, 0.3145769238471985, 0.022457949817180634, 0.16389574110507965, 0.1482015997171402, 0.031027942895889282, 0.19978362321853638, 0.2668631374835968, 0.36750558018684387, 0.011666582897305489], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2944885790348053, 0.4442541003227234, 0.23191171884536743, 0.34655869007110596, 0.3803069293498993, 0.1610577404499054, 0.40373408794403076, 0.23726427555084229, 0.007023083046078682, 0.1797049343585968, 0.2834782898426056, 0.2308986634016037, 0.02788349613547325, 0.16493883728981018, 0.399476557970047, 0.18358832597732544, 0.13522276282310486, 0.101139135658741, 0.2876495122909546, 0.36067822575569153, 0.13983052968978882, 0.473541259765625, 0.41944894194602966, 0.3015606701374054], dtype='float32').reshape([24]),
            paddle.to_tensor([0.210261270403862, 0.11558692157268524, 0.035339441150426865, 0.31872788071632385, 0.03914574533700943, 0.4641133248806, 0.4894435405731201, 0.03173826262354851, 0.4929988980293274, 0.11295997351408005, 0.09806621074676514, 0.20642395317554474, 0.27753889560699463, 0.36542102694511414, 0.24806630611419678, 0.4631028175354004, 0.46921294927597046, 0.15908433496952057, 0.43871504068374634, 0.38346433639526367, 0.2908690869808197, 0.15367622673511505, 0.025288693606853485, 0.1481976956129074], dtype='float32').reshape([24]),
            paddle.to_tensor([0.13284683227539062, 0.1758752316236496, 0.042791906744241714, 0.4577891528606415, 0.43712136149406433, 0.353365033864975, 0.11218146234750748, 0.055198609828948975, 0.21941062808036804, 0.03145081549882889, 0.07789026200771332, 0.3751186728477478, 0.22090844810009003, 0.26018407940864563, 0.39959442615509033, 0.1740182489156723, 0.00970826018601656, 0.3144039511680603, 0.438784658908844, 0.3337060511112213, 0.31947246193885803, 0.32254308462142944, 0.20271390676498413, 0.259082168340683], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c90942bd49320d5970099011fb472974(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.21867406368255615, 0.185573548078537, 0.09283154457807541, 0.3355415463447571, 0.16436026990413666, 0.08045028150081635, 0.28951528668403625, 0.4686836004257202, 0.36361053586006165, 0.0031946273520588875, 0.3777281641960144, 0.08334773778915405, 0.2416459619998932, 0.05254291370511055, 0.26914259791374207, 0.4831743538379669, 0.1503356546163559, 0.0035816296003758907, 0.4442974925041199, 0.25849518179893494, 0.4205015301704407, 0.40226563811302185, 0.2796693444252014, 0.037189021706581116], dtype='float32').reshape([24]),
            paddle.to_tensor([0.19413214921951294, 0.244939386844635, 0.3522573709487915, 0.3503223657608032, 0.21563495695590973, 0.2575867474079132, 0.49028873443603516, 0.271028608083725, 0.37496933341026306, 0.05788913741707802, 0.01676870882511139, 0.3231029808521271, 0.35714271664619446, 0.3665867745876312, 0.22269971668720245, 0.17141622304916382, 0.08363718539476395, 0.4798189401626587, 0.28495681285858154, 0.422641783952713, 0.19642342627048492, 0.25299620628356934, 0.30637791752815247, 0.21281877160072327], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4202168881893158, 0.1921951323747635, 0.44943559169769287, 0.44891980290412903, 0.3836713135242462, 0.16888967156410217, 0.45047515630722046, 0.4337797164916992, 0.2574981153011322, 0.3280634880065918, 0.08543006330728531, 0.08530265092849731, 0.07720888406038284, 0.4131051301956177, 0.3148009777069092, 0.3148633539676666, 0.34146615862846375, 0.28526851534843445, 0.08987265825271606, 0.3796204924583435, 0.009271563962101936, 0.25318366289138794, 0.34154030680656433, 0.12900042533874512], dtype='float32').reshape([24]),
            paddle.to_tensor([0.18868215382099152, 0.4657372236251831, 0.27547499537467957, 0.23963436484336853, 0.2721947431564331, 0.4500746726989746, 0.4687051475048065, 0.27307558059692383, 0.008102905936539173, 0.1768769919872284, 0.34032535552978516, 0.03464164584875107, 0.3617400825023651, 0.34694451093673706, 0.08217991143465042, 0.41580095887184143, 0.18363045156002045, 0.012124894186854362, 0.4513026475906372, 0.18175382912158966, 0.15904554724693298, 0.28667283058166504, 0.0976809710264206, 0.11618759483098984], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_33f0082a3f8e8a8e4d9d66a465ad2081(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 53, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([53], dtype='float32', min=0, max=0.5),
            paddle.uniform([53], dtype='float32', min=0, max=0.5),
            paddle.uniform([53], dtype='float32', min=0, max=0.5),
            paddle.uniform([53], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ca5de77edc194fda1cc1361e06e857e4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 117, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([117], dtype='float32', min=0, max=0.5),
            paddle.uniform([117], dtype='float32', min=0, max=0.5),
            paddle.uniform([117], dtype='float32', min=0, max=0.5),
            paddle.uniform([117], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_152fd115ab4a06dff3ec0e607b85f99f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 480, 480], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_19bfeb9d1390554a176a988d47dbbeec(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1280, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3019214b1a524867018977a9c8c32605(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 128, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_02e7800b11254683dcab17377e035093(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 112, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_524ed3569788c4f7144cecd30f44c32d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6a886269d62cb56d912b07cb9927fafd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 256, 128], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.05046892538666725, 0.30279532074928284, 0.12033936381340027, 0.4464344382286072, 0.22837434709072113, 0.14483314752578735, 0.23711378872394562, 0.07659819722175598, 0.49570003151893616, 0.19511300325393677, 0.45766326785087585, 0.30447638034820557, 0.2707829177379608, 0.1210821121931076, 0.036635611206293106, 0.10385343432426453, 0.030750697478652, 0.3493516147136688, 0.012563146650791168, 0.27591708302497864, 0.42157411575317383, 0.07908046245574951, 0.2096026986837387, 0.23150570690631866], dtype='float32').reshape([24]),
            paddle.to_tensor([0.11117777973413467, 0.147276371717453, 0.19656185805797577, 0.42795297503471375, 0.24792785942554474, 0.23343661427497864, 0.17914338409900665, 0.11306913197040558, 0.49428635835647583, 0.46383264660835266, 0.4255434572696686, 0.199851393699646, 0.12451246380805969, 0.3045874536037445, 0.2153412103652954, 0.494742751121521, 0.4973370432853699, 0.2174026370048523, 0.1516178697347641, 0.15347737073898315, 0.39288529753685, 0.18269531428813934, 0.16530202329158783, 0.017106305807828903], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2607640027999878, 0.4859274923801422, 0.27975955605506897, 0.45977964997291565, 0.12423789501190186, 0.21633993089199066, 0.21109168231487274, 0.4238916039466858, 0.31153056025505066, 0.3037596344947815, 0.013669042848050594, 0.22722184658050537, 0.23592402040958405, 0.09007124602794647, 0.48450925946235657, 0.48678645491600037, 0.04906114935874939, 0.15371738374233246, 0.16618219017982483, 0.3628576695919037, 0.3232089579105377, 0.37004444003105164, 0.10800729691982269, 0.3794534504413605], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2904355227947235, 0.36367565393447876, 0.36367523670196533, 0.3955625295639038, 0.008608939126133919, 0.26496151089668274, 0.29722487926483154, 0.38868916034698486, 0.19586588442325592, 0.3053799867630005, 0.3430350422859192, 0.028032299131155014, 0.12415401637554169, 0.11221016198396683, 0.4289218485355377, 0.18693090975284576, 0.04881482571363449, 0.36861351132392883, 0.1528347134590149, 0.16241446137428284, 0.2584785223007202, 0.4083753526210785, 0.29325249791145325, 0.23264694213867188], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_030a189242f5991b19d718cc1ef8c891(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.1654892861843109, 0.25994110107421875, 0.13075940310955048, 0.2591438889503479, 0.49611344933509827, 0.18190641701221466, 0.07040993869304657, 0.08551978319883347, 0.2896004319190979, 0.4318086802959442, 0.04499385505914688, 0.004662317223846912, 0.2055802047252655, 0.27734822034835815, 0.3415158987045288, 0.09574465453624725, 0.33242860436439514, 0.306903600692749, 0.34963664412498474, 0.35489654541015625, 0.46799522638320923, 0.4993237257003784, 0.0138955507427454, 0.05559384450316429], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3073572814464569, 0.23805169761180878, 0.08839035034179688, 0.46032410860061646, 0.30631494522094727, 0.34195929765701294, 0.48165813088417053, 0.4689890444278717, 0.036225344985723495, 0.2085743099451065, 0.4885958135128021, 0.2585841715335846, 0.2948368787765503, 0.45863911509513855, 0.31791332364082336, 0.1467168629169464, 0.11334634572267532, 0.25901561975479126, 0.3394401967525482, 0.49238404631614685, 0.4515936076641083, 0.017736760899424553, 0.4631001651287079, 0.3169633150100708], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4948125183582306, 0.34601423144340515, 0.18174850940704346, 0.21695934236049652, 0.27548062801361084, 0.16630057990550995, 0.3825843334197998, 0.34796229004859924, 0.2623618543148041, 0.37353238463401794, 0.08081191778182983, 0.29831165075302124, 0.03464963659644127, 0.41417109966278076, 0.39394107460975647, 0.42367541790008545, 0.2384917438030243, 0.07527501881122589, 0.10370931774377823, 0.14774198830127716, 0.17182238399982452, 0.29521867632865906, 0.020610874518752098, 0.4681874215602875], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4624192416667938, 0.3167896568775177, 0.24942077696323395, 0.23323388397693634, 0.49741753935813904, 0.40327081084251404, 0.028880983591079712, 0.37705129384994507, 0.35292312502861023, 0.06484273821115494, 0.031850799918174744, 0.07411372661590576, 0.25614985823631287, 0.47763702273368835, 0.34538519382476807, 0.36924248933792114, 0.41766178607940674, 0.13532839715480804, 0.06591140478849411, 0.029497047886252403, 0.3536505699157715, 0.06429918110370636, 0.42073366045951843, 0.047976214438676834], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d9fa08f2e72b43faf7e10a2ca98279e0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.06418363004922867, 0.4791744351387024, 0.3529255986213684, 0.051538269966840744, 0.19334350526332855, 0.3617425262928009, 0.12965956330299377, 0.08776213973760605, 0.14060215651988983, 0.3946850001811981, 0.24447545409202576, 0.1513255536556244, 0.036948416382074356, 0.4927477538585663, 0.024596763774752617, 0.10774105042219162, 0.14591659605503082, 0.37857162952423096, 0.3437751829624176, 0.24437366425991058, 0.3704403042793274, 0.34125620126724243, 0.48168084025382996, 0.23470622301101685, 0.4548243582248688, 0.44125431776046753, 0.2284604161977768, 0.3865279257297516], dtype='float32').reshape([28]),
            paddle.to_tensor([0.3047066628932953, 0.02669001929461956, 0.3151649534702301, 0.36641737818717957, 0.4990558326244354, 0.297381728887558, 0.02436753734946251, 0.4825502634048462, 0.4435965120792389, 0.4636121094226837, 0.2821086645126343, 0.1259244978427887, 0.004104004707187414, 0.0531943142414093, 0.4023876488208771, 0.326255738735199, 0.006697166711091995, 0.2601209878921509, 0.45204201340675354, 0.23672983050346375, 0.09416504949331284, 0.3283625841140747, 0.3094480633735657, 0.13028697669506073, 0.2527950704097748, 0.07146017998456955, 0.08932383358478546, 0.08623073250055313], dtype='float32').reshape([28]),
            paddle.to_tensor([0.17146609723567963, 0.1397956758737564, 0.12315939366817474, 0.38826388120651245, 0.2209727168083191, 0.2577719986438751, 0.0872393324971199, 0.3762890696525574, 0.06449628621339798, 0.2913847267627716, 0.4818335175514221, 0.3190336525440216, 0.14382202923297882, 0.26613569259643555, 0.41129711270332336, 0.17047755420207977, 0.4802275598049164, 0.343426913022995, 0.3268762230873108, 0.3744032084941864, 0.4008707106113434, 0.3550449013710022, 0.021055597811937332, 0.47574302554130554, 0.20419380068778992, 0.4035881459712982, 0.2579089403152466, 0.13272175192832947], dtype='float32').reshape([28]),
            paddle.to_tensor([0.19542154669761658, 0.014504977501928806, 0.014874355867505074, 0.060061655938625336, 0.17607341706752777, 0.2935936450958252, 0.03635547682642937, 0.0436829999089241, 0.4267376959323883, 0.05569624528288841, 0.0021631699055433273, 0.3528096079826355, 0.05824655294418335, 0.2785874307155609, 0.25846371054649353, 0.4882429540157318, 0.023421775549650192, 0.3568367063999176, 0.4141804873943329, 0.35374870896339417, 0.39274224638938904, 0.046264320611953735, 0.06152268871665001, 0.3351156711578369, 0.23646283149719238, 0.0021229558624327183, 0.49560803174972534, 0.47133713960647583], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_44a53eeb7c6158b7d6197b15a493430d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 56, 112], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.26323172450065613, 0.17931249737739563, 0.1902986615896225, 0.32010790705680847, 0.2352399080991745, 0.2608188986778259, 0.3023111820220947, 0.11947332322597504], dtype='float32').reshape([8]),
            paddle.to_tensor([0.41237616539001465, 0.1238502785563469, 0.4034448266029358, 0.2512516379356384, 0.42407092452049255, 0.3876914978027344, 0.13462898135185242, 0.4788665771484375], dtype='float32').reshape([8]),
            paddle.to_tensor([0.4174915552139282, 0.43277618288993835, 0.2415396124124527, 0.43210160732269287, 0.3882071375846863, 0.16219259798526764, 0.32664892077445984, 0.16991062462329865], dtype='float32').reshape([8]),
            paddle.to_tensor([0.10107740759849548, 0.2721925377845764, 0.3314192593097687, 0.44545090198516846, 0.4780575931072235, 0.27689632773399353, 0.28862735629081726, 0.36873528361320496], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6e6945ed562776cb8cb49b68bfd721fb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 100], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_959f4eb8cf48d48b76194c825afd590b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 304, 304], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5f35f5ad20a4138972922db14005ece2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 4, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8b276af27dd764bcd946035676941f18(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.45006710290908813, 0.06457580626010895, 0.35906982421875, 0.028586026281118393, 0.0023388860281556845, 0.3482840657234192, 0.150233194231987, 0.2111940085887909, 0.23660318553447723, 0.10517918318510056, 0.2957359850406647, 0.27627336978912354], dtype='float32').reshape([12]),
            paddle.to_tensor([0.1039246916770935, 0.20292721688747406, 0.21332015097141266, 0.46777069568634033, 0.007745038252323866, 0.2532414495944977, 0.15955962240695953, 0.01037125289440155, 0.23902767896652222, 0.23856911063194275, 0.06205643713474274, 0.3064134418964386], dtype='float32').reshape([12]),
            paddle.to_tensor([0.29270827770233154, 0.4108833968639374, 0.12466496974229813, 0.4247003495693207, 0.2532799243927002, 0.11655738204717636, 0.47117161750793457, 0.280819296836853, 0.3128032386302948, 0.0525398813188076, 0.15021248161792755, 0.08468504995107651], dtype='float32').reshape([12]),
            paddle.to_tensor([0.15930861234664917, 0.1883602738380432, 0.26710811257362366, 0.013555950485169888, 0.2513647973537445, 0.1386926770210266, 0.2649901807308197, 0.3984575569629669, 0.446508526802063, 0.20621414482593536, 0.12377007305622101, 0.05939004197716713], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_a02f1e2888bea69fc79b31a122ed24eb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7af1867cf062e8e55bd074c997d4b02c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_376eb4e64c16e7dc177259bc5f5ae981(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b8cad70198a579ef46dd2d7f8f14031f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e58ed3e90c06992381b31f7b32cf3514(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, None], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e1b19a292b33d2adecafe5164671e766(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_47253853fe7e4c594fd18626e3ff76f7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d50b2ed5144cbdd1ca5042fed72047fc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9fb4d2c35d0d8e55697ed2f596b3c41c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b9a89d6c5d649626b0ab5c3dc05a8ee8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_90d324a1f7a05e6c93e4c5157ae1ba91(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_51f2ac2cd50c5d1639b2f39112e37803(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0a8f2e0b225571cbdd0378af5cd9cb3a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0b4b8879c39d03f646f9a91b3184e36c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2cfd04874ac8e2deccd00b885bbd9725(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_adfbf8b3643a0b20cce605267bdb05fc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 4, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_95aba9563c2d8490e87f0ebea1f79ef9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f68ee94db56350745cead1204ae65147(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3ddfa948ffba0daaf2bab1fc3fe10349(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_95752e262f48b523693ff1b90222dec0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4ecd3e332c82c379847f781922767736(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_80eaa0c7bfdbd2b1e435a1a302663c21(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_41d527d62e185f65abb0ee3c1bfbf1f1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 4, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2aca463f726fc557a6eb6fffd721807b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1c27edec4a7eb3e614a35b687d2cc820(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e0667d9bad3e3f9948ecf0e9ab7356b3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a14b87cec90c13cb6254bcbcfcb2843(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_093e50fc34ac9d700f8462a8951e7dd6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_62213fc8161303858ca4ce0a0316e3dc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e2bf442264d3c3537af1dd050f480205(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1920, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_306980a35aa819da3afc5c4d630f72be(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f29f53619829634831b832b49653cd59(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4f14c932d02b6507f9245815661bb29a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d84f6f9a89ad1fa3a5427852edccd2f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 320, 320], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aaf74873a9105f0af43d9af182072d3c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_72b552375f023e383d17aa69b279f370(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad27f36a6a00300e6d2db1669fcbc05c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9b9a6a813dd98d200bcb55dbd392a4aa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_78c4f2c3f90cca266e143cbe6eb5de63(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 840, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9580c5aad3dc54b081d5076a4dec374f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 20, 32, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_35dbfde51f98a7f8586486c5e8c555a0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4998577f2715fb1654cdd7b9ad85a960(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_723b30814165a2832e63237aa12019f6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 320, 320], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1e09db09cad87ecfbedcedd6cddcd4d3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 75, 75], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4e066cb3d7c6f78d55cfbada4f260ec9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b7377c3d7f0a477016a5dc92e1a42451(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2eb2104ff4f2d8c45a218e5646e56e39(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f7304499082d0cdfd12079a9338d57ba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f15ba70e4e8d2e88b323c4a8eb5175e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_504e7b0e17e361e8bbfbcfd1c468bd6d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_32f082526f4718640e33f8509d211cda(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_08e61d2bb0735c24358435bd74a7ffb5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 300, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7ea9f08a9b8f521acd61667dd33f31da(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 18, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_488b2e1de9d2ce6934a4a2197e547553(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 52, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8691d18211a75347bf7b588c139f54f1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7bfe4867b2ee50af53c3d7bdc5f7f58f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 16, 256, 512], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6857ca0c0268dc8b0debff2e586c173e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d9d060b5e64903648e88dc3cc8cf97c0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_41bdd673393c791b8d2e8d2405883fa4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8991372857f1063deb0639e3e0988529(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 18, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e4cab3560020f8e75167f6010b432790(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3473e493d0e249ac682f4c69cbbe0a25(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 52, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0657ef22db0d18caaee4c49eefa3e86c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a0a46aa0a89a0eb975d77352fd88c9ed(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d86166d29866b8690a7ca1699e996024(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c264d3b0c0b1a61a28086f770ee56d74(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 8, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ac25a9f438fbee8212f63b2d4fc03f5b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d888a268af70f20a6e8eb7f00a9b04c0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8908757f1e09639d6bcae97a8afe8842(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_edf331ca5fa8f99449330bf1525d87ae(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bb603ef7e789ce3a3c734fe1a71e5586(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e0c71bc41aeac4c359b0d5c6e94ea65c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_60ec4ec8f0d673e48eaae3861d000368(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1dc0990adccbbca8c2f6291e6ea35d61(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6072bbc3aa1bc3812bd0572c3fc3ee0b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_58335340d600825583357bb7285fdaf2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 5, 5], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5294ebd8899a711fe2314802f5ecba45(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a8a7335ee76d483408684aa7a0d340cd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 16, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8e4a97bc06557d54f21f41f06e3995ea(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e04a6d914746c47bf4f361eb509d1ae5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_759a950012c9a587d288dd903532d5ef(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b198c962704a1b49b5b64a507202a445(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 8, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3b283aa56dc1a8591160c9e27af049ee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_22166f4a1b8e7b18c57ffc83278e54fa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0ffdf4b0113987366d210bdd01ca249a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7c8aafac5a643847685bb567642debfc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 32, 128, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9b2c2d5669307fc6f4b29b51d01f95fd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bec5c67fc72cf90ddb9c224701278864(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9f5b52cc907400eaaf11705256ec7039(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4607809436989505002a1958a7354e68(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 208, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e28a464948bafd11a8012207f50f6fb3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 16, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_69d4d8d352c35b330e34d3483128f699(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4213f15def29e76c2417266d316d747c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6b4849f1d1bd3486ed197dbbd329b78d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_720e8ed98eeae62b50a77b14408c2b10(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7fc0ea8eca85f35aafbd5d37c26a8733(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 432, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_acd0d714098c95881a5ad397bed31056(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_de25d736449bbdfaf418d14f121fd0ec(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9bd488df21cd55f3ba5ac049a495705b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 16, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7b169e5997ed90c117631737166d16c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_25dec4d33c0318100e2218ef5aa8a01b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 1, 2], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_019561d773e1092e6a64ee236ce8da12(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 304, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a77b7cfb6a89a98f582848027cf0d454(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_425450a29c03def74064017764b43a41(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e2e08385fc1ffddf21545f9dd55ed8b5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6a135dd1b999265b811e0cba22b585f9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 88, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_578658953af6c5334a305c1d756ab95f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 88, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4ff6ea202881aecbdab6f3d61896bf38(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_be7b23f378ba717a9d80dcf4b4dd4ee5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ba5d32036b331464bee1a454f001a9f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1c0d78493af12479591cd66912c25938(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b9857c98fc49adea29001e09fea3d68d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_be7a980d242db80da503d468c7b6bc00(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 8, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_35ff2db50ad7ca1bf33f0c22454b1841(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_755455be86c40f73334f229d81cfb610(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0f55a3fe994b3b3ea44afa5ed331d5a0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8ba8b0345268f0dc8f4ea2e68bcb1214(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5ac548483b1eb78cd51c68228482ed73(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_17dc6585acd08b6f30f9c456f2d4d68f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b54f59a7929886bfe02b395062ae82a0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0dabc235b01a7023f14e74ac18c8cdbc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 350, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1b831d030b659453f9f36a2daaa0d61f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1469e5829ad6dd174a4d4301d92df512(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 38, 38], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_88c91ea1acff8ca87cb913f04c8fb8c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f20b7d281ea20a4072d0f48c8a19c847(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b2dcd37d98e3adf79f9f85ea7f694dc4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_52fb4c7709facf72bb75957b3bc13cea(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 52, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3039e15c3bff93f37bc753a08dedbe9e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 2, None], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4a21513bcc72ad90df1926e60f25981d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5b297cbdbc20fd574274afcfdf60015f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_08b46924600e40a736dc15f1f0c34c6d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ceb994a879f8829a5efe1033d526d81f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 350, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f4cec7d657e259f4c2913c4230392b9f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4313045e50e143d0b04f4beab2d1ffe6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 17, 17], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_809a14e7883394d5783c5ecea2723f94(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9e30fae60f3b8093db147f92de4ca394(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 8, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7f61c38bb31decba43362a7297fa1282(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 6, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b38f647774f7456158edb7bbe2614217(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 175, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_82cb6c9abe3eac1e80505b61c84166b2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d0216cd2c002adfdd03e4fb5ed82c99f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bde20d420d7966db0d5ad3bba41cdf3e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f490811295f67042d6aa6ee33d9fc946(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b60fcd0b6f26428bc5c7770bad7ffa1d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_47a6a1a509697146a8e9e71dd351270d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cf0dfdaf0c4824bd9e3f298c3ab3d32d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a674dc3c5ebc2981cf0bf4f72a31dacb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_419833893c17c3b5d1c376f82c06ab01(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e09aa6a49354526ead685eacadee9c30(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a00bb4e257ab0a36e29ebfeeff0d842b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 200, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_88d2ee044265eec9b2d13f287f9e5975(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a8300eb81472fa39883d842ffa8e5226(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_acd1fa80420f1a6d3d273063bc1956d7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f49279ac3043eda0a82805a7a8ea9779(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_32edf953fce616597ad2b44f8310ea33(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2763dff4561e1c891db3c3a2068e1058(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_73abd023f1b54ec5f4d5e00139e06a6e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 8, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_21c434ef269a633bfadd6adaf8a4a98d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_398861261c8746d2881cedaba6979814(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_22e747980adc24d3e691b98fe7f97177(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7aa782216c8bde1b95745d01bda60367(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 27, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a9f563c62a91878b553c08e2cc15b4df(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 20, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_57a31e312c2e0e2f69d658b889ce8c6e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 61, 61], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_92c7cf44ca9e699763bcfc4145325e45(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6fd75af07736e9d01ce333ebaf269a6d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7368ec1ffb57fc339f15151366d9e8e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 26], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1be5aad69366d175e7b219fd7a176437(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8f3d8a328339bec099dcaac963428d56(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1888, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad7841ec75393e1bda8df1d78b1bb062(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9ea90a6b5fb2e8b263179eabec9990ff(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cdb498724826f2d1d9074560e678eafe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 28, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6aa048fb1689e5cd251f916ed4cda099(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 17, 17], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_43cbbb0e3509424a4b9cc93860dc6aaf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_14fc0a24ced80903662f15f415902531(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2c8dbe695d486f315b97329be5e529ba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1632, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_639564a6a54579140c8cf73996e45a72(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6019f3d38c8be9d7dcead7fde144fc69(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e61071eff7204eee14630444e919506c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_589998bbd81988b3e1d94e3b20397c1b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_73b2de52b3a25edbe06de17c3b6ab65a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, None], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_93b7eb5220c982e74801e19028ab6cea(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 4, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_424f752909aaaad1bfdd43dd820623a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1893a07ad07eac7754dc602161ea858e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 76, 76], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cdef1e178dcf7458f57f9c3d0f14101c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a977517bd9688cb9754372ab263ef171(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f87c021f4db14462734097720e6b742(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 208, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_432bc6d74660ffe150b7bb18a72891b2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b1da824bbe8be5568b51258a56eb234e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1056, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1642fa8b2e044a5d3f3bf262d50d5508(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_795d9cc3f8e663f9f242086e329c6e31(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c9114ff816b2f3e8d535e52e7d6050e7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 16, 256, 512], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_53dd2771c402a2715696773cf64d4723(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4443cb96dab2b94b7b32f581252245ca(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d8a247c0ec9ff71630a7b32ce2c9213d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 76, 76], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_56388e95fe8a66c4dec6933f7243c2be(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ff40ca29a1d7f7c404a184f6433a19a0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1440, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a26556ca24a31378aa5d2b7a3043ef68(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_90ba953ddfc60bc98ceb008c571446d0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_84a078b7f89b849f5e6f8da37b96b34c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 162, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6c5ec2ea31615061027a4c86396cf6be(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 960, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2d81571c6c5da8d099f4e8488504251f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 728, 19, 19], dtype='float16'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_06188808edfa48caa40aa1fad819536e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_782b1acaa87910c071667b87c8ca249c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_433314088006787be77fb1fc69ad92d2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a4d9198f3e3167489c2e0fb8793391b3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_13c08fa84333383f04cb6725995513f2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b6eae07d54d9ef5dea95b59f6544ff89(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e68a28f7acc5e1fec128bfef6425f8d8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 17, 17], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_54af904919118db57b8a0801eea6f455(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a4da64c4daa8327aeab4500c67f4a858(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 48, 48], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6cf4c086dbf6fa4fa6d2360431a57ca3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 432, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9479e2f3222000ddf2ecb70587464185(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_11949321c9995b8a52b792790afc1e4b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 236, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4ea47978706cc46255775a9741b672c9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 12, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_62c0c2ffe276d6c810cba03d6d5dadf7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_72afea2bd83e26c58d662a0ddac1de3c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_46f1db9af75718b28443482c27151a5b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_31f4a4025c50f577a0d40bf10d25fb87(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cfea30a48e65df8701477c50ffaabbaf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8cabf75de230bd48aadcb8135ae89fee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a84edadf5bb6023f4da6bbef1abdfe8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 16, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eb989c26d3976af75d654c8ead02f5b3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 4, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8861c9bac0c52fa7c8171476d14c50da(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_40febeaac423e00552e9dbdc617b7213(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f632f6b71e76f85dde9e6a9e74d99131(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bf44cd9e9ae104c61b72ccf5ce9bf251(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1b1de1cb6185e0e58217db08ea6109f6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 32, 128, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9a6c60662dbee1ec9a8016799a0067fc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f3652582614c52d5e020edd850bbf63a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 20, 32, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ba12f00683fce2176c748a4d1787d506(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c96ef0ab05eec9f0e73cc37e470d0097(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_476fda35bc4beeae5de4624189dfb962(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 32, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_407935324dee569608d3b6e8e126ddf7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 2, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bde2707e5c148515f5b22de093b4d6fd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0126aeef335dbb2dd53ff29c62ab0a41(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3c414d695d1891198249a313f1480f9d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_25ecfef120be3bf167579a0043641a4e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_408d245d25126df77149f7d08aa9340f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bd62a60ad014fc4d8d3bbdad0dc812e6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 62, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9765f38f093f55ef89226fee7f91366d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4fd88af2ada1b12474fa31f895c5a800(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ff9a641fa2df9e7c95e9352624577d70(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2c9a65811391b48c16663893efad06f2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f3c8740f8eb41d61ae8d759be2f100d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 19, 19], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b42e2b0e2baa229ef0a8f97466061c29(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_25903a5acdd264202599b2e994bbd6cc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_29b7631c13ce6f45783542634b6f9809(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 400, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e85b60b290717e59a466822d1e42eb3d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_148cd4160d3a517d70ebea43e8198ce1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_82dd2e51bf82f64805cdeb9e77f57b2e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_83f47e87307cfd4dd8a195d9d35fd4d1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4b14494f518840e1d8943d5a28f48e86(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 416, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9f84fbdce742b74de5544e057cd28838(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b163aa986bc09bfc647f09b83b07825e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 608, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a2e2e20ea35ca3f2eb4df3088dc67f39(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_22078a374c6d5d4d8c3827de97839e41(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 88, 61, 61], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_011bb84273957ef1e8fcf8977296eb19(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a36275c4e1ca7ee9b59d383a698ba621(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d47d6ea8c05fac573da3b91bfaffb8ca(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 704, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9c791f891b65821f9042c01087883c7a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1044, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d0ce1bc02a3fa4134e621d76919466ca(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5441acb8165a81688c6b7dc201fc8c0a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 20, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d3a35937d9c5845f9df447ced66c3c0f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c624d63392a8c8c6e4d759243edecf88(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a37223a4fd84d051cac78ea33cd6f506(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6fa389afc2ca633de225c3e7427fda16(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 208, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c7d2d72869661ab929394d1ec970ae31(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1805ee5f66925e2e2281b2c7beb70074(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f43c1722e1112a30fd429a884406979(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d7ef8646f6fce33d8636affb4ac762fb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 38, 38], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_26add3720a47ff50de0328dcec26b585(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3b30e6c17eef017676cfb2b3323b0c80(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b192f3bed431296b739a7e68dd3cf5b3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 8, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_deaa4bea46982ee0218f6e86772f7fed(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b37a9d0f63cb1d8171d304dd8f545e82(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e1a2bde34d7432d425068fe9e4eca25b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bce7f856811e02f9600c9a863f63cc58(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_85e67324a9e48c7e743d2bd01f5caa73(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_067694d2f6a1153403fd29036112d116(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 35, 35], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_70478f5bc84b794c41b5b155e2f71fa1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_299dae75cca55f0032f06e8de76caac3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_11b14a1ae41b0a31593836f12c423b4d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 3, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_156d2b02725383e937bf08c7a755c7a7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_47ca2c4ca14cb788357420b8389cdcbd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 25, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a77ec7dc329722caafa62532aed3870(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 304, 304], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c70905879475ab161d7af4f4c9614458(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 17, 17], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a40598a92049528676957c4113ab8b71(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e5fdb4037aea50a357085c04275bec90(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 17, 17], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_192bfa443e2ebf2c6bfa87b81cc89e2d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4411473bb05eeca76341b499dfcf4c73(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e9100befd923b9387a58d311c9f1d4af(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 2, 2], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c0e228c07c5e1b9d3db71607a0a58d57(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 350, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f830538a49880f7faa7090ed914b6a21(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9f150bcd6de150d77ac8d3cac5019fbc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e0186a6004e5dbd78c27b1d461e918a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 87, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6f8d2811b8281ef9cc6d7e408dbcf600(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bc261f501c79d13332c58d7ddd9be9c6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-06'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_70e9da8c86903b09f59bb5b27e4c9293(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e2466faae82137046237da50a7dbcf3f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1245b3cb3e107d262d7ecf1acbf2e1e0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_11a1f24d7a4be003f6b0979b40654476(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f924a2e0c96ef51562722729350c13a3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 8, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bb18f953b4e05538bdade2c7611d012d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_99be4f5ea99d94116b6830a47a351b9e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_42b2574f34c443a4523c2ee045cbc4df(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bd9785b1c71ce71addc5a0faecd777f4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f3ef53dd7ffd9dd955963e7797739157(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3669a9be9c0c3f4eccfeccb2035ed7b6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 8, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_680b0435e1e42580f10b70824ad2c173(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b3f81b916e59f28b56d7c33903e8a638(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_89131f01c734536b69f798872c41bf81(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0531163f843401361762cdccecb950eb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9864e859e39ce967be5cf9baf8b4d3e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 366, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0f299a987c28e8264dec78dad28a831c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 28, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_76ab480359ef928c8720cc2e4d4649b2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 8, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9ff4635bbd75d4d9fac9dcc46fc4bae0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_59c4e0bc731681aacc4f2d01dffa1b58(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_645d76d5c9cb991a81f41522b47a9679(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bb6c2f0038982adc38c6daa844fa9d46(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1d4c603928c952a4be367cbf82cfad9d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_206d8378e199fbf2586f83ad79a8b719(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 82, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1ef9a37669994e790928e31cdc463b1e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 26], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c4c699c445b062d30c0454b975ad4443(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4834a71b097f80307b114c152580156b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a9d9a15467cfdfd89cbb3d3fc8c28aaa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1440, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cd3b91bc227b5b65664dfc2688a3b672(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b82eb8b23f613db89ec89a2c78416e3c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 48, 48], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_66686e954e0e07c03d3f70fb917de5e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 88, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ce3c5438c2af00accb1b7081761e25eb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dcda6b8c7486d0b9bc82aaf2f1b677d1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 26], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_86bbe428ad7cdf86b46d39572e0601ab(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f966f37784637dd7f934a44ee272a76d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1776, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d39b6179e48b5737892563f03fbd1dd8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_42db160513b08a34ac34c58fd9f4cdfe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_487c1fe21a345440b06740e79a052423(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_acaafb69840e887faefad3ca97ae8bba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 12, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9132d1421c6776b558135871c20de887(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a918669557d320cf0b2c025803eb766b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 100], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5e737435984eade283e467402a97dbae(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d2f89acc798a3450b177fda556d9c640(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 64, 48], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b2f70fc54e606b9ae002fdec0cfc7df6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_614edde15238b37cab52731fd37f39b7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7941a58505ea31eaa83ce3690d3d3db9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ee746a5623fda017a4125de9688f8805(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_37c9be2f2073a048ee4868212a97123c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 175, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_78ac0280d335b5d020d80c7278bc659f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 728, 19, 19], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a688e009b68e15cd63b76c86ddc74bb7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 19, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_56d19fb8aa02332835fbe6c0ab8ab7f2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 432, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_15ee5177d1b96df05331b81cd2a8b8d0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4a1101b6ba9ad30c33ffb1728d76a0d3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 232, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3066684247f0145c25f963b02c805430(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9e5b5017ef481ffe84d96a7e526c5fdc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_96c65253665b360fd1a7b9c2f0fbf56f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b081fb752a993ad40bd7ed7adfc6290f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2a07ea5061d7ad2a0c11974d825fb43b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 360, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8b69adeff2c71ee7b815f794bb461a6a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_31c2fa12be1f4c1fc369656d85ae0b8b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c411f6f33680b03a4a7622bf3bbfefdd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c3c82948e8ceddbaefb14f1f6e0adfe2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4295435d2d0a847d799e7a323e990fc1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e8d0c135d4805cf056ec128baf6021b6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 504, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_025a876c127e7008591dd621431ad86e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 17, 17], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2c4c3cc386cf13265b1440a9178fc404(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_954e05d384686e7f64edaf73adcd531e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d0f61ef67fec7264cd1f3036a26ed6ea(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_714efb99a8ff6570f582a86d8c671fbc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 122, 122], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_346cb3eb7a1be8bbb2058808f9fe9edf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 4, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_636654b5ec89cb97d90c57cd6bdfefee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9e0e872a60af4fbfabce465b69e0efeb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 61, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3c644406c3cf29f49b29643c4b71b219(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_117f8489baa6e501d70eb4bff85cf1d9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 4, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_867f6b2e4842a55cae6550a09fcf98b1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 232, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c2dc45c893631346faf0c93366b70b07(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5fa00bf5118cba4b4368cd3a35cf721a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 2, 4], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9e6e8c696fb140f453fb8604690d225f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_20c4c08b1310eb3f64a0bced48ef615d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f1c461cb5f6f22728e44806e063a738c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bf9ee670235dff52a659518f1e62b055(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fcc6dde260acc7188025c4c6de7f0c74(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 61, 61], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d558e69ba7079f1bfbd6bffa22b02a32(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 4, 4], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dbb0395720298544a11586fc9d5563b5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 416, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_968feae99ca59e88c788c7ab6dddb91d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0967bad01a11b325be2075c12bcf6e8d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 38, 38], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1901bf125bb1a77e4198b2c97ca6df6b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_54a5f985cd6257bfcb5e076f9e76e1e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 88, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bc4581fc47f74a192165f23667354af1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9d4af7c9a02c07253705daca1609ea05(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_758efcb799911c6f0c6d536cf461001a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5ed73f86ee2dc70f7d877855c00607d3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 4, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_506dcb579ce2bd0471091f30183046ce(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3890f5a700857b435490454b960a454b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bd66dcd297483cb71457afafd752e6f1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 10, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b22610661c3aba4c91d66f9c02532a06(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e161b0e414bcce57157ab486158e34a5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 32, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8e8f73e3bbd4fb2d7bba6e6d36e22f91(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 2, 4], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_842b6ae9eb48422206b3058acc979f12(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 152, 152], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0e0a708a01ea26658a7494dbfc156383(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 960, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_825e97677a5e98b9abcd3a54e281a971(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 140, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_086aa9ecd36198eb2f410fb51ec5d3c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 38, 38], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1bd086ea33b1b0f960e73d3baae21f66(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_354d921a3759c2556cc892a56f9f4f21(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 224, 224], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_282105b8a4b9005639370dd9fb1097c9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, None], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c0eb1e58665b6937952e19232a64ffa1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_78b820bce8ae8b83dfbfd35bf290faf9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 38, 38], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b182fcdc516e36875fcde1c5e04c6870(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 4, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_26198720caabf032d5962371af520b5a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 4, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0aa32758ff3449ee07ce9667f89e0a15(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_efc5e703d69a415622d3df8531a072f6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b09ad1b67d807cc3d7f03b035f0d8c90(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1728, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_78c34eb38582027a3bfe5a17bdc8a49d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d5a30160af175087d67221cd759bd282(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ecc600a71fc16850cf7a760f447c83f3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d60be19e367192a55056eb8a81936ab8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_65710063276558e901a5a7cc7b2c59de(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f6df65240ac8925a89299b808655ce54(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, None], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ba222da548968190858df4cf68bc622e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_45d43e72ade43b488209dfb3f3e161b1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 2, 27], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_79abae8720848df294fc716c787f83ee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a35dfd75f4a39aec9d44b367a36edb83(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c112c4673f3887b2d0c4814080b7d845(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ea3d781e827ac74579346115b20b8002(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_14c847de69b40a056a624183fdd9b27f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7ac957503cb9abd3088c2dcf6c9489dc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_903c56f67d56a4bf30536590ac5a0891(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_93cb196bfa65ee5c3f96ce05d181a31a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f2b51560d87b84e91350c0e6b6f086d2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c4bbb9b8e17842bbd03aecaca5e7438b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1344, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0f091825d71d4e1ffa027da64f5d4394(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 35, 35], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f1ffb74d8fc89de617e0bc961f4e6da(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 232, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8cff36adecaaee29f89c236933757419(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b0a4b7c90da363fc3fd7250aa8d5641d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 256, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cfd61e99ed9054a30920fda1d94f7475(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2ae2ff6147168e6bdf1ebed988f5fba8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c9b3f21cd8e4297e6ac1d8630524b493(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f636fa30a0565d79092b2b22dd6d384f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_480a729c402cc1c9b50f2fa16dc802a3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_85ae8fa92315e18b68b8dc16979bcd5d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b94beeb62600b02926793987f4e3ec6c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2109b8194661f3138acf2c8b007d6b84(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0e05ff9dec9e57dda66ca079eab7ed32(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6acbf0634458293ad1d55909d3372c67(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 100], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b3a9c28ba2de97e4bf731fe5fcce0b52(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1104, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6ed2f9a7a3b3c0ddc363ed2cf9c5621f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 5, 5], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a09f14130ee5b9fa144a8e7689b473f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 832, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a78c095564d577f66b6b6211bede521c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 16, 128, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9437e9a2ff510eb57d949f967612d7d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_21cdbaf050444108fe284ba07c7eab75(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 75, 75], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_29cce43550466adc735962c12bec1544(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d1eff69a71e3ffa184d13ff92f3df2ab(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 4, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6672a4101d2c1bf5bf196ed91fc6113c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fd4d8fd92633f0918a445794c425b03f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 448, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aadc1f2d06d73f2d1a2e0c0c53deebd9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 300, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dea9f847be393beab56b023670ca37aa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 128, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f0e3c2be8a5f1a675a0f7eb63f584142(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2e6e13cef89b90578915c4c6583801aa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f268700c4da013d6a83d4a923b88f716(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-06'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ee7c94107106bd22819111f42d41d7dd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ce8fcec7c43c5e4a66ab86f6974b50ed(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dcc0d2be3e3ead42d6e2ee26a70970cb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c12e0a77c8c7a291cb84244dc2e185b1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1776, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c4bfbc0e06310a32dd8ec11ccf8397dd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9819e83c80fad20383c50c93b943ba45(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bc61aa3e884868c6d7ea2b51aefc111f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 47, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_68a3a37a967d00e45a032a75e100cbbe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_831f90d1946f5d88b73a11bf9e88126f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_93dffa82bac9460712e64f88ed24475c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_634fbe3fe8596f1608e8d2bffa59c189(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_011b6c68b397738302a4af50e26ee9a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a5af6cbfbe32b6178579798aa12e5117(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_11552d4e360ad473a97ca3997cbdb69b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d0af63b422c5434e9c5ad57c6b33b9d3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7de4059ee7757ed73afab8a6a75785f0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1632, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a5751d1bddadb2e498a869c2225f2f15(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fff5f20a5fd67f2fd593178094d7ab30(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 2, 27], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2c016dd7817b554333a16cd34b1a9c4d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 88, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3cedf31f95c555c6bdb8a2150ea8087b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ab7a00ca20552836652977003ef332fe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8b71e647fe0089f035928ef765b0f2b4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 147, 147], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8c5167bc8aa6796e64bc2519c4f3a09c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3d0a848e20544900d50d701a3eb9b131(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2518c43ed17c92ba61515166afdb2e58(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aff0a1d31bbbdf620cbc6dfa93b5124e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b9c38b4e2b00f747c31761d68272255f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 32, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d0eb36d2a7b10a53ddf6786bc2ba46c9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8359ec07e6727424210642ef7753057d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 84, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a66c9ea1116f694a0225c128cd0ce6a3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 320, 320], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e68404a4033630df4d02cbe3c9aa4a92(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_140fff394b9bab4c3eb8c208a8bd719b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 840, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eba84ae045edc580a5322d6110ee9b6c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 75, 75], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c3f0b2690804cceb3a998c4afc73fbc7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3f89882aa66b7d6e0e6cf03848a21bf5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3dd51b2ec7b03b02c4aa857a43f3dffc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b88bdd4822487797778819bf35905a09(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3d69c22bbe12a85cc0b2687fa479e150(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 75, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f8c2b56849a92c7300392b789d801624(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2f3626e73caf1cf25b8b075ab4aeac6d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 244, 244], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cc105e3dcf83ddef145c899763feb1e9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d66bd28761544b0977a883bd31ab43f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6fdd1bc8e0e47d357c9681e3afc3aa73(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2016, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6ce9c0d4736e8154caf21aad5b0a853e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_88d417846c710f6bcb097ebfa8ce0714(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1ace33d8bf5c0141d2ececd56c48ff1f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a22ef7c70c72a5961e472030518977ff(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_322b006503f083c81d7cbf2702b856ce(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, None], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9b1661a5cd671a8b70b1304b03ebf241(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5a7448d0c9ed8b89a0a1ac8fbc10cf8a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a001f631eb3482fb4092b22194a111fd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1b08cf49c3e0c08ba7759192916c262b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 906, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_09560c7f59586714659b8574fbdbfc34(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b24b502d328b553d9ef8b792d72b4b61(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_04e521ec6933d773533dce64e3d45124(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1e9860a676272a81feaf9cf26dc4e4db(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bf9ca9a52e7714e3b3f92960025f51ac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3ce24945cf09613f0e8e068528b5b165(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_89218944f1439edc3cce4e16883d0d01(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_af48b53061b6c74277cd331b33b43783(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c793602ede73f9274317587bec0f6eb4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_937f0b1ae193bd9da7f88fe70cc20312(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_626f3382699b88b9e7045ebd7392ddf0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 832, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6a5770448924111a18dd88356c399321(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_faab88388cbfec387fbee6d3da415e38(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2208, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_10a96d116bdf16eac747ffae2f89d57e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1660dc9441cdb2a96059f89ab09eaa94(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0c90e6f36ee62d83e7a99e0bfa0cb061(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 28, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4cc0a0f67bde72d35892dd309ed5aa8a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_60fafc0aedd1d0ef03e639ab59eed917(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_540325bcccbcf7dc27dcd00ff9f304dc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 35, 35], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4804a0d2e5fe69e735374909976ae0f7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1200, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_76d55cfaa6d0232d25592b0177013e3a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b2a27438a910061f2374bc6ff571b104(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1952, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_378be52d1342297d3b33c6f3d55b3bee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_48202d5ecfe341ee2cc8a38321d39991(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6894ddba73590d8743b835ce9fdde0b8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_04f7d080fabba449b0b04ad4c7ae09a7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_acd83c9a1d85a8d1ea6e14e7c158d54e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 88, 61, 61], dtype='float16'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4eeff78ab9f44a29f6819ea991ca46eb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 350, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f9f28d79bb8fc25e95db546ff0ad50b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 200, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ec58cf47a015169c640dba668a5ac0d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 4, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bcbd2bd27060e5444ea5c4872a8dff65(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_653a2863d504e0a421142e940bd29d6e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_474d841a3d7fd81c6525c72f0f53949e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 76, 76], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_98d6280b617eb479b02b2ca845c2b604(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 300, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cf58e051a021fc0201a846c5b7961f9a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1248, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e298979dcd323edbf2ba8f2f311beba3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 2, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3046179a193fe152f8a0c8032c94fb97(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c7e92a9b76af9d3c0da8e14097506bd7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_df0a64ffe0bd41c2fc472b41150c17d5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9d0fff58868f807bae16be1c5473db29(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1cf9fed88898a52836a6b9e7175f3073(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 48, 48], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_76c4233d2f19f89ed4488de2d0bad695(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c3c5069945d6e6e01fc969e96a2de2bb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a90ad7972227a94e7fe7981f2ebabe70(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8de152baa28ab7e9da7e2432f6719840(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 38, 38], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2eae31498e57b686bccc36b149fb23bc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cc5107c08852c64c8bad5e05415fdd87(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 2, 4], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6e7ff31e0b879dcba9b6ebba828ef520(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 972, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f546035d8aac286bca7b3ebe029dd18e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 26, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8014a0a060c0e4e6a65b292adf06a740(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_91d3aedc21153cdc01dfdc591767df6d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6e3ca2d981870e25e3d4d5e6d1a4bf5e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_895bf33b9928199543d8314581d69c80(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_24e7b3aaa2ad76692a7ac50c0a6fad88(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e418ce2e68184e8ffa573e948b40ac86(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 864, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f76191763e5b0e9c94d9a20705e3d44(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 570, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_76819a13296acb267a71536c7d3a5599(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_644964eb153e3db1922193903e2afb1f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bec13ebeff44409d3eef899327206ea0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7e323d9cc4d140b84a43a2e2bc7c76b5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 175, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6f601b8e703e0031cc4699e666abcfeb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, None], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a571297f364fbba0735c6f470addef2a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b16fefd9c298c968f9c51950d5c477df(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1248, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aca3f1beaff9d735bbb1a39a91103cc0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_307ff8dfcbd3f3dc711b97745f106617(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_41bee5f0c71d054ae209ee5ad94286ec(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 106, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_926de857b9640246d3a861bb96dc7238(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8cc70e99c5eb0598dfe49f580581078e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_75ccfb5156a59ebd3b59de71bebace12(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 26], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7b82cf83ff45043ef66c84f20c5b0beb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 26, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0651a63728a76e450adae5c8c420ebc3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 728, 38, 38], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4e1aa685230bf4542d74141d26c0a6c8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cc82439e4f1db73811bcc5c3210c2e04(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 200, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ea0e8d966d9ebe90e2e6cf8e9a0f999a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1006a06063d0d7187b8ed43c448efc37(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7e952e52418ffc8a014ade38876a7a85(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 352, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_58210f89e6aa313e3e1f81e847ac0802(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 28, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_95c2a976b83a4958b1be6cfc2b6c3597(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 360, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1b872f8572aae335a7c38cb1cc0c210f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3336a67ab9f984ea2d3fdb4dcba03618(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 12, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_467937c1b73b0a6445e3ed6e1e91f50b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_13672e74ed748a972307260bf7e23292(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 152, 152], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7ca719eb7eda5d71abaf08e1fe892374(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1044, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fe2cbcb53d2ab5f6bbc138deb132c192(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_448c0f99cf80e2fd7cb8976990d3f7f2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9ff1465bf90f692f92fcdbb9f0609310(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8de2a97e06d91e33f36e2fe3b70e55ea(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a294fe275f6cd8d1f3c173f6f1207c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5fc991c75e15990436c661ad4f31bc2c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 76, 76], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dfd191ac4d5b9dbeaa0c4a7641e92bf5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1008, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c69868eec40a5ad5dc2d6eb4fedf8513(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f5b7b0fafa1d70e4588dab9864032d2a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_40574a8c62f9d3b897f69456cf690e39(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 35, 35], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b755acb08dfb84146f7673f238a48da2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2c4b0279f4467672e4b0396db0c2bcdc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dd18401b50f195d1b69ba1a453845f02(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2560, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cea65a08f23b566a04190ff1bc59e5e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_263b59190fe88b474344cef4ce31e75c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_09f774ffd15045b465da35e642040316(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 62, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_be4e7ef61b88705310d92d4ae3c098fe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 640, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_76d92055e5e8b62b69ae9755a09f3f70(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1296, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_903d1293b7e98fd85bcbb07e234cacbf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f0e9367ac584b9476f9ca96ce73f7233(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e8b98632cfbae41426fc220f445e6486(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0dbd9fe2e1e04bf2270b23b74c3d9df4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 12, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a9301b445d7a6ea9af96c2efe8af0364(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_224a201907eed15b1de5c8ae3bed5fc2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4f79fd4b24bb5012a8cdfb4b3210f826(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1cc6aad071abf8ea64d3fe3ef4684e56(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 4, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5cb185b3b482fcf1e63a6baf58c67717(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1872, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e7f539abfc4c607a5f18a9fdd1e81b06(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f3218f271032194f87b942bbcdc87929(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9ef2f6d566b5486e9623c562160de48a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1296, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3cb3ebb4edf0758fe4ae9d28bb2db0fb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_163c4646273b5e8bce82af6b7496fc22(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280], dtype='float16'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0a83b1b40261c8df6f7d4e11860b3fb7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c5f8394951e89737c107542c164c5fc4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d1427fd56e8a37dcd84075fe241cdac1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 350, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b0beaf0879818191f4dbac2d4c9de54e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 832, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad0d3f301065e9667c07af51436d8926(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 4, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b260b97a7dd4463e7f763f97d23bbb0c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_51bab3307d196b0917f4c943a4dae42e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c5860002cd4776fbf03869697a41cb9a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8947a0ba9763d29f6df3181faa00e811(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cf01bb348290461e0b99bb3ee855ed13(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 49], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a411859b7a06d507d70073643073f19(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 19, 19], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5e860e3a3156f27bbc563cb769888402(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1824, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_99af99761d0b8f2f67a9a4bce2b0398d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8db2c8ea7e3b8c1d864304caad894b96(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 1, 26], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fb39e8e283e78cd77af02a9e175f5e0f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 64, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_04553fcf88c43fb8aadb6eb006c585a8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 256, 512], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6872978f377bda77c1b49c5e0bf899f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3eba81a3bc8b921c66b1715a61894576(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ed693f230fbb7e9cdd184fea7edf650d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1908cae05be05cf11f8e07dc0f2507d0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f0d93ff64af4d85361149e262f90dc83(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0003daa770852f0bc16a1add0a2c4e28(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_75b1be54a495016f7a339c8c158ba5c3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_31b49c768d52ebc24a46e57240ade1a2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_98e11c90b7a6bbcdbacaa094234728d8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c35518f3334794ee130ef13ab20e2243(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 960, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_84ee48c3345df24cca6b87e894987d91(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 19, 19], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_534bd8605a1bc9db4c21abd12de320b4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 4, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_672672e9aaa178cbcfa2285d1b93598e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_14b082ceadee49af49a550bae9fc8c51(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 528, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dd271ad96c296143ba2bfc64860ac86a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d90e79439c86f700f2883ab09af3ca8b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eefcbc750dc9b0e65d40ff7907f1312b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, None], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f01e32f14cd76fc4e89468d4c13603c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_001883c96c6f4644672acdea4d262848(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 100], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aa301bbd5a1e416e0b921d7e268a8e4a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f6d9f98a7283b104ec057a546d2c2fe1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 19, 19], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b9f67ea0ac53699cff422db5b35f4e7f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e82b94524c25643933cc36c58a30e558(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_31ca4dbd630951b82d9b5bd88e830717(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_373728b1490bc33e9f944e7dc2e95787(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 16, 512, 1024], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f445cb80822c337d55dac39a368586d8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 12, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2d1cf063b27b9372e920c977c556bfdd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c892c60e1007241eb02b17ef2d1c1ece(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6a86473a8eb8714791b7ea8aceca3b47(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f1a082db6ea56cc6f834213b72feb187(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_43c844305a33618763210ee186f27102(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-06'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_59a74a1aa3538b67792395a662b46c96(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_365d00131505c7d39b3fc12f494811a5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d746ac404df2cb95d7525b307d9e10ec(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c22e309c5201e4b0852ec0fa242bab74(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_04d13aaf40278fc81288610b41085998(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c0cfa8e556c17c27752ea924bef83b51(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 2, 2], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_df260ae3a5b7b61ca1667b85607c9a67(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 12, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5cf4bac0db298058a7de96a85d35606e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 168, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1921d031b73340a7414904caf27a3935(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ba70d8ff83ade7d49521c28915f714e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ab1ad85a256f22d05c0ac5a2c9d383eb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dc77783f4f885beba79336191518261a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e73dbcb3579ad48eae5a945696942094(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f7cfab02d7eee8ea96de3e3e634ec8cf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_399ad121ba8b7abf1eedd38cf9d4f3c3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7b12fe585d6439f409f2d665b43cdde5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3f59838f0a9cd17f718200550a2bdf3a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 102, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c401d1176ab51469a816b7bc7ba19212(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 48, 48], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d4c980e711c4e8864589d6bba9225820(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9aaffcd1723b899daa7ac8d9d877163f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 24, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c48808d6c4a1774361a66dc6663beaaa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2906dd3f3e0640042e9e677d7525ed36(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f19d743a92f26dc8cbc59531e4a073fd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a9f5f909b34ff4c21dedcc1fa91a151a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 150, 150], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_195a3ff1f6a14a915f756aa330dc479e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 740, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_79a5d13472e01c086f5eba705a3482a5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_12c379cc113a403f53a4caf467d6d562(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b87e946dbb1b9643a1a946af8e4fc59e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 32, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_88907b574d986afac9f3889d9e2f029f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9ab99cbf2437e4d5fd27a05ac0ad88a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 636, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fafb45dd41fc17f3a0094974515819a2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7efaa2655c87330018e1ae7b3862dde2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 2, 27], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_92329144f48a3e23c44fb1c13743ae94(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 228, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad8305bcdf7029aba50842d7fed98e6e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad543cd8907ef994e0ba07c7d7732da5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 2, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_552489f69ad39a25d3ae7d7767b1907d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 4, 256, 512], dtype='float16'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_02825ebecc52b0450c7e5430b04e81b6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 16, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_970c9da2f86e92b99b203dec83780b08(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_69864754ba1223a990da32a7ad8aec62(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7757654bfe9b9d46a4cd48c8a31d3306(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ab836b831e4fd09aff2ca9c06012a5f7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 6, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5e1344a34fc3a134b70131e7016a7e01(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 75, 75], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_31a7804c24cf00b89a64598a2aa3f789(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6b99617946dbcbd795bd7875ad9ea3de(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 122, 122], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3f5da1d25591a17618f32d5f4cdb2abd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 17, 17], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_90033cb242d6a196769e55d5671fa6a7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_00b7f414cd5d1873df88c7b75f6f611f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bb759b99bdfcb9011ade436d7f22edac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 432, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_37e9aada9535d6dfb765f3e078755a94(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4f5421ea7cc936dcbad01422103e791b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 960, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_25a90f7243b0e1836ddf4f434eac60d0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_43c10356140810fa3f3a87f08f368419(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6275e405292e7be448c4e45a93e79d9b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 224, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3307b0fc511040ca02006ee72fb33cd8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_75f5ed0c9a39ff313160be27327a8985(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 224, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b4b14c7ab1450b3a7d537c3eba9ce1d5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_77f52497a6de9a22ebc3d5a809e41670(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2af563fefd23155cf2ef7a456335678a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_189ca6ed72ffb5645192b7ab85ce4e09(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8bd61de5b447285a1a98100e41b57a01(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 168, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_01f1d79d398e2cfc61a4d9da75324daf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_86de788e6bb94bd4959b9b0ba3ac5b8d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7a26ecf10bc78b5487d0ed21b0a39662(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c5344cdfe72e997e5159f4cf3d17f3fa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1344, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dbba9c6fdc045c77d11e5ce051888a27(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4011c567561a3bf8cf22c9e03146685a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 52, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_db9f06a06e5ec263aa27a6578d0add80(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 200, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9289ce5059ee500d56412e66c3fb5acc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9e5a434e12da2696a9f9d868b9f45477(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 53, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0cfcadeda21a27d8481eb1381f9e54f7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 256, 512], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_361174189e20d55ed399f9d9a385b3e5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 117, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_045547d9fd3b70e80400ddb0704629b5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0f48c60a5de0a31df82507672ae23681(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_04ec187a23fb298c7d6e878a60cd785c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_32afe5eddedac1a313481c9673ebea20(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f0eda39cb0cd71973c376984841f0139(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 128, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f010ac89e95313785804b533a6f28f66(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 56, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0449ac4c00d9aa171a40fc10cc3bfa5a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 100], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ae5b13e0e939198ab229111bdbeea01b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 304, 304], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_016ed345c77e700944f86327d3e197d6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9199062195c29090c0af0cba91381c55(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_21a16052f5089a16cac936325c38f956(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ddf56b24aa755252d82b107dabd2bb60(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bafe13e1cc66d01a31631a32bf01915c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7281c73301669007031a5922202d9dc6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_07b63dcebc8a727bf977076bbf0b3bc0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 200, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f674ecd4caee1f22c0094e96a755c269(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 12, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_08c45d993c57945b8989d36d0b06f306(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1504, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_44420878aef6a461745211eca5545482(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_46b849d24e80432a68cb996293a9a1cf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0483512e4e012119dfa48bef933ef5bf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5144b12e6f28571ed44049cf5b11694a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ba50b5cc13932f74cac90d833003cee7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5be7e6bac60987a818142ffaeaa572e7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a3e21fe52e3c2c2a118074fc71e41be9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b2209e036a8d188c0ae50a19c154061e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eea460abdf980da44cc6e72122d0288b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e12387775509b6aa385a0c26a8955671(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_adbe90a6483c6d7a731fd2e1e9398487(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1488, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_454d1a24aa37124b5dc12d58d14943d6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a5874bd6cd476b15d682e91765085065(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 96, 96], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f89ec4b18ab354fc7c6cf31b335ec0d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 35, 35], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4c58552bb28ad7d4dac823baed3c6698(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1856, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f0ade45bd152ef82b588f2a11bdaab9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 32, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_02d589f4fd9bc2be1304315eb1756767(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2, 112, 224], dtype='float16'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0bd5599424b42dedca8b043b614148ef(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2112, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_012624d96a179bea34276af76c94f5da(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3b849076f67f776bce6a44e2f283a361(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 16, 512, 1024], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d4400d0065e88fba0edb22f813447e7e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_876d3325e90e8844fa413da7c1376370(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4c61f7bc0e45296f623737d3903f679f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9578368f12d97c0d445fae3606a50612(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c99838ebd7cb34868e5934a9df014868(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bdad8bf219e6ee27eb339ed3428412a6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_24cb734c54a26d6072a990b24082626f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_49740e807080b999383343e1de877743(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9fc8111224fa03c870923298a0763440(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_038d2626f2f1364e422ac477d1de94c8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 350, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ce24ad847c2f00a65300d4a5d0606410(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_601deb1983c5cb746610281b55124c97(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d529bf5776f65ea06d894b0b3e3899d7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1664, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_def4008dc88fc8e348ea94877e806f89(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_86243190853738b96a63e9b23bce77ba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a235d6f6c02c5b2da51e817edea4be49(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 624, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3ff10796688edddd66d59da0c3e0da2e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 228, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d39e7200e9bf7d28cc6a48a96099dc48(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 832, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_11d9df73c6718586e31773eefe4f9f88(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f89151d5524d3ee789d2b4cd1a607c9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_216919a91b32dc576df34acf88a4bbd2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 150, 150], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f350b4c2e9ee3864e1625b6b59db562b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0c2f69248d988e152e721152baaefc6f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e85aaae890c2775c3c0da038a97fc088(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1104, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7bac455786c9c12769ec77051cc472e1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_25f103fa3896da099d3cde17bf1dc78e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 224, 224], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7f6dbe06aa383090672c873099fd6dfa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_92d6fd2dc40fd765a0c08202e2f8e538(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 728, 38, 38], dtype='float16'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bd8359a9fa2df57fc823821810233f7b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1984, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0e7108aa4d5e6ec1d2977be007d1d6e5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_19f30ddf3f63d31426e6ec22a02a60f8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1584, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f79d05a5e1260dd3828d3c4e9cb9c6e2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 24, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8a2a6ead45870594f2d0f3f9c27bc82c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2064, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_df574d33db59afb378e0feda4f1d4f51(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_de5634a75203069b0dbf3fbeeff796df(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e688e2824ed9df8542cecce3c58a2d73(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f9d1da8241b988564b20c5f52acfb12(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5bc89a0588a01a52fe89ef44e204e770(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eb54a6198d0d098b5df559ad16cf3d3e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1872, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2780c15474f5780d697bcab8c75e8283(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 184, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_629b5c94a3254f00139132c6cfefec6c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_61880af5b4bf23d7514c17811f77f734(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2423ebd1101a36394ab1b611cdd61dc9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0b2e3d95f27029d02d70f310dffa5275(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b460480ca04be6d2344634c87163ebc1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e1010f71f59d34d315de368d1204f260(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 2, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6dfff219623c2d00fe823701d0bb2d6b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_75e478ce1c9870e0491a349dfe73dce1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 49], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5e07108a5fd9f5f4cdc70500da5a1d42(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4e0c57e49b7bb71ad7507c34d30b87c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 236, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7141b8d9126bf91af1d01faeff73cc6e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d5d4acd8a1de3893d6b69b05d68ff56f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 12, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9d25370aca2ae679810e076223d2c19d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 4, 512, 1024], dtype='float16'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eb0b82455812b1e069ad7fe596f1bf3d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 32, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dd5a4aad19df3645493d32f6bec951d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 32, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4847d91611badf5406733cc49e5f50e5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_461425de9bf2a33cced22dafb4c617e0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_06d57437dde5bc9c97fe8a1ae10cbb06(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cc3eda264da511fb1672e23fe0d95a53(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 96, 96], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2efda438036f5a8927b64a011fdf01af(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5c24ba61461480d4c6d3eb014c3fc86d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 8, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b6c7a69c80a82f51877a679621124dc0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 48, 48], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_138aa3c869fc81b18cd947421698b73a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 320, 320], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_04c277ea268a9056176872ab5d1a5d4a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 19, 19], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9d82ba9551361ce0e94ae8e857ddafeb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_815a31a17cf07eefa6f7f921cee037bd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 175, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_754aecdb21f90647937ecba1c265a35c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3d0dedb55bd15e3225ad133db883e64e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, None], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b5932dbf13639be2f7d7c2f7e01d89fe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0384eb41ea893946136c8462fa41d819(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_892560361a7154478f5b2366eb05ea43(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cd1ad2d50aa07808a6d27b1f1abf8ef2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_27a114a1ea98a13a7bb957d7d400fa15(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_484b9a7cf89e7e836bef6dfb32fc77f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b5f3fe04440aef0974d3c167829a2095(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 816, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4d0b99242f00e944a18b9bafd47ea522(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1716243e54b0b8be76f839f145a0844c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_11304b7dd07aac514dcf1769ed48cdf2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 448, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bdfecce40297b641425a981e6aa610fb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1792, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e039cbf6ffd8b4b9597ef7e012ea5b2c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_829e10c433363b93c63afda25e46f6be(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_16ee940437a12012167b722195d2b167(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 740, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4d4fb26491e469e694653a760f51e107(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_897e67fbd5d9de8a56c074c35802522f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 570, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2bb8c3500ffce4b5f421ae1a79c4e03f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 138, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7ceff3051ce8ffe6a3af6962933f1156(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 24, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_03d2fb3dfca39e5be82fbabc33a13c2c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3254ec6234d29c6b79ccc83ef1b64d71(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bb7bdcfc57af9f15c0c2fa0f429ffa87(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_499000feeabf66e84f457239b361acaa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 208, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_16756718fec4c09e92352396baa2db01(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1920, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7e483b644adfab6a13faad290fd32bc8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1392, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b9bb75184127fca2f1c57ca008a66b75(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 38, 38], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_754f3b2eceb3c969cafb4bcbd526a9be(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_24cad3ff89579e26f4b24bc3de0dc351(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 366, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c4d19e8d910b39d7d8899b665bb1f6a7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 702, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2e4bcdc5bafdc514906583519befe93c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 704, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_677631133c87b517e069a44dc621d676(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2064, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_25b4477b7535e9e0765139032b8778a7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 175, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b79a03b9889171b06565fd33ddcec551(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_96a1b2b0a9e248c70d83dee2efbe4820(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 4, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1b68c0dd3defc98dbe90fd15975e97f8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2b3dcda2a21fd69aef4c26fca2eaccad(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b242da51ea0bf8a59aa10b12507e499e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1976f70b33b0bd4b60b0ccc3db188442(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 832, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bfc693cb64eafdcc84b0e80cbfb15276(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 24, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dea0f47511d5ea514ec4b33b6dcf178e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6884bef74e02ae00c3bff219ebcfee46(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 100], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_22abb3e6174fe3d2c487d3c21111b5bc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_75fe4f0c8a04a0ae02b307b58d9938d7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 50, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_621e043c1b9c17d3e7dba8409765afa5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 912, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8592516a8edd37f0f9798a35009b999b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3fb28a18176d8047772168f049a5f096(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a61a6fdaddd8ab64e00f90b107930627(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_73a66f5a4072a54571f8e6b41c24a31b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 102, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_caa476a83ca48a09742ad8c422eaae6b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 2, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_972b3a4697e46b3ca787702f45ce6c14(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 61, 61], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_22236779444c94f02ab1a29f69c66e9a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a65a037fa38df36bc9206503c5bc49ed(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 256, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c2219a45e63d1019663f2e68962d68dc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b7d925347799e3b4b1f0dcf60a0ba371(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_05ccffff69f6576dc66e7c98f18af5b9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 26], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fc855164fbc439186f3dc281de66f87c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6868ec7b78224230b1c6b5ca3874bf9b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 1, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8676f53cab2101556a3aa705d3f615f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 350, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e1c59463e9917d9924e85605f4931ef3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1216, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0988a78b8dc830e8cc8cd52428e02408(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_318c00cd40030242db3f705a93b2891a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1584, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6547322fd2cf406a2d8f4d79f0d12efa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_faca234a3b0e486e94372fd901fbd331(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_390e91adf11e1069414d8e99c9dae65e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 175, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b8cc8eed43baa042d1303635c404d3fb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a21a55bb7927674d19430ed6366df76(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_275d357f63fd1ce0ac4dca3ad5d569df(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 410, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0d6acac54a26f9a7b8028cc698d0c0d7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 147, 147], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9f78f5cb4cf7428f7add9adec1b269a8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_71b303d09dce0592b239a8d2f459e33c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e24cbc95fb090ef9681d5e1937fd754f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 800, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_72b5909584deb756eebecd96ec2248ba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_68999480cca03708b0919edb32d1a515(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_90cd01123450907824b1892589e818bb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 75, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7eb422b6e701c3e8483c93d320397fb9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a6de8498df15d65b6c1e88fdee42bb3c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_801eff40f6ce09cb8d1af376fd7f30ac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4cefbe2f0fcd0a3c7d12d04da6294ddd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 25, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e790ff09b57e113e0ae53401cbebfed7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3f81715057e631b27e8769c1ecebc423(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 82, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0924953f854a529bce7b5983b306f26a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4fe064b7d072a933fc3df6cde2593246(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d0b1a0fda2fcb56df040381307a1519b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c879500ef43c9583e591cf88058373fe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6ae3234e602e64832c0c8da733fef782(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_00e024924948f9530c4c5682daa6652b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fd6c62a5b14788f5279b50233d0e159f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_23134b2b217e82c7d766d0ca62787db4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 150, 150], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_da28452afd87fa7ec6700e3a52e92bcc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_90e8782fd11c47935d861c75e6382c54(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a06f1475253de64e3094a1f5e5fc06e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 10, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3e2f62cc2e8f268471a9d1db21d2dad5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_458b69cc397b881816c00af070304cd6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1824, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a175dad4f9465d67843636e9bee397e5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 32, 256, 512], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ec88a9a1602c9efb410e4e001ac243c8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ecff041ed4f2865598c13f6893c95147(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 320, 320], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cf3bea7e81eba2f8803fd04ad3ecbdef(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8756cd79d20184e2edd37a3b21596ae0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 61, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d4c51ec0a78417bef59b68da57c0db62(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f8b6f0326bb25d20671547dea2ea67b8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f7667cbaf3d2a84b08da0bcbc1ff90d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a0459a49b41db6a39edb87eaa20aa89c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 176, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_be909b13142b62385fa32c76b9f65ef9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a0aff5902af7fa4414fbcb1026214c4c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 2, 4], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a69be4e44c5f356df59147f61e8e82e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d54dd1a553b1016b2cb9efe5deb82d10(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_37ca527caca66fa6bd8b014b3f6521f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cdeac6b9a7d9d6d44b3055add64d33e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 304, 304], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a245c093e969413a000f5b4cdefe6057(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 100], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9b35cc9659988100db4c2855c4d8b2ac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 35, 35], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f0310f299a03f53c6ac5e812c44939f7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1440, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d611060c19d2b5cb27db87cc3b170ddf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3b17e66f7026f48e8e8b7b11d641c130(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0b8f44bc97a356e5c42c3042b7bd1505(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7e47f00b89a83b58f2b40e9c647ac925(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 16, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_74f7178073080c772e3857dd3fff3ea6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4139c4405ffe0867954eac812c78a849(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6e736b36f0f4b87a9f4b5897e00b971d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aed2ef1f3b09eb7417781b46d6c304a7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 32, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f04b1f723198053247940073cf961599(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 100], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5ea9bcc6c5efaff0e969963c1fbd5d7f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a67d6452b2a2ceec8c222c4c76a8aaf1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1584, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8caf97a8480c55f50ffb5e8a321f1a14(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 304, 304], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_248b29acd43f66463b4bf3cd2bd304b2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 150, 150], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ce70511b552491bd0868bf96927138c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9a1a7d5ba2e42c33e716ce604cfade5b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 96, 96], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fedc7d2ba8af28a5f2c30f5b54772f7f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 61, 61], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e30289a77f655e48c492be98ebf63d03(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8c13024078fbf85f169eb5a282e12cc1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_459f781711e3a8ec07a566bed8fc9870(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 38, 38], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4805c3dc30ed25876b62bc7c6d061d59(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 832, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3f42d5a9dbdc022e1c99000b5a383ec2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 436, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_75c37502a040ce81a897458d73328af1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c316a32894e7c28555759959a17c9869(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ef23543cccc6d8990591be5a7ae61303(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 19, 19], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c7b2e018e3bc863a28446ac27d7cdf15(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 416, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f6cb4dd98a786871b8712340ce32611(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2688, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_73ad91d23619060c1394e490a731523b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 26], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1242730d4df1050dfbe00528d15112b9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6b466436acd8abbe30b6123b8d9f3da3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ea0b2107e75d7da7dece16d2818e62dd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 49], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_81171a73171edbafffb791570671a2bc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 4], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0cab8e769743e9b14df2ada13681fbb1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 12, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_efcdcd8cb72d0add1f8e32f942773658(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 140, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9b2b4704b181faf4a2a3c3b95962d9fc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 704, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e2c7a5f2b9b5f2b02274d479cab4c5e4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 70, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c0bdbfe9de0855b84834472a3b76ddfc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1000], dtype='float16'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2df9b1c06f9256f8108b4a1143df35e1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 2, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bccf32763057437a36894e2efc788dbe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_70d4159fe200b677a52f2592dd80cc4e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_68d42fd40adb5d3b982284414f88d27f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 128, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f6b9111cef8062043c1cb3d4a1b73a7f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_01be06e6f549cb4256fd5fbcd5391ac1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 4, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d733ad76e6def93a6431f82f0276bd67(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 138, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_557d967e46ba84e246ef28143474892e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 88, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d20b18026bf4113dfbc298bef48cc387(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_84021c0cf0b138b0d1fa628478787faa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5544828a7f181373d095ef45852af1a0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2112, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6e3f86add704cec9080165103b6b20f7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_92831198afc76b390f9884e9aaa79e85(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1920, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3c866ab54cebbaed9f5d6da4974354d8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 42, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4410ed18d17cf56c7edb48d8d9487131(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 4, 4], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bfb83320a839d9c940a6fea84319cafc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 636, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_20ed86f8dadd07a4674aa0e32b32eae3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d15a7bc616b152ced00d29a8e4488a03(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_396e1479eb15e2e6034ce5d21e596c0c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7bd3f8b6a0fa274a9e326618eee146d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_67484094503db28bd97a3dc60c1ca3e2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 608, 608], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c3b4a6d65f831f3e96d102564a9c4de6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_66848f63380fe3caf878efaf5f970000(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 56, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4ef7505da06e846b325d100749737dc6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d99a1335c90ba60b4aafbcd7298b901a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 82, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a15e488b34fc5df2fd08f2ab54403dfb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 12, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cfb69fd406c5282be428fef1cb855749(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1680, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ea5dbc47a611c52732e4b437c6ca230c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8176ec319d7b31f697db792d91833999(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 352, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a65cad9da910bfc21515dc93ae466f7c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 8, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d005e9235308a277e8637c9104600f9a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 704, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d7596fea3bbd1ad27288e9ee160c14c4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1728, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ab8d8cc91b147b04c1301f363ffac4e9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_92c0ba69f1d3da47805e4633b3a97d29(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_08670a12885360769ac09c15fed4cd71(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e6bbce1dac03ffdcfed09c99e5d56cb5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1312, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_963b499957681d183a90ee6fbcf07b4e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1442575d9d6d8d21262f054611450fa7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 256, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ff41a1da4d2282704bf6a4f294a4e176(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7dc14e15b0cf7a31536b29ecaebbc806(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2529de372a4867bc4899bd15cd785d5d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_40bdf43a585f465081df6be985c57b1c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 152, 152], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f6596b2204ca91d19f0a77cd570c1cf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1296, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_480b42b318e09534977c677347792d04(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f4a3dfadd1b3aaddc906e16f8f00c76d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 228, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8eb5dbf52e9523fb2b2f2fcfe9efaf89(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fac92df0b62c9b0932a8ac0aa1ab203e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 149, 149], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_609ec7bf2a07fd26a1254d491f9e4e1e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 262, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a7f7776619c52925ca8dffec3fc3eba0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cafb0986540bfd1a3adbb5480dabffdf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 70, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dc07229b50b6f866a2bf90a74fd7609d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 27, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d5c09a718bde4f4f9b2728061f047e4a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f8f872561986ee45e1ad7aed84c9510f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 320, 320], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9bab6ffa8f76d6e5d8dd3c9144454c5c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dfc6beecf20948b2d428320a8ff22420(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 10, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_02e3dc0424af03931aaf0480e7113fca(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fcd0ef96fde17bfe36d316834ad81acf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c23f660599def9fbcf07210411b7c578(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_386d24c874bfe130a74dc32f974e8743(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d7b7f624e1be1fb0d2bd47d575068cac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_736bd2913629b3dcf106e7ca0229d249(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 4, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_87fdaf18cdc8c15e26667373853ec63d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1728, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_86effc94cc6dd89c0a0bf56d23b38aab(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 44, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_899d17da3b944425f83c54ff28b71924(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 906, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cf9747313572e8321f152fb443a5b43a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5881edd31c99b5b3ac47d160a72576b5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 448, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3d76b968cb943f794da8117263870d04(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 1, 2], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5235f72a7f34d2f45209d273b1ca4ce3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1968, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9a4bbba9f675abeabe36ab9163defc76(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2, 112, 224], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c7ec793f5fa22ce0ec1b09f6e26f303d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 320, 320], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7e32ab27fd7a674e8bdf0c4fc681e7e1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_87d1861b61a5421593d15b19010c587b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_092cc30aab8f74baa2505d9006fd7209(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bb16a312934e2197793f27fc62de7457(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 96, 96], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aa2f69b8d2bb435db583b9490adcbad5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_96750a0f5d1bba3bce0b9b30195dd6a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_edbe30540ec177cce554ce48aac27083(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e54c9b446b4ef105808a049283955adf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ce43855413328140ee22b283997d87ec(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 232, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_45cb315e383525e33a9df82784065670(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 26, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1624ac6e972a6b37a9132e27deaa15ee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_635d8c6eee3d97d4ebe54fb73bcada55(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 147, 147], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2f6636f0103abee5641ac26ee4dd8bcc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 544, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cb83104a20b06dc4b6ef79a0d2e04cc1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1504, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_82424129111531e256cacea5e01fc53a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 300, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6f5027e141674adef965d45913bf3e82(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 96, 96], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0aeca0fb119a33c1a5a9a8bebf74ed03(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, None], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e58f60cd2c596281532515892b76457b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a5e52bc3285cb8166af50a2ee45803c4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 81, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c2e753fdbe3567442fc1e03f5e7ae07e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1984, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2dee71c3d502d6667acafa1bf8810326(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1488, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_056c5654ab2a0e7ab0fd6aaeeb9464ae(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 185, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1a961731f39b2fa32ac9da1944219062(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 95, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4cec5627b665aa86e36d6bb0743d780c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_566aec332f1857bb88d57c37943246f9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1200, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0fc98f3322a6295ddc05c1843662f7d0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 1, 26], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7a41d4c7f9022be3baa88964b959dfb3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 224, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a92fd16fd9e3b064b244d6b28a1a5996(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 1, 26], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8cc3eb9723866708e4398e370127417a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 118, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a57c9796087bacba8e8c4e883f980882(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 164, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_26c1b854eb872ba56ebedf9793de3768(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_73d715474df6ef0f0d2778246c30dbba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1184, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_99e42cd7f95916211bc300acf6ff6d97(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e4be4db5ae01ae41df990d1e11794aa8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 19, 19], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8bbf3293f35b054878bddb7e19320873(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 448, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5c4c45fe10373a4887651be55e63693a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1184, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0fc88152d11556dead22b51cf8347e8a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 82, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b22a4c2a1aad45f0f951cac899fb522d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_470c513e8f7f2702c7307c0ef6a7242c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7870053f6ad183e47186933ccabb9856(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1728, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9f7e315e1209de59d7c3d8832718ef93(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_17467ab67ef840c508175cd1d2686d33(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 624, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_127ed08cbf7d19b8962ef5d19cc14847(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_98f6af822451e6d8c0c9ff8cce770453(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_14436615a0f1d12a9ec45300b59a5a48(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f8f7211854d3fd37264b6b4fa71ea8e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 71, 71], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e1fa640922dffe9ebdf7932ff113bf51(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ba5d7d0fced3c68f4e0a7a428e5b7ba9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_87f7d7b8718088814721bc24351b1ad7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 138, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2f3de459085cd99a8d7aa22a10da2843(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 504, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_82af7e822b695e09d38e6ea79a3bf084(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cab09c7c4fa56e2488ea07ea43375fe3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1760, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_068236b963f20ae3772d95db2836bcc6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_50ffa5edbcfdf476d9ac4dce58622b7b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6cd78aa28625ecef49649dd900f1df0a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5b8fa27111695d91eadafafb1a520f56(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_20e3e436c0a2a41e09dca057c88978d3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_16a62df0a4b2e940b226096560e7c1c5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1296, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_18499de8d9e0603581052a3660719c44(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8c241c5d842a81b164e8cd441263071c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1248, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b96bb580d591d5c3b46ed9cd4fc63c4d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 24, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f7a00d309a2128ebc111858770c69ce9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1fc9939ee612c9963c71c7c09e5c4ef8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5b3c8fc32ea41d5175a58db71a369fc3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_11b605be9747462b91e5f789e3a7c2ab(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 624, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a39b08a6d0b8c8bb33c0aae736ac2e0f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f6f835efd8494d188b7d808e1bf6ed16(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 106, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_af156719f364b1bbcc99a25272c7d11e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_37e481209dfa75f478bb7af2b6546dc0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 608, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ca3414dfe7bb7b85fd0ac3ef4a8fd707(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5548a634096bdea06c688a80376ff53e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 35, 35], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cd2bb7faa7685a1a88ad514922fbdfbf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 176, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9a851f4c322cbd327bf4b5a9c7d23882(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 16, 128, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bf10236c687704c2c9a551d17669e0f6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_affec5a07fd82d74e9335cf58367bd07(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 70, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_23598a8f265065965154dcdcde6d11d2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 2, None], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_625ea443ece00e0ece297543991b2b7f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3b2dcaad09e2aad7cfc7870b395b86b4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 2, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b33da5cfb82e732a25322a60aa9f34e6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 244, 244], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e0f6ac9b4958357415fc8b72ec5ef4ad(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 1, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_877a4855048705141aeb14c3e0cbd641(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dd120fd62325ef88316b34cd1cfc1f64(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 50, 350], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cba8ac77abe105a6ebc0c0dc696f46ba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8a82e1bc607f551a13054d8cd7b874b5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 28, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c25a8b8f243c9cfd1cd6059f193a96d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 35, 35], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fa2b8ac4c3930469a8943e54c818190c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3163f9128c785d514dbf57aba22c95fd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c1d8828d7848d83703110219cca6a1f7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6673197adf120ee098622608c376aa51(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_03076558df529649942a6626db697bc8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1200, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1e650a105736cfce972de59b79c9de2e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 162, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_344329caff1bca5104933b5553a8f640(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 38, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f6e78da54ca3a40268fa01c537e98ffc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5b03cf60275a890056ef00c1c753d978(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b7c85a042e09774cd876c6c793c9f2f9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1392, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cf4191e91099a34af466b305863c956e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fb22ed800caf18e500816b81cb5a7da6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 320, 320], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0b1de604e126b1fa09ae4b5eb369b696(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6ef1b1d00a9af60b2d7ce60ffa060a36(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1584, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_68d960ab8f7b2fae2e15eee62fbd6493(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1376, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad8c8da3857e6bf3898e178beaea5391(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 228, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f881908c233cff0d518f7bbddbe95ff2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 234, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dc971ad76fd1055194fa611ddca13446(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 64, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4a3780e143c6593be6a429686d661bb2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 4, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_af0f41bdd5b07e59f95fbca1a3f3f7c7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 70, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f70d253a8c85f173170351faade870e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 8, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eb7c00b666aecb66afa5d87f4384b4de(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 48, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_24286b574aace75bf578db8ff07e0e99(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_07c8ef96c5099df647626e5400c6e7b9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_455292ca3cef8e477c828481e3a2100d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_441ab70336b8cf138794def7506deaa9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 100], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_45e50a0de819dadc779430c6014fe33b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7900aa0afd93247c2471f0ee5a44e050(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1104, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c1cd91d7c1f51f7aaf5ae5a61587df98(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0866e64952cf5cd57c2696e7aa5e52bf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 19, 19], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4461f75cf5c1fb8a609859991616732f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 19, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_07277bba8546a096d2aa0bb67f8b406d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 132, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_358243a6c0028056dc5741d7b08c7e4c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 184, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_68d9ca62f121598cf5fce845fe4e3830(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 224, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e6281efc4237d516bf62349b005066d3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_294c3e3754c446ce8ef00acc72e1fe52(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c1bd354b6b2279703032a8552648a54f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6ee54eaec46cead9dfed8ffc36bfc004(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_07db224a34ba25292369d1e9d0d61db2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, None], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4d152c44fc2b0626740a6b54eee6520a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 234, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_31eec24c3fafd54628b30060097f3435(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ca645b29629be37054ef146b655740b8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 87, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f8079b1546954a0de68d082173b56fe1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 640, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_19bbd1b0980169ab4c479175a10906d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1200, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a0110df3ba1d7944208519d64614db38(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_74aeeefd61bfad3687975db378e50ec8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 42, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_801797befffbea1a2ce19d019107bf8f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 8, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b2696b10e83c1d3c2a92b52e01f50ed6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 128, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a7c051871ccaee599d41bd3baf7a0dde(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 4, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dbd626ea8f6cd8bc2426556814b8ba10(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cfb0a2c14de50deca6e43948761b253b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_238f535c78e73773dc86e6d27b960b9c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e78aae5a583b14bbbcf1ce11fd4a7365(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_203d377c673ee4774c67da3b7df226d8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 702, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7be5f0cb427996b075b93f0c99ba8f58(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3ff41849c1f594b322c9c2499c6faf7a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e751baaa77fd63c21bb116e6d6b3bb51(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 70, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1c44d044048a874d2e962980a95e10bb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1856, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_92b620aef5acafad4c0d157ef777b660(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 8, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c5edb2f18c8e47a8bf1b6d78f784956f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 150, 150], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3af3a6b6d2c5c0ae73a92daa06d7104c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f3a752b0548f4aad8400ccaaa7d67719(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1632, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_18a785bcdfcfcb78b66199742907e263(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d25e7427dd6d57504ddd98f54354eff3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 8, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d03669759404c17eea5fbe0fd1605b35(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1104, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_423b983ffdfc7e65b383ccfeafd45b4c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5195ebe7af6da40d319d41f61bf2ac2c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 3, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_32eaba6084a13cca4928f97bea910403(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 24, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9d30283f0e3a737119e83c43c06fb560(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_28fc9786c8eb51637dc327186b33a0c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 61, 61], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3547c3c3d5ba9688e3cca445783999de(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_20ebcae51176c55d4c98ad38f10d90a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_504102c0c322ef1243f795b3deabb28e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_61a8af54b96b78bb67ab9a5783933e1e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eb0cf3322ed5ca54d3ec6aba3fa1ed38(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 8, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a4cf30a4d5b9ad84515e68b21657b155(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_696637a7fa2dd68d58407021599fbc16(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 24, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_54e8bf0baf56a0e079ef4d3c0a37f1bf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_10a6e0f122747cdb1636ff08d1dba959(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bdd537d775a796d7f54474406ed58350(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1008, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_049f5fe343bd1b155b9420ba95b6c506(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3e2e5292c99d809a2b77c71c9a22725d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_46d4a4697665df3cdd855c4e6229b962(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 8, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3ba232b3e2066a7ca07d9e9f577e151a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 528, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c085bbf49f874f3d448e2e9fa624208b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dcbac1292344de20ab0353af24353e0d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_07eeb3f0cd7e65e6c274d576a8cf26a8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2016, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fa5e018a7eef8e787a8176b3d388e876(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3caf0ad33a9071ab1f2803f58fa730cc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 149, 149], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5e3f12584c8ba623b5a6686df679b43a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 95, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a70b972739753d83ff305a95acc22b14(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 2, 2], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6d9296fab21040021a8756f2db62936e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 48, 48], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2db559d88ad496e3ddf832b3c8e53914(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2ade1c8c123212d0b4da7fdef1129c38(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 912, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_024d3110644a58f47c54fe0c5e6f8dff(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_367efcfceb41a4eac561294e555d7a26(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 208, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f235e37fe90c6c3445d82bf4fa701baa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7f76141c7f28531e80d93ce1df1964fc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 400, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ee6a11d23b5e90b971cddd7859cb3f18(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 350, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b27edf12bc366327fa179e9bda6aa083(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 26, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_935880d6a6dcada2471b24b5bbf2820c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 10, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b34827b700b57c2f272776970c550fa0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5ebf70de1703320a6f0fdaa614413bda(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0243fce825de868f4efe321aa58a736b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_34ebfe9e21ba9c66a500d45fd7f22e41(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3bc7ae257a778d0de8722c35efd20e3a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_463237fde79324bae936444e62ab79a1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 70, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_84568216994c42dc9a78a7638acfc9f2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0b1ae707c8b4ab10db8aec9c3823350f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_20e7dca2d21b620ebcea6442292033d5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e0ef5e6f2b6f232144c21a5064e862d1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8f9a9f3e7b40fc374b85b405c3aff53a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 71, 71], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_43f623d01e33e4dd3ada01998e069b88(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 17, 17], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ff3939c6254965c12dafccc71f346fe4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9ea508b07f7b60324874cbe201e660d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 185, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9e98ccad392217803a8c7c0fa7e146a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2208, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4e20ab64f4cf542c00878428292e57de(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 8, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_842b58828ce8b0b3fea279b71a56f3f1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3b3f6ba1cde7cd5208a94f870259742a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 53, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e799d33cdc34eab378e0e515e5630486(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1408, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f53ebda9bb5560de5f9349dfa1388398(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 17, 17], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dd488a3ab73b95d9e1bdd5c6be0fe6a5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 100], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d49a26835e4478a1258aea1911ea27d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0627566346fd8d16b36302f5776ac6dc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e3d577d4507d8cf60b0d71c3eade4704(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 73, 73], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7d292400455f3928d3db699b9b9c3a07(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 410, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_df0a557d314c037d833d2be6f20c411f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 50, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a7f9e1287adef234b360f8c5ce7bf39b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 608, 608], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cb59aa2c490dbef566260bca50a12cd0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aed7e060052c4499918ab45fb112be96(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2160, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7c7e6f5bbda6509b41494959f924cae4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_defe38e4814345693cd22be6063cb31f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 147, 147], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a89884487612d4804b14f00b0ed71e98(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 800, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eb3355eb5b3a1d649b681e628ff1b3bc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_02d2de9f3d7b29bc3390442d83ae46a2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1776, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5b62c1eb1eea18fc53875f8c7b26385c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4f90e3704bca0bff852ea5a203b4b63b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d3b7ddd8e1f1dc602d6a5a567c3eaf5c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 416, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6da9a98efad0ff7ae24f0aae4d7d96b4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1392, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8662976d923fc316f42c68b348aac645(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2160, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_827369307d64dcfbca3d462bf795cce0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1872, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dd1b6274f2c1be924839d5d12c6206af(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2432, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_70aba72961631a21282cfb34425d9745(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_16fd19de33b48b46334b998f8ea7abd2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4c20f1ab47ef6ecb4fdaaf7c4394d1e6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_33f3e60285f76e0d85b8df1938b63567(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 175, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c9c78c6e93a10811a79f7a80805b6d47(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 162, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_45f7eefe39f0cdc7e697e422c7996796(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 366, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_91169ace885e4a24d300106550e4c6a1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 152, 152], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_458ff5cc93c2853052d9f3baad6cac72(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1664, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c892f8255afcce5721399d8970d0c97d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b9e6dde078f05face473d58a826284c7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 32, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_976b91526a5dc2f3a9e97c4a16e0b14a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f01a7f175598123e3bf7903757624d6d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 4, 512, 1024], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e10ff6dc7d9b5018dce6730c897ea458(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 174, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4f7322eae16189652b7ed5899e4a5aa0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2064, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ca44f8eeb684e8da550a314b2e180d9f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aaee242e34764c026caea234b9e37583(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 48, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6a26da77a4e4fb14b1a41ff1868ad0c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_35aac71abde1014925ad154485383613(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_80e042b16153a6a13000fc69bfd64db1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 64, 48], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cb4fd531d3a3e93a949a1ba0d55bd5d1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_db3075fe5be00cbb4c1ed9b2bee0f102(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 61, 61], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0a6fd276aa8102f6d3d02f53851ec6b3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 28, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_30ab71bcbd86487bbd826b41f391c7ee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 28, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9e245464a02f8d5c9be937d9791fa951(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 4, 256, 512], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cd851a3b357e92bf5843a71272052852(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1488, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_01c50f499900b120da6225e338d2aa5d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1920, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_964f2c00730c0f617a2caab3ff97bb96(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 972, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2166fbdf0dc76caa9eaeb4e7f2a18035(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 164, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_214efea8c0e6c920f8a4b8780c5b7a4b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c681fab815e93f1cbf1a2b6f35ed0f5b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2432, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8d462e81eee1dce8cb3768f466d00faf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f9d88b6dda9e692e5c8155ab0c881f8c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6ba1fd2d16db2400e955308f53f3c0e0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1344, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_264a1ceeb5ec3adbfe3fddce6195166a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bbec8392257533d466fa69c0a1ee8f14(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7896920255674cc01e20a46599623ade(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8d97cda29ed22e28b86427ac2b826eac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 544, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_acfdc8595c1523b3ad35de36da9e8119(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_89fc545d308c9959a47926c571a761f1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_43c0695f75d5141303c5c0768d82a893(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c7756dc33bcfe086f44d82256691cc50(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_441e62d2bdc69610d368e2d17dd3cadd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1760, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d45ea446e0bee54a250317f48b43b7e5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 262, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ba9b51392051e1265ee4a8d918f6e698(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1056, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d685f2531a4e8e1ab9346744dc305143(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7ff7fed0f17c0351313ecfc46dc48cb3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 48, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e2e914dd9ce790e1723068f6c824bad3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1568, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_171e444d3443a35c9e028717a8c7c8bb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 352, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_67a3ffd7441e4f21b97d12c4ee68c34a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2016, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b5f7017c2ff4851109f25ef4646d6508(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 81, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_22d7cb24c63d8d8d9eb4cb0b9a2aa78d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 816, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3ef1128a7f0385fe2e93b55d907ff2c7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d655591997fa9244a08e06784e6c676f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 3, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c198ebb05e54a668041e8d96c745ecc4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1248, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e69a017b00574952acf834813ed3760c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1600, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_379943e81a6a4c6bf2afc9dd9ce81850(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7f9778ea4534f3fc63e1557084e6046b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1120, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_445ddf41551001cd9d5b69548dc48794(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 100], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_97114e27a4e86e0b0689cef0f1ac820d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7ebeea026c6e9dc157fb83b21e461b0b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad4300dd0dd7c1655f8157362f7a3fce(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 1, 26], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7185d170951224540264a15c38f820bd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 162, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0acfc461b9347a995634a111ab651e07(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2560, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b4aace2371eeae83ccd23970301645e6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 122, 122], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_08a77721120b46e1693801de3678d379(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 117, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cbf87a3922b26ccb48d13f6f2f36adbb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f2e4983afb6846aad56ac221ef11a43b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dcb2f2f8150f2b965cf23f091bee3fcc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 151, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ef279ca824491cb465daea4379cfef4f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f13d91383492b6e76763077e02aa2a30(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 14, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_316a5c1bf1092b7bd8bd6dd93656af77(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 270, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6d6ef9509cacb40b8f843af35ad1060a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_864143a2abfd0f3d4c024275d30b97e4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1056, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b8f3f640971c1628767991bd97777cc4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8ff360b784807127d8dbb229d9af68fe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d892ba2b1df01c09159a3b7835f549af(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_98ee279dc273eba3588a15388601fa2c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1888, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ffba008e796ae2ad1f7d7fca90d15a0f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1718dfd92c51788ed97747a2f0e216bb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e84b9c88555246fdc19a330a2dc95ed7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ec3b3a2a6bb81c72145d5e0bbcee4810(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1312, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_39d2d19b7d303851549f0d5fdbc4f59a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1680, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b4c0fe2fca612284865477be8e7df52a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f8f826ad0801bba2f1aa15a16f0af589(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_84035fb2f3979b4ba0f62b4fa8c68bf3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1344, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e14df009148bd5c0fa59a070b1ebf16e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ed4c9c36390ae7eb199866066fe8c806(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1216, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5cf4d4b147717d00b0b75a61caac84ce(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e528e1003f083ad06760d210da39d461(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c6baed37cb404a4869b3e13de7f6e733(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 736, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7ffe1a6ab63c287d150f682527662f65(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_16c5998ca4ababc65fb3747fe0c754f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a6a02bcf79acd87c4a2d312c69458e8d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 88, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_517c329316f36622f36ec2c73e504025(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 14, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_80088b0ee55d4d921d63acab21fdb431(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1952, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8289b332e10e28c5a84932d651624d7d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 118, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_43a85f8db69f9254d3acc022ca1f59a2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 32, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_df6f636f40813b72b55d61f3fe6a52b4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1776, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_672dd7aa42f3a021e6d00bf92e9b1d76(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0727011728786dc7406d169be3df53e4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 528, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_70f635c2e3f70bc2b9b030d020760420(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 58, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6431429537b11f1f7cafb96123d13ff8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1472, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_40567c7d44448ac41e165254a7ef9497(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 96, 96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_12e434c0aba4bdedd142af1d54405487(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_16a6edfa5b6409b415d09ad05808ddc5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ccfefb6a7a202d41a9529e931fb5938e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1408, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_814f4928eb7842b35504dbba4522c3f2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d9207fb6e42df320687e1abddc8cfd8e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 138, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cc775f221a7a29440383ed934d6f629d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_624cb3d959ca392f1eb717c362b9df66(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1696, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_57546f95726f7922fd067f54dd9b651f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 2, 27], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f323dba5e9c868990c2fd1d31f924e8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 32, 256, 512], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_467e0b54e8b3ce1fe94e1c2291ee10b1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_54fcde5cf795d819b395aaf19da10d8f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1872, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2f0e1860449b1b7ebd00d0c0044deec2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d092b5f6fce32e2a8e87b36f36408ab1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_695ade52943d861cbb70a061e1e9a339(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 2, 2], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_073057315402ed17b5b67313b7b57236(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f8dc552edc4be255e46b47d76908b345(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ac9c506333fd23e58f317f5b39f87cf8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3cf26023911c7eda97b16db699ae0ff0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 352, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0d4ca7005d2aaa8cc3db49df521f1e39(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1968, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_21ef9c97a89964f4ef1bc3e4fd9758f8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a512991055d5115589257aad5b14b81d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 2, 2], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_db7396cc0f24295e5ab6004b01dde58a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f1e8941418a2f8bb9b0465e9d839d2ef(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 366, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e60b25b6ed143be25e842d03da8550fd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 304, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_263cd5f275bc1c540115cfb0253257d0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_32f7e7e456ac4fa76328f4c94180176e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 82, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a64af683a2d6deaf7a7b974a51c6ca37(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_659275afe5a1f942293ad5b944b482d6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f159a255d63dc15c3c3578e376ed7fba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c03fa0a42d9b996e79b755dcd7c02fac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1632, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_67829caec9fb9189d7dae66df909adaa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 38, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f987c0d4fff88db50576a915fbddc61(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2064, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e878490e40f031cbdc6c269e2f5bef62(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6ec6c331c3423d959a4fcd62b6abf104(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1200, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bafe67d17663c70b06eb0ebcf0421bd9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1968, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad432a6b8fec317a29146d7760c1bc85(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2112, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_963e64b1c5f9da6455d21443ca2e14b1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d8efe7286b43a5f44d9b4957f89df325(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e910b95a8213a95415372c2921f6d8cb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1792, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9ddf65cc5820d79f0cf44cf133b6ab42(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 528, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d740db72a5878b910e548c0335883331(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 1, 2], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f93a4f741072d1bd0bddce02bc3e8f18(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2688, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_99b9674fb106caee579db82db9d6fed3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 150, 150], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c1772c848ed84edbc3c4009987e7f26a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1680, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_72066883c5c8cf4c909ee2905df02c64(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 8, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2a5d33304f33681fe1edc766fa8ea08d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4e1519cd585b63180b56b6e0fc574dc2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ee81696f9229deba291d9f8d23cd674d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 2, 2], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_475036204effe55e8c35258b4f43447a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1968, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c85ba9321ecc7a325e149f98792a3fe7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 174, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1d1a1451341812987def2e844afb08fd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 2, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_95f3a13b0cc6136f39a3708c17c63e9c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1b6c71d21f96d7faf2a2db0f5a70d973(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c47b8a59da9ba8b283ceaee01f4f4e4f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 7, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fc756a9190ee566e71ff2d981c920b58(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_be55e6b7bc3740624a173e767643bfa0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 17, 17], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fef8fe758bca37ef2d38ea6431493539(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 118, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5044464335106b9f394e8b42a34c2418(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9d9a59cc1149d5482d170ab28e98dede(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-06'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0552be49233cc4ef96bce93ad608f02d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 47, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fd9bb3eff78417dce39f1c4c17582da6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 151, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_beab501839800f2f2cd590f0c425f144(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_87b6cbb642368169a0a6a927e0de77b2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1600, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4192dcd65a629e7e519714b3d3047b86(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 118, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ec29c855527613d9aa5cb61230da837f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 44, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_49c30032d4e97e0f384c253072b5a30c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_debd32127a4e41e44c2010993e6a022e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 4], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_658067f7e912b60287bb6ef6f7f62f00(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 432, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_26aa9a3de6019caaec46f3a13bf74266(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 256, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4ae532ac2d899558c963968af9e9c8cd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 304, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_83391cfece5cd7542dbfff9afea65200(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 864, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0b78a4c3f6f276413f0a6ddcb7b8c12f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1376, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_75ee84f7236948cee9ff459d8bebe011(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1680, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9bca70962b358c3795b4a1775aa1749e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1ad5322a25dffead414186b218aab2b4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_09d7974ad6e02321106838df9e145c82(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_127ee73f1be2c7092762a34c121d6b79(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 12, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d610cbf6adb1ac86386c4dbe83923177(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 624, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bf4172b5e8d85da0334345b3af72fd78(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 48, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_19eafe3a5d3f9df80de4387e50945935(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2112, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f79b3849c0ff7b562d2f8d37623473b9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 8, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b606cf513c8f64479a9b2e2cbb83813c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_42c7fa3df3c9ea87887339f90ff4d1e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1056, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_541c5e8fce76898fc2f665d66f8d8d58(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2016, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5d9f2c3bc38fb6027110b24fa6671d9f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 175, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2b078752569198765df0efc82d9b357e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 84, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_31ac0e1776ae9a4a2dc523abd9dcaa95(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_09c329535e4dfc090393750161f37a90(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1824, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ca1c2938c7fadc96b0dc8dfa9209c1c9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 82, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f18595d2024ae3d46e77191cf6bf85e6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2d4a06ca563163a1911037f377fd7b54(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_36df1ca41ea7c15a1af97a21087edcdd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8e8f0133986ffb4135513dfd7b141efb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 44, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8bb03deac1cb8964dbb9828bfc08fab8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1200, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_237e94dd03dbb3d597731d100450d5a0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aa7e0b68c846dc747dd1bd9131491ec0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 73, 73], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dadc711cf95ce5c6825bd88d9a162838(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1120, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_512d64956c2da18527392987c6edf207(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e8261252208644d1e6de9fe1fff3934f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1568, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_462e07d96819e4c7122493866f197baa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_752486afaea0903136dae08f821d2b70(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_26858e4bcc071dbb21f5b46f3b0410fd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 24, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cac9f33a1d94791f207118c5166b9344(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1696, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8272a9122063f5942bfd66068ee30970(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 704, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c181f9728f90d171e0887bcbb8afe44d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_99613e5121b570118b8f970abbba7f46(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_00c9d6d672498715a61d2ae2a6f98c7d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 88, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4662f7dbf1bfba37b6e568739b9af629(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 208, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f38ab147038e4d8fcfdba47ca449d86c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1864570f21287a00d91d55c94bd03f33(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 3, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2fc9a891a92f336694b4fffc9b756281(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 736, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dea80a9adc2725dd7126b65cfeff8f4a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4e126e17b02f24fa0aeed65cc485aa65(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2413fec6a4581a7335100a8ec156542a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 132, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dbd9633630d82736e0e78f1238847b14(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1488, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c347cb6185eb6b1a8768caffbd4c20b9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 436, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f9f6ec79ccff23b64aeae5231be51af2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 304, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dfaf72fe729072533518bfce8d3ee0f6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 49], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d115a16c90d0f6858881e4f05a9f4700(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f67d1a33739513a1ccf71884ffd49a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 28, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d817aba74df7b35ce1e36288368ef6b4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1440, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_152dac02533dcdd72854cac3edf3c57d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 12, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7f27863bb556262ce9ca842c1183b000(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 44, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bf527dc1fcf1a6fd8a5f4eaf7405ac3e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 122, 122], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a3bd1bafc24393f61078257ba8e253d7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a31fbafeb6158af7b3bc553784331220(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d2edb31d7ed5044dc395a2ce8ae41db4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3cc17f3071d80ab3eb49c5addce341bf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 50, 350], dtype='float16'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_777ac0525f84696e6440d3ec6ec78be2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_148a3483c0f7584668318beaeeb76022(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 270, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8264cd0ed09d33c4efb6b54b04181ed2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 58, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f8cdcba81ed75913b22b320109803f41(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 704, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2e9afbd276ea8658a969c5b6afad74cc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2dd377f23ba6d8724508ee0503e7e7c0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1392, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d7f6d683fd92f78f719a0cf8725cd79b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 200, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7134ccac61be0ea0fd0aefa3dabd9403(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 432, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_29630784f95f784bcc017bebd7e94415(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 608, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_69ec786644c8aff47383c886f382f69b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 1, 2], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f5147e6dcf0a2f6457540422bd0a3b0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1824, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f96500f1380943bc67b1f56886e95fb3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_742a892484ff8f554a435063c08680e4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 7, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_45037d8ea16b2d2e3487e5b2fae77fa2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 4, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_579400ce34160557edeed1dea40b82a1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1472, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_745afe881745275c03bbd782f2897adb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 608, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None




if __name__ == '__main__':
    unittest.main()